seed:  666
save trained model at:  ../trained_models/trained_classifier_model_31.pt
save loss at:  ./results/train_classifier_results_31.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  14780
number of pockets in validation set:  3162
number of pockets in test set:  3183
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKMCNMMEmbeddingNet(
    (conv0): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=22, out_features=128, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(128, 256)
  )
  (fc1): Linear(in_features=256, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0008
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b6f7f026490>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.006466017298511, acc: 0.3993234100135318, val loss: 1.7973029274189496, acc: 0.4497153700189753, test loss: 1.7829525186099462, acc: 0.4483191957273013
epoch: 2, train loss: 1.7447368872181812, acc: 0.46596752368064953, val loss: 1.6029404480950429, acc: 0.5148640101201771, test loss: 1.5878599752916918, acc: 0.513980521520578
epoch: 3, train loss: 1.6396247025594337, acc: 0.49851150202976996, val loss: 1.5414264806835505, acc: 0.5170777988614801, test loss: 1.5493174841896078, acc: 0.5133521834747093
epoch: 4, train loss: 1.5635916918637143, acc: 0.5246955345060893, val loss: 1.4874982405583517, acc: 0.5309930423782416, test loss: 1.4856613761705308, acc: 0.5347156770342444
epoch: 5, train loss: 1.564587805719595, acc: 0.5230040595399188, val loss: 1.5928340006447683, acc: 0.521505376344086, test loss: 1.5918384839482476, acc: 0.5271756204838203
epoch: 6, train loss: 1.497620419849401, acc: 0.5462110960757781, val loss: 1.4964239068004166, acc: 0.5332068311195446, test loss: 1.4961147789980607, acc: 0.5438265786993403
epoch: 7, train loss: 1.4453741748858853, acc: 0.5629228687415426, val loss: 1.6524093926216816, acc: 0.4873497786211259, test loss: 1.6743503679017244, acc: 0.48601947847942195
epoch: 8, train loss: 1.4080970255383296, acc: 0.5759133964817321, val loss: 1.481986913403554, acc: 0.547438330170778, test loss: 1.4925511215903868, acc: 0.552937480364436
epoch: 9, train loss: 1.3732494534271817, acc: 0.5871447902571042, val loss: 1.3196510056164505, acc: 0.5790638836179633, test loss: 1.336054163761121, acc: 0.5790135092679862
epoch: 10, train loss: 1.3373625463013397, acc: 0.5947902571041949, val loss: 1.3876167455102582, acc: 0.562618595825427, test loss: 1.3962259149536411, acc: 0.5645617342130066
epoch: 11, train loss: 1.288107612394351, acc: 0.6075778078484438, val loss: 1.2384347716590258, acc: 0.6091081593927894, test loss: 1.2473173335480159, acc: 0.6079170593779454
epoch: 12, train loss: 1.2551520498582893, acc: 0.6213802435723951, val loss: 1.3082756403501392, acc: 0.5939278937381404, test loss: 1.302005601386173, acc: 0.5981778196669808
epoch: 13, train loss: 1.2173140325468834, acc: 0.6324763193504737, val loss: 1.272368454013272, acc: 0.6046805819101835, test loss: 1.2727675046180726, acc: 0.6050895381715363
epoch: 14, train loss: 1.1947092608605412, acc: 0.638700947225981, val loss: 1.2752383263483595, acc: 0.5923466160657812, test loss: 1.291221152301708, acc: 0.5972353125981779
epoch: 15, train loss: 1.1487512250069192, acc: 0.6502029769959404, val loss: 1.3256701311682086, acc: 0.5743200506008855, test loss: 1.3398131128755186, acc: 0.5824693685202639
epoch: 16, train loss: 1.185204807831244, acc: 0.6441136671177267, val loss: 1.223019787361922, acc: 0.6192283364958887, test loss: 1.206637839644801, acc: 0.6273955387998743
epoch: 17, train loss: 1.1263424710749934, acc: 0.6593369418132612, val loss: 1.089377996515881, acc: 0.6635041113219481, test loss: 1.089984219889951, acc: 0.6625824693685203
epoch: 18, train loss: 1.0834097518004648, acc: 0.6684709066305818, val loss: 1.2695419103717744, acc: 0.610056925996205, test loss: 1.2637501027349627, acc: 0.6170279610430411
epoch: 19, train loss: 1.0811502771545327, acc: 0.6719891745602166, val loss: 1.2506520541539152, acc: 0.6113219481340924, test loss: 1.2343821010985092, acc: 0.6173421300659755
epoch: 20, train loss: 1.0678937662599535, acc: 0.6768606224627876, val loss: 1.0762353189228306, acc: 0.6748893105629349, test loss: 1.0673844896694642, acc: 0.6760917373546969
epoch: 21, train loss: 1.038046350563008, acc: 0.6824763193504736, val loss: 1.0972170403002184, acc: 0.6612903225806451, test loss: 1.0931944528017035, acc: 0.6622683003455859
epoch: 22, train loss: 1.0001151053605448, acc: 0.6941813261163735, val loss: 1.039994555888939, acc: 0.6865907653383935, test loss: 1.0523326157250197, acc: 0.6745208922400251
epoch: 23, train loss: 0.9961431894315274, acc: 0.7016914749661705, val loss: 1.1235721860785488, acc: 0.6571790006325111, test loss: 1.125965700709816, acc: 0.6672950047125353
epoch: 24, train loss: 1.046637027592717, acc: 0.6792286874154263, val loss: 1.0486482323761759, acc: 0.6739405439595193, test loss: 1.0613199744702435, acc: 0.6754633993088281
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.7896576741551514, acc: 0.7022327469553451, val loss: 0.8512610659276587, acc: 0.6650853889943074, test loss: 0.8470735783626402, acc: 0.6729500471253534
epoch: 26, train loss: 0.7593399913288421, acc: 0.7058186738836265, val loss: 0.8342984787351643, acc: 0.6859582542694497, test loss: 0.8512092163230951, acc: 0.6748350612629594
epoch: 27, train loss: 0.7438417146912447, acc: 0.7133288227334236, val loss: 0.8233989699591119, acc: 0.6818469323213157, test loss: 0.8318854268152235, acc: 0.6789192585611059
epoch: 28, train loss: 0.7494586024297267, acc: 0.7118403247631935, val loss: 0.8043171156063478, acc: 0.6872232764073372, test loss: 0.8303563340894463, acc: 0.6786050895381716
epoch: 29, train loss: 0.7095267888496306, acc: 0.7275372124492557, val loss: 0.8887123340749047, acc: 0.6593927893738141, test loss: 0.875141981896686, acc: 0.6786050895381716
epoch: 30, train loss: 0.7208117271631109, acc: 0.7249661705006766, val loss: 0.900547917472796, acc: 0.6505376344086021, test loss: 0.8988426145876574, acc: 0.6525290606346215
epoch: 31, train loss: 0.7060316733320285, acc: 0.7294993234100136, val loss: 0.8443534730130702, acc: 0.6824794433902593, test loss: 0.8339781293770145, acc: 0.6776625824693685
epoch: 32, train loss: 0.7135109077771075, acc: 0.725981055480379, val loss: 0.8625446893533976, acc: 0.6714104996837444, test loss: 0.8629232246921454, acc: 0.6729500471253534
epoch: 33, train loss: 0.7276809111015723, acc: 0.71765899864682, val loss: 0.8446081232074543, acc: 0.6837444655281467, test loss: 0.8424962454534274, acc: 0.6826892868363179
epoch: 34, train loss: 0.6878059788711661, acc: 0.7349120433017592, val loss: 0.8657950450166126, acc: 0.6650853889943074, test loss: 0.8614961694255401, acc: 0.6691800188501413
epoch: 35, train loss: 0.6790157111797668, acc: 0.737212449255751, val loss: 0.8156966520667754, acc: 0.6929158760278304, test loss: 0.7908311199550168, acc: 0.6971410618912975
epoch: 36, train loss: 0.6620997654569004, acc: 0.7391069012178619, val loss: 0.862916965412234, acc: 0.6552814674256799, test loss: 0.846283089241965, acc: 0.6672950047125353
epoch: 37, train loss: 0.6547455236295563, acc: 0.7438430311231394, val loss: 0.8503481370598964, acc: 0.6774193548387096, test loss: 0.8716083698740105, acc: 0.683631793905121
epoch: 38, train loss: 0.6597328755465186, acc: 0.7415426251691475, val loss: 0.7842519126460192, acc: 0.698292220113852, test loss: 0.7835247922610308, acc: 0.706880301602262
epoch: 39, train loss: 0.6545305333698876, acc: 0.7444519621109608, val loss: 0.7614742762525197, acc: 0.6970271979759646, test loss: 0.7672941220319762, acc: 0.7018535972353126
epoch: 40, train loss: 0.6497815851268329, acc: 0.7476319350473613, val loss: 0.7954210735890475, acc: 0.6761543327008223, test loss: 0.7905615430712662, acc: 0.6808042726987119
epoch: 41, train loss: 0.6302940403979589, acc: 0.7551420838971583, val loss: 0.8138634247692381, acc: 0.6859582542694497, test loss: 0.7848133272320229, acc: 0.6949418787307572
epoch: 42, train loss: 0.645629770313774, acc: 0.7487821380243572, val loss: 0.9199711103216144, acc: 0.6552814674256799, test loss: 0.9179229394767107, acc: 0.6647816525290606
epoch: 43, train loss: 0.6307203935350875, acc: 0.7515561569688769, val loss: 0.7925613968527672, acc: 0.6824794433902593, test loss: 0.7832549066240968, acc: 0.6955702167766258
epoch: 44, train loss: 0.6557921197159526, acc: 0.7461434370771313, val loss: 0.7889805962057674, acc: 0.700506008855155, test loss: 0.7861718994395896, acc: 0.6965127238454288
epoch: 45, train loss: 0.615567948321367, acc: 0.7575778078484439, val loss: 0.7625976037104472, acc: 0.7188488298545225, test loss: 0.7559953628753365, acc: 0.7106503298774741
epoch: 46, train loss: 0.5992232156380588, acc: 0.762381596752368, val loss: 0.7671620946832279, acc: 0.7017710309930424, test loss: 0.7887679111122373, acc: 0.690229343386742
epoch: 47, train loss: 0.6064097013621917, acc: 0.7627198917456022, val loss: 0.7478550279690901, acc: 0.7160025300442757, test loss: 0.7490735037537293, acc: 0.7156770342444235
epoch: 48, train loss: 0.5885595589916502, acc: 0.7726657645466847, val loss: 0.8858374424330574, acc: 0.6701454775458571, test loss: 0.899295894297421, acc: 0.6622683003455859
epoch: 49, train loss: 0.5797512949964836, acc: 0.7675236806495264, val loss: 0.7022475084875446, acc: 0.7194813409234662, test loss: 0.7102246551891187, acc: 0.7251021049324536
epoch: 50, train loss: 0.5916256688444476, acc: 0.7667794316644113, val loss: 0.8525619544536535, acc: 0.681214421252372, test loss: 0.8256525558905522, acc: 0.6880301602262017
epoch: 51, train loss: 0.5832917184242538, acc: 0.7685385656292287, val loss: 0.7039189872524544, acc: 0.7292852624920936, test loss: 0.7085782646421288, acc: 0.7279296261388627
epoch: 52, train loss: 0.6421421501891378, acc: 0.7469553450608931, val loss: 0.9995651000817315, acc: 0.6385199240986718, test loss: 0.9774077978292951, acc: 0.648444863336475
epoch: 53, train loss: 0.6144474260700572, acc: 0.7589309878213802, val loss: 0.6957103946705093, acc: 0.7172675521821632, test loss: 0.6855942928075716, acc: 0.7313854853911405
epoch: 54, train loss: 0.5664838844287702, acc: 0.7756427604871448, val loss: 0.7677833032638193, acc: 0.6897533206831119, test loss: 0.7873432998046012, acc: 0.6943135406848885
epoch: 55, train loss: 0.583231485470706, acc: 0.7669824086603518, val loss: 0.7120256904255808, acc: 0.7254901960784313, test loss: 0.721749229323291, acc: 0.7285579641847314
epoch: 56, train loss: 0.55264569280596, acc: 0.7831529093369418, val loss: 0.7763761753224633, acc: 0.7052498418722327, test loss: 0.762237886494149, acc: 0.706880301602262
epoch: 57, train loss: 0.5475426670017681, acc: 0.7861299052774019, val loss: 0.7837195360230766, acc: 0.6976597090449083, test loss: 0.7720727475604058, acc: 0.704052780395853
epoch: 58, train loss: 0.5336192790484396, acc: 0.7892422192151556, val loss: 0.6916047233633419, acc: 0.7340290955091714, test loss: 0.7162313423102807, acc: 0.7244737668865849
epoch: 59, train loss: 0.5642556520856927, acc: 0.7767253044654939, val loss: 0.7917523545452796, acc: 0.7011385199240987, test loss: 0.7971315148713414, acc: 0.702167766258247
epoch: 60, train loss: 0.5428191558272655, acc: 0.7836265223274695, val loss: 0.7452808432304581, acc: 0.717583807716635, test loss: 0.7468575459023972, acc: 0.7115928369462771
epoch: 61, train loss: 0.5237973503476067, acc: 0.7922192151556157, val loss: 0.8382674082406785, acc: 0.6869070208728653, test loss: 0.8127949667770908, acc: 0.6889726672950047
epoch: 62, train loss: 0.5381524454272971, acc: 0.7860622462787551, val loss: 0.7320385199419537, acc: 0.7109424414927261, test loss: 0.7523412947155865, acc: 0.7027961043041157
epoch: 63, train loss: 0.5564548603575349, acc: 0.7815290933694181, val loss: 0.6830412564889906, acc: 0.7296015180265655, test loss: 0.6779447057282366, acc: 0.7376688658498272
epoch: 64, train loss: 0.5095891569560855, acc: 0.7974289580514209, val loss: 0.7510378556791398, acc: 0.7163187855787476, test loss: 0.7569480301859691, acc: 0.7169337103361608
epoch: 65, train loss: 0.49893648367934684, acc: 0.8021650879566983, val loss: 0.720500486700841, acc: 0.7450980392156863, test loss: 0.7143373630947937, acc: 0.7417530631479736
epoch: 66, train loss: 0.49202205328238025, acc: 0.8039242219215156, val loss: 0.6514846885000732, acc: 0.7416192283364959, test loss: 0.6398675549603318, acc: 0.744894753377317
epoch: 67, train loss: 0.4928162246337602, acc: 0.8006765899864682, val loss: 0.8545674113515508, acc: 0.7030360531309298, test loss: 0.8394140425696149, acc: 0.7075086396481307
epoch: 68, train loss: 0.4990643143815181, acc: 0.8013531799729364, val loss: 0.7459738189702091, acc: 0.7270714737507906, test loss: 0.7166685010141223, acc: 0.7373546968268929
epoch: 69, train loss: 0.49261611966222163, acc: 0.8034506089309879, val loss: 0.7117923670822424, acc: 0.7280202403542062, test loss: 0.6847844241439192, acc: 0.7345271756204839
epoch: 70, train loss: 0.47746050369110415, acc: 0.8066982408660351, val loss: 0.6985917625210091, acc: 0.7400379506641366, test loss: 0.7081555066301956, acc: 0.727301288092994
epoch: 71, train loss: 0.4726544589731787, acc: 0.8105548037889039, val loss: 0.7548352723480553, acc: 0.706831119544592, test loss: 0.739084021834656, acc: 0.7109644989004085
epoch: 72, train loss: 0.5035568332317234, acc: 0.7987821380243573, val loss: 0.7946403893694555, acc: 0.7204301075268817, test loss: 0.8082982370248951, acc: 0.7200754005655042
epoch: 73, train loss: 0.4606867019071953, acc: 0.8163734776725304, val loss: 0.7735407014650155, acc: 0.724225173940544, test loss: 0.7676706347044556, acc: 0.7216462456801759
epoch: 74, train loss: 0.48186065589300836, acc: 0.8080514208389716, val loss: 0.7182238287446169, acc: 0.74573055028463, test loss: 0.7005533837238423, acc: 0.7423814011938423
epoch: 75, train loss: 0.48146534512910855, acc: 0.8055480378890393, val loss: 0.7605201483225235, acc: 0.7213788741302973, test loss: 0.7741307523165939, acc: 0.7222745837260446
epoch: 76, train loss: 0.4887942250107235, acc: 0.8060893098782138, val loss: 0.6475374704671012, acc: 0.7413029728020241, test loss: 0.6653599744317968, acc: 0.7373546968268929
epoch: 77, train loss: 0.4600255704700382, acc: 0.8158998646820027, val loss: 0.680433564364043, acc: 0.7454142947501581, test loss: 0.6996149667733277, acc: 0.7313854853911405
epoch: 78, train loss: 0.46874416438265326, acc: 0.8141407307171854, val loss: 0.6501114023101247, acc: 0.7498418722327641, test loss: 0.6582353273503181, acc: 0.7433239082626453
epoch: 79, train loss: 0.45504058805144365, acc: 0.8163058186738836, val loss: 0.6801134666862705, acc: 0.7539531941808981, test loss: 0.6378232099483345, acc: 0.7518064718818724
epoch: 80, train loss: 0.4618223278141796, acc: 0.8158322056833559, val loss: 0.9222066797537566, acc: 0.6821631878557874, test loss: 0.9203254888913859, acc: 0.6804901036757776
epoch: 81, train loss: 0.45031990153218476, acc: 0.8228687415426251, val loss: 0.7114538594486591, acc: 0.7428842504743833, test loss: 0.7182325015006633, acc: 0.7389255419415646
epoch: 82, train loss: 0.434334845310619, acc: 0.8237483085250338, val loss: 0.6648628943182109, acc: 0.7469955724225174, test loss: 0.6657128793954324, acc: 0.7474081055607917
epoch: 83, train loss: 0.45588801692722614, acc: 0.81617050067659, val loss: 0.6985914339226006, acc: 0.7530044275774826, test loss: 0.7027665674405543, acc: 0.7496072887213321
epoch: 84, train loss: 0.42264738895089765, acc: 0.8299729364005413, val loss: 0.7770540604178171, acc: 0.726122707147375, test loss: 0.7815745871583134, acc: 0.7207037386113729
epoch: 85, train loss: 0.47022375014379963, acc: 0.8160351826792963, val loss: 0.6716393028762354, acc: 0.7501581277672359, test loss: 0.6842132458850119, acc: 0.742067232170908
epoch: 86, train loss: 0.43521591097479423, acc: 0.8237483085250338, val loss: 0.7341153327608922, acc: 0.7365591397849462, test loss: 0.7465355907863241, acc: 0.7326421614828778
epoch: 87, train loss: 0.44071997375062416, acc: 0.8228687415426251, val loss: 0.7347364694087435, acc: 0.711258697027198, test loss: 0.7686485056977507, acc: 0.708765315739868
epoch: 88, train loss: 0.433677610750934, acc: 0.8261163734776725, val loss: 0.7473373064732416, acc: 0.7413029728020241, test loss: 0.7676901657776828, acc: 0.7370405278039586
epoch: 89, train loss: 0.41748540591806454, acc: 0.8342354533152909, val loss: 0.6928504375925855, acc: 0.7460468058191019, test loss: 0.6938987245658566, acc: 0.746779767514923
epoch: 90, train loss: 0.45048988860740713, acc: 0.8172530446549391, val loss: 0.7917267847935812, acc: 0.7191650853889943, test loss: 0.7661530529595081, acc: 0.7260446120012567
Epoch    90: reducing learning rate of group 0 to 1.5000e-03.
epoch: 91, train loss: 0.3658130875782328, acc: 0.8487144790257104, val loss: 0.6089152204017712, acc: 0.7802024035420619, test loss: 0.6236293072450322, acc: 0.7715991203267358
epoch: 92, train loss: 0.297201984838478, acc: 0.8769959404600812, val loss: 0.6165513087495228, acc: 0.7855787476280834, test loss: 0.625398758256956, acc: 0.7753691486019478
epoch: 93, train loss: 0.2855159360315545, acc: 0.8807171853856562, val loss: 0.6208605992499973, acc: 0.7852624920936117, test loss: 0.6252752384448853, acc: 0.7847942192899781
epoch: 94, train loss: 0.2856417226775251, acc: 0.8813937753721245, val loss: 0.6421286683531972, acc: 0.7798861480075902, test loss: 0.6379067182840509, acc: 0.7744266415331448
epoch: 95, train loss: 0.2971342228584909, acc: 0.8748985115020298, val loss: 0.7026075396938916, acc: 0.7643896268184693, test loss: 0.6917780290562291, acc: 0.7571473452717562
epoch: 96, train loss: 0.2745236262215329, acc: 0.8848443843031123, val loss: 0.6540309214124492, acc: 0.7792536369386465, test loss: 0.6628616194361004, acc: 0.7712849513038015
epoch: 97, train loss: 0.2710435922841097, acc: 0.8871447902571042, val loss: 0.6722218243854096, acc: 0.7808349146110057, test loss: 0.6812439764815609, acc: 0.7766258246936852
epoch: 98, train loss: 0.25572732189753383, acc: 0.8953315290933694, val loss: 0.700978312139493, acc: 0.7773561037318153, test loss: 0.698367092619143, acc: 0.7725416273955388
epoch: 99, train loss: 0.2637373320304654, acc: 0.889851150202977, val loss: 0.7095489552471925, acc: 0.7637571157495257, test loss: 0.7292666693285056, acc: 0.7628023876845743
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.20746852219185422, acc: 0.8866035182679296, val loss: 0.5475703495201772, acc: 0.7729285262492094, test loss: 0.5427014286174289, acc: 0.7756833176248822
epoch: 101, train loss: 0.17859958277180973, acc: 0.9008795669824087, val loss: 0.6402560328472121, acc: 0.7716635041113219, test loss: 0.6630287431824181, acc: 0.7577756833176249
epoch: 102, train loss: 0.18677991275374395, acc: 0.892489851150203, val loss: 0.5743070754124197, acc: 0.7748260594560404, test loss: 0.5818019196779669, acc: 0.7697141061891297
epoch: 103, train loss: 0.18534725666933358, acc: 0.8935723951285521, val loss: 0.5933046861210629, acc: 0.7798861480075902, test loss: 0.604066511013955, acc: 0.7709707822808671
epoch: 104, train loss: 0.18503474531057562, acc: 0.8941813261163735, val loss: 0.66466230881659, acc: 0.7517394054395952, test loss: 0.6744126873573641, acc: 0.7480364436066603
epoch: 105, train loss: 0.21850497086251378, acc: 0.8788227334235453, val loss: 0.5709305507785379, acc: 0.7634408602150538, test loss: 0.5639909651906394, acc: 0.7587181903864278
epoch: 106, train loss: 0.1919586239390186, acc: 0.8901894451962111, val loss: 0.5904434352340312, acc: 0.7735610373181531, test loss: 0.585159206293006, acc: 0.763430725730443
epoch: 107, train loss: 0.1686082815285142, acc: 0.9023680649526387, val loss: 0.6054298967443487, acc: 0.7691334598355472, test loss: 0.6076006502465485, acc: 0.7596606974552309
epoch: 108, train loss: 0.17802272802760702, acc: 0.8960081190798376, val loss: 0.631220682748581, acc: 0.7685009487666035, test loss: 0.6040277455611832, acc: 0.7728557964184731
epoch: 109, train loss: 0.1851933368770292, acc: 0.8926251691474966, val loss: 0.6300253427006036, acc: 0.7558507273877293, test loss: 0.6535455581342653, acc: 0.7483506126295947
epoch: 110, train loss: 0.18424006967486484, acc: 0.8918132611637348, val loss: 0.6218754639737143, acc: 0.7517394054395952, test loss: 0.6197887256465316, acc: 0.7442664153314483
epoch: 111, train loss: 0.190436542961691, acc: 0.8882273342354533, val loss: 0.5777625238344988, acc: 0.7751423149905123, test loss: 0.5895645396723674, acc: 0.7800816839459629
epoch: 112, train loss: 0.18749841121762628, acc: 0.8906630581867389, val loss: 0.55125450100196, acc: 0.7843137254901961, test loss: 0.5702243761712187, acc: 0.7747408105560791
epoch: 113, train loss: 0.17577576938114245, acc: 0.8951962110960758, val loss: 0.7643585513469013, acc: 0.732764073371284, test loss: 0.7391715845221287, acc: 0.7310713163682061
epoch: 114, train loss: 0.15503587322725496, acc: 0.9055480378890393, val loss: 0.624098584593134, acc: 0.7741935483870968, test loss: 0.6203368780862375, acc: 0.7693999371661954
epoch: 115, train loss: 0.1836426598011076, acc: 0.8955345060893098, val loss: 0.674692494668665, acc: 0.7375079063883618, test loss: 0.6818590018122292, acc: 0.7408105560791706
epoch: 116, train loss: 0.17750483113914445, acc: 0.8959404600811908, val loss: 0.5860187352872362, acc: 0.7802024035420619, test loss: 0.6152060074585997, acc: 0.7700282752120641
epoch: 117, train loss: 0.15589576265163446, acc: 0.9013531799729364, val loss: 0.6466470554913998, acc: 0.7561669829222012, test loss: 0.6488557823266842, acc: 0.7618598806157713
epoch: 118, train loss: 0.16839279382614386, acc: 0.9002706359945872, val loss: 0.6374059770018295, acc: 0.7675521821631879, test loss: 0.6271570039701806, acc: 0.760603204524034
epoch: 119, train loss: 0.15527690001366426, acc: 0.9031799729364005, val loss: 0.6840989686204692, acc: 0.7469955724225174, test loss: 0.6597534218113115, acc: 0.7558906691800189
epoch: 120, train loss: 0.16076863004809627, acc: 0.9012855209742896, val loss: 0.6182789543171158, acc: 0.7703984819734345, test loss: 0.6076933672845345, acc: 0.782909205152372
epoch: 121, train loss: 0.16458837787982414, acc: 0.8983761840324763, val loss: 0.5947057070572266, acc: 0.7805186590765338, test loss: 0.5951329958802156, acc: 0.7785108388312912
epoch: 122, train loss: 0.1590764848442297, acc: 0.9012178619756428, val loss: 0.6412357874568092, acc: 0.7558507273877293, test loss: 0.6595053364936189, acc: 0.7577756833176249
epoch: 123, train loss: 0.15866714242911306, acc: 0.9035859269282814, val loss: 0.6051284316821162, acc: 0.7748260594560404, test loss: 0.5836423236620415, acc: 0.7775683317624882
epoch: 124, train loss: 0.15122109819128, acc: 0.9070365358592692, val loss: 0.6399670539966947, acc: 0.7647058823529411, test loss: 0.6198328649926552, acc: 0.7668865849827207
epoch: 125, train loss: 0.1502191090414424, acc: 0.9080514208389716, val loss: 0.6417973613829495, acc: 0.7691334598355472, test loss: 0.654985018694809, acc: 0.760603204524034
epoch: 126, train loss: 0.15562041915640618, acc: 0.9029093369418133, val loss: 0.6888794617740358, acc: 0.7359266287160026, test loss: 0.7151047192090269, acc: 0.7354696826892868
epoch: 127, train loss: 0.14411955640949964, acc: 0.9098782138024357, val loss: 0.6570488093096873, acc: 0.7786211258697027, test loss: 0.668890054593699, acc: 0.7662582469368521
epoch: 128, train loss: 0.14092974005032619, acc: 0.9119079837618403, val loss: 0.6201192338884367, acc: 0.7735610373181531, test loss: 0.5999128365718903, acc: 0.7684574300973924
epoch: 129, train loss: 0.15576412214397256, acc: 0.9041271989174561, val loss: 0.6085760423007606, acc: 0.7647058823529411, test loss: 0.6369142198277988, acc: 0.7612315425699026
epoch: 130, train loss: 0.15991286429323265, acc: 0.901150202976996, val loss: 0.5988924790604969, acc: 0.7713472485768501, test loss: 0.6028101251741793, acc: 0.7628023876845743
epoch: 131, train loss: 0.15017637874986547, acc: 0.9060893098782138, val loss: 0.6154645195043517, acc: 0.7691334598355472, test loss: 0.5798320288632675, acc: 0.7722274583726044
epoch: 132, train loss: 0.19521738590423082, acc: 0.8828822733423546, val loss: 0.6761496451593214, acc: 0.7545857052498419, test loss: 0.6495233253604243, acc: 0.7580898523405593
epoch: 133, train loss: 0.17053295720736616, acc: 0.8952638700947226, val loss: 0.6466910983851105, acc: 0.7637571157495257, test loss: 0.6125315678333135, acc: 0.7728557964184731
epoch: 134, train loss: 0.1481098130930099, acc: 0.9058863328822734, val loss: 0.6076619234513362, acc: 0.775774826059456, test loss: 0.6167827135655927, acc: 0.7665724159597863
epoch: 135, train loss: 0.1376194158531171, acc: 0.9089309878213803, val loss: 0.5984755838166151, acc: 0.7789373814041746, test loss: 0.6014858146826876, acc: 0.7756833176248822
epoch: 136, train loss: 0.12562034705172373, acc: 0.9185385656292286, val loss: 0.6778727588134804, acc: 0.7691334598355472, test loss: 0.6839868857581174, acc: 0.7722274583726044
epoch: 137, train loss: 0.15064616817098833, acc: 0.9075778078484439, val loss: 0.6146721594850901, acc: 0.7694497153700189, test loss: 0.5871845969530609, acc: 0.7700282752120641
epoch: 138, train loss: 0.12775426711458637, acc: 0.9145466847090663, val loss: 0.6858223840302112, acc: 0.7599620493358634, test loss: 0.6828766569865038, acc: 0.7628023876845743
epoch: 139, train loss: 0.11900643878925153, acc: 0.9192828146143437, val loss: 0.6620433872670481, acc: 0.7770398481973435, test loss: 0.6573821624944017, acc: 0.7709707822808671
epoch: 140, train loss: 0.13842468951527578, acc: 0.9121109607577808, val loss: 0.6946412830093404, acc: 0.7530044275774826, test loss: 0.6962146817832633, acc: 0.7524348099277411
epoch: 141, train loss: 0.14007074900263863, acc: 0.9104194857916103, val loss: 0.6243282878496916, acc: 0.7792536369386465, test loss: 0.6150249592583021, acc: 0.7737983034872762
Epoch   141: reducing learning rate of group 0 to 7.5000e-04.
epoch: 142, train loss: 0.10079755220910047, acc: 0.933491204330176, val loss: 0.6255197442837112, acc: 0.7839974699557243, test loss: 0.6027506180166112, acc: 0.7885642475651901
epoch: 143, train loss: 0.07015036141275553, acc: 0.9498646820027064, val loss: 0.6427151075576696, acc: 0.7877925363693865, test loss: 0.645329916810525, acc: 0.7851083883129123
epoch: 144, train loss: 0.05968304420235354, acc: 0.9558186738836265, val loss: 0.6696102329630553, acc: 0.7868437697659709, test loss: 0.6514391528335413, acc: 0.7844800502670437
epoch: 145, train loss: 0.05078560322099674, acc: 0.9625845737483085, val loss: 0.6910924256262637, acc: 0.7915876027830487, test loss: 0.6790972072075194, acc: 0.7895067546339931
epoch: 146, train loss: 0.04831277951396528, acc: 0.9638024357239513, val loss: 0.7103854255688333, acc: 0.7982289690069576, test loss: 0.6827201510689148, acc: 0.7951617970468112
epoch: 147, train loss: 0.04464829461056744, acc: 0.9669824086603518, val loss: 0.7070521536241371, acc: 0.7836812144212524, test loss: 0.6895820558951256, acc: 0.7923342758404022
epoch: 148, train loss: 0.04450591239115382, acc: 0.9666441136671178, val loss: 0.7193487384966247, acc: 0.795382669196711, test loss: 0.7047434302571807, acc: 0.7939051209550738
epoch: 149, train loss: 0.04682572310505604, acc: 0.9633288227334236, val loss: 0.7320817028096324, acc: 0.7871600253004427, test loss: 0.7144606816779882, acc: 0.785736726358781
epoch: 150, train loss: 0.05688482827137707, acc: 0.9603518267929635, val loss: 0.74029046021859, acc: 0.7839974699557243, test loss: 0.7103596884163902, acc: 0.787621740496387
epoch: 151, train loss: 0.07152915840340887, acc: 0.9502706359945873, val loss: 0.6861789777714719, acc: 0.7719797596457938, test loss: 0.6973610421677187, acc: 0.7725416273955388
epoch: 152, train loss: 0.07445111121473197, acc: 0.9495263870094722, val loss: 0.6710034010155452, acc: 0.7884250474383302, test loss: 0.6724304837548355, acc: 0.781024191014766
epoch: 153, train loss: 0.06307106035004127, acc: 0.9537212449255751, val loss: 0.691384533927985, acc: 0.7858950031625553, test loss: 0.6925775676338395, acc: 0.7788250078542256
epoch: 154, train loss: 0.05547673105386984, acc: 0.9579161028416779, val loss: 0.6903514343300324, acc: 0.7833649588867805, test loss: 0.6978808079655725, acc: 0.7841658812441094
epoch: 155, train loss: 0.05385983255017594, acc: 0.9589986468200271, val loss: 0.7252873680698653, acc: 0.7688172043010753, test loss: 0.7268808739686439, acc: 0.7788250078542256
epoch: 156, train loss: 0.05146423304637488, acc: 0.9614343707713126, val loss: 0.6966974625174265, acc: 0.7906388361796332, test loss: 0.6943514571787612, acc: 0.7907634307257304
epoch: 157, train loss: 0.05773128407814506, acc: 0.9582543978349121, val loss: 0.7010457478015956, acc: 0.790955091714105, test loss: 0.6874273609822926, acc: 0.7882500785422557
epoch: 158, train loss: 0.055653284908510514, acc: 0.9598782138024358, val loss: 0.7357049710685735, acc: 0.7811511701454775, test loss: 0.7329871815553521, acc: 0.7785108388312912
epoch: 159, train loss: 0.04554894612509763, acc: 0.9651556156968877, val loss: 0.7413764242730814, acc: 0.786527514231499, test loss: 0.7138405229698515, acc: 0.7939051209550738
epoch: 160, train loss: 0.07356416552210371, acc: 0.9512855209742895, val loss: 0.6592741144525334, acc: 0.7852624920936117, test loss: 0.651926215924147, acc: 0.781024191014766
epoch: 161, train loss: 0.05067912466592169, acc: 0.9626522327469553, val loss: 0.7461796597089291, acc: 0.7836812144212524, test loss: 0.7455610615106787, acc: 0.7869934024505184
epoch: 162, train loss: 0.03886329321157948, acc: 0.9705683355886333, val loss: 0.7444553289286778, acc: 0.7839974699557243, test loss: 0.7489232395266797, acc: 0.7832233741753063
epoch: 163, train loss: 0.06699871216801087, acc: 0.952638700947226, val loss: 0.831174990257683, acc: 0.7413029728020241, test loss: 0.8632056973769235, acc: 0.7364121897580899
epoch: 164, train loss: 0.06924360127103829, acc: 0.9518267929634642, val loss: 0.6987835831615007, acc: 0.7839974699557243, test loss: 0.6944019112465631, acc: 0.7913917687715991
epoch: 165, train loss: 0.07997146247204327, acc: 0.9419485791610284, val loss: 0.7237356659130987, acc: 0.7732447817836812, test loss: 0.7029584681804248, acc: 0.7759974866478165
epoch: 166, train loss: 0.07583688930362584, acc: 0.946211096075778, val loss: 0.6785502807766783, acc: 0.7849462365591398, test loss: 0.6721330429823789, acc: 0.7854225573358466
epoch: 167, train loss: 0.05958420601402471, acc: 0.9562246278755074, val loss: 0.6777779869758804, acc: 0.7786211258697027, test loss: 0.690898233863718, acc: 0.782909205152372
epoch: 168, train loss: 0.05462486542892391, acc: 0.9598105548037889, val loss: 0.7169326470668824, acc: 0.7881087919038583, test loss: 0.7232849971700531, acc: 0.7822808671065034
epoch: 169, train loss: 0.04379985356988216, acc: 0.9665087956698241, val loss: 0.7459601211366889, acc: 0.7893738140417458, test loss: 0.7466715375395118, acc: 0.7869934024505184
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.03182250645682115, acc: 0.9652232746955345, val loss: 0.6371079622228865, acc: 0.7896900695762176, test loss: 0.652997969120432, acc: 0.7882500785422557
epoch: 171, train loss: 0.03328391400459493, acc: 0.9671853856562923, val loss: 0.6513122718293481, acc: 0.7602783048703352, test loss: 0.6510740433928729, acc: 0.7656299088909834
epoch: 172, train loss: 0.045097848610682806, acc: 0.957510148849797, val loss: 0.6308780017342468, acc: 0.7760910815939279, test loss: 0.6280300826573949, acc: 0.77913917687716
epoch: 173, train loss: 0.051144028576506816, acc: 0.9508795669824086, val loss: 0.6042030533984218, acc: 0.7802024035420619, test loss: 0.605247789608245, acc: 0.781024191014766
epoch: 174, train loss: 0.035025527451252904, acc: 0.9629228687415426, val loss: 0.5956579857729116, acc: 0.7896900695762176, test loss: 0.6242084850352626, acc: 0.7873075714734528
epoch: 175, train loss: 0.031487985436725194, acc: 0.9658322056833559, val loss: 0.6187237515470636, acc: 0.7820999367488931, test loss: 0.6423550568546771, acc: 0.7926484448633365
epoch: 176, train loss: 0.024677996654996048, acc: 0.9719215155615697, val loss: 0.6455653900590447, acc: 0.7896900695762176, test loss: 0.6342666581333938, acc: 0.7825950361294376
epoch: 177, train loss: 0.02345169687615922, acc: 0.974424898511502, val loss: 0.6060012381413102, acc: 0.7938013915243517, test loss: 0.6239195830260962, acc: 0.7913917687715991
epoch: 178, train loss: 0.021221127670071768, acc: 0.9742219215155615, val loss: 0.6522985303198364, acc: 0.7884250474383302, test loss: 0.6758237419883447, acc: 0.7832233741753063
epoch: 179, train loss: 0.030862177474895575, acc: 0.9685385656292287, val loss: 0.6514621985855923, acc: 0.7685009487666035, test loss: 0.6584291841636991, acc: 0.7665724159597863
epoch: 180, train loss: 0.0587675296758202, acc: 0.9460757780784844, val loss: 0.6065323945467115, acc: 0.7811511701454775, test loss: 0.6068566476178926, acc: 0.7753691486019478
epoch: 181, train loss: 0.05175454989882219, acc: 0.9491204330175913, val loss: 0.5778171719817102, acc: 0.7792536369386465, test loss: 0.5917646553843717, acc: 0.7706566132579328
epoch: 182, train loss: 0.04448824644653336, acc: 0.9527740189445196, val loss: 0.5951545603436658, acc: 0.7858950031625553, test loss: 0.611101757495088, acc: 0.7788250078542256
epoch: 183, train loss: 0.07796271994347018, acc: 0.9308525033829499, val loss: 0.5380391505305937, acc: 0.7802024035420619, test loss: 0.5597933315833784, acc: 0.7741124725102105
epoch: 184, train loss: 0.059117166218076896, acc: 0.9448579161028416, val loss: 0.5763341945458785, acc: 0.782416192283365, test loss: 0.5998748442379889, acc: 0.7712849513038015
epoch: 185, train loss: 0.0468898883399444, acc: 0.9540595399188092, val loss: 0.6027387167214593, acc: 0.7786211258697027, test loss: 0.5925280328370699, acc: 0.7744266415331448
epoch: 186, train loss: 0.03937820933648311, acc: 0.9600811907983762, val loss: 0.6392206051619577, acc: 0.7748260594560404, test loss: 0.6250766718346407, acc: 0.7772541627395538
epoch: 187, train loss: 0.037907290135047754, acc: 0.9605548037889039, val loss: 0.5980751227910273, acc: 0.7893738140417458, test loss: 0.590150807362999, acc: 0.7917059377945335
epoch: 188, train loss: 0.029650192758186904, acc: 0.9672530446549391, val loss: 0.6122870831305585, acc: 0.7881087919038583, test loss: 0.5951903870448053, acc: 0.7967326421614829
epoch: 189, train loss: 0.03924267759624773, acc: 0.9604871447902571, val loss: 0.5946757737629629, acc: 0.7802024035420619, test loss: 0.5976665362673588, acc: 0.7832233741753063
epoch: 190, train loss: 0.03496026900043668, acc: 0.9628552097428958, val loss: 0.6370111467112928, acc: 0.7868437697659709, test loss: 0.6209205144087623, acc: 0.7891925856110588
epoch: 191, train loss: 0.03927227078416512, acc: 0.9608254397834912, val loss: 0.5899156409679223, acc: 0.7855787476280834, test loss: 0.5944814427708666, acc: 0.7863650644046497
epoch: 192, train loss: 0.038807737802096086, acc: 0.9587280108254398, val loss: 0.6069405534311159, acc: 0.7890575585072739, test loss: 0.6051234233840022, acc: 0.7904492617027961
Epoch   192: reducing learning rate of group 0 to 3.7500e-04.
epoch: 193, train loss: 0.026703732324630546, acc: 0.9704330175913396, val loss: 0.6076411634664759, acc: 0.7915876027830487, test loss: 0.5975736588604825, acc: 0.7954759660697456
epoch: 194, train loss: 0.015987098555512856, acc: 0.9796346414073072, val loss: 0.6288613962418365, acc: 0.7956989247311828, test loss: 0.6144762242922721, acc: 0.7942192899780082
epoch: 195, train loss: 0.013884583842362175, acc: 0.9822056833558863, val loss: 0.6335944438719584, acc: 0.7985452245414295, test loss: 0.620724433928288, acc: 0.7961043041156142
epoch: 196, train loss: 0.011579736936156902, acc: 0.9863328822733424, val loss: 0.6417090516539784, acc: 0.7956989247311828, test loss: 0.629244143619052, acc: 0.7967326421614829
epoch: 197, train loss: 0.012238405246529551, acc: 0.9852503382949932, val loss: 0.6484313394961517, acc: 0.7941176470588235, test loss: 0.639610160756628, acc: 0.7932767829092051
epoch: 198, train loss: 0.010679191986101406, acc: 0.9873477672530446, val loss: 0.6492616370235493, acc: 0.7868437697659709, test loss: 0.6427773377672891, acc: 0.7929626138862708
epoch: 199, train loss: 0.011604207521493526, acc: 0.9868741542625169, val loss: 0.6491355197781027, acc: 0.7988614800759013, test loss: 0.6560213440362664, acc: 0.7917059377945335
epoch: 200, train loss: 0.014685965360158674, acc: 0.9852503382949932, val loss: 0.6552828765232152, acc: 0.7928526249209361, test loss: 0.6492215127567431, acc: 0.7926484448633365
best val acc 0.7988614800759013 at epoch 199.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9996    0.9978    0.9987      5337
           1     0.9974    0.9556    0.9760       810
           2     0.9809    1.0000    0.9903      2100
           3     0.9973    1.0000    0.9986       737
           4     0.9495    1.0000    0.9741       677
           5     0.9992    0.9985    0.9989      1323
           6     0.9974    0.9897    0.9935      1164
           7     0.9905    0.9952    0.9929       421
           8     0.9877    1.0000    0.9938       401
           9     0.9826    0.9975    0.9900       396
          10     0.9968    0.9984    0.9976       627
          11     0.9932    1.0000    0.9966       291
          12     0.9865    0.8429    0.9091       261
          13     1.0000    0.9872    0.9936       235

    accuracy                         0.9926     14780
   macro avg     0.9899    0.9831    0.9860     14780
weighted avg     0.9927    0.9926    0.9925     14780

train confusion matrix:
[[9.97751546e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.87371182e-04 7.49484729e-04
  7.49484729e-04 0.00000000e+00 0.00000000e+00 3.74742365e-04
  1.87371182e-04 0.00000000e+00]
 [0.00000000e+00 9.55555556e-01 0.00000000e+00 0.00000000e+00
  4.32098765e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.23456790e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 7.55857899e-04 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.98488284e-01 0.00000000e+00 0.00000000e+00
  7.55857899e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 8.59106529e-04
  0.00000000e+00 8.59106529e-04 9.89690722e-01 0.00000000e+00
  0.00000000e+00 6.01374570e-03 8.59106529e-04 0.00000000e+00
  1.71821306e-03 0.00000000e+00]
 [4.75059382e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.95249406e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.52525253e-03 0.00000000e+00
  0.00000000e+00 9.97474747e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.59489633e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.98405104e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 1.57088123e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  8.42911877e-01 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.25531915e-03
  4.25531915e-03 0.00000000e+00 4.25531915e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.87234043e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8701    0.8793    0.8747      1143
           1     0.8655    0.8555    0.8605       173
           2     0.8009    0.8222    0.8114       450
           3     0.8072    0.8481    0.8272       158
           4     0.8467    0.8759    0.8610       145
           5     0.8321    0.8233    0.8277       283
           6     0.6041    0.5944    0.5992       249
           7     0.8101    0.7111    0.7574        90
           8     0.6265    0.6118    0.6190        85
           9     0.8462    0.7857    0.8148        84
          10     0.8226    0.7612    0.7907       134
          11     0.8182    0.5806    0.6792        62
          12     0.1333    0.2143    0.1644        56
          13     0.8286    0.5800    0.6824        50

    accuracy                         0.7989      3162
   macro avg     0.7509    0.7102    0.7264      3162
weighted avg     0.8058    0.7989    0.8012      3162

validation confusion matrix:
[[8.79265092e-01 4.37445319e-03 2.79965004e-02 8.74890639e-04
  8.74890639e-04 5.24934383e-03 2.88713911e-02 8.74890639e-03
  1.22484689e-02 4.37445319e-03 2.62467192e-03 1.74978128e-03
  2.18722660e-02 8.74890639e-04]
 [2.31213873e-02 8.55491329e-01 5.78034682e-03 5.78034682e-03
  6.93641618e-02 1.15606936e-02 1.15606936e-02 0.00000000e+00
  1.15606936e-02 0.00000000e+00 0.00000000e+00 5.78034682e-03
  0.00000000e+00 0.00000000e+00]
 [6.66666667e-02 0.00000000e+00 8.22222222e-01 1.55555556e-02
  2.22222222e-03 1.11111111e-02 2.44444444e-02 0.00000000e+00
  2.22222222e-03 0.00000000e+00 8.88888889e-03 2.22222222e-03
  4.22222222e-02 2.22222222e-03]
 [5.69620253e-02 1.26582278e-02 1.89873418e-02 8.48101266e-01
  0.00000000e+00 6.32911392e-03 1.26582278e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.89873418e-02 6.32911392e-03
  1.89873418e-02 0.00000000e+00]
 [0.00000000e+00 4.82758621e-02 6.89655172e-03 6.89655172e-03
  8.75862069e-01 4.13793103e-02 6.89655172e-03 0.00000000e+00
  6.89655172e-03 0.00000000e+00 6.89655172e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.88692580e-02 1.41342756e-02 2.47349823e-02 1.41342756e-02
  2.82685512e-02 8.23321555e-01 2.12014134e-02 0.00000000e+00
  2.12014134e-02 0.00000000e+00 0.00000000e+00 7.06713781e-03
  7.06713781e-03 0.00000000e+00]
 [1.04417671e-01 0.00000000e+00 9.23694779e-02 2.00803213e-02
  4.01606426e-03 4.81927711e-02 5.94377510e-01 8.03212851e-03
  0.00000000e+00 2.40963855e-02 2.00803213e-02 0.00000000e+00
  7.22891566e-02 1.20481928e-02]
 [1.88888889e-01 1.11111111e-02 1.11111111e-02 0.00000000e+00
  0.00000000e+00 1.11111111e-02 4.44444444e-02 7.11111111e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  2.22222222e-02 0.00000000e+00]
 [2.23529412e-01 3.52941176e-02 1.17647059e-02 0.00000000e+00
  0.00000000e+00 5.88235294e-02 1.17647059e-02 1.17647059e-02
  6.11764706e-01 0.00000000e+00 1.17647059e-02 0.00000000e+00
  2.35294118e-02 0.00000000e+00]
 [3.57142857e-02 0.00000000e+00 0.00000000e+00 2.38095238e-02
  0.00000000e+00 0.00000000e+00 1.07142857e-01 0.00000000e+00
  0.00000000e+00 7.85714286e-01 1.19047619e-02 1.19047619e-02
  2.38095238e-02 0.00000000e+00]
 [2.23880597e-02 7.46268657e-03 3.73134328e-02 2.23880597e-02
  0.00000000e+00 2.23880597e-02 7.46268657e-02 7.46268657e-03
  1.49253731e-02 0.00000000e+00 7.61194030e-01 0.00000000e+00
  2.23880597e-02 7.46268657e-03]
 [2.25806452e-01 0.00000000e+00 4.83870968e-02 1.61290323e-02
  0.00000000e+00 1.61290323e-02 0.00000000e+00 1.61290323e-02
  6.45161290e-02 0.00000000e+00 3.22580645e-02 5.80645161e-01
  0.00000000e+00 0.00000000e+00]
 [1.96428571e-01 0.00000000e+00 2.14285714e-01 8.92857143e-02
  0.00000000e+00 8.92857143e-02 1.25000000e-01 0.00000000e+00
  1.78571429e-02 1.78571429e-02 3.57142857e-02 0.00000000e+00
  2.14285714e-01 0.00000000e+00]
 [6.00000000e-02 0.00000000e+00 6.00000000e-02 4.00000000e-02
  0.00000000e+00 0.00000000e+00 2.20000000e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.00000000e-02 5.80000000e-01]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8719    0.8795    0.8757      1145
           1     0.8373    0.7943    0.8152       175
           2     0.8140    0.8248    0.8194       451
           3     0.7848    0.7799    0.7823       159
           4     0.8299    0.8356    0.8328       146
           5     0.8357    0.8239    0.8298       284
           6     0.5635    0.5680    0.5657       250
           7     0.8118    0.7582    0.7841        91
           8     0.6168    0.7586    0.6804        87
           9     0.8193    0.7907    0.8047        86
          10     0.8246    0.6912    0.7520       136
          11     0.6964    0.6094    0.6500        64
          12     0.1628    0.2456    0.1958        57
          13     0.8108    0.5769    0.6742        52

    accuracy                         0.7917      3183
   macro avg     0.7343    0.7098    0.7187      3183
weighted avg     0.7987    0.7917    0.7941      3183

test confusion matrix:
[[8.79475983e-01 4.36681223e-03 1.57205240e-02 2.62008734e-03
  8.73362445e-04 7.86026201e-03 2.88209607e-02 8.73362445e-03
  2.18340611e-02 4.36681223e-03 3.49344978e-03 5.24017467e-03
  1.48471616e-02 1.74672489e-03]
 [1.71428571e-02 7.94285714e-01 5.71428571e-03 1.14285714e-02
  9.14285714e-02 2.85714286e-02 1.71428571e-02 0.00000000e+00
  1.71428571e-02 0.00000000e+00 0.00000000e+00 1.14285714e-02
  5.71428571e-03 0.00000000e+00]
 [6.87361419e-02 2.21729490e-03 8.24833703e-01 4.43458980e-03
  0.00000000e+00 4.43458980e-03 3.10421286e-02 0.00000000e+00
  2.21729490e-03 2.21729490e-03 8.86917960e-03 0.00000000e+00
  4.87804878e-02 2.21729490e-03]
 [1.88679245e-02 1.25786164e-02 6.28930818e-02 7.79874214e-01
  0.00000000e+00 3.14465409e-02 6.28930818e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 6.28930818e-03 1.88679245e-02
  6.28930818e-03 0.00000000e+00]
 [6.84931507e-03 7.53424658e-02 0.00000000e+00 0.00000000e+00
  8.35616438e-01 7.53424658e-02 6.84931507e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.87323944e-02 3.52112676e-03 3.16901408e-02 1.05633803e-02
  2.81690141e-02 8.23943662e-01 2.81690141e-02 0.00000000e+00
  7.04225352e-03 0.00000000e+00 0.00000000e+00 1.40845070e-02
  1.40845070e-02 0.00000000e+00]
 [1.20000000e-01 1.60000000e-02 7.60000000e-02 4.80000000e-02
  0.00000000e+00 2.40000000e-02 5.68000000e-01 8.00000000e-03
  8.00000000e-03 2.40000000e-02 1.60000000e-02 4.00000000e-03
  7.20000000e-02 1.60000000e-02]
 [2.19780220e-01 0.00000000e+00 1.09890110e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 7.58241758e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.09890110e-02 0.00000000e+00]
 [1.14942529e-01 1.14942529e-02 3.44827586e-02 0.00000000e+00
  0.00000000e+00 2.29885057e-02 1.14942529e-02 2.29885057e-02
  7.58620690e-01 0.00000000e+00 1.14942529e-02 0.00000000e+00
  1.14942529e-02 0.00000000e+00]
 [9.30232558e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.30232558e-02 1.16279070e-02
  1.16279070e-02 7.90697674e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.41176471e-02 7.35294118e-03 5.14705882e-02 2.20588235e-02
  0.00000000e+00 2.94117647e-02 9.55882353e-02 0.00000000e+00
  1.47058824e-02 0.00000000e+00 6.91176471e-01 7.35294118e-03
  3.67647059e-02 0.00000000e+00]
 [1.40625000e-01 1.56250000e-02 1.56250000e-02 6.25000000e-02
  0.00000000e+00 0.00000000e+00 4.68750000e-02 1.56250000e-02
  6.25000000e-02 0.00000000e+00 1.56250000e-02 6.09375000e-01
  1.56250000e-02 0.00000000e+00]
 [1.75438596e-01 0.00000000e+00 2.63157895e-01 7.01754386e-02
  0.00000000e+00 3.50877193e-02 8.77192982e-02 0.00000000e+00
  1.75438596e-02 3.50877193e-02 7.01754386e-02 0.00000000e+00
  2.45614035e-01 0.00000000e+00]
 [1.15384615e-01 0.00000000e+00 1.92307692e-02 1.92307692e-02
  0.00000000e+00 0.00000000e+00 2.11538462e-01 0.00000000e+00
  0.00000000e+00 1.92307692e-02 1.92307692e-02 0.00000000e+00
  1.92307692e-02 5.76923077e-01]]
---------------------------------------
program finished.
