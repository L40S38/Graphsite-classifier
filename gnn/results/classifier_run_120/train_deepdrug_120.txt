seed:  20
save trained model at:  ../trained_models/trained_classifier_model_120.pt
save loss at:  ./results/train_classifier_results_120.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['1p16B00', '3pyzA00', '2is6C00', '6q4uA00', '2a5jA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['2z02B01', '2qu8A00', '5ajxA01', '4hg0A00', '3tk1A00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2af6e9e8c610>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 1.981301987031458, acc: 0.39972771398129514; test loss: 1.704229538898833, acc: 0.4800283620893406
epoch: 2, train loss: 1.7096293582512074, acc: 0.47040369361903633; test loss: 1.6014118386734972, acc: 0.505317891751359
epoch: 3, train loss: 1.6328821079369986, acc: 0.495915709719427; test loss: 1.594908220439452, acc: 0.499172772394233
epoch: 4, train loss: 1.5750656978356086, acc: 0.5205990292411508; test loss: 1.506722488912728, acc: 0.5400614511935713
epoch: 5, train loss: 1.519156286190625, acc: 0.5407837101929679; test loss: 1.4781468140546437, acc: 0.5459702198061924
epoch: 6, train loss: 1.4900468367436264, acc: 0.5492482538179235; test loss: 1.3974272773159335, acc: 0.5755140628692981
epoch: 7, train loss: 1.4624256931621957, acc: 0.5589558423108796; test loss: 1.3805728779704654, acc: 0.5771685180808319
epoch: 8, train loss: 1.4263817846979687, acc: 0.5694921273825027; test loss: 1.366129591627849, acc: 0.5818955329709289
epoch: 9, train loss: 1.3886050926350526, acc: 0.5813306499348881; test loss: 1.3636945942788787, acc: 0.5849680926494919
epoch: 10, train loss: 1.3727649906845336, acc: 0.5856517106665088; test loss: 1.3596684116191318, acc: 0.5948948239186953
epoch: 11, train loss: 1.348657952309558, acc: 0.5904463123002249; test loss: 1.2929818875342785, acc: 0.6019853462538407
epoch: 12, train loss: 1.3354383425640894, acc: 0.5921628980703209; test loss: 1.4092012964729663, acc: 0.5776412195698416
epoch: 13, train loss: 1.335873373347626, acc: 0.5901503492364153; test loss: 1.3726098042124977, acc: 0.5700779957456866
epoch: 14, train loss: 1.3056685475184253, acc: 0.6015745234994673; test loss: 1.2860489166028053, acc: 0.6076577641219569
epoch: 15, train loss: 1.2875004833910584, acc: 0.6054812359417545; test loss: 1.4255252499806912, acc: 0.5714961002127157
epoch: 16, train loss: 1.2658136391676402, acc: 0.6141233574049959; test loss: 1.3054963800817538, acc: 0.603167099976365
epoch: 17, train loss: 1.2619253569302988, acc: 0.615662365336806; test loss: 1.3099369832568348, acc: 0.5896951075395888
epoch: 18, train loss: 1.251334952972225, acc: 0.6182076476855688; test loss: 1.2373101576032213, acc: 0.6208934058142284
epoch: 19, train loss: 1.2337652044323362, acc: 0.6237125606724281; test loss: 1.2318535344140296, acc: 0.6126211297565587
epoch: 20, train loss: 1.2512393890082576, acc: 0.6200426186811886; test loss: 1.3594135715729856, acc: 0.5913495627511227
epoch: 21, train loss: 1.2099481417580413, acc: 0.631111637267669; test loss: 1.2060381803803342, acc: 0.6251477192153155
epoch: 22, train loss: 1.1985082048289915, acc: 0.6325914525867172; test loss: 1.2506699954835103, acc: 0.612148428267549
epoch: 23, train loss: 1.1954885470284307, acc: 0.635195927548242; test loss: 1.253386882539901, acc: 0.6090758685889861
epoch: 24, train loss: 1.1813664713512162, acc: 0.6379187877352906; test loss: 1.2637198255350053, acc: 0.6173481446466557
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9539005813463716, acc: 0.6397537587309103; test loss: 0.9481070116602538, acc: 0.6407468683526353
epoch: 26, train loss: 0.949331356891446, acc: 0.6431277376583402; test loss: 0.9800650149872613, acc: 0.630583786338927
epoch: 27, train loss: 0.9182560407820434, acc: 0.6499348881259619; test loss: 0.9876911094131776, acc: 0.6256204207043252
epoch: 28, train loss: 0.9268933451075491, acc: 0.64650171658577; test loss: 1.021418370019457, acc: 0.597021980619239
epoch: 29, train loss: 0.91254715502749, acc: 0.6498756955131999; test loss: 1.0732383606873965, acc: 0.5750413613802884
epoch: 30, train loss: 0.9055039987022727, acc: 0.6533680596661536; test loss: 1.0030340328478018, acc: 0.6216024580477428
epoch: 31, train loss: 0.8952686006082675, acc: 0.6581626612998698; test loss: 0.9130330324116623, acc: 0.6452375324982274
epoch: 32, train loss: 0.8906599829800104, acc: 0.657985083461584; test loss: 0.8943855514539891, acc: 0.6523280548333728
epoch: 33, train loss: 0.8839899337868924, acc: 0.6655617378951106; test loss: 0.9830002646670503, acc: 0.6175844953911604
epoch: 34, train loss: 0.8756866161465094, acc: 0.6636083816739671; test loss: 1.0173987597896257, acc: 0.6192389506026944
epoch: 35, train loss: 0.8692861680760389, acc: 0.664555463478158; test loss: 0.9097811812854101, acc: 0.6551642637674309
epoch: 36, train loss: 0.849462029552042, acc: 0.6715993843968273; test loss: 0.9314782711890989, acc: 0.6277475774048689
epoch: 37, train loss: 0.8578832295500655, acc: 0.6682254054693975; test loss: 1.047743383352144, acc: 0.5974946821082486
epoch: 38, train loss: 0.8411908148025724, acc: 0.6733159701669231; test loss: 1.1342524090886144, acc: 0.583077286693453
epoch: 39, train loss: 0.8296669561989677, acc: 0.6786433053154967; test loss: 0.8938142995241545, acc: 0.6499645473883243
epoch: 40, train loss: 0.8146213486559563, acc: 0.6830827512726412; test loss: 0.8884336504353555, acc: 0.661073032380052
epoch: 41, train loss: 0.8098482147425438, acc: 0.6855096483958802; test loss: 1.0395012109788477, acc: 0.6133301819900733
epoch: 42, train loss: 0.80228050923339, acc: 0.6895939386764531; test loss: 0.9770646836283753, acc: 0.624438666981801
epoch: 43, train loss: 0.7929282670925958, acc: 0.6927903397655972; test loss: 0.93255493066689, acc: 0.6286929803828882
epoch: 44, train loss: 0.8054961942562312, acc: 0.6867526932638807; test loss: 1.028724907250924, acc: 0.6175844953911604
epoch: 45, train loss: 0.7827602042112602, acc: 0.6955723925654078; test loss: 0.9605537110596677, acc: 0.6223115102812574
epoch: 46, train loss: 0.7825266011539995, acc: 0.6949212738250267; test loss: 0.841872838224764, acc: 0.6691089576932168
epoch: 47, train loss: 0.7597004555944608, acc: 0.7052799810583639; test loss: 0.892878922072744, acc: 0.6454738832427322
epoch: 48, train loss: 0.7589268707602465, acc: 0.6990055641055997; test loss: 0.8413638462273119, acc: 0.6797447411959348
epoch: 49, train loss: 0.7459587711927749, acc: 0.7062270628625548; test loss: 0.8867798714287257, acc: 0.6518553533443631
epoch: 50, train loss: 0.7486210686717073, acc: 0.7036817805137919; test loss: 0.8231551466301468, acc: 0.6733632710943039
epoch: 51, train loss: 0.7462603976433836, acc: 0.7062862554753166; test loss: 0.8144933811121026, acc: 0.6865989127865753
epoch: 52, train loss: 0.7295543226995905, acc: 0.7143364508109388; test loss: 0.879219260178568, acc: 0.6601276294020326
epoch: 53, train loss: 0.7283796802616492, acc: 0.7129158281046526; test loss: 0.8135409231192675, acc: 0.6771448830063814
epoch: 54, train loss: 0.7295647127128457, acc: 0.7119687463004617; test loss: 0.8909780675427791, acc: 0.6561096667454502
epoch: 55, train loss: 0.71052794151226, acc: 0.7211436012785605; test loss: 0.8191531632589522, acc: 0.6752540770503427
epoch: 56, train loss: 0.7147172206997998, acc: 0.7169409257724636; test loss: 0.7780594899563664, acc: 0.6979437485228078
epoch: 57, train loss: 0.7047411747355284, acc: 0.7180655854149403; test loss: 0.7892924265161639, acc: 0.6903805246986529
epoch: 58, train loss: 0.725764172922807, acc: 0.708476382147508; test loss: 0.9006478203241776, acc: 0.6393287638856062
epoch: 59, train loss: 0.6972040994725905, acc: 0.720670060376465; test loss: 1.0579410721240836, acc: 0.600094540297802
epoch: 60, train loss: 0.698885990773995, acc: 0.7220314904699894; test loss: 0.8535632224996951, acc: 0.6643819428031198
epoch: 61, train loss: 0.70067555808846, acc: 0.7197821711850361; test loss: 0.8256087157214063, acc: 0.674781375561333
epoch: 62, train loss: 0.6749970781121492, acc: 0.7305552267077069; test loss: 0.8582873359635997, acc: 0.6830536516190026
epoch: 63, train loss: 0.6823959723088506, acc: 0.730436841482183; test loss: 0.819256693836195, acc: 0.6858898605530608
epoch: 64, train loss: 0.6918757261997908, acc: 0.7226234165976086; test loss: 0.8233275833323728, acc: 0.680926494918459
epoch: 65, train loss: 0.6698560885696054, acc: 0.7346395169882799; test loss: 0.7875506777595442, acc: 0.6934530843772158
epoch: 66, train loss: 0.6769723269845432, acc: 0.7319758494139932; test loss: 0.7783979826349932, acc: 0.690144173954148
epoch: 67, train loss: 0.6573329711999077, acc: 0.7365928732094235; test loss: 0.8067143032156923, acc: 0.6844717560860317
Epoch    67: reducing learning rate of group 0 to 1.5000e-03.
epoch: 68, train loss: 0.595930940691107, acc: 0.7583165620930508; test loss: 0.7125391426505673, acc: 0.7272512408414087
epoch: 69, train loss: 0.5630152112734562, acc: 0.7725227891559133; test loss: 0.7464219050271041, acc: 0.7151973528716615
epoch: 70, train loss: 0.5554199839211278, acc: 0.7748313010536285; test loss: 0.7330016626079887, acc: 0.7159064051051761
epoch: 71, train loss: 0.5463134766302405, acc: 0.7775541612406771; test loss: 0.781669675754447, acc: 0.7050342708579532
epoch: 72, train loss: 0.5438762318210177, acc: 0.7770806203385817; test loss: 0.7413108810919786, acc: 0.7263058378633893
epoch: 73, train loss: 0.5465503468857272, acc: 0.7795667100745827; test loss: 0.7487912736472101, acc: 0.718978964783739
epoch: 74, train loss: 0.533687092650497, acc: 0.7842429264827749; test loss: 0.7217133600248059, acc: 0.7258331363743796
epoch: 75, train loss: 0.5367997689643413, acc: 0.7762519237599148; test loss: 0.7455861242745565, acc: 0.71756086031671
epoch: 76, train loss: 0.5199894406420953, acc: 0.7859595122528709; test loss: 0.7676511301100606, acc: 0.7170881588277003
epoch: 77, train loss: 0.5242049009422868, acc: 0.786669823606014; test loss: 0.7212885526640425, acc: 0.726778539352399
epoch: 78, train loss: 0.514245534184502, acc: 0.7868474014442998; test loss: 0.796908026692772, acc: 0.7047979201134483
epoch: 79, train loss: 0.5166391521653466, acc: 0.788268024150586; test loss: 0.7274154369162206, acc: 0.7253604348853699
epoch: 80, train loss: 0.5070074999333839, acc: 0.7908724991121108; test loss: 0.9432638924444745, acc: 0.6705270621602458
epoch: 81, train loss: 0.4995663474794957, acc: 0.7931218183970641; test loss: 0.6959150278154093, acc: 0.7338690616875443
epoch: 82, train loss: 0.5008495328439627, acc: 0.7979164200307801; test loss: 0.73911103856279, acc: 0.7244150319073505
epoch: 83, train loss: 0.49381965431247477, acc: 0.7949567893926838; test loss: 0.8341527753788032, acc: 0.6903805246986529
epoch: 84, train loss: 0.4993005265522895, acc: 0.7909316917248728; test loss: 0.7407272745604437, acc: 0.7194516662727488
epoch: 85, train loss: 0.4930864999149486, acc: 0.7956671007458269; test loss: 0.7724881622983445, acc: 0.7045615693689435
epoch: 86, train loss: 0.4856347915998391, acc: 0.7960222564223984; test loss: 0.7575923965434266, acc: 0.7182699125502245
epoch: 87, train loss: 0.48671967355657697, acc: 0.7954895229075412; test loss: 0.7146290627756572, acc: 0.7284329945639328
epoch: 88, train loss: 0.48502251636746285, acc: 0.8033621404048775; test loss: 0.8074219488303055, acc: 0.7104703379815647
epoch: 89, train loss: 0.47663817056272245, acc: 0.8012312063454481; test loss: 0.7690565108802127, acc: 0.7239423304183408
epoch: 90, train loss: 0.46999555463492837, acc: 0.8066769267195454; test loss: 0.7317996917112418, acc: 0.7341054124320492
epoch: 91, train loss: 0.4599837034400912, acc: 0.8036581034686872; test loss: 0.792200504423343, acc: 0.709997636492555
epoch: 92, train loss: 0.46835850670138124, acc: 0.8066769267195454; test loss: 0.8038543482194187, acc: 0.7232332781848263
epoch: 93, train loss: 0.4655218441112092, acc: 0.8056114596898307; test loss: 0.8463189851530504, acc: 0.7007799574568659
epoch: 94, train loss: 0.46302004120569196, acc: 0.8062625784302119; test loss: 0.795980605186511, acc: 0.7062160245804774
epoch: 95, train loss: 0.4419257602028922, acc: 0.8157925890848822; test loss: 0.8165799363288888, acc: 0.703852517135429
epoch: 96, train loss: 0.44481116276076943, acc: 0.8112939505149758; test loss: 0.8231123897432576, acc: 0.7031434649019145
epoch: 97, train loss: 0.44718272804974013, acc: 0.8098733278086895; test loss: 0.7601097435907279, acc: 0.7286693453084377
epoch: 98, train loss: 0.4384021150426525, acc: 0.8164437078252634; test loss: 0.7658469563707114, acc: 0.7215788229732923
epoch: 99, train loss: 0.4486791648758337, acc: 0.8101692908724991; test loss: 0.7487792144343521, acc: 0.71756086031671
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.33973554288323454, acc: 0.8178643305315496; test loss: 0.6483734963605714, acc: 0.7194516662727488
epoch: 101, train loss: 0.32726099142678833, acc: 0.8221853912631704; test loss: 0.6199757035701051, acc: 0.7279602930749232
epoch: 102, train loss: 0.3238597819618012, acc: 0.819166568012312; test loss: 0.6555326168093663, acc: 0.7173245095722052
epoch: 103, train loss: 0.342115889713865, acc: 0.8146679294424056; test loss: 0.6671383981585249, acc: 0.7135428976601277
epoch: 104, train loss: 0.32994073374646504, acc: 0.818456256659169; test loss: 0.6726116921089693, acc: 0.7201607185062633
epoch: 105, train loss: 0.3274690634832871, acc: 0.8174499822422162; test loss: 0.6311536386091358, acc: 0.7272512408414087
epoch: 106, train loss: 0.33044384259210613, acc: 0.8165620930507873; test loss: 0.6604220196023615, acc: 0.7147246513826518
epoch: 107, train loss: 0.3225283833231406, acc: 0.824079554871552; test loss: 0.6614100264759104, acc: 0.7140155991491374
epoch: 108, train loss: 0.3218844529614867, acc: 0.8211791168462176; test loss: 0.6265590296826219, acc: 0.734341763176554
epoch: 109, train loss: 0.3279536614774232, acc: 0.8163845152125014; test loss: 0.6328921783508575, acc: 0.7274875915859135
epoch: 110, train loss: 0.33093056525156117, acc: 0.817627560080502; test loss: 0.7800030768607925, acc: 0.6806901441739541
epoch: 111, train loss: 0.32665865319260934, acc: 0.818456256659169; test loss: 0.6656832787985498, acc: 0.7239423304183408
epoch: 112, train loss: 0.31853580224354533, acc: 0.8213566946845033; test loss: 0.632116856966084, acc: 0.7244150319073505
epoch: 113, train loss: 0.30961786572349215, acc: 0.823250858292885; test loss: 0.6913669621569697, acc: 0.7137792484046325
epoch: 114, train loss: 0.310356719336228, acc: 0.825736948028886; test loss: 0.6394981431273584, acc: 0.7284329945639328
epoch: 115, train loss: 0.3175313949154526, acc: 0.8215342725227892; test loss: 0.7053614842979036, acc: 0.7003072559678563
epoch: 116, train loss: 0.32212285075622915, acc: 0.8194625310761217; test loss: 0.6267989723994022, acc: 0.7395414795556606
epoch: 117, train loss: 0.31286294661119196, acc: 0.820883153782408; test loss: 0.6785644481887042, acc: 0.7182699125502245
epoch: 118, train loss: 0.30430685175838335, acc: 0.8230732804545993; test loss: 0.6642859986250464, acc: 0.718978964783739
epoch: 119, train loss: 0.3075413281245529, acc: 0.823250858292885; test loss: 0.6838706377070441, acc: 0.7159064051051761
epoch: 120, train loss: 0.2966737917637534, acc: 0.830590742275364; test loss: 0.6641321031231882, acc: 0.7326873079650201
epoch: 121, train loss: 0.30944947221864255, acc: 0.8214750799100272; test loss: 0.6638669871292169, acc: 0.7159064051051761
epoch: 122, train loss: 0.3118782439562955, acc: 0.8239019770332663; test loss: 0.6283645531833102, acc: 0.7293783975419522
epoch: 123, train loss: 0.3062816848818531, acc: 0.8220670060376465; test loss: 0.6526402750637368, acc: 0.7234696289293311
epoch: 124, train loss: 0.31226487375369816, acc: 0.8175091748549781; test loss: 0.6518225188064846, acc: 0.7244150319073505
epoch: 125, train loss: 0.3091018916806399, acc: 0.8259737184799337; test loss: 0.6661165952964248, acc: 0.7081068305365162
epoch: 126, train loss: 0.2963625159406995, acc: 0.8269799928968865; test loss: 0.6715917507826197, acc: 0.7154337036161664
epoch: 127, train loss: 0.28442987303409517, acc: 0.8318929797561264; test loss: 0.7006850478159454, acc: 0.7248877333963601
Epoch   127: reducing learning rate of group 0 to 7.5000e-04.
epoch: 128, train loss: 0.2416355512851905, acc: 0.8545045578311826; test loss: 0.6543141491050783, acc: 0.7442684944457575
epoch: 129, train loss: 0.2140663823983197, acc: 0.8664022729963301; test loss: 0.6605276071385722, acc: 0.7452138974237769
epoch: 130, train loss: 0.21065690761009334, acc: 0.8682964366047118; test loss: 0.6955852440924432, acc: 0.7376506735996219
epoch: 131, train loss: 0.21865095426073242, acc: 0.8647448798389961; test loss: 0.682094632522684, acc: 0.7367052706216024
epoch: 132, train loss: 0.21134490571890524, acc: 0.8676453178643305; test loss: 0.6625002365083003, acc: 0.7508863152918932
epoch: 133, train loss: 0.20734699099031698, acc: 0.8669350065111874; test loss: 0.7186710367358286, acc: 0.7284329945639328
epoch: 134, train loss: 0.20445862854706884, acc: 0.8684740144429975; test loss: 0.6915333542072077, acc: 0.737414322855117
epoch: 135, train loss: 0.2055275105041372, acc: 0.8701314076003315; test loss: 0.7249256842695943, acc: 0.7241786811628457
epoch: 136, train loss: 0.20608494848778533, acc: 0.8664022729963301; test loss: 0.7100165212613644, acc: 0.7393051288111557
epoch: 137, train loss: 0.20671696696667272, acc: 0.8704273706641411; test loss: 0.7326295859310481, acc: 0.74048688253368
epoch: 138, train loss: 0.20835777919073986, acc: 0.867290162187759; test loss: 0.70808031147069, acc: 0.7352871661545733
epoch: 139, train loss: 0.20437694957224706, acc: 0.8687699775068072; test loss: 0.7176127599165305, acc: 0.7390687780666509
epoch: 140, train loss: 0.198893130645888, acc: 0.8701906002130934; test loss: 0.7314609746452878, acc: 0.7402505317891751
epoch: 141, train loss: 0.20014347392164689, acc: 0.8701314076003315; test loss: 0.7686551886792725, acc: 0.7293783975419522
epoch: 142, train loss: 0.19559534008374252, acc: 0.8723807268852847; test loss: 0.7036330303213777, acc: 0.7456865989127865
epoch: 143, train loss: 0.2059809977509107, acc: 0.8691251331833787; test loss: 0.7326453631438704, acc: 0.7265421886078941
epoch: 144, train loss: 0.2042179339112961, acc: 0.8692435184089026; test loss: 0.7225832637673263, acc: 0.7333963601985346
epoch: 145, train loss: 0.19702897882290785, acc: 0.8706049485024269; test loss: 0.717553147805326, acc: 0.7364689198770976
epoch: 146, train loss: 0.19411854077242133, acc: 0.8732686160767136; test loss: 0.7227468335073458, acc: 0.7326873079650201
epoch: 147, train loss: 0.18395320601550533, acc: 0.8798389960932875; test loss: 0.7101103613715284, acc: 0.7428503899787284
epoch: 148, train loss: 0.19161589969731482, acc: 0.8757547058127145; test loss: 0.7088736195678098, acc: 0.7461593004017962
epoch: 149, train loss: 0.19260161214359162, acc: 0.8717296081449035; test loss: 0.7636201596941911, acc: 0.7319782557315055
epoch: 150, train loss: 0.201647081954939, acc: 0.8694210962471883; test loss: 0.7215124479396156, acc: 0.738832427322146
epoch: 151, train loss: 0.1889831092395175, acc: 0.874630046170238; test loss: 0.7404625314371266, acc: 0.7381233750886316
epoch: 152, train loss: 0.1841631406748155, acc: 0.8782999881614775; test loss: 0.7509239583228446, acc: 0.735759867643583
epoch: 153, train loss: 0.18374673102231395, acc: 0.8780040250976678; test loss: 0.77171803078621, acc: 0.7281966438194281
epoch: 154, train loss: 0.1918931165658272, acc: 0.8710192967917604; test loss: 0.7521046968927566, acc: 0.7359962183880879
epoch: 155, train loss: 0.1834803813746713, acc: 0.8796022256422399; test loss: 0.7643783202382076, acc: 0.7376506735996219
epoch: 156, train loss: 0.18626504933091753, acc: 0.8777672546466201; test loss: 0.7449207260264372, acc: 0.7274875915859135
epoch: 157, train loss: 0.1804990438488414, acc: 0.8789511069018586; test loss: 0.8021867329334429, acc: 0.7215788229732923
epoch: 158, train loss: 0.19174632392148935, acc: 0.8751035870723334; test loss: 0.8046399529339208, acc: 0.7163791065941858
epoch: 159, train loss: 0.17915652484862468, acc: 0.8779448324849058; test loss: 0.7515835191194286, acc: 0.7426140392342235
epoch: 160, train loss: 0.19751526410937859, acc: 0.8737421569788091; test loss: 0.7457674725685128, acc: 0.7289056960529425
epoch: 161, train loss: 0.17989092460359724, acc: 0.8790694921273825; test loss: 0.7287968510686577, acc: 0.7456865989127865
epoch: 162, train loss: 0.17891467844147196, acc: 0.881969930152717; test loss: 0.763621754607983, acc: 0.7416686362562042
epoch: 163, train loss: 0.17477593486007323, acc: 0.8836273233100509; test loss: 0.7790375820318595, acc: 0.7324509572205152
epoch: 164, train loss: 0.17278607259512765, acc: 0.8831537824079555; test loss: 0.7955310013355642, acc: 0.7317419049870008
epoch: 165, train loss: 0.16737134163162617, acc: 0.8871196874630046; test loss: 0.8036854947833298, acc: 0.7218151737177972
epoch: 166, train loss: 0.17682158923705146, acc: 0.8840416715993844; test loss: 0.7541801857345354, acc: 0.7331600094540298
epoch: 167, train loss: 0.1853738474580569, acc: 0.8793654551911921; test loss: 0.7210142462001684, acc: 0.7376506735996219
epoch: 168, train loss: 0.17821089533520715, acc: 0.8814371966378596; test loss: 0.7713605122373505, acc: 0.7322146064760104
epoch: 169, train loss: 0.16951358012261442, acc: 0.8856398721439565; test loss: 0.7752286765175884, acc: 0.7430867407232333
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.12320617858836026, acc: 0.8878299988161478; test loss: 0.7172677097690215, acc: 0.7307965020089813
epoch: 171, train loss: 0.12664609961718853, acc: 0.8858174499822422; test loss: 0.6779344032853807, acc: 0.737414322855117
epoch: 172, train loss: 0.11526532542095871, acc: 0.8913815555818634; test loss: 0.6665359006032852, acc: 0.7324509572205152
epoch: 173, train loss: 0.11519728099804483, acc: 0.8901977033266248; test loss: 0.6897686279515965, acc: 0.7348144646655637
epoch: 174, train loss: 0.12676197597722802, acc: 0.8856398721439565; test loss: 0.7328746326987948, acc: 0.7187426140392342
epoch: 175, train loss: 0.13650312259964398, acc: 0.879483840416716; test loss: 0.6540205575309164, acc: 0.7263058378633893
epoch: 176, train loss: 0.11858353016643788, acc: 0.8878891914289097; test loss: 0.6785931606941882, acc: 0.7376506735996219
epoch: 177, train loss: 0.12100275599919114, acc: 0.8870013022374807; test loss: 0.7099499170787508, acc: 0.720633419995273
epoch: 178, train loss: 0.11338022943553917, acc: 0.8855214869184326; test loss: 0.6919107177921909, acc: 0.7376506735996219
Epoch   178: reducing learning rate of group 0 to 3.7500e-04.
epoch: 179, train loss: 0.09881593437378712, acc: 0.899017402628152; test loss: 0.6756799183002292, acc: 0.7471047033798156
epoch: 180, train loss: 0.08908250610788108, acc: 0.9067716348999645; test loss: 0.6989902213402489, acc: 0.7395414795556606
epoch: 181, train loss: 0.08356139949049148, acc: 0.9130460518527288; test loss: 0.6904496240210122, acc: 0.7433230914677381
epoch: 182, train loss: 0.08722137404349735, acc: 0.9120989700485379; test loss: 0.7031603251977667, acc: 0.7437957929567478
epoch: 183, train loss: 0.07991188216208706, acc: 0.9149994080738724; test loss: 0.7205978699875509, acc: 0.7362325691325927
epoch: 184, train loss: 0.07997314967819258, acc: 0.9131644370782527; test loss: 0.7261370625289668, acc: 0.738832427322146
epoch: 185, train loss: 0.08587685219279348, acc: 0.9109743104060614; test loss: 0.7248705528893107, acc: 0.7447411959347672
epoch: 186, train loss: 0.0826068667334834, acc: 0.9134604001420623; test loss: 0.7192428339122177, acc: 0.7381233750886316
epoch: 187, train loss: 0.07930938988161945, acc: 0.9146442523973008; test loss: 0.7188018566889843, acc: 0.7385960765776413
epoch: 188, train loss: 0.0782475419641529, acc: 0.9142299041079673; test loss: 0.7373790524299475, acc: 0.7378870243441267
epoch: 189, train loss: 0.07887333796103918, acc: 0.9139931336569196; test loss: 0.7400241344991065, acc: 0.7376506735996219
epoch: 190, train loss: 0.07682176863013238, acc: 0.9163608381673967; test loss: 0.756335245016139, acc: 0.7324509572205152
epoch: 191, train loss: 0.07914808819431433, acc: 0.9133420149165384; test loss: 0.7699059376449513, acc: 0.7402505317891751
epoch: 192, train loss: 0.08151384261344698, acc: 0.9131052444654907; test loss: 0.7355924156421579, acc: 0.7355235168990782
epoch: 193, train loss: 0.077224587302554, acc: 0.9135787853675861; test loss: 0.7575954157137977, acc: 0.7348144646655637
epoch: 194, train loss: 0.07753252998087919, acc: 0.916064875103587; test loss: 0.7457965079900137, acc: 0.7383597258331364
epoch: 195, train loss: 0.07986185636459137, acc: 0.9113886586953948; test loss: 0.7416115487841843, acc: 0.7300874497754668
epoch: 196, train loss: 0.0774984078414979, acc: 0.9141707114952053; test loss: 0.7278952299914612, acc: 0.7409595840226897
epoch: 197, train loss: 0.07920127281218121, acc: 0.9138155558186338; test loss: 0.7444607392745896, acc: 0.7350508154100686
epoch: 198, train loss: 0.07757358032139686, acc: 0.9135787853675861; test loss: 0.7621571459237128, acc: 0.7385960765776413
epoch: 199, train loss: 0.0796730802736791, acc: 0.912809281401681; test loss: 0.764326545539068, acc: 0.7423776884897187
epoch: 200, train loss: 0.0806430663401465, acc: 0.9097312655380608; test loss: 0.751936189448177, acc: 0.7437957929567478
best test acc 0.7508863152918932 at epoch 132.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9251    0.9577    0.9411      6100
           1     0.9697    0.8974    0.9321       926
           2     0.8481    0.9213    0.8832      2400
           3     0.9024    0.8992    0.9008       843
           4     0.9062    0.9483    0.9268       774
           5     0.9223    0.9656    0.9435      1512
           6     0.7343    0.7917    0.7619      1330
           7     0.9365    0.7672    0.8434       481
           8     0.8949    0.8734    0.8840       458
           9     0.9267    0.9513    0.9389       452
          10     0.9577    0.8201    0.8835       717
          11     0.9280    0.7357    0.8208       333
          12     0.6667    0.0201    0.0390       299
          13     0.8203    0.7807    0.8000       269

    accuracy                         0.8960     16894
   macro avg     0.8813    0.8093    0.8214     16894
weighted avg     0.8941    0.8960    0.8884     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8146    0.8643    0.8387      1525
           1     0.8411    0.7759    0.8072       232
           2     0.7051    0.7637    0.7332       601
           3     0.7938    0.7299    0.7605       211
           4     0.8424    0.7990    0.8201       194
           5     0.7843    0.8466    0.8142       378
           6     0.4922    0.5706    0.5285       333
           7     0.7609    0.5785    0.6573       121
           8     0.5424    0.5565    0.5494       115
           9     0.7436    0.7632    0.7532       114
          10     0.8296    0.6222    0.7111       180
          11     0.5484    0.4048    0.4658        84
          12     0.0000    0.0000    0.0000        75
          13     0.6939    0.5000    0.5812        68

    accuracy                         0.7509      4231
   macro avg     0.6709    0.6268    0.6443      4231
weighted avg     0.7408    0.7509    0.7434      4231

---------------------------------------
program finished.
