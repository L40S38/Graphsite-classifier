seed:  666
save trained model at:  ../trained_models/trained_classifier_model_15.pt
save loss at:  ./results/train_classifier_results_15.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  144
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): PNAEmbeddingNet(
    (convs): ModuleList(
      (0): PNAConv(11, 80, towers=4, edge_dim=1)
      (1): PNAConv(80, 80, towers=4, edge_dim=1)
      (2): PNAConv(80, 80, towers=4, edge_dim=1)
      (3): PNAConv(80, 80, towers=4, edge_dim=1)
    )
    (batch_norms): ModuleList(
      (0): BatchNorm(80)
      (1): BatchNorm(80)
      (2): BatchNorm(80)
      (3): BatchNorm(80)
    )
    (set2set): Set2Set(80, 160)
  )
  (fc1): Linear(in_features=160, out_features=80, bias=True)
  (fc2): Linear(in_features=80, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2aea4754f5b0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.231488355826624, acc: 0.3495860690836426, val loss: 1.9769237505330193, acc: 0.3987189751801441, test loss: 1.9885726550224603, acc: 0.38997879109225875
epoch: 2, train loss: 2.0102072128118396, acc: 0.38892377961747077, val loss: 1.921997396921138, acc: 0.40298905791299705, test loss: 1.9158280884347312, acc: 0.40853658536585363
epoch: 3, train loss: 1.9353359748389494, acc: 0.4105623751070511, val loss: 1.8527464578397947, acc: 0.42727515345609823, test loss: 1.8487725932929322, acc: 0.4395546129374337
epoch: 4, train loss: 1.8471866263003816, acc: 0.43379960034256354, val loss: 1.9853680544227863, acc: 0.418468107819589, test loss: 1.9976419871584103, acc: 0.4159597030752916
epoch: 5, train loss: 1.7812189550968773, acc: 0.4536111904082215, val loss: 1.978607562563723, acc: 0.3907125700560448, test loss: 1.9717557364531564, acc: 0.40164369034994696
epoch: 6, train loss: 1.7287053491196018, acc: 0.4717670568084499, val loss: 1.7562447441779108, acc: 0.47104350146784096, test loss: 1.7373738106841878, acc: 0.47879109225874866
epoch: 7, train loss: 1.6814238052588681, acc: 0.48718241507279475, val loss: 1.6566303316548503, acc: 0.477982385908727, test loss: 1.648022469382018, acc: 0.493372216330859
epoch: 8, train loss: 1.6435690411424215, acc: 0.5056237510705109, val loss: 1.8116442849867052, acc: 0.4526287696824126, test loss: 1.804360420807548, acc: 0.4559915164369035
epoch: 9, train loss: 1.6111924966199038, acc: 0.5104196403083071, val loss: 1.555607736158409, acc: 0.5201494528956498, test loss: 1.5414386372045394, acc: 0.5357900318133616
epoch: 10, train loss: 1.5777645038939052, acc: 0.519269197830431, val loss: 1.5691757580105832, acc: 0.5206832132372565, test loss: 1.5675666280414746, acc: 0.5283669141039237
epoch: 11, train loss: 1.5472033340455191, acc: 0.5318298601198972, val loss: 1.508916268069998, acc: 0.5348278622898318, test loss: 1.5063923548033988, acc: 0.5365853658536586
epoch: 12, train loss: 1.5171755661326003, acc: 0.5419354838709678, val loss: 1.4903511218398546, acc: 0.5431011475847345, test loss: 1.4710251369759488, acc: 0.545068928950159
epoch: 13, train loss: 1.5002470597042004, acc: 0.5523836711390236, val loss: 1.5419577187399944, acc: 0.5270883373365359, test loss: 1.5353072786887403, acc: 0.528101802757158
epoch: 14, train loss: 1.4739672653817328, acc: 0.5547816157579217, val loss: 1.46906510537104, acc: 0.5561782759540966, test loss: 1.4495660211981796, acc: 0.5625662778366914
epoch: 15, train loss: 1.44030269998637, acc: 0.5680274050813588, val loss: 1.8722191927812881, acc: 0.489191353082466, test loss: 1.842912627050252, acc: 0.4941675503711559
epoch: 16, train loss: 1.4127903125982368, acc: 0.580188409934342, val loss: 1.558484538828878, acc: 0.5345609821190286, test loss: 1.5331657226113473, acc: 0.5432131495227995
epoch: 17, train loss: 1.378524390139513, acc: 0.5924636026263203, val loss: 1.713625332181028, acc: 0.4843875100080064, test loss: 1.680706558905123, acc: 0.49390243902439024
epoch: 18, train loss: 1.376578418289019, acc: 0.5885812161004853, val loss: 1.5052208320153437, acc: 0.5519081932212436, test loss: 1.4839765919354662, acc: 0.546659597030753
epoch: 19, train loss: 1.3539480318180535, acc: 0.6003425635169855, val loss: 1.4798586924043629, acc: 0.5716573258606885, test loss: 1.48083635858109, acc: 0.5689289501590669
epoch: 20, train loss: 1.334866628087387, acc: 0.605081358835284, val loss: 1.5631966153748804, acc: 0.5545769949292767, test loss: 1.54626092381088, acc: 0.5556733828207847
epoch: 21, train loss: 1.3030034408887863, acc: 0.6118755352554953, val loss: 1.4752429075676312, acc: 0.5631171603949826, test loss: 1.437190661880396, acc: 0.5652173913043478
epoch: 22, train loss: 1.3012885501580072, acc: 0.6138167285184127, val loss: 1.3134216195206532, acc: 0.6079530290899386, test loss: 1.3059629961642836, acc: 0.6047189819724285
epoch: 23, train loss: 1.268667262532253, acc: 0.6248358549814444, val loss: 1.3151767243376915, acc: 0.6111555911395783, test loss: 1.313045213798956, acc: 0.6158536585365854
epoch: 24, train loss: 1.2740030756617422, acc: 0.6260348272908935, val loss: 1.561812533293847, acc: 0.5070723245262877, test loss: 1.5301503882301954, acc: 0.5082184517497349
epoch 25, gamma increased to 1.
epoch: 25, train loss: 1.0243212876645218, acc: 0.6345989152155296, val loss: 1.2868004873144425, acc: 0.5580464371497198, test loss: 1.290685986979516, acc: 0.559915164369035
epoch: 26, train loss: 1.0176334019426003, acc: 0.6310019982871824, val loss: 1.1446573877449127, acc: 0.5916733386709367, test loss: 1.1271520847986005, acc: 0.6002120890774125
epoch: 27, train loss: 0.9965492595239194, acc: 0.6415643733942336, val loss: 1.1351316902902815, acc: 0.6031491860154791, test loss: 1.1225680473753403, acc: 0.6193001060445387
epoch: 28, train loss: 0.991016055977484, acc: 0.6437910362546388, val loss: 1.2029193194032766, acc: 0.5775286896183613, test loss: 1.1643293887408388, acc: 0.5858960763520679
epoch: 29, train loss: 0.9949603160585365, acc: 0.6417927490722238, val loss: 1.0903376437264314, acc: 0.6127568721643982, test loss: 1.077578070876707, acc: 0.6084305408271474
epoch: 30, train loss: 0.9602022652111495, acc: 0.6507564944333428, val loss: 1.1388461007606134, acc: 0.5948759007205765, test loss: 1.0924841302070738, acc: 0.6060445387062566
epoch: 31, train loss: 0.9624756665152208, acc: 0.6483014558949471, val loss: 1.186653888063301, acc: 0.5644515612489992, test loss: 1.136530147377532, acc: 0.584305408271474
epoch: 32, train loss: 0.9528929791040772, acc: 0.6528118755352555, val loss: 1.3545918187873662, acc: 0.5681878836402455, test loss: 1.3812772264725837, acc: 0.5580593849416755
epoch: 33, train loss: 0.9387920553739364, acc: 0.6608621181844133, val loss: 1.1963022245799568, acc: 0.562583400053376, test loss: 1.1510532193067597, acc: 0.573170731707317
epoch: 34, train loss: 0.9282942424663094, acc: 0.6604624607479304, val loss: 1.0702207594322528, acc: 0.6180944755804644, test loss: 1.0419802932445976, acc: 0.6312301166489925
epoch: 35, train loss: 0.9248210626467956, acc: 0.659834427633457, val loss: 1.2493572328642333, acc: 0.5785962103015746, test loss: 1.2173032548243017, acc: 0.5925238600212089
epoch: 36, train loss: 0.9008117342655434, acc: 0.6665144162146731, val loss: 1.3308077627797428, acc: 0.5286896183613558, test loss: 1.3288515319500596, acc: 0.5214740190880169
epoch: 37, train loss: 0.9045345436597667, acc: 0.6669711675706538, val loss: 1.1653959436164654, acc: 0.5906058179877235, test loss: 1.164088707207875, acc: 0.5824496288441146
epoch: 38, train loss: 0.9040312820695109, acc: 0.6673137310876391, val loss: 1.177619043877833, acc: 0.5855350947424607, test loss: 1.1800479277699776, acc: 0.5840402969247084
epoch: 39, train loss: 0.8835527682964577, acc: 0.6720525264059378, val loss: 1.0958537417855427, acc: 0.6087536696023486, test loss: 1.0792801437044195, acc: 0.623541887592789
epoch: 40, train loss: 0.8812886413900368, acc: 0.6743933771053383, val loss: 1.7055833108717198, acc: 0.40459033893781693, test loss: 1.6956868097107078, acc: 0.41410392364793214
epoch: 41, train loss: 0.8613698032866741, acc: 0.683128746788467, val loss: 1.1768071874797201, acc: 0.6020816653322658, test loss: 1.1108049273111662, acc: 0.6174443266171792
epoch: 42, train loss: 0.8705708524277643, acc: 0.6778190122751927, val loss: 1.239520061101982, acc: 0.5852682145716573, test loss: 1.2273412651499913, acc: 0.5861611876988335
epoch: 43, train loss: 0.8480334639821501, acc: 0.6825007136739937, val loss: 1.1482490594144628, acc: 0.6130237523352015, test loss: 1.1501238434590313, acc: 0.6121420996818664
epoch: 44, train loss: 0.8442578801351923, acc: 0.6819868683985155, val loss: 1.1630429113455254, acc: 0.6023485455030692, test loss: 1.1630964451099124, acc: 0.6110816542948038
epoch: 45, train loss: 0.8357974606686036, acc: 0.6890665144162147, val loss: 1.1901233510268603, acc: 0.5860688550840673, test loss: 1.1737090555253742, acc: 0.5935843054082715
Epoch    45: reducing learning rate of group 0 to 1.5000e-03.
epoch: 46, train loss: 0.7483452951128811, acc: 0.7128746788467029, val loss: 0.9103172850475204, acc: 0.6733386709367494, test loss: 0.9055367344763221, acc: 0.6686108165429481
epoch: 47, train loss: 0.7221433678559498, acc: 0.7221238938053097, val loss: 1.188903440371621, acc: 0.6135575126768081, test loss: 1.1879546122738058, acc: 0.6277836691410392
epoch: 48, train loss: 0.718529626753478, acc: 0.7229803025977733, val loss: 1.077483957724346, acc: 0.6119562316519882, test loss: 1.0612630792286084, acc: 0.616914103923648
epoch: 49, train loss: 0.7040310989541052, acc: 0.7284042249500429, val loss: 1.0308874305292928, acc: 0.6621297037630104, test loss: 1.0281014054666016, acc: 0.6580063626723224
epoch: 50, train loss: 0.686934660112657, acc: 0.7312589209249215, val loss: 1.0579478330283856, acc: 0.6253002401921537, test loss: 1.0452985798945988, acc: 0.6280487804878049
epoch: 51, train loss: 0.671058243579467, acc: 0.7425064230659435, val loss: 1.2685682242922633, acc: 0.5874032559380838, test loss: 1.2311754210250627, acc: 0.5996818663838812
epoch: 52, train loss: 0.6724587061151314, acc: 0.7397088210105623, val loss: 1.0090778072515996, acc: 0.6458500133440085, test loss: 0.9765962822439814, acc: 0.6492576882290562
epoch: 53, train loss: 0.6697048780886132, acc: 0.7411932629174993, val loss: 0.9763470062549062, acc: 0.6613290632506005, test loss: 0.956141605104141, acc: 0.658271474019088
epoch: 54, train loss: 0.6554392491821694, acc: 0.7439337710533828, val loss: 1.1283652538104092, acc: 0.6325060048038431, test loss: 1.1343002979854004, acc: 0.630965005302227
epoch: 55, train loss: 0.6506481814581838, acc: 0.7445047102483585, val loss: 0.9755182733909906, acc: 0.6658660261542567, test loss: 0.952061379259237, acc: 0.6704665959703076
epoch: 56, train loss: 0.6424300286656205, acc: 0.7491864116471596, val loss: 0.9503102665237659, acc: 0.6621297037630104, test loss: 0.9305554442542229, acc: 0.6794803817603393
epoch: 57, train loss: 0.6369652762094499, acc: 0.7467884670282615, val loss: 0.9436298119153281, acc: 0.6663997864958634, test loss: 0.9442290307861118, acc: 0.6606574761399788
epoch: 58, train loss: 0.6279296282288553, acc: 0.7503853839566086, val loss: 0.9612940375188335, acc: 0.6736055511075527, test loss: 0.9594403889626739, acc: 0.6784199363732768
epoch: 59, train loss: 0.6241447919578237, acc: 0.7530687981729945, val loss: 1.0111944368116563, acc: 0.6573258606885508, test loss: 0.9758455635760521, acc: 0.661983032873807
epoch: 60, train loss: 0.618444470524822, acc: 0.754610333999429, val loss: 1.2198262627932241, acc: 0.5991459834534294, test loss: 1.1819083019379972, acc: 0.6063096500530223
epoch: 61, train loss: 0.6137384102884239, acc: 0.7553525549528975, val loss: 1.0101918852548775, acc: 0.6717373899119295, test loss: 0.9830025660776763, acc: 0.6736479321314952
epoch: 62, train loss: 0.5840477071964091, acc: 0.7677419354838709, val loss: 1.0792954128393657, acc: 0.6511876167600748, test loss: 1.0454136867664778, acc: 0.6574761399787911
epoch: 63, train loss: 0.5871879032976519, acc: 0.7665429631744219, val loss: 1.1758381331606422, acc: 0.6319722444622364, test loss: 1.1254669486022577, acc: 0.6447507953340403
epoch: 64, train loss: 0.5963574805090913, acc: 0.7590636597202398, val loss: 1.2978708573681914, acc: 0.6204963971176941, test loss: 1.2887338636283026, acc: 0.623541887592789
epoch: 65, train loss: 0.5860953550440157, acc: 0.7648872395089923, val loss: 1.131740751029779, acc: 0.6279690419001868, test loss: 1.128257047839281, acc: 0.6230116648992576
epoch: 66, train loss: 0.5761691111208812, acc: 0.7691692834713103, val loss: 1.1609316939253917, acc: 0.622097678142514, test loss: 1.1544444090741057, acc: 0.6134676564156946
epoch: 67, train loss: 0.5952497196034163, acc: 0.7635169854410505, val loss: 0.9896377007230174, acc: 0.6722711502535361, test loss: 1.0141643222543872, acc: 0.6643690349946978
epoch: 68, train loss: 0.5732379512551373, acc: 0.7682557807593491, val loss: 1.1881355051043703, acc: 0.6191619962636776, test loss: 1.193750771586235, acc: 0.6158536585365854
epoch: 69, train loss: 0.551304571086803, acc: 0.7763631173280046, val loss: 1.0971533106364089, acc: 0.6495863357352548, test loss: 1.0917414468520519, acc: 0.6553552492046659
epoch: 70, train loss: 0.5635775745544576, acc: 0.77082500713674, val loss: 1.011325508904896, acc: 0.6655991459834534, test loss: 1.0154188296195685, acc: 0.6550901378579003
epoch: 71, train loss: 0.5471812851116584, acc: 0.7797316585783614, val loss: 1.2985539062200688, acc: 0.6138243928476115, test loss: 1.2600471816664662, acc: 0.6203605514316013
epoch: 72, train loss: 0.5505657476865663, acc: 0.7795603768198687, val loss: 1.0917755952350991, acc: 0.6562583400053376, test loss: 1.09916132609513, acc: 0.6598621420996819
epoch: 73, train loss: 0.5457323432240934, acc: 0.7769911504424779, val loss: 1.0544808620066524, acc: 0.6663997864958634, test loss: 1.0504183459825505, acc: 0.6662248144220573
epoch: 74, train loss: 0.5389118915893267, acc: 0.7804167856123323, val loss: 1.0505498264960236, acc: 0.6541232986389112, test loss: 1.0168475161680868, acc: 0.6635737009544008
epoch: 75, train loss: 0.5329913326954113, acc: 0.779788752497859, val loss: 1.0940633548365486, acc: 0.6503869762476648, test loss: 1.077609559511203, acc: 0.6569459172852599
epoch: 76, train loss: 0.535075308083467, acc: 0.7830431059092207, val loss: 1.0184795875564587, acc: 0.6728049105951428, test loss: 0.9884205253859972, acc: 0.6731177094379639
epoch: 77, train loss: 0.5161003363734818, acc: 0.7868683985155581, val loss: 1.1213290300056207, acc: 0.6351748065118762, test loss: 1.1105728062187754, acc: 0.6447507953340403
epoch: 78, train loss: 0.5443695688771752, acc: 0.779788752497859, val loss: 1.0110209200074904, acc: 0.66693354683747, test loss: 0.9954948694556793, acc: 0.6834570519618239
epoch: 79, train loss: 0.5124811835453029, acc: 0.7905795032829004, val loss: 1.1298234961909042, acc: 0.661862823592207, test loss: 1.1077280499649453, acc: 0.6680805938494168
epoch: 80, train loss: 0.5192217581482435, acc: 0.7881244647445047, val loss: 1.0746542628618887, acc: 0.6626634641046171, test loss: 1.0692622143289316, acc: 0.6569459172852599
epoch: 81, train loss: 0.5099887274212065, acc: 0.787953182986012, val loss: 1.0836603922114743, acc: 0.6434480918067788, test loss: 1.0657252303363156, acc: 0.6444856839872747
epoch: 82, train loss: 0.49872653040585097, acc: 0.7921210391093348, val loss: 1.1786039032107645, acc: 0.6378436082199093, test loss: 1.1817462674380612, acc: 0.6306998939554613
epoch: 83, train loss: 0.4998888231058853, acc: 0.7926919783043106, val loss: 1.160714501853558, acc: 0.6498532159060582, test loss: 1.1528931149495873, acc: 0.6572110286320254
epoch: 84, train loss: 0.4876417323347163, acc: 0.7960605195546674, val loss: 1.2556003260555222, acc: 0.6365092073658927, test loss: 1.2404532466688187, acc: 0.6344114528101803
epoch: 85, train loss: 0.4863625477628302, acc: 0.8003425635169854, val loss: 1.0799271529726833, acc: 0.6626634641046171, test loss: 1.046030782186846, acc: 0.6680805938494168
epoch: 86, train loss: 0.47713680264611535, acc: 0.8009135027119612, val loss: 1.1833018099813102, acc: 0.6439818521483853, test loss: 1.1795187153214488, acc: 0.6466065747613998
epoch: 87, train loss: 0.4895643522645486, acc: 0.7968027405081359, val loss: 1.02863096436851, acc: 0.6672004270082733, test loss: 1.0048806773516432, acc: 0.6853128313891834
epoch: 88, train loss: 0.4940826820639791, acc: 0.7950328290037111, val loss: 1.0763312180200704, acc: 0.6698692287163064, test loss: 1.0516082829660474, acc: 0.6786850477200425
epoch: 89, train loss: 0.47208506818366264, acc: 0.8047958892377962, val loss: 1.0474371419514152, acc: 0.6770749933279957, test loss: 1.0463319959974238, acc: 0.6794803817603393
epoch: 90, train loss: 0.4730945644091444, acc: 0.7993148729660291, val loss: 1.1990067365171435, acc: 0.6450493728315986, test loss: 1.1642967438520835, acc: 0.6492576882290562
epoch: 91, train loss: 0.47294725773847, acc: 0.8007993148729661, val loss: 1.1034476773275004, acc: 0.6621297037630104, test loss: 1.1216159437013709, acc: 0.6614528101802757
epoch: 92, train loss: 0.48185686458698723, acc: 0.8025121324578932, val loss: 1.220887208958451, acc: 0.6530557779556979, test loss: 1.2254256087168656, acc: 0.6580063626723224
epoch: 93, train loss: 0.476342994238423, acc: 0.8021695689409077, val loss: 1.2086072350808197, acc: 0.6421136909527622, test loss: 1.2171808881951793, acc: 0.6423647932131495
epoch: 94, train loss: 0.4512604429420321, acc: 0.8087924636026264, val loss: 1.155728284982417, acc: 0.6610621830797971, test loss: 1.1142947672533459, acc: 0.6654294803817603
epoch: 95, train loss: 0.46053340309114754, acc: 0.8058235797887525, val loss: 1.0413780597995241, acc: 0.6776087536696024, test loss: 1.0089290202106422, acc: 0.6789501590668081
epoch: 96, train loss: 0.4626675496348442, acc: 0.806965458178704, val loss: 1.1002083658408508, acc: 0.6642647451294369, test loss: 1.099526209012069, acc: 0.6672852598091198
epoch: 97, train loss: 0.44944980643219584, acc: 0.8113616899800171, val loss: 1.3392405933719143, acc: 0.6407792900987457, test loss: 1.3435532158025376, acc: 0.6484623541887593
epoch: 98, train loss: 0.437481937051466, acc: 0.8122751926919783, val loss: 1.124712201287214, acc: 0.6610621830797971, test loss: 1.1066271851702434, acc: 0.6654294803817603
epoch: 99, train loss: 0.4585839493493165, acc: 0.8062803311447331, val loss: 1.2488114999903976, acc: 0.6423805711235655, test loss: 1.2380172499669824, acc: 0.6572110286320254
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3399545055438953, acc: 0.8147873251498715, val loss: 0.9222042913719404, acc: 0.6762743528155858, test loss: 0.9071240737354768, acc: 0.6810710498409331
epoch: 101, train loss: 0.33005511741994825, acc: 0.816328860976306, val loss: 0.9930996478701516, acc: 0.6570589805177476, test loss: 0.9793718017930064, acc: 0.6715270413573701
epoch: 102, train loss: 0.3357063430694046, acc: 0.8097630602340851, val loss: 1.214545273084083, acc: 0.6271684013877769, test loss: 1.2282019506582906, acc: 0.63016967126193
epoch: 103, train loss: 0.3399046149435569, acc: 0.8131316014844419, val loss: 0.8822995372730985, acc: 0.6864157993061115, test loss: 0.8731617179174818, acc: 0.6874337221633086
epoch: 104, train loss: 0.3208872712818922, acc: 0.816728518412789, val loss: 0.9810548300166623, acc: 0.6623965839338137, test loss: 0.9576291625219084, acc: 0.6648992576882291
epoch: 105, train loss: 0.3278793046704504, acc: 0.8141592920353983, val loss: 0.9406120313273324, acc: 0.6728049105951428, test loss: 0.9126528122265797, acc: 0.6643690349946978
epoch: 106, train loss: 0.32421942626854844, acc: 0.8165001427347988, val loss: 1.2537414322288825, acc: 0.6020816653322658, test loss: 1.2634119097526606, acc: 0.5999469777306469
epoch: 107, train loss: 0.3368047534058239, acc: 0.8088495575221238, val loss: 0.9401918022607784, acc: 0.6725380304243395, test loss: 0.9276307933783101, acc: 0.6736479321314952
epoch: 108, train loss: 0.3232181567618958, acc: 0.8176420211247503, val loss: 0.8871493702224964, acc: 0.6816119562316519, test loss: 0.8701453606498329, acc: 0.693796394485684
epoch: 109, train loss: 0.32235421449533025, acc: 0.8180987724807308, val loss: 1.0161972669623582, acc: 0.6642647451294369, test loss: 1.009748853087299, acc: 0.6712619300106044
epoch: 110, train loss: 0.3199976984820909, acc: 0.8159862974593206, val loss: 1.1968144341599378, acc: 0.5900720576461169, test loss: 1.1853472206777125, acc: 0.5845705196182397
epoch: 111, train loss: 0.3187098556694017, acc: 0.8165572366542964, val loss: 1.1958915240484014, acc: 0.6373098478783027, test loss: 1.1754081510915988, acc: 0.6314952279957582
epoch: 112, train loss: 0.3379756213034217, acc: 0.8122751926919783, val loss: 0.9191175284053537, acc: 0.6736055511075527, test loss: 0.8920871358155446, acc: 0.676829268292683
epoch: 113, train loss: 0.2947914235836228, acc: 0.822894661718527, val loss: 0.984584942257242, acc: 0.6655991459834534, test loss: 0.9834126277035645, acc: 0.6709968186638389
epoch: 114, train loss: 0.31427695550919943, acc: 0.8169568940907793, val loss: 1.1859905725960926, acc: 0.6242327195089404, test loss: 1.2125262729942103, acc: 0.6240721102863203
Epoch   114: reducing learning rate of group 0 to 7.5000e-04.
epoch: 115, train loss: 0.23800361574019427, acc: 0.8492149586069083, val loss: 0.9611016157820665, acc: 0.6810781958900454, test loss: 0.9227287434318032, acc: 0.6924708377518558
epoch: 116, train loss: 0.2187940954770696, acc: 0.8597202397944619, val loss: 1.015333970515035, acc: 0.6952228449426208, test loss: 1.0220730950698367, acc: 0.6964475079533404
epoch: 117, train loss: 0.205755683766955, acc: 0.864401941193263, val loss: 1.0525822396083675, acc: 0.685882038964505, test loss: 1.0248050111980984, acc: 0.6959172852598091
epoch: 118, train loss: 0.20334919272424015, acc: 0.8663431344561804, val loss: 1.0823942265623183, acc: 0.6792100346944222, test loss: 1.0897608777055812, acc: 0.6773594909862142
epoch: 119, train loss: 0.20933781899011988, acc: 0.8630316871253212, val loss: 1.0386243694968182, acc: 0.6816119562316519, test loss: 1.039066131142771, acc: 0.6823966065747614
epoch: 120, train loss: 0.19394828864771266, acc: 0.8730231230373965, val loss: 1.2204398604943525, acc: 0.6565252201761409, test loss: 1.1767480497269212, acc: 0.6776246023329798
epoch: 121, train loss: 0.20015551212631633, acc: 0.8666286040536683, val loss: 1.0551703264467043, acc: 0.6770749933279957, test loss: 1.03768079253042, acc: 0.6869034994697774
epoch: 122, train loss: 0.20216903570478997, acc: 0.8658292891807022, val loss: 1.0497146141443183, acc: 0.6805444355484388, test loss: 1.0522952119593656, acc: 0.6816012725344645
epoch: 123, train loss: 0.20451007592926493, acc: 0.8656580074222096, val loss: 1.0380437333074162, acc: 0.6842807579396851, test loss: 1.0356404537992792, acc: 0.6882290562036055
epoch: 124, train loss: 0.2056464244255569, acc: 0.8639451898372823, val loss: 1.0106031801148927, acc: 0.6714705097411262, test loss: 1.016374169705655, acc: 0.6770943796394485
epoch: 125, train loss: 0.19137772381339727, acc: 0.8725092777619183, val loss: 1.1228997881074636, acc: 0.6575927408593542, test loss: 1.0834253050489617, acc: 0.6537645811240721
epoch: 126, train loss: 0.18193140503358338, acc: 0.873365686554382, val loss: 1.1330799647003675, acc: 0.6626634641046171, test loss: 1.1002249448448578, acc: 0.6651643690349947
epoch: 127, train loss: 0.20403551626062516, acc: 0.8655438195832144, val loss: 1.1194474989171024, acc: 0.6639978649586336, test loss: 1.101417718105579, acc: 0.6675503711558854
epoch: 128, train loss: 0.18954999045903023, acc: 0.8731373108763917, val loss: 1.206429697972856, acc: 0.663464104617027, test loss: 1.1220451003041272, acc: 0.6638388123011665
epoch: 129, train loss: 0.17970169004443165, acc: 0.8799314872966029, val loss: 1.1264130282726548, acc: 0.6837469975980784, test loss: 1.0733579122881116, acc: 0.6879639448568399
epoch: 130, train loss: 0.1805288979284361, acc: 0.8770767913217242, val loss: 1.0725627632690868, acc: 0.6818788364024553, test loss: 1.0287213698424564, acc: 0.6874337221633086
epoch: 131, train loss: 0.18575345922326347, acc: 0.8736511561518698, val loss: 1.1135438680410195, acc: 0.680811315719242, test loss: 1.1119817660949642, acc: 0.6762990455991517
epoch: 132, train loss: 0.24117587803433085, acc: 0.8522980302597774, val loss: 0.9899099727598928, acc: 0.6573258606885508, test loss: 1.006811378426921, acc: 0.6633085896076352
epoch: 133, train loss: 0.21845242191375136, acc: 0.8620039965743649, val loss: 1.2081416995168401, acc: 0.6437149719775821, test loss: 1.1792373070661235, acc: 0.6484623541887593
epoch: 134, train loss: 0.18921224823976088, acc: 0.8734227804738796, val loss: 1.0274834106023834, acc: 0.6938884440886042, test loss: 1.0030828485559684, acc: 0.6985683987274656
epoch: 135, train loss: 0.1633809447416128, acc: 0.8836996859834427, val loss: 1.1536812494047362, acc: 0.6722711502535361, test loss: 1.1299208175086772, acc: 0.676033934252386
epoch: 136, train loss: 0.16801446921530022, acc: 0.8829574650299743, val loss: 1.1048803294153573, acc: 0.6789431545236189, test loss: 1.1023578526232933, acc: 0.6800106044538706
epoch: 137, train loss: 0.1751811007746553, acc: 0.8775906365972024, val loss: 1.0506530459925496, acc: 0.6952228449426208, test loss: 1.0438039865372417, acc: 0.6998939554612937
epoch: 138, train loss: 0.17587070107766298, acc: 0.8760491007707679, val loss: 1.272900637385366, acc: 0.6437149719775821, test loss: 1.2472239079177063, acc: 0.648727465535525
epoch: 139, train loss: 0.1626636512171293, acc: 0.8823865258349985, val loss: 1.1190433935512056, acc: 0.6810781958900454, test loss: 1.1128476390403756, acc: 0.6871686108165429
epoch: 140, train loss: 0.17833260455677383, acc: 0.8732514987153868, val loss: 1.2020186915027322, acc: 0.6653322658126501, test loss: 1.1885516355298105, acc: 0.6757688229056203
epoch: 141, train loss: 0.17307907409695875, acc: 0.8777619183556952, val loss: 1.2688301381728857, acc: 0.6519882572724847, test loss: 1.2528294797666757, acc: 0.6585365853658537
epoch: 142, train loss: 0.18752056975518502, acc: 0.8722809020839281, val loss: 1.2452251961748537, acc: 0.6639978649586336, test loss: 1.224781576295167, acc: 0.6747083775185578
epoch: 143, train loss: 0.15335841665446945, acc: 0.8872395089922923, val loss: 1.0912865849854567, acc: 0.7013610888710968, test loss: 1.0891213224902512, acc: 0.6990986214209968
epoch: 144, train loss: 0.16863339473920108, acc: 0.8772480730802169, val loss: 1.2297796733480535, acc: 0.6370429677074994, test loss: 1.1874050043547015, acc: 0.6426299045599152
epoch: 145, train loss: 0.16298367548186268, acc: 0.8853554096488724, val loss: 1.3307230796596352, acc: 0.6231651988257273, test loss: 1.3278444049719915, acc: 0.6219512195121951
epoch: 146, train loss: 0.16732505119905247, acc: 0.8789608906651442, val loss: 1.104824570487078, acc: 0.6984254069922605, test loss: 1.066563662516856, acc: 0.6990986214209968
epoch: 147, train loss: 0.17034964388756965, acc: 0.8795318298601199, val loss: 1.2297911449276608, acc: 0.6533226581265013, test loss: 1.1598867439642184, acc: 0.6524390243902439
epoch: 148, train loss: 0.17014265065699552, acc: 0.8775335426777048, val loss: 1.2054120708790848, acc: 0.6829463570856685, test loss: 1.155599451772987, acc: 0.6802757158006363
epoch: 149, train loss: 0.1566434854103979, acc: 0.8852983157293748, val loss: 1.1256937071072568, acc: 0.6970910061382439, test loss: 1.0759136932146511, acc: 0.7017497348886532
epoch: 150, train loss: 0.16176027483151295, acc: 0.8848986582928918, val loss: 1.1169231516347111, acc: 0.680811315719242, test loss: 1.1153246115020072, acc: 0.6890243902439024
epoch: 151, train loss: 0.172693822293189, acc: 0.8773051669997145, val loss: 1.1258623374758767, acc: 0.6589271417133707, test loss: 1.1249278600496553, acc: 0.665694591728526
epoch: 152, train loss: 0.15838606126336346, acc: 0.8829574650299743, val loss: 1.3776376720807189, acc: 0.6415799306111556, test loss: 1.3821375212639033, acc: 0.6365323435843054
epoch: 153, train loss: 0.18903641650209146, acc: 0.8721667142449329, val loss: 1.1892210571550197, acc: 0.6605284227381906, test loss: 1.2070552138483284, acc: 0.6564156945917285
epoch: 154, train loss: 0.15205654794238072, acc: 0.8895232657721953, val loss: 1.1478952781213008, acc: 0.6786762743528156, test loss: 1.144249798257935, acc: 0.6813361611876988
epoch: 155, train loss: 0.15951151199555894, acc: 0.8847844704538966, val loss: 1.1880027113961067, acc: 0.6744061916199626, test loss: 1.1479813732751987, acc: 0.6694061505832449
epoch: 156, train loss: 0.1478802552529141, acc: 0.8899800171281759, val loss: 1.1064844776669724, acc: 0.7010942087002936, test loss: 1.1089020913125096, acc: 0.6988335100742312
epoch: 157, train loss: 0.14597791382926484, acc: 0.8905509563231516, val loss: 1.3267757713174324, acc: 0.6573258606885508, test loss: 1.2789100126648758, acc: 0.6646341463414634
epoch: 158, train loss: 0.14910549146003327, acc: 0.8903225806451613, val loss: 1.1208383922485279, acc: 0.6944222044302109, test loss: 1.1236584773119282, acc: 0.7006892895015907
epoch: 159, train loss: 0.1527799319172804, acc: 0.8880388238652583, val loss: 1.2092389346505281, acc: 0.6730717907659461, test loss: 1.1574951864123217, acc: 0.672322375397667
epoch: 160, train loss: 0.16174973775634008, acc: 0.8813017413645446, val loss: 1.1369398358919984, acc: 0.685882038964505, test loss: 1.0873554951312812, acc: 0.690880169671262
epoch: 161, train loss: 0.16451772802213993, acc: 0.8821010562375107, val loss: 1.1530274734008399, acc: 0.6866826794769149, test loss: 1.1905776737603504, acc: 0.6874337221633086
epoch: 162, train loss: 0.15231791099045844, acc: 0.8859263488438481, val loss: 1.083551238497512, acc: 0.6810781958900454, test loss: 1.0786492776516765, acc: 0.6781548250265111
epoch: 163, train loss: 0.15256911239137727, acc: 0.8880388238652583, val loss: 1.113042282714569, acc: 0.6882839605017347, test loss: 1.1357375728489865, acc: 0.6839872746553552
epoch: 164, train loss: 0.15141801652467968, acc: 0.8864401941193263, val loss: 1.1865443503789848, acc: 0.6765412329863891, test loss: 1.1467215999692268, acc: 0.6731177094379639
epoch: 165, train loss: 0.14532397914164208, acc: 0.8869540393948044, val loss: 1.172071136886163, acc: 0.6856151587937016, test loss: 1.1508208835617988, acc: 0.690084835630965
Epoch   165: reducing learning rate of group 0 to 3.7500e-04.
epoch: 166, train loss: 0.11313400658290454, acc: 0.9097345132743363, val loss: 1.1460636933390096, acc: 0.7026954897251134, test loss: 1.1306859340799196, acc: 0.7030752916224814
epoch: 167, train loss: 0.09362519602630263, acc: 0.9193833856694262, val loss: 1.202659223440459, acc: 0.7077662129703763, test loss: 1.1845522912916424, acc: 0.7075821845174973
epoch: 168, train loss: 0.09062802484298889, acc: 0.9200685127033971, val loss: 1.1928668913600728, acc: 0.7096343741659995, test loss: 1.1805685418534506, acc: 0.7025450689289502
epoch: 169, train loss: 0.08687189368649684, acc: 0.9216100485298315, val loss: 1.2316656887674446, acc: 0.7058980517747532, test loss: 1.22298497802253, acc: 0.7083775185577943
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.05683414724905628, acc: 0.9230373965172709, val loss: 1.1509964948086093, acc: 0.7005604483586869, test loss: 1.1419511167520178, acc: 0.7030752916224814
epoch: 171, train loss: 0.056559521780843024, acc: 0.9233228661147588, val loss: 1.1457848837129396, acc: 0.7080330931411796, test loss: 1.1228669434311787, acc: 0.7054612937433722
epoch: 172, train loss: 0.051948975882052426, acc: 0.9266343134456181, val loss: 1.1818190327255702, acc: 0.7072324526287697, test loss: 1.1645288327086643, acc: 0.707051961823966
epoch: 173, train loss: 0.05739747578931678, acc: 0.9221809877248073, val loss: 1.1231526231842102, acc: 0.7048305310915399, test loss: 1.0946519956356142, acc: 0.711558854718982
epoch: 174, train loss: 0.05771485174996255, acc: 0.9235512417927491, val loss: 1.1457497842603916, acc: 0.7034961302375233, test loss: 1.110779659998379, acc: 0.7091728525980912
epoch: 175, train loss: 0.058634243889805114, acc: 0.9225806451612903, val loss: 1.1175339029157896, acc: 0.7048305310915399, test loss: 1.0960375528193986, acc: 0.7089077412513256
epoch: 176, train loss: 0.05911428214029826, acc: 0.9205252640593776, val loss: 1.1923448132743255, acc: 0.699759807846277, test loss: 1.2117950912497835, acc: 0.6948568398727466
epoch: 177, train loss: 0.06563252506258145, acc: 0.9182415072794747, val loss: 1.2088875148275358, acc: 0.6802775553776355, test loss: 1.2074152185707052, acc: 0.6831919406150583
epoch: 178, train loss: 0.061226047050976735, acc: 0.9208678275763631, val loss: 1.1060325515279779, acc: 0.7048305310915399, test loss: 1.0706262116973246, acc: 0.7009544008483564
epoch: 179, train loss: 0.06347841613702933, acc: 0.9177847559234942, val loss: 1.123766624803444, acc: 0.7002935681878837, test loss: 1.094715794096064, acc: 0.6996288441145281
epoch: 180, train loss: 0.06314373546554605, acc: 0.9182415072794747, val loss: 1.1544669915238222, acc: 0.6826794769148652, test loss: 1.0909572474397757, acc: 0.6927359490986215
epoch: 181, train loss: 0.06408525066837868, acc: 0.9159577504995718, val loss: 1.1120506980306535, acc: 0.6981585268214572, test loss: 1.0910312942330935, acc: 0.7006892895015907
epoch: 182, train loss: 0.058934168360410606, acc: 0.921153297173851, val loss: 1.1519840805887507, acc: 0.699759807846277, test loss: 1.1195599907655605, acc: 0.7057264050901378
epoch: 183, train loss: 0.05749466793301648, acc: 0.9218384242078219, val loss: 1.1338232614404207, acc: 0.6976247664798505, test loss: 1.0984439253174993, acc: 0.7110286320254506
epoch: 184, train loss: 0.05627348448761082, acc: 0.9226948330002854, val loss: 1.1521865663001594, acc: 0.6986922871630638, test loss: 1.1371289984671713, acc: 0.693796394485684
epoch: 185, train loss: 0.06184696292123419, acc: 0.9196117613474165, val loss: 1.2673868388534069, acc: 0.6706698692287163, test loss: 1.1907228333826156, acc: 0.6871686108165429
epoch: 186, train loss: 0.0667496949319427, acc: 0.914187838995147, val loss: 1.1600049084143413, acc: 0.6946890846010142, test loss: 1.1597631796799435, acc: 0.6993637327677624
epoch: 187, train loss: 0.06790588138071496, acc: 0.9148158721096203, val loss: 1.1801896131544518, acc: 0.6946890846010142, test loss: 1.1269601886876695, acc: 0.6959172852598091
epoch: 188, train loss: 0.06589640920592008, acc: 0.915786468741079, val loss: 1.1073268141338022, acc: 0.7021617293835068, test loss: 1.0675449812526034, acc: 0.7083775185577943
epoch: 189, train loss: 0.06272716752370187, acc: 0.9190408221524408, val loss: 1.107768527024073, acc: 0.6949559647718174, test loss: 1.1051094580050618, acc: 0.6927359490986215
epoch: 190, train loss: 0.06705327879189764, acc: 0.9144733085926349, val loss: 1.1707514681942086, acc: 0.6933546837469976, test loss: 1.1614531649006008, acc: 0.6895546129374337
epoch: 191, train loss: 0.06878626538265646, acc: 0.9147587781901227, val loss: 1.177851849121509, acc: 0.6821457165732586, test loss: 1.1367859093280763, acc: 0.6914103923647932
epoch: 192, train loss: 0.0639846513127042, acc: 0.9171567228090208, val loss: 1.1735177447645067, acc: 0.6826794769148652, test loss: 1.121111073903542, acc: 0.6842523860021209
epoch: 193, train loss: 0.05832481473139821, acc: 0.919155009991436, val loss: 1.144841442497565, acc: 0.6970910061382439, test loss: 1.1262665124864872, acc: 0.6998939554612937
epoch: 194, train loss: 0.06423458826055263, acc: 0.9181273194404795, val loss: 1.2837359382210969, acc: 0.6626634641046171, test loss: 1.2849835513884544, acc: 0.66118769883351
epoch: 195, train loss: 0.07317186440318577, acc: 0.9083642592063945, val loss: 1.1297740785478114, acc: 0.6898852415265546, test loss: 1.0650635462171325, acc: 0.6990986214209968
epoch: 196, train loss: 0.0609495636115083, acc: 0.9197259491864116, val loss: 1.1418672090916753, acc: 0.6984254069922605, test loss: 1.1148580302615687, acc: 0.6972428419936373
epoch: 197, train loss: 0.06074867155886772, acc: 0.9186982586354553, val loss: 1.1759925391599595, acc: 0.6893514811849479, test loss: 1.1413589013090568, acc: 0.6858430540827147
epoch: 198, train loss: 0.05829591922081269, acc: 0.9210391093348559, val loss: 1.2191875784563198, acc: 0.682412596744062, test loss: 1.1535279515692185, acc: 0.6924708377518558
epoch: 199, train loss: 0.06047548292310552, acc: 0.9188124464744505, val loss: 1.2007743810252058, acc: 0.6770749933279957, test loss: 1.1561667718159179, acc: 0.6850477200424178
epoch: 200, train loss: 0.0594599246874541, acc: 0.9209820154153583, val loss: 1.110444200621117, acc: 0.6944222044302109, test loss: 1.086884503905619, acc: 0.7001590668080594
best val acc 0.7096343741659995 at epoch 168.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9951    0.9968    0.9960      5337
           1     0.8971    0.9580    0.9266      2502
           2     0.9887    0.9741    0.9813       810
           3     0.9251    0.9668    0.9455      1840
           4     0.9863    0.9796    0.9830       737
           5     0.9738    0.9882    0.9809       677
           6     0.8688    0.9864    0.9239      1323
           7     0.7932    0.9471    0.8633       907
           8     0.9722    0.9952    0.9836       421
           9     0.9804    0.9975    0.9889       401
          10     0.9752    0.9924    0.9837       396
          11     0.9970    1.0000    0.9985       332
          12     0.8852    0.9932    0.9361       295
          13     0.9861    0.9725    0.9792       291
          14     0.8529    0.1111    0.1966       261
          15     0.8881    0.4980    0.6381       494
          16     0.7297    0.1055    0.1843       256
          17     0.9913    0.9745    0.9828       235

    accuracy                         0.9415     17515
   macro avg     0.9270    0.8576    0.8596     17515
weighted avg     0.9399    0.9415    0.9306     17515

train confusion matrix:
[[9.96814690e-01 3.74742365e-04 0.00000000e+00 1.87371182e-04
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  2.06108301e-03 1.87371182e-04 0.00000000e+00 0.00000000e+00
  0.00000000e+00 3.74742365e-04 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.99680256e-04 9.58033573e-01 0.00000000e+00 1.99840128e-03
  2.39808153e-03 0.00000000e+00 3.39728217e-02 1.19904077e-03
  0.00000000e+00 7.99360512e-04 0.00000000e+00 0.00000000e+00
  3.99680256e-04 0.00000000e+00 7.99360512e-04 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.23456790e-03 9.74074074e-01 0.00000000e+00
  2.46913580e-03 2.09876543e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.23456790e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.63043478e-03 1.19565217e-02 0.00000000e+00 9.66847826e-01
  0.00000000e+00 0.00000000e+00 5.43478261e-04 5.43478261e-04
  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.43478261e-04
  0.00000000e+00 0.00000000e+00 1.63043478e-03 1.63043478e-02
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 5.42740841e-03 0.00000000e+00 0.00000000e+00
  9.79647218e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.49253731e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 1.18168390e-02 0.00000000e+00
  0.00000000e+00 9.88183161e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.20937264e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.86394558e-01 0.00000000e+00
  0.00000000e+00 7.55857899e-04 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 7.55857899e-04
  0.00000000e+00 0.00000000e+00]
 [8.82028666e-03 1.10253583e-03 0.00000000e+00 0.00000000e+00
  1.10253583e-03 0.00000000e+00 2.20507166e-03 9.47078280e-01
  0.00000000e+00 0.00000000e+00 1.10253583e-03 0.00000000e+00
  2.64608600e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.10253583e-02 1.10253583e-03]
 [2.37529691e-03 2.37529691e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.95249406e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.97506234e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.49376559e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 2.52525253e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.92424242e-01 0.00000000e+00
  0.00000000e+00 2.52525253e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.52525253e-03]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.38983051e-03 0.00000000e+00 0.00000000e+00 3.38983051e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.93220339e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.87285223e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.43642612e-03 1.37457045e-02 0.00000000e+00 0.00000000e+00
  3.43642612e-03 9.72508591e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.14942529e-02 8.62068966e-01 0.00000000e+00 1.14942529e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.83141762e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.11111111e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.02429150e-03 6.07287449e-03 0.00000000e+00 2.73279352e-01
  0.00000000e+00 0.00000000e+00 2.20647773e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.97975709e-01
  0.00000000e+00 0.00000000e+00]
 [2.73437500e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 8.47656250e-01
  0.00000000e+00 0.00000000e+00 1.95312500e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.05468750e-01 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 4.25531915e-03 0.00000000e+00 4.25531915e-03
  0.00000000e+00 0.00000000e+00 1.70212766e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.74468085e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8524    0.8591    0.8558      1143
           1     0.5737    0.6604    0.6141       536
           2     0.8634    0.8035    0.8323       173
           3     0.7404    0.6878    0.7132       394
           4     0.7682    0.7342    0.7508       158
           5     0.7947    0.8276    0.8108       145
           6     0.7016    0.7562    0.7279       283
           7     0.4378    0.4897    0.4623       194
           8     0.7551    0.8222    0.7872        90
           9     0.5357    0.5294    0.5325        85
          10     0.8046    0.8333    0.8187        84
          11     0.9180    0.7887    0.8485        71
          12     0.3333    0.4127    0.3688        63
          13     0.5488    0.7258    0.6250        62
          14     0.1111    0.0536    0.0723        56
          15     0.4000    0.2286    0.2909       105
          16     0.3750    0.0545    0.0952        55
          17     0.5238    0.4400    0.4783        50

    accuracy                         0.7096      3747
   macro avg     0.6132    0.5949    0.5936      3747
weighted avg     0.7044    0.7096    0.7032      3747

validation confusion matrix:
[[0.85914261 0.03324584 0.00262467 0.01137358 0.00174978 0.00174978
  0.00174978 0.02624672 0.01487314 0.01399825 0.00612423 0.
  0.00262467 0.01312336 0.00349956 0.00262467 0.         0.00524934]
 [0.08022388 0.66044776 0.00559701 0.06716418 0.02052239 0.00373134
  0.07276119 0.02985075 0.         0.00559701 0.00186567 0.00373134
  0.02238806 0.00559701 0.00559701 0.0130597  0.         0.00186567]
 [0.02312139 0.00578035 0.80346821 0.         0.01156069 0.06936416
  0.04046243 0.         0.00578035 0.01156069 0.         0.00578035
  0.01156069 0.00578035 0.         0.00578035 0.         0.        ]
 [0.06598985 0.13451777 0.00253807 0.68781726 0.00507614 0.00253807
  0.01269036 0.03553299 0.         0.         0.         0.00253807
  0.         0.00253807 0.01015228 0.03045685 0.         0.00761421]
 [0.         0.13924051 0.         0.01265823 0.73417722 0.
  0.00632911 0.03164557 0.         0.         0.00632911 0.
  0.0443038  0.01265823 0.00632911 0.00632911 0.         0.        ]
 [0.0137931  0.02068966 0.05517241 0.         0.         0.82758621
  0.05517241 0.         0.         0.02068966 0.         0.00689655
  0.         0.         0.         0.         0.         0.        ]
 [0.00706714 0.1024735  0.01060071 0.         0.01060071 0.03886926
  0.75618375 0.00706714 0.         0.02120141 0.         0.
  0.00353357 0.00706714 0.         0.03533569 0.         0.        ]
 [0.14948454 0.12371134 0.         0.01030928 0.0257732  0.
  0.01546392 0.48969072 0.00515464 0.         0.01030928 0.
  0.08762887 0.         0.03608247 0.         0.01546392 0.03092784]
 [0.05555556 0.04444444 0.         0.         0.         0.
  0.         0.03333333 0.82222222 0.         0.         0.
  0.02222222 0.02222222 0.         0.         0.         0.        ]
 [0.16470588 0.07058824 0.01176471 0.01176471 0.         0.01176471
  0.03529412 0.01176471 0.02352941 0.52941176 0.         0.
  0.02352941 0.10588235 0.         0.         0.         0.        ]
 [0.04761905 0.02380952 0.         0.01190476 0.01190476 0.
  0.         0.04761905 0.         0.         0.83333333 0.
  0.01190476 0.         0.         0.         0.01190476 0.        ]
 [0.01408451 0.08450704 0.02816901 0.05633803 0.         0.01408451
  0.         0.         0.         0.01408451 0.         0.78873239
  0.         0.         0.         0.         0.         0.        ]
 [0.07936508 0.20634921 0.01587302 0.01587302 0.03174603 0.
  0.         0.17460317 0.01587302 0.         0.         0.
  0.41269841 0.01587302 0.01587302 0.         0.         0.01587302]
 [0.09677419 0.01612903 0.         0.         0.01612903 0.
  0.         0.         0.03225806 0.06451613 0.03225806 0.
  0.01612903 0.72580645 0.         0.         0.         0.        ]
 [0.14285714 0.5        0.         0.10714286 0.01785714 0.
  0.01785714 0.07142857 0.         0.         0.         0.
  0.01785714 0.01785714 0.05357143 0.03571429 0.         0.01785714]
 [0.06666667 0.2        0.         0.23809524 0.00952381 0.00952381
  0.20952381 0.         0.         0.02857143 0.         0.
  0.         0.         0.00952381 0.22857143 0.         0.        ]
 [0.12727273 0.21818182 0.         0.01818182 0.03636364 0.
  0.         0.38181818 0.         0.01818182 0.01818182 0.
  0.05454545 0.         0.03636364 0.         0.05454545 0.03636364]
 [0.14       0.         0.         0.06       0.04       0.
  0.         0.22       0.         0.         0.06       0.
  0.         0.         0.02       0.         0.02       0.44      ]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8382    0.8550    0.8465      1145
           1     0.5778    0.6294    0.6025       537
           2     0.7829    0.7829    0.7829       175
           3     0.7051    0.6962    0.7006       395
           4     0.7708    0.6981    0.7327       159
           5     0.7887    0.7671    0.7778       146
           6     0.6889    0.7641    0.7245       284
           7     0.4268    0.5231    0.4700       195
           8     0.6195    0.7692    0.6863        91
           9     0.5765    0.5632    0.5698        87
          10     0.8250    0.7674    0.7952        86
          11     0.9385    0.8472    0.8905        72
          12     0.5672    0.5938    0.5802        64
          13     0.5152    0.5312    0.5231        64
          14     0.0645    0.0351    0.0455        57
          15     0.6078    0.2897    0.3924       107
          16     0.1818    0.0357    0.0597        56
          17     0.5778    0.5000    0.5361        52

    accuracy                         0.7025      3772
   macro avg     0.6140    0.5916    0.5953      3772
weighted avg     0.6960    0.7025    0.6959      3772

test confusion matrix:
[[0.85502183 0.02882096 0.00436681 0.01222707 0.00087336 0.00087336
  0.00262009 0.02882096 0.02445415 0.01135371 0.00262009 0.00087336
  0.00174672 0.01484716 0.00436681 0.00174672 0.00087336 0.00349345]
 [0.07635009 0.62942272 0.00558659 0.07635009 0.01862197 0.00744879
  0.08938547 0.05214153 0.00744879 0.00931099 0.         0.
  0.00931099 0.00558659 0.00372439 0.00558659 0.         0.00372439]
 [0.02857143 0.02285714 0.78285714 0.00571429 0.         0.09714286
  0.01714286 0.         0.         0.02857143 0.00571429 0.
  0.         0.00571429 0.00571429 0.         0.         0.        ]
 [0.07341772 0.1164557  0.00253165 0.69620253 0.00506329 0.
  0.01012658 0.03037975 0.         0.         0.00253165 0.00253165
  0.00253165 0.         0.02278481 0.02531646 0.         0.01012658]
 [0.01886792 0.14465409 0.00628931 0.03773585 0.69811321 0.
  0.         0.03144654 0.         0.         0.00628931 0.
  0.03144654 0.01257862 0.00628931 0.         0.         0.00628931]
 [0.01369863 0.01369863 0.0890411  0.         0.00684932 0.76712329
  0.0890411  0.         0.         0.00684932 0.         0.00684932
  0.         0.         0.         0.00684932 0.         0.        ]
 [0.01760563 0.1056338  0.01760563 0.00704225 0.02464789 0.01408451
  0.76408451 0.00352113 0.00352113 0.02464789 0.         0.00352113
  0.         0.00352113 0.00352113 0.00704225 0.         0.        ]
 [0.08205128 0.15384615 0.         0.05128205 0.02564103 0.
  0.02564103 0.52307692 0.01025641 0.00512821 0.01025641 0.
  0.05641026 0.         0.02051282 0.00512821 0.01025641 0.02051282]
 [0.16483516 0.03296703 0.01098901 0.         0.         0.
  0.         0.         0.76923077 0.01098901 0.         0.
  0.         0.01098901 0.         0.         0.         0.        ]
 [0.17241379 0.04597701 0.05747126 0.01149425 0.         0.
  0.01149425 0.         0.03448276 0.56321839 0.         0.
  0.01149425 0.08045977 0.01149425 0.         0.         0.        ]
 [0.08139535 0.02325581 0.         0.01162791 0.         0.
  0.01162791 0.02325581 0.         0.01162791 0.76744186 0.
  0.         0.         0.         0.         0.04651163 0.02325581]
 [0.05555556 0.02777778 0.02777778 0.02777778 0.         0.
  0.         0.         0.01388889 0.         0.         0.84722222
  0.         0.         0.         0.         0.         0.        ]
 [0.015625   0.15625    0.         0.         0.015625   0.
  0.         0.1875     0.         0.015625   0.015625   0.
  0.59375    0.         0.         0.         0.         0.        ]
 [0.21875    0.046875   0.015625   0.015625   0.03125    0.
  0.015625   0.046875   0.03125    0.015625   0.015625   0.
  0.         0.53125    0.015625   0.         0.         0.        ]
 [0.12280702 0.49122807 0.         0.12280702 0.03508772 0.
  0.01754386 0.07017544 0.03508772 0.         0.         0.
  0.01754386 0.         0.03508772 0.01754386 0.01754386 0.01754386]
 [0.07476636 0.18691589 0.00934579 0.22429907 0.         0.03738318
  0.1682243  0.         0.         0.         0.         0.
  0.         0.         0.00934579 0.28971963 0.         0.        ]
 [0.125      0.10714286 0.         0.07142857 0.         0.
  0.         0.51785714 0.         0.         0.03571429 0.
  0.05357143 0.         0.03571429 0.         0.03571429 0.01785714]
 [0.19230769 0.01923077 0.         0.01923077 0.03846154 0.
  0.         0.15384615 0.         0.         0.03846154 0.
  0.         0.         0.01923077 0.         0.01923077 0.5       ]]
---------------------------------------
program finished.
