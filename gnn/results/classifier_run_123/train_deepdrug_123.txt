seed:  23
save trained model at:  ../trained_models/trained_classifier_model_123.pt
save loss at:  ./results/train_classifier_results_123.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['1nn5A01', '4ncnA00', '3mlaA00', '5l0rA02', '2p5sB00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['6cpgA00', '4jy0A00', '4xpdB01', '1ea6B00', '5bswB00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b9c98b2ed60>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.012611083509062, acc: 0.3922694447732923; test loss: 1.7536141006300225, acc: 0.454975183171827
epoch: 2, train loss: 1.7079988167922964, acc: 0.4757310287676098; test loss: 1.771545033172016, acc: 0.4509572205152446
epoch: 3, train loss: 1.6279577529824132, acc: 0.5058600686634308; test loss: 1.5438643668848018, acc: 0.5199716379106594
epoch: 4, train loss: 1.5652480493343213, acc: 0.5224340002367704; test loss: 1.4676964848747605, acc: 0.5592058614984637
epoch: 5, train loss: 1.527880868347803, acc: 0.5398366283887771; test loss: 1.476552326102765, acc: 0.5438430631056488
epoch: 6, train loss: 1.4868450024515494, acc: 0.5502545282348763; test loss: 1.4249115038132223, acc: 0.5514062869298039
epoch: 7, train loss: 1.4442375717155183, acc: 0.5617970877234522; test loss: 1.4515576775094854, acc: 0.5686598912786576
epoch: 8, train loss: 1.420403624041004, acc: 0.5726293358588848; test loss: 1.3659252456051045, acc: 0.5842590404159773
epoch: 9, train loss: 1.3926098648579117, acc: 0.5764176630756481; test loss: 1.3964118656044844, acc: 0.5646419286220752
epoch: 10, train loss: 1.3819924647305384, acc: 0.57985083461584; test loss: 1.3594481425487073, acc: 0.5743323091467738
epoch: 11, train loss: 1.3539492541182545, acc: 0.5864212146324139; test loss: 1.3209582235029307, acc: 0.5814228314819192
epoch: 12, train loss: 1.3299721544051686, acc: 0.5948265656446076; test loss: 1.2996190043381557, acc: 0.6041125029543843
epoch: 13, train loss: 1.3198247565960086, acc: 0.598496507635847; test loss: 1.2799894434429178, acc: 0.604585204443394
epoch: 14, train loss: 1.3032339187688455, acc: 0.6010417899846099; test loss: 1.3479398683125572, acc: 0.5759867643583078
epoch: 15, train loss: 1.2989796095661006, acc: 0.6061915472948975; test loss: 1.2644143220635065, acc: 0.6076577641219569
epoch: 16, train loss: 1.2630152068251588, acc: 0.6153072096602344; test loss: 1.2631642648272694, acc: 0.6130938312455684
epoch: 17, train loss: 1.2631190202215572, acc: 0.6141825500177578; test loss: 1.2959885195450591, acc: 0.5941857716851808
epoch: 18, train loss: 1.2468267255271415, acc: 0.6198650408429028; test loss: 1.355099469317637, acc: 0.5811864807374143
epoch: 19, train loss: 1.2515858544165304, acc: 0.6158399431750917; test loss: 1.2236038159884564, acc: 0.6232569132592768
epoch: 20, train loss: 1.2203970181669612, acc: 0.6288623179827157; test loss: 1.257443013187842, acc: 0.6067123611439376
epoch: 21, train loss: 1.2056605280031323, acc: 0.6307564815910974; test loss: 1.2264164742835164, acc: 0.6239659654927913
epoch: 22, train loss: 1.2032369169325776, acc: 0.6341896531312892; test loss: 1.2415685845841418, acc: 0.6178208461356653
epoch: 23, train loss: 1.1877601296150628, acc: 0.6353143127737658; test loss: 1.189393270441162, acc: 0.6346017489955094
epoch: 24, train loss: 1.1751005150544456, acc: 0.6390434473777673; test loss: 1.1751327954181625, acc: 0.6341290475064997
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9540874790168732, acc: 0.6378595951225287; test loss: 0.9558959077701195, acc: 0.630583786338927
epoch: 26, train loss: 0.923065701433649, acc: 0.6464425239730082; test loss: 0.9426869506448247, acc: 0.6431103757976837
epoch: 27, train loss: 0.9197171132522489, acc: 0.6540191784065349; test loss: 1.0225383178811918, acc: 0.6140392342235879
epoch: 28, train loss: 0.9245628143635476, acc: 0.652302592636439; test loss: 0.956142178665969, acc: 0.6372016071850626
epoch: 29, train loss: 0.912877670001374, acc: 0.6518290517343436; test loss: 0.9653375517304866, acc: 0.6239659654927913
epoch: 30, train loss: 0.902044295135639, acc: 0.6527169409257725; test loss: 0.9705157716120425, acc: 0.6329472937839754
epoch: 31, train loss: 0.9088152895118268, acc: 0.6504676216408192; test loss: 1.1202180013676903, acc: 0.6005672417868116
epoch: 32, train loss: 0.8903649048973459, acc: 0.6601160175210133; test loss: 0.9354896048399551, acc: 0.6355471519735287
epoch: 33, train loss: 0.8805841884557151, acc: 0.6630164555463478; test loss: 0.9545392406322071, acc: 0.6263294729378398
epoch: 34, train loss: 0.8785246534151087, acc: 0.6618917959038711; test loss: 0.9946026049221528, acc: 0.6197116520917041
epoch: 35, train loss: 0.875727670269209, acc: 0.6653841600568249; test loss: 0.9821011848242337, acc: 0.6253840699598203
epoch: 36, train loss: 0.8604709362932899, acc: 0.6692316798863501; test loss: 1.0022900217223523, acc: 0.6348380997400142
epoch: 37, train loss: 0.8629774597674217, acc: 0.6679294424055878; test loss: 0.9455116788593203, acc: 0.6294020326164027
Epoch    37: reducing learning rate of group 0 to 1.5000e-03.
epoch: 38, train loss: 0.8029681896336391, acc: 0.688942819936072; test loss: 0.8113831890528153, acc: 0.6780902859844008
epoch: 39, train loss: 0.7714644576849226, acc: 0.6965786669823606; test loss: 0.8216056153160606, acc: 0.67950839045143
epoch: 40, train loss: 0.7623594450120801, acc: 0.7004853794246478; test loss: 0.8081907239317527, acc: 0.6927440321437013
epoch: 41, train loss: 0.7605787178701489, acc: 0.69870960104179; test loss: 0.8871640077129771, acc: 0.6653273457811392
epoch: 42, train loss: 0.7523941180614857, acc: 0.7058719071859831; test loss: 0.7784793442852586, acc: 0.7026707634129048
epoch: 43, train loss: 0.7374480903903676, acc: 0.7063454480880786; test loss: 0.8659827868669655, acc: 0.6634365398251004
epoch: 44, train loss: 0.7362248941301608, acc: 0.7094234639516989; test loss: 0.8516164414287036, acc: 0.676435830772867
epoch: 45, train loss: 0.7312988413993291, acc: 0.71007458269208; test loss: 0.8333408657863782, acc: 0.6839990545970219
epoch: 46, train loss: 0.7208706000168196, acc: 0.7129158281046526; test loss: 0.7800774908437709, acc: 0.69463483809974
epoch: 47, train loss: 0.7083169568554528, acc: 0.7203740973126553; test loss: 0.8296129097754835, acc: 0.6776175844953911
epoch: 48, train loss: 0.7147472972727054, acc: 0.7158754587427489; test loss: 0.8721573573707323, acc: 0.6726542188607895
epoch: 49, train loss: 0.7031872234175696, acc: 0.7213211791168462; test loss: 0.9341891304986242, acc: 0.6603639801465374
epoch: 50, train loss: 0.6981305148356819, acc: 0.7215579495678939; test loss: 0.8572024366473002, acc: 0.6738359725833136
epoch: 51, train loss: 0.697942839559894, acc: 0.7196637859595123; test loss: 0.824828807187965, acc: 0.6939257858662254
epoch: 52, train loss: 0.6941558905038464, acc: 0.7219131052444655; test loss: 0.7772649948378878, acc: 0.691562278421177
epoch: 53, train loss: 0.6737800098190904, acc: 0.7285426778738013; test loss: 0.8001075176391386, acc: 0.6943984873552351
epoch: 54, train loss: 0.670814085637548, acc: 0.7306736119332308; test loss: 0.8073089858708554, acc: 0.7012526589458756
epoch: 55, train loss: 0.6780610392259341, acc: 0.7248727358825618; test loss: 0.7915517204170164, acc: 0.6934530843772158
epoch: 56, train loss: 0.661867595952376, acc: 0.7336332425713271; test loss: 0.7563082786407687, acc: 0.7076341290475066
epoch: 57, train loss: 0.6596199722421004, acc: 0.733751627796851; test loss: 0.8125829639177856, acc: 0.69463483809974
epoch: 58, train loss: 0.6475880965264122, acc: 0.739374926009234; test loss: 0.7950252455984326, acc: 0.7111793902150791
epoch: 59, train loss: 0.6617961695762419, acc: 0.7345803243755179; test loss: 0.7619067797055624, acc: 0.7135428976601277
epoch: 60, train loss: 0.6386820539661, acc: 0.74263051971114; test loss: 0.749875040151244, acc: 0.7071614275584968
epoch: 61, train loss: 0.641163040078207, acc: 0.7402036225879011; test loss: 0.7721960914244863, acc: 0.7050342708579532
epoch: 62, train loss: 0.6317011755896106, acc: 0.7447022611578075; test loss: 0.8071712223639924, acc: 0.7003072559678563
epoch: 63, train loss: 0.6438955186569748, acc: 0.7399076595240914; test loss: 0.767475039083832, acc: 0.7133065469156228
epoch: 64, train loss: 0.6181590727790511, acc: 0.7516869894637149; test loss: 0.816437156564996, acc: 0.6965256440557788
epoch: 65, train loss: 0.6074567499126332, acc: 0.75245649342962; test loss: 0.8334618736682731, acc: 0.67950839045143
epoch: 66, train loss: 0.6053884552966579, acc: 0.7514502190126672; test loss: 0.7619582731598123, acc: 0.7166154573386906
epoch: 67, train loss: 0.6093376498540167, acc: 0.7522789155913342; test loss: 0.7878654204314269, acc: 0.7057433230914677
epoch: 68, train loss: 0.5976195641773134, acc: 0.7535811530720966; test loss: 0.7582617334033602, acc: 0.7118884424485937
epoch: 69, train loss: 0.5888143467157991, acc: 0.75736948028886; test loss: 0.7494548445079265, acc: 0.7239423304183408
epoch: 70, train loss: 0.5868365309415406, acc: 0.7582573694802889; test loss: 0.7790624026400179, acc: 0.7130701961711179
epoch: 71, train loss: 0.5864599453664834, acc: 0.76382147507991; test loss: 0.7523433358019561, acc: 0.7241786811628457
epoch: 72, train loss: 0.5744396036188039, acc: 0.7640582455309577; test loss: 0.8219915534609797, acc: 0.6955802410777594
epoch: 73, train loss: 0.5849816017818688, acc: 0.7599739552503848; test loss: 0.8221615277630475, acc: 0.7024344126683999
epoch: 74, train loss: 0.5728573832439081, acc: 0.764709364271339; test loss: 0.7662776287947104, acc: 0.708343181281021
epoch: 75, train loss: 0.563238310626206, acc: 0.7696223511305789; test loss: 0.7624954469545996, acc: 0.7116520917040888
epoch: 76, train loss: 0.5565414906634767, acc: 0.7715757073517225; test loss: 0.7874032280682334, acc: 0.7055069723469629
epoch: 77, train loss: 0.5749188441700719, acc: 0.7610986148928613; test loss: 0.7542341060318357, acc: 0.7154337036161664
epoch: 78, train loss: 0.568496501832456, acc: 0.7667811057180064; test loss: 0.7235341150973091, acc: 0.7305601512644765
epoch: 79, train loss: 0.5451338327719896, acc: 0.7739434118621996; test loss: 0.8019656197338966, acc: 0.7064523753249823
epoch: 80, train loss: 0.5464954121629063, acc: 0.774416952764295; test loss: 0.7564807006918886, acc: 0.7142519498936422
epoch: 81, train loss: 0.5425288548790747, acc: 0.7757191902450574; test loss: 0.740152431250013, acc: 0.7310328527534862
epoch: 82, train loss: 0.5369442766963398, acc: 0.7794483248490588; test loss: 0.8078942207037993, acc: 0.6969983455447885
epoch: 83, train loss: 0.5315456652805003, acc: 0.7803954066532497; test loss: 0.9111482118833322, acc: 0.6613093831245569
epoch: 84, train loss: 0.550018758954163, acc: 0.7743577601515331; test loss: 0.7225292968118875, acc: 0.734341763176554
epoch: 85, train loss: 0.518277154090744, acc: 0.7854267787380135; test loss: 0.7439871780473948, acc: 0.7208697707397779
epoch: 86, train loss: 0.532887145113039, acc: 0.7813424884574405; test loss: 0.7884480488889185, acc: 0.705270621602458
epoch: 87, train loss: 0.5218539239210929, acc: 0.7843613117082988; test loss: 0.7895065070156565, acc: 0.7029071141574096
epoch: 88, train loss: 0.5090684935131256, acc: 0.7895702616313484; test loss: 0.7960404262732061, acc: 0.706925076813992
epoch: 89, train loss: 0.5022726126912219, acc: 0.7886823724399195; test loss: 0.8267494599832144, acc: 0.7111793902150791
epoch: 90, train loss: 0.4983194249472534, acc: 0.7915828104652539; test loss: 0.7531441145545059, acc: 0.7319782557315055
epoch: 91, train loss: 0.5358318400775326, acc: 0.7765478868237244; test loss: 0.7902794774050297, acc: 0.7166154573386906
epoch: 92, train loss: 0.49505096977386753, acc: 0.794009707588493; test loss: 0.8110463002758707, acc: 0.7090522335145356
epoch: 93, train loss: 0.5001615648109051, acc: 0.789096720729253; test loss: 0.7888634735121848, acc: 0.7149610021271567
epoch: 94, train loss: 0.4869096800746672, acc: 0.7961406416479223; test loss: 0.75911255779911, acc: 0.7350508154100686
epoch: 95, train loss: 0.5000695180325617, acc: 0.7924706996566828; test loss: 0.7683559218018761, acc: 0.7263058378633893
epoch: 96, train loss: 0.47305507784173717, acc: 0.8049603409494495; test loss: 0.8543364332799116, acc: 0.7050342708579532
epoch: 97, train loss: 0.4803653607548264, acc: 0.8014679767964958; test loss: 0.8292668430384269, acc: 0.7036161663909242
epoch: 98, train loss: 0.47547140085966727, acc: 0.8000473540902096; test loss: 0.7519156847178189, acc: 0.735759867643583
epoch: 99, train loss: 0.4720310949779649, acc: 0.8050787261749733; test loss: 0.816051751776695, acc: 0.7133065469156228
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.38253502880269513, acc: 0.8017639398603055; test loss: 0.6421720628844342, acc: 0.7215788229732923
epoch: 101, train loss: 0.35773567148845153, acc: 0.8120634544808808; test loss: 0.6283022279971544, acc: 0.7281966438194281
epoch: 102, train loss: 0.35144946074731337, acc: 0.8125369953829762; test loss: 0.6913886326645824, acc: 0.7218151737177972
epoch: 103, train loss: 0.35144532448344784, acc: 0.8072096602344028; test loss: 0.6879491071784448, acc: 0.7272512408414087
epoch: 104, train loss: 0.35510295551529125, acc: 0.8076240085237363; test loss: 0.6378154963135184, acc: 0.7305601512644765
epoch: 105, train loss: 0.34822522167822473, acc: 0.8106428317745945; test loss: 0.6451088501414743, acc: 0.7352871661545733
epoch: 106, train loss: 0.36371435595642565, acc: 0.8057890375281165; test loss: 0.7285785999063678, acc: 0.6943984873552351
epoch: 107, train loss: 0.348592193518727, acc: 0.809518172132118; test loss: 0.6491602113471756, acc: 0.726778539352399
epoch: 108, train loss: 0.3411882496894824, acc: 0.8130697288978336; test loss: 0.7222027593232644, acc: 0.7177972110612149
epoch: 109, train loss: 0.35672219391333165, acc: 0.8051971114004972; test loss: 0.6675304011288785, acc: 0.7170881588277003
Epoch   109: reducing learning rate of group 0 to 7.5000e-04.
epoch: 110, train loss: 0.3030496435474754, acc: 0.8298804309222209; test loss: 0.6245734298857525, acc: 0.7471047033798156
epoch: 111, train loss: 0.27154448903621664, acc: 0.8431395761808926; test loss: 0.7072129222524558, acc: 0.7289056960529425
epoch: 112, train loss: 0.2664436415831013, acc: 0.8456256659168936; test loss: 0.6369004009520013, acc: 0.7473410541243205
epoch: 113, train loss: 0.26164404481271886, acc: 0.8475790221380372; test loss: 0.6444343483157824, acc: 0.7475774048688253
epoch: 114, train loss: 0.2636325981781527, acc: 0.8461583994317509; test loss: 0.6511319003131085, acc: 0.7473410541243205
epoch: 115, train loss: 0.2586104566333025, acc: 0.8524328163845152; test loss: 0.6768550065864882, acc: 0.7369416213661073
epoch: 116, train loss: 0.25879599817465, acc: 0.8478157925890849; test loss: 0.6967693654900886, acc: 0.7312692034979911
epoch: 117, train loss: 0.2579831388789238, acc: 0.845329702853084; test loss: 0.6731300060418277, acc: 0.7319782557315055
epoch: 118, train loss: 0.2590641118308221, acc: 0.846099206818989; test loss: 0.67868758791926, acc: 0.7397778303001654
epoch: 119, train loss: 0.26255030477380026, acc: 0.8476382147507991; test loss: 0.6831724129228044, acc: 0.7355235168990782
epoch: 120, train loss: 0.2548655318913268, acc: 0.8491180300698473; test loss: 0.6934035465638989, acc: 0.7381233750886316
epoch: 121, train loss: 0.25391540134871826, acc: 0.851012193678229; test loss: 0.6956732700237904, acc: 0.7397778303001654
epoch: 122, train loss: 0.23987771315420536, acc: 0.8563395288268024; test loss: 0.7287616034935789, acc: 0.7376506735996219
epoch: 123, train loss: 0.24091843485733427, acc: 0.8565171066650882; test loss: 0.7041042441595984, acc: 0.7303238005199716
epoch: 124, train loss: 0.24909557517681008, acc: 0.8540310169290872; test loss: 0.6939756427919067, acc: 0.7390687780666509
epoch: 125, train loss: 0.23938739121073135, acc: 0.854267787380135; test loss: 0.789902929492212, acc: 0.7130701961711179
epoch: 126, train loss: 0.2416186220401474, acc: 0.8543269799928969; test loss: 0.8157980003077435, acc: 0.7026707634129048
epoch: 127, train loss: 0.23786832839762148, acc: 0.8567538771161359; test loss: 0.7411086997701833, acc: 0.7378870243441267
epoch: 128, train loss: 0.2415527008448294, acc: 0.8544453652184207; test loss: 0.6898303610076579, acc: 0.7383597258331364
epoch: 129, train loss: 0.23831829742325739, acc: 0.8539126317035634; test loss: 0.8452104894975713, acc: 0.7059796738359726
epoch: 130, train loss: 0.24468393182280332, acc: 0.8511897715165148; test loss: 0.8300313291975692, acc: 0.7109430394705744
epoch: 131, train loss: 0.22803549429707604, acc: 0.8580561145968983; test loss: 0.7078755722911441, acc: 0.7463956511463011
epoch: 132, train loss: 0.22600773512801997, acc: 0.862377175328519; test loss: 0.6960890987358372, acc: 0.7409595840226897
epoch: 133, train loss: 0.22954010513247408, acc: 0.861607671362614; test loss: 0.7474041860792773, acc: 0.7381233750886316
epoch: 134, train loss: 0.22983960485026528, acc: 0.8584704628862317; test loss: 0.7561206924692729, acc: 0.7246513826518554
epoch: 135, train loss: 0.23641529676828693, acc: 0.8545637504439446; test loss: 0.7125259766593326, acc: 0.7430867407232333
epoch: 136, train loss: 0.22610938568975822, acc: 0.8579377293713745; test loss: 0.775296162048769, acc: 0.7296147482864571
epoch: 137, train loss: 0.21153399574766502, acc: 0.8678820883153783; test loss: 0.7465948859052268, acc: 0.74048688253368
epoch: 138, train loss: 0.2166641794733427, acc: 0.8626731383923286; test loss: 0.7386251590799086, acc: 0.743559442212243
epoch: 139, train loss: 0.2159450124520105, acc: 0.8665206582218539; test loss: 0.7266766292438583, acc: 0.7390687780666509
epoch: 140, train loss: 0.22126989022675275, acc: 0.8638569906475672; test loss: 0.7520898549840175, acc: 0.7421413377452138
epoch: 141, train loss: 0.2187715004721633, acc: 0.8639161832603292; test loss: 0.8566142323732207, acc: 0.7062160245804774
epoch: 142, train loss: 0.22498906400340032, acc: 0.8629099088433764; test loss: 0.7409549265487119, acc: 0.7244150319073505
epoch: 143, train loss: 0.21504419915906434, acc: 0.8649224576772819; test loss: 0.7486413230120614, acc: 0.7383597258331364
epoch: 144, train loss: 0.2100946773798406, acc: 0.8706049485024269; test loss: 0.7405333269913127, acc: 0.7456865989127865
epoch: 145, train loss: 0.21749919268369985, acc: 0.8637977980348053; test loss: 0.760124467727113, acc: 0.734341763176554
epoch: 146, train loss: 0.1942315995428595, acc: 0.8710192967917604; test loss: 0.7428621227536689, acc: 0.7414322855116994
epoch: 147, train loss: 0.2058533540213194, acc: 0.8688291701195691; test loss: 0.7484510398536561, acc: 0.7367052706216024
epoch: 148, train loss: 0.20516932322010284, acc: 0.8703681780513792; test loss: 0.72989084021303, acc: 0.7452138974237769
epoch: 149, train loss: 0.19973038172064214, acc: 0.8723807268852847; test loss: 0.7435334241579002, acc: 0.7395414795556606
epoch: 150, train loss: 0.19710452315002078, acc: 0.8737421569788091; test loss: 0.7418946126650705, acc: 0.7428503899787284
epoch: 151, train loss: 0.20376125354696, acc: 0.8711968746300461; test loss: 0.7130141617755353, acc: 0.7426140392342235
epoch: 152, train loss: 0.19377704589722833, acc: 0.8730318456256659; test loss: 0.7585073371103711, acc: 0.7329236587095249
epoch: 153, train loss: 0.2003064047758207, acc: 0.8694802888599503; test loss: 0.7444310668398939, acc: 0.7409595840226897
epoch: 154, train loss: 0.20722548964391366, acc: 0.8674677400260448; test loss: 0.7292979599820387, acc: 0.7378870243441267
epoch: 155, train loss: 0.2154663282836755, acc: 0.8647448798389961; test loss: 0.7200777731277621, acc: 0.7303238005199716
epoch: 156, train loss: 0.19483105173241644, acc: 0.8742748904936664; test loss: 0.7496360771144939, acc: 0.7456865989127865
epoch: 157, train loss: 0.20401551291951628, acc: 0.8683556292174737; test loss: 0.7357739804343844, acc: 0.7350508154100686
epoch: 158, train loss: 0.20143866775740643, acc: 0.8719071859831893; test loss: 0.7150987207227439, acc: 0.7530134719924367
epoch: 159, train loss: 0.18796675971298935, acc: 0.8768793654551912; test loss: 0.7545039555728366, acc: 0.7367052706216024
epoch: 160, train loss: 0.19650069328614406, acc: 0.8722623416597609; test loss: 0.7474934535791788, acc: 0.7371779721106122
epoch: 161, train loss: 0.18846941045597346, acc: 0.8760506688765242; test loss: 0.7749554449941103, acc: 0.7409595840226897
epoch: 162, train loss: 0.1846275376867116, acc: 0.8782407955487155; test loss: 0.8014994535398833, acc: 0.7293783975419522
epoch: 163, train loss: 0.17688105090512346, acc: 0.8841600568249083; test loss: 0.8088576770566986, acc: 0.7383597258331364
epoch: 164, train loss: 0.19722772453774556, acc: 0.8724399194980467; test loss: 0.7866785783797002, acc: 0.7199243677617585
epoch: 165, train loss: 0.1944078031260967, acc: 0.8733278086894756; test loss: 0.8052666840387613, acc: 0.7284329945639328
epoch: 166, train loss: 0.1914787175204608, acc: 0.8741565052681425; test loss: 0.7766393216556197, acc: 0.7312692034979911
epoch: 167, train loss: 0.18976138442017362, acc: 0.8766425950041434; test loss: 0.784792693223099, acc: 0.7336327109430395
epoch: 168, train loss: 0.17899984900162771, acc: 0.8841600568249083; test loss: 0.7676197420871729, acc: 0.7492318600803592
epoch: 169, train loss: 0.17958839410393143, acc: 0.8809044631230023; test loss: 0.7958286217103695, acc: 0.735759867643583
Epoch   169: reducing learning rate of group 0 to 3.7500e-04.
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.11177846000288258, acc: 0.8955250384751983; test loss: 0.6785610076018017, acc: 0.7523044197589223
epoch: 171, train loss: 0.10136802815676187, acc: 0.9036936190363443; test loss: 0.6868210708353993, acc: 0.7551406286929804
epoch: 172, train loss: 0.08914154033428312, acc: 0.9124541257251095; test loss: 0.7094141378157022, acc: 0.748050106357835
epoch: 173, train loss: 0.08873872265327297, acc: 0.9125725109506334; test loss: 0.7193029512330111, acc: 0.7501772630583786
epoch: 174, train loss: 0.08464313578511097, acc: 0.9131052444654907; test loss: 0.7290167327863729, acc: 0.7456865989127865
epoch: 175, train loss: 0.0819885421465371, acc: 0.9155321415887298; test loss: 0.7396255522240873, acc: 0.7485228078468447
epoch: 176, train loss: 0.08800682785635348, acc: 0.9135195927548242; test loss: 0.7241946146917017, acc: 0.748050106357835
epoch: 177, train loss: 0.09020793535423036, acc: 0.9074819462531076; test loss: 0.7342281962534238, acc: 0.7421413377452138
epoch: 178, train loss: 0.08861934184336558, acc: 0.9086657985083462; test loss: 0.7392880100078038, acc: 0.7447411959347672
epoch: 179, train loss: 0.09060334807191923, acc: 0.9094353024742512; test loss: 0.7343462892977292, acc: 0.7421413377452138
epoch: 180, train loss: 0.09064734945801627, acc: 0.9089025689593939; test loss: 0.7415859663185006, acc: 0.7468683526353108
epoch: 181, train loss: 0.0895641882784468, acc: 0.9115070439209186; test loss: 0.7136389994502941, acc: 0.752777121247932
epoch: 182, train loss: 0.08999277312739426, acc: 0.9084882206700604; test loss: 0.7441404624854214, acc: 0.7426140392342235
epoch: 183, train loss: 0.08979285515362347, acc: 0.9080738723807269; test loss: 0.7562828122005765, acc: 0.7466320018908059
epoch: 184, train loss: 0.08789878085077452, acc: 0.9082514502190127; test loss: 0.7470833425176986, acc: 0.752540770503427
epoch: 185, train loss: 0.08923057368398066, acc: 0.908724991121108; test loss: 0.7712565749792082, acc: 0.7362325691325927
epoch: 186, train loss: 0.08938963100890994, acc: 0.9075411388658695; test loss: 0.7283987135674141, acc: 0.7456865989127865
epoch: 187, train loss: 0.09150324420871375, acc: 0.9074227536403456; test loss: 0.751489126268106, acc: 0.7407232332781848
epoch: 188, train loss: 0.09606082778548833, acc: 0.90387119687463; test loss: 0.7472602237457081, acc: 0.7369416213661073
epoch: 189, train loss: 0.09028391703756883, acc: 0.9059429383212975; test loss: 0.7636848018964507, acc: 0.7345781139210589
epoch: 190, train loss: 0.09201349866655545, acc: 0.9061205161595833; test loss: 0.759308729180769, acc: 0.7393051288111557
epoch: 191, train loss: 0.09409133024267632, acc: 0.9060613235468213; test loss: 0.7565877509156572, acc: 0.7359962183880879
epoch: 192, train loss: 0.08718128513482615, acc: 0.9080146797679649; test loss: 0.775526145535196, acc: 0.7400141810446703
epoch: 193, train loss: 0.08889231949734154, acc: 0.90878418373387; test loss: 0.7404918821673392, acc: 0.743559442212243
epoch: 194, train loss: 0.08624848049886837, acc: 0.9121581626612999; test loss: 0.7445525719014445, acc: 0.7419049870007091
epoch: 195, train loss: 0.08066147113267637, acc: 0.9106191547294897; test loss: 0.7691181347291821, acc: 0.7419049870007091
epoch: 196, train loss: 0.08663780115191098, acc: 0.9073635610275838; test loss: 0.7367962893795048, acc: 0.7367052706216024
epoch: 197, train loss: 0.08335027770504551, acc: 0.9117438143719664; test loss: 0.7684767591674343, acc: 0.7414322855116994
epoch: 198, train loss: 0.08713820828313416, acc: 0.9067716348999645; test loss: 0.7874143449332072, acc: 0.7397778303001654
epoch: 199, train loss: 0.08663628969663019, acc: 0.9065940570616787; test loss: 0.7614090180064567, acc: 0.7348144646655637
epoch: 200, train loss: 0.08287807204101838, acc: 0.9102639990529182; test loss: 0.7698726141264015, acc: 0.7395414795556606
best test acc 0.7551406286929804 at epoch 171.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9670    0.9815    0.9742      6100
           1     0.9775    0.9384    0.9576       926
           2     0.8868    0.9762    0.9294      2400
           3     0.9542    0.9395    0.9468       843
           4     0.9490    0.9858    0.9670       774
           5     0.9584    0.9755    0.9669      1512
           6     0.8789    0.8346    0.8562      1330
           7     0.9295    0.8503    0.8882       481
           8     0.8808    0.9192    0.8996       458
           9     0.9252    0.9845    0.9539       452
          10     0.9310    0.9414    0.9362       717
          11     0.9310    0.9730    0.9515       333
          12     0.7143    0.0502    0.0938       299
          13     0.8683    0.7844    0.8242       269

    accuracy                         0.9376     16894
   macro avg     0.9109    0.8668    0.8675     16894
weighted avg     0.9342    0.9376    0.9302     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8227    0.8702    0.8458      1525
           1     0.8174    0.8103    0.8139       232
           2     0.7190    0.7621    0.7399       601
           3     0.8478    0.7393    0.7899       211
           4     0.8495    0.8144    0.8316       194
           5     0.7893    0.8228    0.8057       378
           6     0.4955    0.4955    0.4955       333
           7     0.6979    0.5537    0.6175       121
           8     0.5118    0.5652    0.5372       115
           9     0.7727    0.7456    0.7589       114
          10     0.7435    0.7889    0.7655       180
          11     0.5921    0.5357    0.5625        84
          12     0.1667    0.0133    0.0247        75
          13     0.5625    0.3971    0.4655        68

    accuracy                         0.7551      4231
   macro avg     0.6706    0.6367    0.6467      4231
weighted avg     0.7443    0.7551    0.7474      4231

---------------------------------------
program finished.
