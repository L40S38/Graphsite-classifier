seed:  666
save trained model at:  ../trained_models/trained_classifier_model_12.pt
save loss at:  ./results/train_classifier_results_12.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=96, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=96, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=96, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b579e033280>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=1, reduction=mean)
begin training...
epoch: 1, train loss: 2.273051981404615, acc: 0.3268055952041108, val loss: 2.030100101402737, acc: 0.4016546570589805, test loss: 2.045621624680617, acc: 0.3947507953340403
epoch: 2, train loss: 1.9793753805822079, acc: 0.41198972309449045, val loss: 1.8631850849128195, acc: 0.42273819055244194, test loss: 1.8657242531235372, acc: 0.42762460233297983
epoch: 3, train loss: 1.8900932282473133, acc: 0.43482729089351985, val loss: 1.8055476046639314, acc: 0.455564451561249, test loss: 1.8119094718172593, acc: 0.45121951219512196
epoch: 4, train loss: 1.8116525946975672, acc: 0.4545246931201827, val loss: 1.8012136778005574, acc: 0.46650653856418467, test loss: 1.7922986676306132, acc: 0.45811240721102864
epoch: 5, train loss: 1.7711184973380514, acc: 0.46468741079075077, val loss: 1.697280104522359, acc: 0.4851881505204163, test loss: 1.693979832931389, acc: 0.48197242841993637
epoch: 6, train loss: 1.7220195630057484, acc: 0.48153011704253496, val loss: 1.7373531319664293, acc: 0.4686415799306112, test loss: 1.7207621451020874, acc: 0.48064687168610815
epoch: 7, train loss: 1.692413876487499, acc: 0.49174992863260064, val loss: 1.6454574645663185, acc: 0.4955964771817454, test loss: 1.627925772176441, acc: 0.49946977730646874
epoch: 8, train loss: 1.6668793461372469, acc: 0.4936340279760206, val loss: 1.660161413190713, acc: 0.4913263944488924, test loss: 1.649555193657334, acc: 0.5042417815482503
epoch: 9, train loss: 1.6128226775834467, acc: 0.5100770767913217, val loss: 1.6364333027039715, acc: 0.5105417667467307, test loss: 1.625900196751001, acc: 0.5127253446447508
epoch: 10, train loss: 1.6089775200336618, acc: 0.5106480159862975, val loss: 1.6314690552109046, acc: 0.4921270349613024, test loss: 1.6206811523639764, acc: 0.49469777306468715
epoch: 11, train loss: 1.5591036973937458, acc: 0.5248073080216957, val loss: 1.573165194206248, acc: 0.5076060848678943, test loss: 1.5627022790757115, acc: 0.5164369034994698
epoch: 12, train loss: 1.531208419874672, acc: 0.5351984013702541, val loss: 1.6141568643428943, acc: 0.5049372831598612, test loss: 1.6075572823416264, acc: 0.49946977730646874
epoch: 13, train loss: 1.5168390780831962, acc: 0.5393662574935769, val loss: 1.5488380348011115, acc: 0.5196156925540433, test loss: 1.544392857546518, acc: 0.5209437963944857
epoch: 14, train loss: 1.5098706195465401, acc: 0.5415929203539823, val loss: 1.5947818166260914, acc: 0.5068054443554844, test loss: 1.5896633290789413, acc: 0.5079533404029692
epoch: 15, train loss: 1.471212356035416, acc: 0.5524407650585212, val loss: 1.9256654924668086, acc: 0.4355484387510008, test loss: 1.927372412110184, acc: 0.433457051961824
epoch: 16, train loss: 1.4364816901481665, acc: 0.5592349414787325, val loss: 1.5147690412869033, acc: 0.537496663997865, test loss: 1.5083970626616654, acc: 0.5397667020148462
epoch: 17, train loss: 1.4226384162426404, acc: 0.5673422780473879, val loss: 1.5162604827387096, acc: 0.5268214571657326, test loss: 1.4951122649021897, acc: 0.5373806998939554
epoch: 18, train loss: 1.4071087458254166, acc: 0.5686554381958322, val loss: 1.4448402575693164, acc: 0.5457699492927676, test loss: 1.416646269447351, acc: 0.563626723223754
epoch: 19, train loss: 1.3733079396347778, acc: 0.5818441335997716, val loss: 1.545956936014917, acc: 0.5361622631438484, test loss: 1.5351399371631578, acc: 0.5397667020148462
epoch: 20, train loss: 1.3784391580089446, acc: 0.5772766200399657, val loss: 1.4046035151434544, acc: 0.5647184414198025, test loss: 1.4124755849261805, acc: 0.5673382820784729
epoch: 21, train loss: 1.3422624981542877, acc: 0.5882386525834998, val loss: 1.445836101495841, acc: 0.5572457966373099, test loss: 1.4395948614320724, acc: 0.556998939554613
epoch: 22, train loss: 1.31832059805236, acc: 0.5945760776477306, val loss: 1.4134564044668352, acc: 0.5655190819322125, test loss: 1.3955645346312862, acc: 0.5676033934252386
epoch: 23, train loss: 1.3010966897317078, acc: 0.6018840993434199, val loss: 1.9158422939294428, acc: 0.4384841206298372, test loss: 1.9184637380682903, acc: 0.43478260869565216
epoch: 24, train loss: 1.294805114860709, acc: 0.6050242649157864, val loss: 1.3776937124727247, acc: 0.567654123298639, test loss: 1.366788436347581, acc: 0.5710498409331919
epoch 25, gamma increased to 1.
epoch: 25, train loss: 1.0416405479839248, acc: 0.6107907507850414, val loss: 1.1238801607870565, acc: 0.5874032559380838, test loss: 1.1421446104949757, acc: 0.5821845174973489
epoch: 26, train loss: 1.0164405534922856, acc: 0.6215244076505853, val loss: 1.086054353864154, acc: 0.5964771817453963, test loss: 1.0866435982389642, acc: 0.6018027571580064
epoch: 27, train loss: 1.0108842371192757, acc: 0.6188980873536968, val loss: 1.1021523112960583, acc: 0.5916733386709367, test loss: 1.1227955974930797, acc: 0.5959703075291622
epoch: 28, train loss: 1.0118415121143285, acc: 0.6218669711675706, val loss: 1.3977689072390318, acc: 0.5014678409394182, test loss: 1.4033070333689166, acc: 0.5119300106044539
epoch: 29, train loss: 0.9779432141675358, acc: 0.627976020553811, val loss: 1.1994936512030059, acc: 0.5596477181745396, test loss: 1.2006088696254273, acc: 0.5641569459172853
epoch: 30, train loss: 1.0213977172073214, acc: 0.6187268055952041, val loss: 1.1244355161762059, acc: 0.5756605284227382, test loss: 1.1539946353321744, acc: 0.5713149522799575
epoch: 31, train loss: 0.9616393279457582, acc: 0.6370539537539253, val loss: 1.0226874641014285, acc: 0.6362423271950894, test loss: 1.0394716682514988, acc: 0.6261930010604454
epoch: 32, train loss: 0.9686068207918832, acc: 0.6311732800456752, val loss: 1.1513566391163264, acc: 0.5679210034694422, test loss: 1.1661358954165673, acc: 0.5707847295864263
epoch: 33, train loss: 0.9511087601606144, acc: 0.6409934341992578, val loss: 1.001089706018762, acc: 0.6247664798505471, test loss: 1.0304280513164727, acc: 0.6275185577942736
epoch: 34, train loss: 0.9258030491966946, acc: 0.6475021410219811, val loss: 1.0543259395291908, acc: 0.611689351481185, test loss: 1.0784011380164267, acc: 0.5996818663838812
epoch: 35, train loss: 0.9259753451495725, acc: 0.6491007707679132, val loss: 1.016694439994134, acc: 0.6135575126768081, test loss: 1.0183170921349955, acc: 0.6285790031813362
epoch: 36, train loss: 0.9104882277218641, acc: 0.6512132457893235, val loss: 1.007213870970637, acc: 0.6111555911395783, test loss: 1.013061085030484, acc: 0.6195652173913043
epoch: 37, train loss: 0.9019337005898369, acc: 0.6557807593491293, val loss: 1.0924226342438697, acc: 0.5863357352548706, test loss: 1.131440496646966, acc: 0.5768822905620361
epoch: 38, train loss: 0.8883170799973553, acc: 0.6601198972309449, val loss: 1.2490036100713864, acc: 0.5751267680811316, test loss: 1.2630566597490522, acc: 0.5800636267232238
epoch: 39, train loss: 0.9018127761021907, acc: 0.6553811019126463, val loss: 1.096609726886225, acc: 0.5972778222578062, test loss: 1.0943468236468124, acc: 0.6031283138918345
epoch: 40, train loss: 0.8795254298594554, acc: 0.6637168141592921, val loss: 1.2422067547658937, acc: 0.5743261275687216, test loss: 1.2571118947430608, acc: 0.5625662778366914
epoch: 41, train loss: 0.8666780879692864, acc: 0.6634313445618042, val loss: 0.9679163360646607, acc: 0.6325060048038431, test loss: 0.9951314809845715, acc: 0.6283138918345705
epoch: 42, train loss: 0.8480609513540592, acc: 0.6722809020839281, val loss: 1.065048553862825, acc: 0.6135575126768081, test loss: 1.0883260961554841, acc: 0.6044538706256628
Epoch    42: reducing learning rate of group 0 to 1.5000e-03.
epoch: 43, train loss: 0.7711388694875893, acc: 0.695575221238938, val loss: 0.8840009171643955, acc: 0.6621297037630104, test loss: 0.9216099930719758, acc: 0.6577412513255567
epoch: 44, train loss: 0.7286618527466319, acc: 0.716186126177562, val loss: 0.8872666562561166, acc: 0.6688017080330931, test loss: 0.9284186466880466, acc: 0.6670201484623541
epoch: 45, train loss: 0.7121335242293475, acc: 0.7185269768769627, val loss: 0.9768617407048197, acc: 0.6503869762476648, test loss: 1.0139848589265081, acc: 0.6423647932131495
epoch: 46, train loss: 0.7046645740866491, acc: 0.7200685127033971, val loss: 0.8915692588504804, acc: 0.677341873498799, test loss: 0.9045284221685578, acc: 0.6678154825026511
epoch: 47, train loss: 0.7065166090782322, acc: 0.7242363688267199, val loss: 0.9211805025227457, acc: 0.6637309847878302, test loss: 0.9575358901018808, acc: 0.6542948038176034
epoch: 48, train loss: 0.694923718837272, acc: 0.7248073080216957, val loss: 0.9087951904237828, acc: 0.6682679476914866, test loss: 0.9166201597053957, acc: 0.6670201484623541
epoch: 49, train loss: 0.6856777575708205, acc: 0.7265772195261204, val loss: 0.8557367454632652, acc: 0.677341873498799, test loss: 0.8741276213624697, acc: 0.6829268292682927
epoch: 50, train loss: 0.6742793352032879, acc: 0.7260633742506423, val loss: 0.905862677342293, acc: 0.671203629570323, test loss: 0.9437174933586242, acc: 0.6614528101802757
epoch: 51, train loss: 0.6832256612218246, acc: 0.7268055952041108, val loss: 0.9000422742037828, acc: 0.670402989057913, test loss: 0.918572649849561, acc: 0.6633085896076352
epoch: 52, train loss: 0.6558909127006455, acc: 0.7354838709677419, val loss: 0.9091227763425136, acc: 0.6621297037630104, test loss: 0.9517819350496456, acc: 0.6595970307529162
epoch: 53, train loss: 0.6555970427481542, acc: 0.7339423351413075, val loss: 0.9025525355739914, acc: 0.671203629570323, test loss: 0.9246939500341487, acc: 0.6622481442205727
epoch: 54, train loss: 0.646933412990875, acc: 0.7353696831287468, val loss: 0.8888153681158224, acc: 0.6720042700827329, test loss: 0.8856478366973165, acc: 0.6755037115588547
epoch: 55, train loss: 0.649500441687331, acc: 0.7387382243791036, val loss: 0.935775868187022, acc: 0.6631972244462236, test loss: 0.9305806529104773, acc: 0.676829268292683
epoch: 56, train loss: 0.6274474669584982, acc: 0.7465600913502712, val loss: 0.8610474085979599, acc: 0.688550840672538, test loss: 0.8813095047264059, acc: 0.6834570519618239
epoch: 57, train loss: 0.6189332369699023, acc: 0.7453611190408221, val loss: 0.8750653914651904, acc: 0.6818788364024553, test loss: 0.9094936918479135, acc: 0.6792152704135737
epoch: 58, train loss: 0.611973510603887, acc: 0.7495860690836426, val loss: 0.986373824630383, acc: 0.655724579663731, test loss: 0.9769262180348596, acc: 0.6614528101802757
epoch: 59, train loss: 0.6267156661186359, acc: 0.7470168427062518, val loss: 0.8958508132901992, acc: 0.6802775553776355, test loss: 0.92360556922055, acc: 0.6770943796394485
epoch: 60, train loss: 0.6109721722464679, acc: 0.7496431630031402, val loss: 0.888805664091515, acc: 0.685882038964505, test loss: 0.901144130611723, acc: 0.6829268292682927
epoch: 61, train loss: 0.5865018023819778, acc: 0.7594633171567228, val loss: 0.8882847110971248, acc: 0.6786762743528156, test loss: 0.9287396700991553, acc: 0.6781548250265111
epoch: 62, train loss: 0.5834723740041919, acc: 0.76117613474165, val loss: 0.9004087254051084, acc: 0.6717373899119295, test loss: 0.9311267118565225, acc: 0.676033934252386
epoch: 63, train loss: 0.5847737789596451, acc: 0.7588352840422495, val loss: 0.9054700829742875, acc: 0.6786762743528156, test loss: 0.9489666823996972, acc: 0.6699363732767762
epoch: 64, train loss: 0.5877290931423154, acc: 0.7558664002283757, val loss: 0.9128184987285279, acc: 0.685882038964505, test loss: 0.9246926343074406, acc: 0.6707317073170732
epoch: 65, train loss: 0.5850792828880308, acc: 0.7609477590636597, val loss: 0.9234609113937065, acc: 0.6816119562316519, test loss: 0.945659192217749, acc: 0.6831919406150583
epoch: 66, train loss: 0.5723659809473546, acc: 0.7616328860976306, val loss: 0.9654401589432556, acc: 0.6733386709367494, test loss: 0.9911740503786477, acc: 0.6646341463414634
epoch: 67, train loss: 0.5425204140371572, acc: 0.7752783328575507, val loss: 0.9183437544280827, acc: 0.6845476381104884, test loss: 0.9322411212536841, acc: 0.6823966065747614
epoch: 68, train loss: 0.5483742200712324, acc: 0.771624322009706, val loss: 0.8945069921980675, acc: 0.6757405924739792, test loss: 0.9265025754242914, acc: 0.6765641569459173
epoch: 69, train loss: 0.5332883705726665, acc: 0.7774479017984585, val loss: 0.9071309422377685, acc: 0.678142514011209, test loss: 0.9088882070078198, acc: 0.6855779427359491
epoch: 70, train loss: 0.5443928162506435, acc: 0.772423636882672, val loss: 0.90207972658899, acc: 0.6800106752068321, test loss: 0.9026396332717523, acc: 0.6831919406150583
epoch: 71, train loss: 0.5183945364639686, acc: 0.7777904653154439, val loss: 0.9601475940820915, acc: 0.6690685882038965, test loss: 0.9680323004090521, acc: 0.6675503711558854
epoch: 72, train loss: 0.5201795319165974, acc: 0.784870111333143, val loss: 0.9524418696868505, acc: 0.6877502001601281, test loss: 0.9822322712976126, acc: 0.672322375397667
epoch: 73, train loss: 0.518939995006803, acc: 0.7837282329431916, val loss: 0.9463147142902514, acc: 0.6869495596477182, test loss: 0.9516465704362693, acc: 0.6834570519618239
epoch: 74, train loss: 0.5275314725129359, acc: 0.7776762774764487, val loss: 0.9320240954192187, acc: 0.6744061916199626, test loss: 0.9423831722769227, acc: 0.672322375397667
epoch: 75, train loss: 0.5188777300347065, acc: 0.7804738795318299, val loss: 0.9426686504156327, acc: 0.6752068321323725, test loss: 0.9672860383734597, acc: 0.6709968186638389
epoch: 76, train loss: 0.49801277746788064, acc: 0.7915500999143591, val loss: 0.9959209910831294, acc: 0.66693354683747, test loss: 0.997188474315348, acc: 0.6601272534464475
epoch: 77, train loss: 0.5352010708382019, acc: 0.7784184984299172, val loss: 0.9376252313470853, acc: 0.677341873498799, test loss: 0.9472364036806821, acc: 0.6808059384941676
epoch: 78, train loss: 0.5059500230828796, acc: 0.7893234370539538, val loss: 1.015379571583801, acc: 0.6709367493995196, test loss: 1.029051700047951, acc: 0.6709968186638389
epoch: 79, train loss: 0.5030821410785428, acc: 0.792634884384813, val loss: 0.9242847506319773, acc: 0.6845476381104884, test loss: 0.9266168485896332, acc: 0.6784199363732768
epoch: 80, train loss: 0.49370673340523274, acc: 0.7918926634313446, val loss: 0.9559164855331939, acc: 0.6688017080330931, test loss: 0.9801319039891911, acc: 0.6667550371155886
epoch: 81, train loss: 0.4916247270156954, acc: 0.7890950613759634, val loss: 0.9309582049158818, acc: 0.6954897251134241, test loss: 0.9405463740529842, acc: 0.6943266171792153
epoch: 82, train loss: 0.5040854586589824, acc: 0.7920639451898372, val loss: 0.9559961557706451, acc: 0.6714705097411262, test loss: 0.9866245488220409, acc: 0.6643690349946978
epoch: 83, train loss: 0.48906902304520583, acc: 0.795717956037682, val loss: 0.8748142815985169, acc: 0.6941553242594075, test loss: 0.9057551302560946, acc: 0.7025450689289502
epoch: 84, train loss: 0.49260715050184145, acc: 0.7910362546388809, val loss: 0.9002171916137036, acc: 0.6802775553776355, test loss: 0.9303766968141558, acc: 0.6808059384941676
epoch: 85, train loss: 0.4721367981853806, acc: 0.8016557236654296, val loss: 0.9081291672004392, acc: 0.6944222044302109, test loss: 0.9297626122184421, acc: 0.690880169671262
epoch: 86, train loss: 0.44704294225810903, acc: 0.8077076791321725, val loss: 0.9950572302858767, acc: 0.6749399519615693, test loss: 1.0245882860549813, acc: 0.6699363732767762
epoch: 87, train loss: 0.4584972965693494, acc: 0.8038252926063374, val loss: 0.9258195860786123, acc: 0.6882839605017347, test loss: 0.9506140597172532, acc: 0.679745493107105
epoch: 88, train loss: 0.4392896532280732, acc: 0.808963745361119, val loss: 0.938695042172145, acc: 0.6981585268214572, test loss: 0.9562431206000319, acc: 0.6948568398727466
epoch: 89, train loss: 0.4616416004906713, acc: 0.8038252926063374, val loss: 0.9868757508422713, acc: 0.6869495596477182, test loss: 1.0354221468892102, acc: 0.679745493107105
epoch: 90, train loss: 0.44983671617242493, acc: 0.8043962318013131, val loss: 0.9576885629597428, acc: 0.6968241259674406, test loss: 0.9761737598973392, acc: 0.690880169671262
epoch: 91, train loss: 0.4440536136205898, acc: 0.8119897230944905, val loss: 0.9916114293005042, acc: 0.6794769148652255, test loss: 1.024878260692382, acc: 0.6648992576882291
epoch: 92, train loss: 0.44743684259374383, acc: 0.8073080216956894, val loss: 1.011193651309292, acc: 0.6816119562316519, test loss: 0.9658279072568373, acc: 0.689289501590668
epoch: 93, train loss: 0.4101858657783962, acc: 0.819811590065658, val loss: 0.971184721812586, acc: 0.7002935681878837, test loss: 0.9647884093311913, acc: 0.7020148462354189
epoch: 94, train loss: 0.4238572199753683, acc: 0.8162717670568085, val loss: 0.9144172703453086, acc: 0.6981585268214572, test loss: 0.936406578517036, acc: 0.6927359490986215
epoch: 95, train loss: 0.4159103435582513, acc: 0.8187268055952042, val loss: 1.024976827699342, acc: 0.6792100346944222, test loss: 1.0255558098866857, acc: 0.6834570519618239
epoch: 96, train loss: 0.40607780295373913, acc: 0.819811590065658, val loss: 1.025673175195328, acc: 0.671203629570323, test loss: 1.0372649847133795, acc: 0.6694061505832449
epoch: 97, train loss: 0.43992389307136437, acc: 0.8113616899800171, val loss: 0.928838764663566, acc: 0.6949559647718174, test loss: 0.9336378389805524, acc: 0.6967126193001061
epoch: 98, train loss: 0.4210214872769278, acc: 0.8170710819297745, val loss: 1.0030805804998613, acc: 0.6976247664798505, test loss: 0.9829390059093909, acc: 0.693796394485684
epoch: 99, train loss: 0.4017577505353312, acc: 0.8260348272908935, val loss: 0.9443298800646353, acc: 0.7029623698959168, test loss: 0.9651215981577512, acc: 0.7009544008483564
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3161232221783348, acc: 0.8202683414216386, val loss: 0.9158300896596998, acc: 0.6818788364024553, test loss: 0.9020248378448243, acc: 0.6842523860021209
epoch: 101, train loss: 0.3035879224002821, acc: 0.825578075934913, val loss: 0.875107298095862, acc: 0.692820923405391, test loss: 0.8772244866807033, acc: 0.6922057264050901
epoch: 102, train loss: 0.3089061558774087, acc: 0.824892948900942, val loss: 0.9116287777383772, acc: 0.6722711502535361, test loss: 0.8841303933336778, acc: 0.6800106044538706
epoch: 103, train loss: 0.30213519998220184, acc: 0.822894661718527, val loss: 0.8854078194730467, acc: 0.6901521216973578, test loss: 0.9209959529743215, acc: 0.6876988335100742
epoch: 104, train loss: 0.3251550511091463, acc: 0.8183271481587211, val loss: 1.079248853705869, acc: 0.6266346410461703, test loss: 1.0870876755972303, acc: 0.6166489925768823
epoch: 105, train loss: 0.4087801974193389, acc: 0.7826434484727377, val loss: 0.9094486715063783, acc: 0.6637309847878302, test loss: 0.901165524317882, acc: 0.6659597030752916
epoch: 106, train loss: 0.31848656177656876, acc: 0.8185555238367114, val loss: 0.7886881029443611, acc: 0.6960234854550307, test loss: 0.8051941614767906, acc: 0.6988335100742312
epoch: 107, train loss: 0.2938126365355073, acc: 0.8293462746217528, val loss: 0.8578877143297381, acc: 0.6901521216973578, test loss: 0.8651614419223143, acc: 0.6983032873806999
epoch: 108, train loss: 0.30106596816123776, acc: 0.8246074793034541, val loss: 0.8189971399587219, acc: 0.7045636509207366, test loss: 0.8169723458406402, acc: 0.6990986214209968
epoch: 109, train loss: 0.27108132429133813, acc: 0.8385383956608621, val loss: 0.8490998805094185, acc: 0.6925540432345877, test loss: 0.8683310532241206, acc: 0.6898197242841994
epoch: 110, train loss: 0.2954951684712479, acc: 0.8221524407650586, val loss: 0.8199044780028735, acc: 0.7077662129703763, test loss: 0.8365615541836363, acc: 0.7051961823966065
epoch: 111, train loss: 0.29089593824202287, acc: 0.8284327719097916, val loss: 0.8147480246986998, acc: 0.7013610888710968, test loss: 0.8342669879897149, acc: 0.6969777306468717
epoch: 112, train loss: 0.2879879651241836, acc: 0.8255209820154153, val loss: 0.8815763499980486, acc: 0.6842807579396851, test loss: 0.8790119280870747, acc: 0.6831919406150583
epoch: 113, train loss: 0.2966658270498565, acc: 0.8257493576934056, val loss: 0.821581341853166, acc: 0.6933546837469976, test loss: 0.8355332796292998, acc: 0.6956521739130435
epoch: 114, train loss: 0.26153732362455073, acc: 0.8371681415929203, val loss: 0.9517930899742988, acc: 0.6784093941820123, test loss: 0.9568243137978547, acc: 0.6762990455991517
epoch: 115, train loss: 0.281698969258365, acc: 0.8287753354267771, val loss: 0.8614554516245341, acc: 0.7000266880170803, test loss: 0.874937126770505, acc: 0.7030752916224814
epoch: 116, train loss: 0.26512781329540197, acc: 0.8341992577790466, val loss: 0.9125850214969644, acc: 0.6792100346944222, test loss: 0.9172818325736489, acc: 0.6725874867444327
epoch: 117, train loss: 0.29838109491145715, acc: 0.826377390807879, val loss: 0.9284461512068033, acc: 0.6869495596477182, test loss: 0.9385708260763741, acc: 0.6869034994697774
epoch: 118, train loss: 0.3033557472128954, acc: 0.8190122751926919, val loss: 0.9268915825663868, acc: 0.6805444355484388, test loss: 0.9192680195052955, acc: 0.6712619300106044
epoch: 119, train loss: 0.29740637255130137, acc: 0.8224950042820439, val loss: 0.8333965975155918, acc: 0.6917534027221778, test loss: 0.8334930715480008, acc: 0.6967126193001061
epoch: 120, train loss: 0.2685403282799586, acc: 0.835341136168998, val loss: 0.8271796693157952, acc: 0.6984254069922605, test loss: 0.8407590497968559, acc: 0.6983032873806999
epoch: 121, train loss: 0.2892641584736396, acc: 0.8272337996003426, val loss: 0.8421652871257373, acc: 0.7048305310915399, test loss: 0.8566590953353607, acc: 0.6906150583244963
Epoch   121: reducing learning rate of group 0 to 7.5000e-04.
epoch: 122, train loss: 0.22328818935072495, acc: 0.8578932343705395, val loss: 0.8692198265448297, acc: 0.7149719775820657, test loss: 0.8853113598181359, acc: 0.7104984093319194
epoch: 123, train loss: 0.17568422945744305, acc: 0.8782186697116757, val loss: 0.870385756580423, acc: 0.7144382172404591, test loss: 0.9254362277236242, acc: 0.7112937433722163
epoch: 124, train loss: 0.17499786590018887, acc: 0.8770196974022266, val loss: 0.9327659298332017, acc: 0.7165732586068855, test loss: 0.9278986590538146, acc: 0.7168610816542949
epoch: 125, train loss: 0.16855571177340628, acc: 0.8827861832714816, val loss: 0.9443603442896834, acc: 0.7104350146784094, test loss: 0.9527483398562904, acc: 0.7128844114528102
epoch: 126, train loss: 0.16643159666164856, acc: 0.8833000285469598, val loss: 0.9311390784634951, acc: 0.7192420603149186, test loss: 0.9432428623939749, acc: 0.7163308589607635
epoch: 127, train loss: 0.1649880284722655, acc: 0.8836425920639452, val loss: 0.9712592737497188, acc: 0.7066986922871631, test loss: 0.9537945523115383, acc: 0.7028101802757158
epoch: 128, train loss: 0.15922123939011595, acc: 0.8880959177847559, val loss: 0.9797187793213493, acc: 0.7058980517747532, test loss: 1.0438476558201892, acc: 0.6990986214209968
epoch: 129, train loss: 0.17363884815344835, acc: 0.882329431915501, val loss: 0.9730349125975382, acc: 0.7099012543368027, test loss: 0.9607335742432643, acc: 0.7171261930010604
epoch: 130, train loss: 0.1576525575436424, acc: 0.8874678846702826, val loss: 0.9517489270843249, acc: 0.7115025353616227, test loss: 0.9578734587004936, acc: 0.7118239660657476
epoch: 131, train loss: 0.16107844812101613, acc: 0.8850699400513845, val loss: 0.9898595068656447, acc: 0.7048305310915399, test loss: 0.9844541916285947, acc: 0.7075821845174973
epoch: 132, train loss: 0.17811543587494197, acc: 0.8801027690550957, val loss: 0.9967851326374617, acc: 0.7000266880170803, test loss: 1.0088961279910544, acc: 0.6996288441145281
epoch: 133, train loss: 0.16378731534221055, acc: 0.8869540393948044, val loss: 0.9631855006468656, acc: 0.7104350146784094, test loss: 0.9705273194884445, acc: 0.7112937433722163
epoch: 134, train loss: 0.14774285325264203, acc: 0.8954610333999429, val loss: 0.9914149550841845, acc: 0.7144382172404591, test loss: 1.0195697221118851, acc: 0.7028101802757158
epoch: 135, train loss: 0.15816500847369577, acc: 0.8871824150727947, val loss: 1.0059047947255713, acc: 0.6946890846010142, test loss: 1.0094780790464497, acc: 0.7110286320254506
epoch: 136, train loss: 0.17552627691397693, acc: 0.8781044818726805, val loss: 0.9450080109686096, acc: 0.7061649319455564, test loss: 1.0037896805072513, acc: 0.7057264050901378
epoch: 137, train loss: 0.14623625352999975, acc: 0.8950613759634599, val loss: 0.9984991462637273, acc: 0.7050974112623432, test loss: 0.9978695133205183, acc: 0.7089077412513256
epoch: 138, train loss: 0.17032408200346877, acc: 0.8848986582928918, val loss: 0.9787337621562093, acc: 0.7026954897251134, test loss: 1.0025284191710067, acc: 0.6975079533404029
epoch: 139, train loss: 0.16223735663419855, acc: 0.8811875535255496, val loss: 0.9858777769922032, acc: 0.7016279690419002, test loss: 1.002624946080488, acc: 0.707051961823966
epoch: 140, train loss: 0.14690351047891567, acc: 0.8942620610904939, val loss: 0.9827111179999807, acc: 0.7016279690419002, test loss: 0.9818389744419814, acc: 0.7107635206786851
epoch: 141, train loss: 0.14408551035627037, acc: 0.8958035969169283, val loss: 0.9782081438058976, acc: 0.7053642914331465, test loss: 1.0338010080040196, acc: 0.6993637327677624
epoch: 142, train loss: 0.1367248707257643, acc: 0.8968883813873822, val loss: 1.0924484920018127, acc: 0.692820923405391, test loss: 1.0692065258926198, acc: 0.7046659597030753
epoch: 143, train loss: 0.14150667971341976, acc: 0.8939194975735084, val loss: 1.0878655534443424, acc: 0.6970910061382439, test loss: 1.0903282759677568, acc: 0.70864262990456
epoch: 144, train loss: 0.15135984048311144, acc: 0.8910077076791322, val loss: 0.9887764275089912, acc: 0.6978916466506538, test loss: 1.0288972260464033, acc: 0.7012195121951219
epoch: 145, train loss: 0.15099334581099746, acc: 0.8884955752212389, val loss: 1.0937339402466797, acc: 0.6906858820389645, test loss: 1.073601112638779, acc: 0.7036055143160127
epoch: 146, train loss: 0.14513647997682175, acc: 0.8968312874678847, val loss: 1.0554784735521063, acc: 0.6864157993061115, test loss: 1.0406790639284687, acc: 0.7006892895015907
epoch: 147, train loss: 0.15881896400124967, acc: 0.8865543819583215, val loss: 0.9904690210043604, acc: 0.7061649319455564, test loss: 0.9985064431187457, acc: 0.7083775185577943
epoch: 148, train loss: 0.15138821729652682, acc: 0.8934056522980303, val loss: 1.088203626535465, acc: 0.6784093941820123, test loss: 1.088853550614633, acc: 0.6914103923647932
epoch: 149, train loss: 0.1717729331509168, acc: 0.8794176420211247, val loss: 0.9659127357453386, acc: 0.7147050974112623, test loss: 1.0184258916597984, acc: 0.7128844114528102
epoch: 150, train loss: 0.13250033942216333, acc: 0.9000285469597488, val loss: 0.9975294088852829, acc: 0.7037630104083267, test loss: 1.0347867808438056, acc: 0.704931071049841
epoch: 151, train loss: 0.13297541736365384, acc: 0.899457607764773, val loss: 1.0301216730983664, acc: 0.7082999733119829, test loss: 1.0489589250226794, acc: 0.7165959703075292
epoch: 152, train loss: 0.15229589584433212, acc: 0.8913502711961176, val loss: 1.0096818562154615, acc: 0.6976247664798505, test loss: 1.0476319559811282, acc: 0.6914103923647932
epoch: 153, train loss: 0.14603320409783355, acc: 0.890893519840137, val loss: 1.0358430985485614, acc: 0.707499332799573, test loss: 1.038947771108795, acc: 0.711558854718982
epoch: 154, train loss: 0.13634704504818226, acc: 0.8988295746502998, val loss: 1.0229152360533853, acc: 0.7013610888710968, test loss: 1.0789284668191241, acc: 0.7012195121951219
epoch: 155, train loss: 0.12806753031473447, acc: 0.899457607764773, val loss: 1.1483764453732175, acc: 0.685882038964505, test loss: 1.1460938435976225, acc: 0.6961823966065748
epoch: 156, train loss: 0.14283299548325457, acc: 0.8926634313445618, val loss: 1.0537867224754127, acc: 0.6994929276754737, test loss: 1.073237333166258, acc: 0.703340402969247
epoch: 157, train loss: 0.13613923233371444, acc: 0.8980302597773338, val loss: 0.9742830801175568, acc: 0.7091006138243928, test loss: 1.068044404336276, acc: 0.7054612937433722
epoch: 158, train loss: 0.11551443464392565, acc: 0.908021695689409, val loss: 1.133161027812627, acc: 0.6973578863090473, test loss: 1.13679251261253, acc: 0.7014846235418876
epoch: 159, train loss: 0.1260972147392199, acc: 0.9014558949471881, val loss: 1.0150634548267683, acc: 0.70402989057913, test loss: 1.0581627764352939, acc: 0.7022799575821845
epoch: 160, train loss: 0.13784510591882249, acc: 0.8961461604339138, val loss: 1.0000428098470393, acc: 0.7045636509207366, test loss: 1.0691300750663653, acc: 0.70864262990456
epoch: 161, train loss: 0.126656389618444, acc: 0.9020839280616614, val loss: 1.1336961160318229, acc: 0.6960234854550307, test loss: 1.1084579167411537, acc: 0.703340402969247
epoch: 162, train loss: 0.1327783371614042, acc: 0.8999143591207537, val loss: 1.0721576784590514, acc: 0.699759807846277, test loss: 1.0689371072095626, acc: 0.7065217391304348
epoch: 163, train loss: 0.1190627547423907, acc: 0.9036825578075935, val loss: 1.1261129763592774, acc: 0.6930878035761943, test loss: 1.1145119851619036, acc: 0.6927359490986215
epoch: 164, train loss: 0.15542469473692339, acc: 0.8899800171281759, val loss: 0.9694040569267751, acc: 0.7069655724579663, test loss: 1.0238746762402111, acc: 0.7107635206786851
epoch: 165, train loss: 0.13372551506667554, acc: 0.9005423922352269, val loss: 1.0265386632069735, acc: 0.7037630104083267, test loss: 1.0817361285046834, acc: 0.6964475079533404
epoch: 166, train loss: 0.1444946977493595, acc: 0.8931201827005424, val loss: 1.014082095512938, acc: 0.6938884440886042, test loss: 1.0089537361646634, acc: 0.7036055143160127
epoch: 167, train loss: 0.13652750066934297, acc: 0.8957465029974307, val loss: 1.0174035738397034, acc: 0.7144382172404591, test loss: 1.058491017633885, acc: 0.7036055143160127
epoch: 168, train loss: 0.13654020582816276, acc: 0.8985441050528119, val loss: 1.0718674534379498, acc: 0.707499332799573, test loss: 1.0413902238216783, acc: 0.7054612937433722
epoch: 169, train loss: 0.1383125903110929, acc: 0.8970596631458749, val loss: 1.0363541946176977, acc: 0.7024286095543101, test loss: 1.0573558506505487, acc: 0.7099681866383881
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.07795809146602801, acc: 0.9105338281473023, val loss: 0.9838890773568562, acc: 0.696290365625834, test loss: 0.9929308107523751, acc: 0.7067868504772005
epoch: 171, train loss: 0.08506812171198591, acc: 0.9059663145874964, val loss: 0.9137937289605244, acc: 0.707499332799573, test loss: 0.974212588586332, acc: 0.7097030752916225
epoch: 172, train loss: 0.07444777830758845, acc: 0.9151013417071082, val loss: 0.9840111841606973, acc: 0.6986922871630638, test loss: 1.0083782680466977, acc: 0.704135737009544
Epoch   172: reducing learning rate of group 0 to 3.7500e-04.
epoch: 173, train loss: 0.05808907731529511, acc: 0.9241221809877248, val loss: 0.9354147747544757, acc: 0.7144382172404591, test loss: 0.9620396520780481, acc: 0.7104984093319194
epoch: 174, train loss: 0.04795944396615913, acc: 0.934798743933771, val loss: 0.9504176324483455, acc: 0.7171070189484922, test loss: 0.9760294982004014, acc: 0.7120890774125133
epoch: 175, train loss: 0.047281610181322785, acc: 0.9354267770482444, val loss: 0.9904230471957929, acc: 0.7149719775820657, test loss: 1.0166498997810662, acc: 0.7152704135737009
epoch: 176, train loss: 0.05056940768931785, acc: 0.9318869540393948, val loss: 1.0265017124631102, acc: 0.7101681345076061, test loss: 1.0318386918793048, acc: 0.7168610816542949
epoch: 177, train loss: 0.04094137279579989, acc: 0.9395375392520696, val loss: 0.9953791514249811, acc: 0.7112356551908193, test loss: 1.029043362982579, acc: 0.721898197242842
epoch: 178, train loss: 0.04004264734121789, acc: 0.9413074507564945, val loss: 1.0201470792086753, acc: 0.7141713370696557, test loss: 1.0521764664230266, acc: 0.7187168610816543
epoch: 179, train loss: 0.04099831575585745, acc: 0.9394233514130745, val loss: 1.0339052680972292, acc: 0.7136375767280491, test loss: 1.055910836601055, acc: 0.718186638388123
epoch: 180, train loss: 0.037543456811726925, acc: 0.9419925777904653, val loss: 1.068027637340178, acc: 0.7131038163864425, test loss: 1.0574084584811585, acc: 0.7197773064687168
epoch: 181, train loss: 0.03827596386028398, acc: 0.9427918926634313, val loss: 1.0482402271609768, acc: 0.7082999733119829, test loss: 1.0780371423743562, acc: 0.7165959703075292
epoch: 182, train loss: 0.04230256948004339, acc: 0.9393662574935769, val loss: 1.052367262118716, acc: 0.7120362957032292, test loss: 1.0694861887368519, acc: 0.7163308589607635
epoch: 183, train loss: 0.03751716237837439, acc: 0.9433628318584071, val loss: 1.072703627894839, acc: 0.7069655724579663, test loss: 1.082614463561413, acc: 0.7118239660657476
epoch: 184, train loss: 0.03892036068917614, acc: 0.9427347987439337, val loss: 1.0943597826539022, acc: 0.7021617293835068, test loss: 1.0995060467138522, acc: 0.7150053022269353
epoch: 185, train loss: 0.0407380973581515, acc: 0.9415358264344847, val loss: 1.0628294814323342, acc: 0.707499332799573, test loss: 1.0809155314497072, acc: 0.7165959703075292
epoch: 186, train loss: 0.0492284447975646, acc: 0.936682843277191, val loss: 1.018845664363624, acc: 0.7061649319455564, test loss: 1.0769644488964707, acc: 0.7097030752916225
epoch: 187, train loss: 0.054498547927723455, acc: 0.9300599486154725, val loss: 1.019450386906103, acc: 0.7141713370696557, test loss: 1.0475556392811263, acc: 0.711558854718982
epoch: 188, train loss: 0.045503407245852286, acc: 0.9371966885526691, val loss: 1.0458478357494752, acc: 0.7141713370696557, test loss: 1.0493543261814218, acc: 0.7131495227995758
epoch: 189, train loss: 0.043366104091724735, acc: 0.9411932629174993, val loss: 1.067841814332305, acc: 0.7045636509207366, test loss: 1.0879607653188048, acc: 0.7102332979851538
epoch: 190, train loss: 0.05122432429093549, acc: 0.9350271196117613, val loss: 1.0318329610409722, acc: 0.7144382172404591, test loss: 1.0510713429617857, acc: 0.7150053022269353
epoch: 191, train loss: 0.046260218068748894, acc: 0.9371395946331715, val loss: 1.0702468204218323, acc: 0.7026954897251134, test loss: 1.0750430626429277, acc: 0.7142099681866384
epoch: 192, train loss: 0.04549402159186999, acc: 0.9393091635740793, val loss: 1.0406912588455979, acc: 0.7069655724579663, test loss: 1.06821931002628, acc: 0.7110286320254506
epoch: 193, train loss: 0.0407604720633846, acc: 0.940165572366543, val loss: 1.042434400261196, acc: 0.7155057379236722, test loss: 1.0929143418689296, acc: 0.7144750795334041
epoch: 194, train loss: 0.040912501906586345, acc: 0.9413074507564945, val loss: 1.0608253358108444, acc: 0.7093674939951962, test loss: 1.1009273587203607, acc: 0.7136797454931071
epoch: 195, train loss: 0.04193991855310094, acc: 0.9386811304596061, val loss: 1.07126369850458, acc: 0.7149719775820657, test loss: 1.1243070041640313, acc: 0.7083775185577943
epoch: 196, train loss: 0.050769566872818896, acc: 0.9332572081073366, val loss: 1.0465125557833554, acc: 0.7099012543368027, test loss: 1.0692779603211775, acc: 0.7120890774125133
epoch: 197, train loss: 0.045146605173775105, acc: 0.9365686554381958, val loss: 1.0593313180767188, acc: 0.7088337336535896, test loss: 1.117737638103368, acc: 0.7051961823966065
epoch: 198, train loss: 0.047612316861148904, acc: 0.9335997716243221, val loss: 1.0558846532932624, acc: 0.7010942087002936, test loss: 1.0774364367775295, acc: 0.7075821845174973
epoch: 199, train loss: 0.06256171207988803, acc: 0.9214387667713388, val loss: 1.0411744552324191, acc: 0.7058980517747532, test loss: 1.0587130515724579, acc: 0.7104984093319194
epoch: 200, train loss: 0.057127968194891035, acc: 0.9284042249500428, val loss: 1.0180440234730712, acc: 0.7088337336535896, test loss: 1.0356995263003594, acc: 0.7118239660657476
best val acc 0.7192420603149186 at epoch 126.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9682    0.9882    0.9781      5337
           1     0.8582    0.8489    0.8535      2502
           2     0.9740    0.9247    0.9487       810
           3     0.8867    0.9527    0.9185      1840
           4     0.9797    0.9172    0.9474       737
           5     0.9143    0.9926    0.9518       677
           6     0.7841    0.9720    0.8680      1323
           7     0.8553    0.8148    0.8346       907
           8     0.9519    0.9406    0.9462       421
           9     0.9419    0.9701    0.9558       401
          10     0.9493    0.9924    0.9704       396
          11     0.9848    0.9759    0.9803       332
          12     0.9313    0.9186    0.9249       295
          13     0.9150    0.9622    0.9380       291
          14     0.8800    0.1686    0.2830       261
          15     0.9231    0.4858    0.6366       494
          16     0.7621    0.6133    0.6797       256
          17     0.9227    0.9149    0.9188       235

    accuracy                         0.9125     17515
   macro avg     0.9102    0.8530    0.8630     17515
weighted avg     0.9140    0.9125    0.9065     17515

train confusion matrix:
[[9.88195616e-01 2.24845419e-03 1.87371182e-04 1.49896946e-03
  0.00000000e+00 0.00000000e+00 1.87371182e-04 1.12422709e-03
  3.18531010e-03 1.12422709e-03 3.74742365e-04 0.00000000e+00
  0.00000000e+00 1.87371182e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.07833733e-02 8.48920863e-01 0.00000000e+00 2.07833733e-02
  1.59872102e-03 0.00000000e+00 9.67226219e-02 1.19904077e-03
  0.00000000e+00 2.39808153e-03 0.00000000e+00 1.19904077e-03
  0.00000000e+00 0.00000000e+00 1.59872102e-03 3.99680256e-03
  0.00000000e+00 7.99360512e-04]
 [0.00000000e+00 0.00000000e+00 9.24691358e-01 0.00000000e+00
  0.00000000e+00 6.79012346e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.46913580e-03 0.00000000e+00 0.00000000e+00
  3.70370370e-03 1.23456790e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.79347826e-02 2.17391304e-02 0.00000000e+00 9.52717391e-01
  0.00000000e+00 0.00000000e+00 5.43478261e-04 2.17391304e-03
  0.00000000e+00 0.00000000e+00 5.43478261e-04 0.00000000e+00
  0.00000000e+00 0.00000000e+00 5.43478261e-04 3.80434783e-03
  0.00000000e+00 0.00000000e+00]
 [1.35685210e-03 5.83446404e-02 0.00000000e+00 0.00000000e+00
  9.17232022e-01 0.00000000e+00 4.07055631e-03 1.08548168e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  6.78426052e-03 1.35685210e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 5.90841950e-03 0.00000000e+00
  0.00000000e+00 9.92614476e-01 1.47710487e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.02343159e-03 1.58730159e-02 1.51171580e-03 0.00000000e+00
  0.00000000e+00 3.77928949e-03 9.72033258e-01 7.55857899e-04
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  7.55857899e-04 0.00000000e+00 0.00000000e+00 2.26757370e-03
  0.00000000e+00 0.00000000e+00]
 [2.86659316e-02 6.06394708e-02 2.20507166e-03 4.41014333e-03
  4.41014333e-03 2.20507166e-03 2.20507166e-03 8.14773980e-01
  1.10253583e-03 0.00000000e+00 7.71775083e-03 0.00000000e+00
  1.10253583e-02 5.51267916e-03 1.10253583e-03 0.00000000e+00
  5.07166483e-02 3.30760750e-03]
 [4.75059382e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.37529691e-03
  9.40617577e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.50118765e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.49376559e-03 9.97506234e-03 7.48129676e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.70074813e-01 0.00000000e+00 4.98753117e-03
  0.00000000e+00 4.98753117e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 2.52525253e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.92424242e-01 0.00000000e+00
  0.00000000e+00 5.05050505e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 3.01204819e-03 3.01204819e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.80722892e-02 0.00000000e+00 9.75903614e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 6.77966102e-03 1.69491525e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.74576271e-02
  0.00000000e+00 3.38983051e-03 0.00000000e+00 0.00000000e+00
  9.18644068e-01 3.38983051e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 3.38983051e-03]
 [3.43642612e-03 0.00000000e+00 6.87285223e-03 0.00000000e+00
  3.43642612e-03 3.43642612e-03 0.00000000e+00 0.00000000e+00
  6.87285223e-03 6.87285223e-03 3.43642612e-03 0.00000000e+00
  3.43642612e-03 9.62199313e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [8.42911877e-02 5.67049808e-01 0.00000000e+00 1.37931034e-01
  1.14942529e-02 0.00000000e+00 0.00000000e+00 2.68199234e-02
  0.00000000e+00 0.00000000e+00 3.83141762e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.68582375e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.21457490e-02 4.04858300e-02 0.00000000e+00 2.48987854e-01
  0.00000000e+00 0.00000000e+00 2.10526316e-01 0.00000000e+00
  0.00000000e+00 2.02429150e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.85829960e-01
  0.00000000e+00 0.00000000e+00]
 [2.34375000e-02 1.56250000e-02 0.00000000e+00 3.90625000e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.73437500e-01
  0.00000000e+00 0.00000000e+00 2.34375000e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  6.13281250e-01 4.68750000e-02]
 [4.25531915e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  8.51063830e-03 0.00000000e+00 0.00000000e+00 4.68085106e-02
  0.00000000e+00 0.00000000e+00 1.27659574e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.27659574e-02 9.14893617e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8402    0.8784    0.8589      1143
           1     0.5899    0.5877    0.5888       536
           2     0.8581    0.7341    0.7913       173
           3     0.7030    0.7208    0.7118       394
           4     0.8239    0.7405    0.7800       158
           5     0.7791    0.8759    0.8247       145
           6     0.6138    0.8198    0.7020       283
           7     0.4293    0.4536    0.4411       194
           8     0.7778    0.7000    0.7368        90
           9     0.6104    0.5529    0.5802        85
          10     0.8372    0.8571    0.8471        84
          11     0.9375    0.8451    0.8889        71
          12     0.5577    0.4603    0.5043        63
          13     0.5897    0.7419    0.6571        62
          14     0.3000    0.0536    0.0909        56
          15     0.6383    0.2857    0.3947       105
          16     0.5000    0.2545    0.3373        55
          17     0.6727    0.7400    0.7048        50

    accuracy                         0.7192      3747
   macro avg     0.6699    0.6279    0.6356      3747
weighted avg     0.7146    0.7192    0.7111      3747

validation confusion matrix:
[[8.78390201e-01 2.53718285e-02 2.62467192e-03 1.83727034e-02
  1.74978128e-03 8.74890639e-04 3.49956255e-03 2.79965004e-02
  9.62379703e-03 9.62379703e-03 4.37445319e-03 0.00000000e+00
  2.62467192e-03 1.04986877e-02 0.00000000e+00 8.74890639e-04
  0.00000000e+00 3.49956255e-03]
 [9.51492537e-02 5.87686567e-01 1.30597015e-02 8.95522388e-02
  1.86567164e-02 3.73134328e-03 1.15671642e-01 3.35820896e-02
  1.86567164e-03 9.32835821e-03 0.00000000e+00 1.86567164e-03
  5.59701493e-03 0.00000000e+00 3.73134328e-03 1.49253731e-02
  0.00000000e+00 5.59701493e-03]
 [2.31213873e-02 1.15606936e-02 7.34104046e-01 5.78034682e-03
  0.00000000e+00 1.15606936e-01 6.35838150e-02 5.78034682e-03
  0.00000000e+00 1.15606936e-02 0.00000000e+00 5.78034682e-03
  5.78034682e-03 1.73410405e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [5.32994924e-02 1.19289340e-01 0.00000000e+00 7.20812183e-01
  5.07614213e-03 0.00000000e+00 1.52284264e-02 3.80710660e-02
  5.07614213e-03 5.07614213e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.53807107e-03 1.01522843e-02 1.77664975e-02
  5.07614213e-03 2.53807107e-03]
 [6.32911392e-03 1.07594937e-01 6.32911392e-03 1.26582278e-02
  7.40506329e-01 0.00000000e+00 3.79746835e-02 4.43037975e-02
  0.00000000e+00 0.00000000e+00 6.32911392e-03 0.00000000e+00
  6.32911392e-03 1.89873418e-02 0.00000000e+00 0.00000000e+00
  1.26582278e-02 0.00000000e+00]
 [6.89655172e-03 0.00000000e+00 3.44827586e-02 0.00000000e+00
  0.00000000e+00 8.75862069e-01 7.58620690e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.89655172e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.41342756e-02 8.12720848e-02 7.06713781e-03 7.06713781e-03
  3.53356890e-03 3.18021201e-02 8.19787986e-01 3.53356890e-03
  3.53356890e-03 7.06713781e-03 0.00000000e+00 3.53356890e-03
  1.41342756e-02 3.53356890e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.90721649e-01 1.18556701e-01 1.03092784e-02 3.60824742e-02
  2.57731959e-02 5.15463918e-03 2.57731959e-02 4.53608247e-01
  0.00000000e+00 0.00000000e+00 3.60824742e-02 0.00000000e+00
  2.06185567e-02 1.03092784e-02 5.15463918e-03 5.15463918e-03
  4.12371134e-02 1.54639175e-02]
 [1.33333333e-01 2.22222222e-02 0.00000000e+00 1.11111111e-02
  1.11111111e-02 0.00000000e+00 0.00000000e+00 2.22222222e-02
  7.00000000e-01 2.22222222e-02 0.00000000e+00 0.00000000e+00
  1.11111111e-02 5.55555556e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.11111111e-02]
 [2.23529412e-01 9.41176471e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 8.23529412e-02 0.00000000e+00
  0.00000000e+00 5.52941176e-01 0.00000000e+00 0.00000000e+00
  1.17647059e-02 3.52941176e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.76190476e-02 1.19047619e-02 0.00000000e+00 1.19047619e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.57142857e-02
  1.19047619e-02 0.00000000e+00 8.57142857e-01 0.00000000e+00
  1.19047619e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.19047619e-02]
 [0.00000000e+00 9.85915493e-02 0.00000000e+00 1.40845070e-02
  1.40845070e-02 1.40845070e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.40845070e-02 0.00000000e+00 8.45070423e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.76190476e-02 2.22222222e-01 0.00000000e+00 0.00000000e+00
  1.58730159e-02 0.00000000e+00 3.17460317e-02 1.90476190e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.60317460e-01 1.58730159e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.58730159e-02]
 [1.29032258e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.61290323e-02 1.61290323e-02 0.00000000e+00
  3.22580645e-02 3.22580645e-02 0.00000000e+00 0.00000000e+00
  3.22580645e-02 7.41935484e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.42857143e-01 4.28571429e-01 0.00000000e+00 2.14285714e-01
  0.00000000e+00 0.00000000e+00 5.35714286e-02 7.14285714e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.78571429e-02 0.00000000e+00 5.35714286e-02 0.00000000e+00
  0.00000000e+00 1.78571429e-02]
 [9.52380952e-02 1.33333333e-01 0.00000000e+00 2.00000000e-01
  0.00000000e+00 9.52380952e-03 2.57142857e-01 0.00000000e+00
  0.00000000e+00 1.90476190e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.85714286e-01
  0.00000000e+00 0.00000000e+00]
 [7.27272727e-02 9.09090909e-02 1.81818182e-02 5.45454545e-02
  1.81818182e-02 0.00000000e+00 1.81818182e-02 3.63636364e-01
  0.00000000e+00 1.81818182e-02 1.81818182e-02 0.00000000e+00
  1.81818182e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  2.54545455e-01 5.45454545e-02]
 [8.00000000e-02 6.00000000e-02 0.00000000e+00 0.00000000e+00
  2.00000000e-02 0.00000000e+00 0.00000000e+00 4.00000000e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.00000000e-02 0.00000000e+00 0.00000000e+00
  4.00000000e-02 7.40000000e-01]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8372    0.8620    0.8494      1145
           1     0.5704    0.6182    0.5934       537
           2     0.8831    0.7771    0.8267       175
           3     0.6927    0.6962    0.6944       395
           4     0.8370    0.7107    0.7687       159
           5     0.7716    0.8562    0.8117       146
           6     0.6875    0.8134    0.7452       284
           7     0.4425    0.5128    0.4751       195
           8     0.7273    0.7912    0.7579        91
           9     0.5789    0.6322    0.6044        87
          10     0.8222    0.8605    0.8409        86
          11     0.9667    0.8056    0.8788        72
          12     0.6250    0.4688    0.5357        64
          13     0.5938    0.5938    0.5938        64
          14     0.1333    0.0351    0.0556        57
          15     0.6346    0.3084    0.4151       107
          16     0.4375    0.2500    0.3182        56
          17     0.5870    0.5192    0.5510        52

    accuracy                         0.7163      3772
   macro avg     0.6571    0.6173    0.6287      3772
weighted avg     0.7120    0.7163    0.7101      3772

test confusion matrix:
[[0.86200873 0.0419214  0.00174672 0.00960699 0.00087336 0.00087336
  0.00349345 0.02008734 0.01484716 0.01484716 0.00349345 0.
  0.00174672 0.01135371 0.00087336 0.         0.00524017 0.0069869 ]
 [0.08193669 0.61824953 0.01117318 0.09497207 0.00744879 0.00558659
  0.09310987 0.03538175 0.00372439 0.01117318 0.0018622  0.00372439
  0.         0.00558659 0.01117318 0.00931099 0.0018622  0.00372439]
 [0.03428571 0.02285714 0.77714286 0.01142857 0.         0.11428571
  0.         0.         0.         0.02285714 0.         0.
  0.00571429 0.01142857 0.         0.         0.         0.        ]
 [0.07594937 0.13164557 0.00253165 0.69620253 0.00506329 0.00253165
  0.00506329 0.03291139 0.00253165 0.         0.00506329 0.
  0.         0.         0.00506329 0.02531646 0.00253165 0.00759494]
 [0.01257862 0.11949686 0.         0.01886792 0.71069182 0.
  0.02515723 0.05031447 0.00628931 0.00628931 0.00628931 0.
  0.03144654 0.00628931 0.         0.         0.00628931 0.        ]
 [0.02054795 0.02739726 0.02054795 0.00684932 0.         0.85616438
  0.06849315 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.01760563 0.08802817 0.01056338 0.00352113 0.00352113 0.02816901
  0.81338028 0.00704225 0.00352113 0.00704225 0.00352113 0.
  0.00352113 0.00352113 0.         0.00352113 0.         0.00352113]
 [0.0974359  0.12820513 0.         0.06153846 0.02564103 0.00512821
  0.04615385 0.51282051 0.01025641 0.         0.01538462 0.
  0.02051282 0.00512821 0.01025641 0.01025641 0.04102564 0.01025641]
 [0.17582418 0.         0.         0.01098901 0.01098901 0.
  0.         0.01098901 0.79120879 0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.26436782 0.03448276 0.         0.         0.01149425 0.
  0.02298851 0.01149425 0.         0.63218391 0.         0.
  0.         0.02298851 0.         0.         0.         0.        ]
 [0.03488372 0.01162791 0.         0.01162791 0.01162791 0.
  0.         0.05813953 0.         0.         0.86046512 0.
  0.         0.         0.         0.         0.01162791 0.        ]
 [0.05555556 0.01388889 0.         0.         0.         0.
  0.         0.01388889 0.         0.08333333 0.         0.80555556
  0.         0.02777778 0.         0.         0.         0.        ]
 [0.046875   0.203125   0.03125    0.         0.015625   0.
  0.         0.203125   0.         0.         0.         0.
  0.46875    0.         0.         0.         0.         0.03125   ]
 [0.15625    0.0625     0.015625   0.         0.015625   0.
  0.         0.         0.03125    0.046875   0.015625   0.
  0.03125    0.59375    0.03125    0.         0.         0.        ]
 [0.15789474 0.35087719 0.         0.21052632 0.01754386 0.
  0.07017544 0.10526316 0.01754386 0.         0.         0.
  0.         0.01754386 0.03508772 0.01754386 0.         0.        ]
 [0.04672897 0.23364486 0.         0.18691589 0.00934579 0.02803738
  0.17757009 0.         0.         0.00934579 0.         0.
  0.         0.         0.         0.30841121 0.         0.        ]
 [0.08928571 0.08928571 0.         0.08928571 0.01785714 0.
  0.01785714 0.33928571 0.         0.         0.05357143 0.
  0.03571429 0.         0.         0.         0.25       0.01785714]
 [0.09615385 0.01923077 0.         0.03846154 0.01923077 0.
  0.         0.28846154 0.         0.         0.         0.
  0.01923077 0.         0.         0.         0.         0.51923077]]
---------------------------------------
program finished.
