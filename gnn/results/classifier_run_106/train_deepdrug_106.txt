seed:  6
save trained model at:  ../trained_models/trained_classifier_model_106.pt
save loss at:  ./results/train_classifier_results_106.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['2y3zA03', '2z02B00', '5nwlH00', '3n3xA02', '1rgcA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['6mn8A02', '5eomI01', '3u4oB00', '6azrC00', '1rffC01']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2ae68e8d6d60>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.029060615411634, acc: 0.3939268379306263; test loss: 1.7379326526224401, acc: 0.45284802647128336
epoch: 2, train loss: 1.725940209924222, acc: 0.47212027938913226; test loss: 1.629744226333521, acc: 0.48404632474592296
epoch: 3, train loss: 1.6306377259093345, acc: 0.49857937729371377; test loss: 1.5438362085292368, acc: 0.5173717797211062
epoch: 4, train loss: 1.5698708993691135, acc: 0.5198887178880076; test loss: 1.5576314730611542, acc: 0.5038997872843299
epoch: 5, train loss: 1.5293568139303728, acc: 0.533384633597727; test loss: 1.4733230331721134, acc: 0.5376979437485228
epoch: 6, train loss: 1.472726631181486, acc: 0.5530957736474488; test loss: 1.4783072230438903, acc: 0.5504608839517845
epoch: 7, train loss: 1.4717485986405665, acc: 0.5541020480644016; test loss: 1.4788735105027209, acc: 0.5462065705506972
epoch: 8, train loss: 1.4229025417521846, acc: 0.5684858529655499; test loss: 1.4130635098682964, acc: 0.566296383833609
epoch: 9, train loss: 1.3886171894495736, acc: 0.5801467976796496; test loss: 1.3478649486973617, acc: 0.5904041597731032
epoch: 10, train loss: 1.3647057357533834, acc: 0.5870131407600332; test loss: 1.3277739051159774, acc: 0.5868588986055306
epoch: 11, train loss: 1.3364273782903744, acc: 0.59121581626613; test loss: 1.4094838369712568, acc: 0.5696052942566769
epoch: 12, train loss: 1.3498302389210155, acc: 0.5916893571682255; test loss: 1.305813855861158, acc: 0.595367525407705
epoch: 13, train loss: 1.2980065087220987, acc: 0.6012193678228958; test loss: 1.3133193211926266, acc: 0.5963129283857244
epoch: 14, train loss: 1.2784552325352248, acc: 0.6104534154137564; test loss: 1.3604744312414405, acc: 0.578586622547861
epoch: 15, train loss: 1.2683359850253157, acc: 0.6148336687581389; test loss: 1.283598710053132, acc: 0.6008035925313164
epoch: 16, train loss: 1.2502603078433236, acc: 0.618089262460045; test loss: 1.3144903853553263, acc: 0.5901678090285984
epoch: 17, train loss: 1.2252649617954388, acc: 0.6258434947318575; test loss: 1.30896839817149, acc: 0.6097849208225006
epoch: 18, train loss: 1.2275919287645338, acc: 0.6271457322126198; test loss: 1.3220235984169317, acc: 0.5932403687071615
epoch: 19, train loss: 1.213530982473524, acc: 0.6306380963655736; test loss: 1.2746780372291444, acc: 0.6211297565587331
epoch: 20, train loss: 1.2030875265944057, acc: 0.6342488457440512; test loss: 1.1980965122830582, acc: 0.6383833609075868
epoch: 21, train loss: 1.1818821172651008, acc: 0.6407600331478631; test loss: 1.2747312676847418, acc: 0.6093122193334909
epoch: 22, train loss: 1.1815648937671301, acc: 0.6438972416242453; test loss: 1.3058819083338002, acc: 0.5979673835972583
epoch: 23, train loss: 1.1726092343162726, acc: 0.6446667455901504; test loss: 1.2194546977556842, acc: 0.6308201370834318
epoch: 24, train loss: 1.159832949128619, acc: 0.6518290517343436; test loss: 1.3266005245570285, acc: 0.6036398014653747
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9344994120870496, acc: 0.6503492364152954; test loss: 0.9767282643630357, acc: 0.6395651146301111
epoch: 26, train loss: 0.9112383507265344, acc: 0.6562093050787262; test loss: 1.0202617853760183, acc: 0.6126211297565587
epoch: 27, train loss: 0.9144767050674233, acc: 0.6570971942701551; test loss: 0.9588362537245185, acc: 0.6360198534625384
epoch: 28, train loss: 0.9068326398199291, acc: 0.6557357641766307; test loss: 0.9350161991962387, acc: 0.6414559205861499
epoch: 29, train loss: 0.89205537771521, acc: 0.6626612998697763; test loss: 0.9252209026631495, acc: 0.6516190025998582
epoch: 30, train loss: 0.8793329955544488, acc: 0.6664496270865397; test loss: 0.9342604914728109, acc: 0.6431103757976837
epoch: 31, train loss: 0.8930725780791667, acc: 0.6625429146442524; test loss: 0.9565466028442058, acc: 0.645710233987237
epoch: 32, train loss: 0.877828572930225, acc: 0.6676334793417782; test loss: 0.9305598618600476, acc: 0.6416922713306547
epoch: 33, train loss: 0.8661015275753333, acc: 0.6710666508819699; test loss: 0.93344731940583, acc: 0.6398014653746159
epoch: 34, train loss: 0.8614209471079701, acc: 0.6758020599029241; test loss: 0.9316563946618676, acc: 0.6485464429212952
epoch: 35, train loss: 0.8576337830179526, acc: 0.6714218065585414; test loss: 1.0790029431989756, acc: 0.6076577641219569
epoch: 36, train loss: 0.8445038894118395, acc: 0.6775778382857819; test loss: 0.9229024718259137, acc: 0.6367289056960529
epoch: 37, train loss: 0.8409987069508501, acc: 0.6738487036817805; test loss: 0.8942916501472814, acc: 0.6520917040888679
epoch: 38, train loss: 0.8359483670479901, acc: 0.6789984609920682; test loss: 1.103004710890108, acc: 0.5967856298747342
epoch: 39, train loss: 0.8301313711127205, acc: 0.679116846217592; test loss: 0.9528523180057072, acc: 0.6483100921767904
epoch: 40, train loss: 0.8408355327898389, acc: 0.6747957854859713; test loss: 0.953821143143793, acc: 0.6327109430394706
epoch: 41, train loss: 0.8248932859848784, acc: 0.6813069728897834; test loss: 1.232940152755896, acc: 0.5407705034270858
epoch: 42, train loss: 0.8075915169783622, acc: 0.6869894637149284; test loss: 1.0591598528548916, acc: 0.598440085086268
epoch: 43, train loss: 0.818114737490867, acc: 0.6843257961406416; test loss: 0.859135661387776, acc: 0.6660363980146538
epoch: 44, train loss: 0.8035861908987286, acc: 0.6868710784894045; test loss: 0.8557352302937157, acc: 0.676435830772867
epoch: 45, train loss: 0.7918654843676199, acc: 0.6975257487865515; test loss: 1.0564366566376944, acc: 0.6133301819900733
epoch: 46, train loss: 0.786886033644066, acc: 0.6943885403101693; test loss: 0.9415356067020937, acc: 0.6445284802647129
epoch: 47, train loss: 0.7882505623817557, acc: 0.6965786669823606; test loss: 1.1870856586569902, acc: 0.5703143464901914
epoch: 48, train loss: 0.7859169667791979, acc: 0.694566118148455; test loss: 0.8610460577324417, acc: 0.6702907114157409
epoch: 49, train loss: 0.7809910241056225, acc: 0.6968154374334083; test loss: 0.8726394377654113, acc: 0.6705270621602458
epoch: 50, train loss: 0.7628442912013118, acc: 0.6969930152716941; test loss: 1.0080970069585018, acc: 0.6220751595367525
epoch: 51, train loss: 0.7721226884371235, acc: 0.702734698709601; test loss: 0.8586623367759472, acc: 0.676199480028362
epoch: 52, train loss: 0.7587907623790263, acc: 0.7036817805137919; test loss: 0.9746694371537434, acc: 0.6327109430394706
epoch: 53, train loss: 0.7614181746925082, acc: 0.7034450100627442; test loss: 0.8417732229866167, acc: 0.6726542188607895
epoch: 54, train loss: 0.7513865210028587, acc: 0.7086539599857937; test loss: 0.8511584906171215, acc: 0.6721815173717797
epoch: 55, train loss: 0.7465071022559923, acc: 0.7062862554753166; test loss: 0.9133214428193235, acc: 0.662491136847081
Epoch    55: reducing learning rate of group 0 to 1.5000e-03.
epoch: 56, train loss: 0.6830566410864827, acc: 0.729667337516278; test loss: 0.7772830975757257, acc: 0.7024344126683999
epoch: 57, train loss: 0.6520504591746402, acc: 0.7416834379069492; test loss: 0.7615740965850188, acc: 0.7040888678799339
epoch: 58, train loss: 0.6349256401077817, acc: 0.7487273588256186; test loss: 0.7896296226845878, acc: 0.7005436067123612
epoch: 59, train loss: 0.6384543635456523, acc: 0.7478394696341897; test loss: 0.8332146281357551, acc: 0.6877806665090995
epoch: 60, train loss: 0.6425884168882179, acc: 0.7431040606132354; test loss: 0.7554558505123881, acc: 0.7085795320255259
epoch: 61, train loss: 0.6331245475610191, acc: 0.7454125725109506; test loss: 1.344865040811797, acc: 0.5530607421413377
epoch: 62, train loss: 0.6241152690362123, acc: 0.7496744406298094; test loss: 0.7955965076149905, acc: 0.7010163082013708
epoch: 63, train loss: 0.6148702793182305, acc: 0.7521013377530484; test loss: 0.8236321838323006, acc: 0.6995982037343418
epoch: 64, train loss: 0.6098997110909536, acc: 0.753285190008287; test loss: 0.7656062569524683, acc: 0.7071614275584968
epoch: 65, train loss: 0.6108239596449188, acc: 0.7534627678465727; test loss: 0.8727187075194555, acc: 0.6769085322618766
epoch: 66, train loss: 0.6060381068892066, acc: 0.756599976322955; test loss: 0.781324859996463, acc: 0.7085795320255259
epoch: 67, train loss: 0.5848043939411619, acc: 0.7637030898543862; test loss: 0.8573241558967541, acc: 0.6813991964074687
epoch: 68, train loss: 0.6690512362832753, acc: 0.7315023085118977; test loss: 0.7860973953101912, acc: 0.7081068305365162
epoch: 69, train loss: 0.5978343635885704, acc: 0.760625073990766; test loss: 1.0960535698820586, acc: 0.6142755849680926
epoch: 70, train loss: 0.5918251520517354, acc: 0.7626376228246715; test loss: 0.7979352759909218, acc: 0.6960529425667691
epoch: 71, train loss: 0.5837817560416982, acc: 0.7657156386882917; test loss: 0.7762839033428418, acc: 0.7116520917040888
epoch: 72, train loss: 0.576361641159026, acc: 0.7667811057180064; test loss: 1.1378793736153308, acc: 0.6175844953911604
epoch: 73, train loss: 0.5713641738713783, acc: 0.7666035278797206; test loss: 0.7872773125165463, acc: 0.7017253604348853
epoch: 74, train loss: 0.5632312627280127, acc: 0.7702142772581981; test loss: 0.7673558921178332, acc: 0.7111793902150791
epoch: 75, train loss: 0.5611923553611423, acc: 0.7699775068071505; test loss: 0.9315183730137825, acc: 0.6589458756795084
epoch: 76, train loss: 0.5522925018332788, acc: 0.772877944832485; test loss: 0.8107605560314455, acc: 0.6960529425667691
epoch: 77, train loss: 0.5580536862640137, acc: 0.7732331005090565; test loss: 0.8008531132253792, acc: 0.7026707634129048
epoch: 78, train loss: 0.5582166587063105, acc: 0.7718124778027702; test loss: 0.77243356303497, acc: 0.7107066887260695
epoch: 79, train loss: 0.5402171755699318, acc: 0.7799218657511543; test loss: 0.7716869185985726, acc: 0.7107066887260695
epoch: 80, train loss: 0.5465023375000632, acc: 0.7766662720492482; test loss: 0.8094043707458778, acc: 0.7014890096903805
epoch: 81, train loss: 0.5331489765370289, acc: 0.7810465253936308; test loss: 0.757016747309451, acc: 0.7227605766958166
epoch: 82, train loss: 0.5329425274669538, acc: 0.7803954066532497; test loss: 0.7508114769339871, acc: 0.711415740959584
epoch: 83, train loss: 0.5201044055333685, acc: 0.7838877708062034; test loss: 0.9389651983067714, acc: 0.6728905696052943
epoch: 84, train loss: 0.5252605207547558, acc: 0.785071623061442; test loss: 0.8089800512106022, acc: 0.7019617111793902
epoch: 85, train loss: 0.517688006335523, acc: 0.7857819344145851; test loss: 0.7827785912768211, acc: 0.7161427558496809
epoch: 86, train loss: 0.5157819445583119, acc: 0.7828814963892506; test loss: 0.8618919653859833, acc: 0.680926494918459
epoch: 87, train loss: 0.5166440202566919, acc: 0.7869065940570616; test loss: 0.806105540058512, acc: 0.7092885842590404
epoch: 88, train loss: 0.5197509622184344, acc: 0.7854859713507755; test loss: 0.792321553553737, acc: 0.7102339872370598
epoch: 89, train loss: 0.502830308478895, acc: 0.7935361666863976; test loss: 0.851602231658849, acc: 0.69463483809974
epoch: 90, train loss: 0.5130011128104903, acc: 0.7879720610867764; test loss: 0.7810822719942732, acc: 0.7090522335145356
epoch: 91, train loss: 0.4968764678576973, acc: 0.7943648632650645; test loss: 0.919698788859945, acc: 0.6617820846135666
epoch: 92, train loss: 0.48976789086235056, acc: 0.8005800876050669; test loss: 0.904659542961905, acc: 0.6821082486409832
epoch: 93, train loss: 0.48956169326502685, acc: 0.795548715520303; test loss: 0.8063696291226355, acc: 0.7156700543606712
epoch: 94, train loss: 0.46894284587111346, acc: 0.8027110216644963; test loss: 0.7676464919079227, acc: 0.7135428976601277
epoch: 95, train loss: 0.4764092605579982, acc: 0.8036581034686872; test loss: 0.7636177063664487, acc: 0.7199243677617585
epoch: 96, train loss: 0.47476601657097445, acc: 0.8043684148218302; test loss: 0.8370360344246864, acc: 0.708343181281021
epoch: 97, train loss: 0.4752028019481599, acc: 0.8034213330176394; test loss: 0.807881368584341, acc: 0.7050342708579532
epoch: 98, train loss: 0.48751200149366103, acc: 0.7969693382265893; test loss: 0.7582899734670179, acc: 0.7177972110612149
epoch: 99, train loss: 0.4710741807147993, acc: 0.8024150586006866; test loss: 0.9555217456772547, acc: 0.6740723233278185
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3627868236642456, acc: 0.8079199715875459; test loss: 0.6564429854422533, acc: 0.7194516662727488
epoch: 101, train loss: 0.35031321818015415, acc: 0.8107020243873565; test loss: 0.6382103264402256, acc: 0.7307965020089813
epoch: 102, train loss: 0.34731686920389787, acc: 0.8116491061915473; test loss: 0.678446220861375, acc: 0.7142519498936422
epoch: 103, train loss: 0.35877540114505396, acc: 0.8067953119450693; test loss: 0.6750330344578583, acc: 0.7135428976601277
epoch: 104, train loss: 0.3564123765694343, acc: 0.8040132591452587; test loss: 0.6408168533907183, acc: 0.7234696289293311
epoch: 105, train loss: 0.3507061538977695, acc: 0.8103468687107849; test loss: 0.6750545845446477, acc: 0.7142519498936422
epoch: 106, train loss: 0.3323245014687946, acc: 0.819225760625074; test loss: 0.6596031404223405, acc: 0.7260694871188844
epoch: 107, train loss: 0.34648392563023145, acc: 0.8075056232982124; test loss: 0.7074288898144341, acc: 0.6882533679981092
epoch: 108, train loss: 0.35488150157391624, acc: 0.8079199715875459; test loss: 0.6607350657888407, acc: 0.7239423304183408
epoch: 109, train loss: 0.3366112158935536, acc: 0.809458979519356; test loss: 0.7066115643278811, acc: 0.7081068305365162
epoch: 110, train loss: 0.34849099369815106, acc: 0.8073872380726885; test loss: 0.6248945448308993, acc: 0.7307965020089813
epoch: 111, train loss: 0.34206514062084953, acc: 0.8106428317745945; test loss: 0.6584340923645625, acc: 0.7109430394705744
epoch: 112, train loss: 0.34244511278688633, acc: 0.8141943885403101; test loss: 0.6595557041693398, acc: 0.7199243677617585
Epoch   112: reducing learning rate of group 0 to 7.5000e-04.
epoch: 113, train loss: 0.287861836752878, acc: 0.8345566473304131; test loss: 0.6206173809446417, acc: 0.7411959347671945
epoch: 114, train loss: 0.2601526407881066, acc: 0.8482893334911803; test loss: 0.6841774399864902, acc: 0.7286693453084377
epoch: 115, train loss: 0.25342674517230734, acc: 0.8503018823250859; test loss: 0.6685909861736167, acc: 0.7352871661545733
epoch: 116, train loss: 0.2503701284240234, acc: 0.8505386527761335; test loss: 0.6867537921665691, acc: 0.7310328527534862
epoch: 117, train loss: 0.24395791770820013, acc: 0.8519592754824198; test loss: 0.6751142081795108, acc: 0.7381233750886316
epoch: 118, train loss: 0.23967771034727892, acc: 0.8567538771161359; test loss: 0.6662541824546837, acc: 0.7326873079650201
epoch: 119, train loss: 0.23747194173901381, acc: 0.8571682254054694; test loss: 0.7169569493805199, acc: 0.7218151737177972
epoch: 120, train loss: 0.24370360092727533, acc: 0.8524920089972772; test loss: 0.6956507083344645, acc: 0.737414322855117
epoch: 121, train loss: 0.23618242944770132, acc: 0.8527287794483248; test loss: 0.6684928704480194, acc: 0.7426140392342235
epoch: 122, train loss: 0.2345239541241865, acc: 0.8545637504439446; test loss: 0.8117259494296429, acc: 0.6917986291656819
epoch: 123, train loss: 0.23537628221923929, acc: 0.8564579140523263; test loss: 0.6746309637544398, acc: 0.7305601512644765
epoch: 124, train loss: 0.230230232837201, acc: 0.8604238191073754; test loss: 0.7093828643531953, acc: 0.7270148900969038
epoch: 125, train loss: 0.23075018843257303, acc: 0.858352077660708; test loss: 0.7052570655536945, acc: 0.7324509572205152
epoch: 126, train loss: 0.23093680611754136, acc: 0.8579377293713745; test loss: 0.6873932488842006, acc: 0.7359962183880879
epoch: 127, train loss: 0.22807525152657404, acc: 0.8579969219841364; test loss: 0.7107776159148103, acc: 0.7296147482864571
epoch: 128, train loss: 0.22632662076787496, acc: 0.8600686634308038; test loss: 0.7236559113214214, acc: 0.7196880170172536
epoch: 129, train loss: 0.22441321641570142, acc: 0.8601870486563277; test loss: 0.8105710540024053, acc: 0.7033798156464193
epoch: 130, train loss: 0.2189717305275312, acc: 0.8645081093879484; test loss: 0.7636039470617609, acc: 0.7263058378633893
epoch: 131, train loss: 0.23500219829459013, acc: 0.8537942464780396; test loss: 0.6787109033798275, acc: 0.7255967856298747
epoch: 132, train loss: 0.22472996366747994, acc: 0.8566946845033739; test loss: 0.6904456439071219, acc: 0.7322146064760104
epoch: 133, train loss: 0.22095470938214581, acc: 0.862377175328519; test loss: 0.7081390284274774, acc: 0.7258331363743796
epoch: 134, train loss: 0.21709611905006398, acc: 0.8644489167751864; test loss: 0.7206127401278448, acc: 0.7303238005199716
epoch: 135, train loss: 0.21428419592974102, acc: 0.8665206582218539; test loss: 0.6973339161447981, acc: 0.7359962183880879
epoch: 136, train loss: 0.21393367898447269, acc: 0.8630282940689002; test loss: 0.7283168672472837, acc: 0.7289056960529425
epoch: 137, train loss: 0.20267449040991237, acc: 0.867230969574997; test loss: 0.7310862759274812, acc: 0.7298510990309619
epoch: 138, train loss: 0.21727400002916197, acc: 0.863146679294424; test loss: 0.6989776692021911, acc: 0.7296147482864571
epoch: 139, train loss: 0.1992797685739231, acc: 0.8711968746300461; test loss: 0.7149541519460766, acc: 0.7329236587095249
epoch: 140, train loss: 0.2087911356514384, acc: 0.8652184207410916; test loss: 0.7412832391879092, acc: 0.7293783975419522
epoch: 141, train loss: 0.20288121612083, acc: 0.871374452468332; test loss: 0.7957935194066443, acc: 0.7144883006381471
epoch: 142, train loss: 0.1977956431094353, acc: 0.8719071859831893; test loss: 0.7554984537317126, acc: 0.7303238005199716
epoch: 143, train loss: 0.19780199120697275, acc: 0.8717888007576654; test loss: 0.7430570332949338, acc: 0.7232332781848263
epoch: 144, train loss: 0.19801563485827095, acc: 0.8719071859831893; test loss: 0.7364032287119805, acc: 0.7270148900969038
epoch: 145, train loss: 0.20500306724775214, acc: 0.8670533917367113; test loss: 0.8009223403786407, acc: 0.7055069723469629
epoch: 146, train loss: 0.20186878508987746, acc: 0.8684740144429975; test loss: 0.7238771976857736, acc: 0.7367052706216024
epoch: 147, train loss: 0.19767359948566118, acc: 0.8687107848940452; test loss: 0.7574517960188886, acc: 0.7293783975419522
epoch: 148, train loss: 0.18631221167191933, acc: 0.8747484313957619; test loss: 0.8038121519063951, acc: 0.7154337036161664
epoch: 149, train loss: 0.19429799323923364, acc: 0.8706049485024269; test loss: 0.7641583402453797, acc: 0.7305601512644765
epoch: 150, train loss: 0.184096157944201, acc: 0.8777080620338582; test loss: 0.7469053235185876, acc: 0.7369416213661073
epoch: 151, train loss: 0.18312344317381463, acc: 0.8759914762637623; test loss: 0.8014313468581479, acc: 0.7213424722287876
epoch: 152, train loss: 0.19664024639527614, acc: 0.8701314076003315; test loss: 0.7117991020876137, acc: 0.7381233750886316
epoch: 153, train loss: 0.19200971907654557, acc: 0.8727950751746182; test loss: 0.793253531497693, acc: 0.7289056960529425
epoch: 154, train loss: 0.17652268026289156, acc: 0.8820883153782408; test loss: 0.773627617939007, acc: 0.7265421886078941
epoch: 155, train loss: 0.18073528978765907, acc: 0.8788919142890967; test loss: 0.756788005898732, acc: 0.7411959347671945
epoch: 156, train loss: 0.17811225704311603, acc: 0.8810228483485261; test loss: 0.8109954177164012, acc: 0.7066887260694871
epoch: 157, train loss: 0.18894053943392594, acc: 0.8760506688765242; test loss: 0.7631272685863086, acc: 0.7310328527534862
epoch: 158, train loss: 0.18074358029663035, acc: 0.8777080620338582; test loss: 0.7415412294473019, acc: 0.7284329945639328
epoch: 159, train loss: 0.185044127419755, acc: 0.8767609802296673; test loss: 0.8056390568370995, acc: 0.722051524462302
epoch: 160, train loss: 0.19366647673031417, acc: 0.8711968746300461; test loss: 0.738959064271765, acc: 0.7319782557315055
epoch: 161, train loss: 0.17083557768094973, acc: 0.8842192494376702; test loss: 0.7443530580952951, acc: 0.7350508154100686
epoch: 162, train loss: 0.1653453132535235, acc: 0.8814371966378596; test loss: 0.7849719910383844, acc: 0.7265421886078941
epoch: 163, train loss: 0.1942025605667212, acc: 0.8716112229193796; test loss: 0.8254293610485585, acc: 0.7014890096903805
Epoch   163: reducing learning rate of group 0 to 3.7500e-04.
epoch: 164, train loss: 0.15795596550946775, acc: 0.8899609328755771; test loss: 0.7435107291435074, acc: 0.7385960765776413
epoch: 165, train loss: 0.13004552893939492, acc: 0.9057653604830117; test loss: 0.7486589534771242, acc: 0.7442684944457575
epoch: 166, train loss: 0.1218313730611905, acc: 0.9122173552740618; test loss: 0.7651279911423541, acc: 0.7494682108248641
epoch: 167, train loss: 0.11502660176938026, acc: 0.9120989700485379; test loss: 0.7774487349249403, acc: 0.7419049870007091
epoch: 168, train loss: 0.11448309090457273, acc: 0.9145850597845389; test loss: 0.8164489046672477, acc: 0.7473410541243205
epoch: 169, train loss: 0.11255783643350808, acc: 0.913756363205872; test loss: 0.8148441466843369, acc: 0.7440321437012527
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.08121063802367035, acc: 0.9145850597845389; test loss: 0.7330424722501556, acc: 0.7485228078468447
epoch: 171, train loss: 0.07696470404530018, acc: 0.9182550017757783; test loss: 0.7147788871858903, acc: 0.7440321437012527
epoch: 172, train loss: 0.07351263799059951, acc: 0.9212146324138748; test loss: 0.7084298449412386, acc: 0.7473410541243205
epoch: 173, train loss: 0.07292408534774728, acc: 0.919379661418255; test loss: 0.7443510312274454, acc: 0.7416686362562042
epoch: 174, train loss: 0.07728546017339219, acc: 0.9149994080738724; test loss: 0.716100497438507, acc: 0.7411959347671945
epoch: 175, train loss: 0.07894570530395868, acc: 0.9162424529418729; test loss: 0.6994669514045003, acc: 0.7452138974237769
epoch: 176, train loss: 0.08341093637544021, acc: 0.9125725109506334; test loss: 0.7352962394719765, acc: 0.7369416213661073
epoch: 177, train loss: 0.08119273972508469, acc: 0.9120989700485379; test loss: 0.7398997431696236, acc: 0.7284329945639328
epoch: 178, train loss: 0.08034633027000004, acc: 0.9117438143719664; test loss: 0.7285888955768359, acc: 0.7367052706216024
epoch: 179, train loss: 0.07949055288424926, acc: 0.9150586006866344; test loss: 0.7186535437094126, acc: 0.7437957929567478
epoch: 180, train loss: 0.0759221219131442, acc: 0.9180774239374926; test loss: 0.7251061642601033, acc: 0.7423776884897187
epoch: 181, train loss: 0.07610021992623489, acc: 0.9148810228483485; test loss: 0.7410695627208467, acc: 0.7454502481682818
epoch: 182, train loss: 0.0745416427123994, acc: 0.9156505268142536; test loss: 0.7093253417433196, acc: 0.7523044197589223
epoch: 183, train loss: 0.07836091319370075, acc: 0.9115070439209186; test loss: 0.7433297381674925, acc: 0.7390687780666509
epoch: 184, train loss: 0.08059981313673112, acc: 0.9097312655380608; test loss: 0.7329583734351393, acc: 0.7352871661545733
epoch: 185, train loss: 0.07794445749216564, acc: 0.915295371137682; test loss: 0.7613000436979838, acc: 0.735759867643583
epoch: 186, train loss: 0.07108320432791304, acc: 0.917840653486445; test loss: 0.7394274506155297, acc: 0.7416686362562042
epoch: 187, train loss: 0.06955348286123703, acc: 0.9181366165502546; test loss: 0.7302227542068456, acc: 0.7471047033798156
epoch: 188, train loss: 0.07011382654882597, acc: 0.9197940097075885; test loss: 0.7564587178339606, acc: 0.7381233750886316
epoch: 189, train loss: 0.07354786828895907, acc: 0.9163608381673967; test loss: 0.7953107066538789, acc: 0.7293783975419522
epoch: 190, train loss: 0.07442645056915662, acc: 0.9155913342014916; test loss: 0.7338137979401508, acc: 0.7371779721106122
epoch: 191, train loss: 0.08072901366412379, acc: 0.9099680359891086; test loss: 0.7420569069699401, acc: 0.7333963601985346
epoch: 192, train loss: 0.07722653474866448, acc: 0.9144074819462531; test loss: 0.7404427378489261, acc: 0.7336327109430395
epoch: 193, train loss: 0.07331928904873387, acc: 0.9166568012312063; test loss: 0.7189513753982504, acc: 0.7348144646655637
epoch: 194, train loss: 0.07264891024879609, acc: 0.9147034450100627; test loss: 0.7665598705794845, acc: 0.7397778303001654
epoch: 195, train loss: 0.07800112634622036, acc: 0.9152361785249201; test loss: 0.7531990573299717, acc: 0.7393051288111557
epoch: 196, train loss: 0.07902989141105886, acc: 0.9116254291464425; test loss: 0.7002204960047903, acc: 0.7421413377452138
epoch: 197, train loss: 0.08624600466749605, acc: 0.9082514502190127; test loss: 0.728924272923119, acc: 0.7381233750886316
epoch: 198, train loss: 0.07329351698063076, acc: 0.9147626376228247; test loss: 0.7407481699118347, acc: 0.7447411959347672
epoch: 199, train loss: 0.07065565400712022, acc: 0.9180182313247307; test loss: 0.7783216092919548, acc: 0.7307965020089813
epoch: 200, train loss: 0.07323457334004056, acc: 0.9129868592399668; test loss: 0.7517319499840552, acc: 0.74048688253368
best test acc 0.7523044197589223 at epoch 182.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9697    0.9866    0.9781      6100
           1     0.9793    0.9719    0.9756       926
           2     0.8706    0.9838    0.9237      2400
           3     0.9677    0.9609    0.9643       843
           4     0.9525    0.9845    0.9682       774
           5     0.9750    0.9782    0.9766      1512
           6     0.9283    0.8962    0.9120      1330
           7     0.9555    0.9376    0.9465       481
           8     0.9485    0.8450    0.8938       458
           9     0.9759    0.9867    0.9813       452
          10     0.9797    0.9442    0.9616       717
          11     0.9611    0.9640    0.9625       333
          12     0.7500    0.0100    0.0198       299
          13     0.8933    0.8401    0.8659       269

    accuracy                         0.9490     16894
   macro avg     0.9362    0.8778    0.8807     16894
weighted avg     0.9468    0.9490    0.9409     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8040    0.8715    0.8364      1525
           1     0.8035    0.7931    0.7983       232
           2     0.7006    0.7903    0.7428       601
           3     0.7756    0.7536    0.7644       211
           4     0.8077    0.8660    0.8358       194
           5     0.8206    0.7381    0.7772       378
           6     0.5241    0.5225    0.5233       333
           7     0.6606    0.5950    0.6261       121
           8     0.5758    0.4957    0.5327       115
           9     0.8333    0.7895    0.8108       114
          10     0.8199    0.7333    0.7742       180
          11     0.5789    0.3929    0.4681        84
          12     0.2500    0.0133    0.0253        75
          13     0.6250    0.4412    0.5172        68

    accuracy                         0.7523      4231
   macro avg     0.6843    0.6283    0.6452      4231
weighted avg     0.7415    0.7523    0.7430      4231

---------------------------------------
program finished.
