seed:  666
save trained model at:  ../trained_models/trained_classifier_model_20.pt
save loss at:  ./results/train_classifier_results_20.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  15
number of pockets in training set:  17282
number of pockets in validation set:  3698
number of pockets in test set:  3720
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=15, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2ac8dae74be0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.106782863225916, acc: 0.36251591251012616, val loss: 1.8479453774643564, acc: 0.42239048134126556, test loss: 1.8615152312863257, acc: 0.4163978494623656
epoch: 2, train loss: 1.830988784504632, acc: 0.4296377734058558, val loss: 1.742920971226215, acc: 0.44077879935100056, test loss: 1.7390345629825388, acc: 0.44247311827956987
epoch: 3, train loss: 1.7767247084238065, acc: 0.4456081472051846, val loss: 1.7250606602369094, acc: 0.4591671173607355, test loss: 1.7379791705839096, acc: 0.4599462365591398
epoch: 4, train loss: 1.7136473773378642, acc: 0.46221502140955906, val loss: 1.6668893928589725, acc: 0.47187669010275823, test loss: 1.6681350287570749, acc: 0.46881720430107526
epoch: 5, train loss: 1.66949802797821, acc: 0.4761023029741928, val loss: 1.6590257062339473, acc: 0.46592752839372636, test loss: 1.6704490353984218, acc: 0.4650537634408602
epoch: 6, train loss: 1.624591927908593, acc: 0.4905682212706863, val loss: 1.596489204078703, acc: 0.4943212547322877, test loss: 1.5847252440708939, acc: 0.4975806451612903
epoch: 7, train loss: 1.56970914477152, acc: 0.509026733017012, val loss: 1.6424574770754643, acc: 0.4872904272579773, test loss: 1.6095026923764137, acc: 0.49435483870967745
epoch: 8, train loss: 1.5194130256321954, acc: 0.5227404235620877, val loss: 1.6535897712697203, acc: 0.4905354245538129, test loss: 1.6731944940423453, acc: 0.48091397849462364
epoch: 9, train loss: 1.4899058798068294, acc: 0.5289897002661729, val loss: 1.509269952387214, acc: 0.5262303948080044, test loss: 1.4886859186234014, acc: 0.5188172043010753
epoch: 10, train loss: 1.4919068044899322, acc: 0.5307834741349381, val loss: 1.4575576276763702, acc: 0.5348837209302325, test loss: 1.4581947967570315, acc: 0.5252688172043011
epoch: 11, train loss: 1.4264814788690114, acc: 0.5458280291632913, val loss: 1.4811424123331816, acc: 0.53677663601947, test loss: 1.4771616740893292, acc: 0.5231182795698924
epoch: 12, train loss: 1.4154301879858753, acc: 0.5563592176831386, val loss: 1.40978855774685, acc: 0.5489453758788534, test loss: 1.3880236871780889, acc: 0.5526881720430108
epoch: 13, train loss: 1.3858119583615494, acc: 0.5648651776414767, val loss: 1.6421903947548586, acc: 0.49621416982152516, test loss: 1.61459792301219, acc: 0.5043010752688172
epoch: 14, train loss: 1.3625242664725425, acc: 0.5690313621108668, val loss: 1.4292939936811695, acc: 0.5351541373715522, test loss: 1.4162394990203202, acc: 0.5419354838709678
epoch: 15, train loss: 1.354676985696515, acc: 0.573139682907071, val loss: 1.3579720019520523, acc: 0.5608436992969172, test loss: 1.359536898007957, acc: 0.5489247311827957
epoch: 16, train loss: 1.3200019561586356, acc: 0.5823978706168268, val loss: 1.36434837661607, acc: 0.5621957815035155, test loss: 1.3460981574109805, acc: 0.5763440860215053
epoch: 17, train loss: 1.2880679227290173, acc: 0.5911931489410948, val loss: 1.3227941909953025, acc: 0.5746349378042185, test loss: 1.3299967242825417, acc: 0.5637096774193548
epoch: 18, train loss: 1.2792831579301522, acc: 0.5974424256451799, val loss: 1.3125572020199054, acc: 0.5803136830719308, test loss: 1.3253696062231577, acc: 0.5790322580645161
epoch: 19, train loss: 1.2634025674053342, acc: 0.5996991088994329, val loss: 1.3024679668018018, acc: 0.5811249323958897, test loss: 1.3020813101081439, acc: 0.5739247311827957
epoch: 20, train loss: 1.2518101207995604, acc: 0.6079157504918412, val loss: 1.261140771399581, acc: 0.6046511627906976, test loss: 1.2561734917343303, acc: 0.6021505376344086
epoch: 21, train loss: 1.235223658837291, acc: 0.6149172549473441, val loss: 1.2311936978587206, acc: 0.6068144943212548, test loss: 1.252722341783585, acc: 0.5924731182795699
epoch: 22, train loss: 1.2127838414819851, acc: 0.6208193496123134, val loss: 1.3675518382749665, acc: 0.5703082747431044, test loss: 1.383845474386728, acc: 0.5744623655913978
epoch: 23, train loss: 1.197301585011261, acc: 0.6292674458974655, val loss: 1.2492103722367562, acc: 0.5976203353163873, test loss: 1.2593457996204336, acc: 0.5889784946236559
epoch: 24, train loss: 1.1790133089727308, acc: 0.6301354009952552, val loss: 1.2377498691439177, acc: 0.6024878312601406, test loss: 1.2311893822044455, acc: 0.603494623655914
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9460010557397622, acc: 0.6329707209813679, val loss: 0.9797027319943731, acc: 0.6146565711195241, test loss: 0.9932833240878197, acc: 0.6088709677419355
epoch: 26, train loss: 0.9297887462389275, acc: 0.6407244531882884, val loss: 0.9356752171782045, acc: 0.6203353163872364, test loss: 0.9397034316934565, acc: 0.6163978494623656
epoch: 27, train loss: 0.9105857185654694, acc: 0.6432125911352853, val loss: 0.9869165380559398, acc: 0.6119524067063278, test loss: 0.9789371562260454, acc: 0.6153225806451613
epoch: 28, train loss: 0.9093966463583099, acc: 0.6408980442078464, val loss: 0.9294085064212847, acc: 0.6300703082747431, test loss: 0.9678130667696717, acc: 0.6190860215053764
epoch: 29, train loss: 0.9026531152890217, acc: 0.6434440458280292, val loss: 0.9538112733736755, acc: 0.6222282314764738, test loss: 0.9631201441569995, acc: 0.6169354838709677
epoch: 30, train loss: 0.890706637373071, acc: 0.6478995486633492, val loss: 0.9682118103528681, acc: 0.6227690643591131, test loss: 0.975270620469124, acc: 0.6190860215053764
epoch: 31, train loss: 0.8851071454876897, acc: 0.650503413956718, val loss: 0.9379442964391879, acc: 0.6292590589507842, test loss: 0.9378841451419297, acc: 0.6271505376344086
epoch: 32, train loss: 0.8641758406138809, acc: 0.6534544612892026, val loss: 0.9806552619273886, acc: 0.6157382368848026, test loss: 0.968169645596576, acc: 0.6206989247311828
epoch: 33, train loss: 0.8587504197651418, acc: 0.659182964934614, val loss: 1.2064917030819304, acc: 0.5508382909680909, test loss: 1.211616559182444, acc: 0.5575268817204301
epoch: 34, train loss: 0.8652855268149392, acc: 0.6566369633144312, val loss: 0.9154878535613683, acc: 0.6352082206598161, test loss: 0.9398874841710573, acc: 0.6319892473118279
epoch: 35, train loss: 0.839479661735585, acc: 0.6652586506191414, val loss: 0.9902215499114577, acc: 0.6127636560302866, test loss: 0.9916541730203936, acc: 0.6209677419354839
epoch: 36, train loss: 0.8670485073625317, acc: 0.6607452841106354, val loss: 0.9192264523617056, acc: 0.6297998918334234, test loss: 0.9382190155726607, acc: 0.635483870967742
epoch: 37, train loss: 0.8310322963643745, acc: 0.6693091077421595, val loss: 0.9078987098629892, acc: 0.644131963223364, test loss: 0.94792663615237, acc: 0.635752688172043
epoch: 38, train loss: 0.8210443301673234, acc: 0.6717972456891563, val loss: 0.9842532722933219, acc: 0.6233098972417523, test loss: 1.0152426965775028, acc: 0.610752688172043
epoch: 39, train loss: 0.8065215285322951, acc: 0.6758477028121745, val loss: 0.9365492987594068, acc: 0.6349378042184964, test loss: 0.9541995007504699, acc: 0.6279569892473118
epoch: 40, train loss: 0.8135224038590824, acc: 0.6750954750607568, val loss: 0.8639003771456465, acc: 0.6600865332612222, test loss: 0.88762331316548, acc: 0.6448924731182796
epoch: 41, train loss: 0.8060592347970509, acc: 0.6819812521698877, val loss: 1.1712825081682385, acc: 0.5570578691184424, test loss: 1.1753616676535659, acc: 0.5653225806451613
epoch: 42, train loss: 0.8091130474901989, acc: 0.6777572040273117, val loss: 0.9024534424038563, acc: 0.6454840454299622, test loss: 0.9257139954515683, acc: 0.6306451612903226
epoch: 43, train loss: 0.7778797692728876, acc: 0.6860317092929059, val loss: 0.8795419469145326, acc: 0.6657652785289345, test loss: 0.8920119367619996, acc: 0.6586021505376344
epoch: 44, train loss: 0.8445051780882126, acc: 0.6632334220576322, val loss: 0.8711818909890076, acc: 0.6552190373174689, test loss: 0.9072877201982724, acc: 0.6451612903225806
epoch: 45, train loss: 0.788408800202544, acc: 0.6851058905219304, val loss: 0.9033484850914947, acc: 0.6479177934018389, test loss: 0.9318092284664031, acc: 0.6306451612903226
epoch: 46, train loss: 0.7614392417956938, acc: 0.6929753500752228, val loss: 0.8659396829187322, acc: 0.6654948620876149, test loss: 0.8810992948470577, acc: 0.6583333333333333
epoch: 47, train loss: 0.7595416131208318, acc: 0.6966786251591252, val loss: 0.8993227409633061, acc: 0.6362898864250947, test loss: 0.9172652085622152, acc: 0.6327956989247312
epoch: 48, train loss: 0.7583415992432978, acc: 0.6947691239439879, val loss: 0.953304798684293, acc: 0.6343969713358573, test loss: 0.9795897545353058, acc: 0.6188172043010752
epoch: 49, train loss: 0.7426617459668121, acc: 0.7015970373799328, val loss: 0.8184408168008741, acc: 0.6730665224445647, test loss: 0.8372113791845178, acc: 0.6682795698924732
epoch: 50, train loss: 0.7728068930188645, acc: 0.692570304362921, val loss: 0.890012236657563, acc: 0.6500811249323959, test loss: 0.9069099395505844, acc: 0.65
epoch: 51, train loss: 0.7427050698039405, acc: 0.7023492651313505, val loss: 0.8345750440707911, acc: 0.6703623580313683, test loss: 0.879943807150728, acc: 0.6629032258064517
epoch: 52, train loss: 0.7250990431806222, acc: 0.7070362226594145, val loss: 0.8283899752367762, acc: 0.673607355327204, test loss: 0.846649544726136, acc: 0.6655913978494624
epoch: 53, train loss: 0.7392528767580788, acc: 0.7042587663464877, val loss: 0.8448473730236856, acc: 0.6746890210924824, test loss: 0.8731462832420103, acc: 0.6618279569892473
epoch: 54, train loss: 0.7372127559888336, acc: 0.7026964471704664, val loss: 0.8988182683968428, acc: 0.638182801514332, test loss: 0.9080219294435234, acc: 0.6352150537634409
epoch: 55, train loss: 0.7201791306900048, acc: 0.7105080430505728, val loss: 0.941952168329011, acc: 0.6457544618712818, test loss: 0.9804758558991135, acc: 0.6255376344086021
epoch: 56, train loss: 0.7118923343134318, acc: 0.7143849091540331, val loss: 0.8132069299129489, acc: 0.6746890210924824, test loss: 0.830984244808074, acc: 0.6647849462365591
epoch: 57, train loss: 0.7001197471371432, acc: 0.716930910774216, val loss: 0.8658020915438382, acc: 0.6798269334775554, test loss: 0.8865984988468949, acc: 0.6658602150537635
epoch: 58, train loss: 0.7131061614311592, acc: 0.7150792732322648, val loss: 0.8835895890606745, acc: 0.6714440237966468, test loss: 0.9250578111217868, acc: 0.6478494623655914
epoch: 59, train loss: 0.7103261274122113, acc: 0.7169887744474019, val loss: 0.8457549073491114, acc: 0.6590048674959438, test loss: 0.8640701729764221, acc: 0.6639784946236559
epoch: 60, train loss: 0.6981616356612603, acc: 0.7179724568915635, val loss: 0.9033924446291637, acc: 0.6535965386695511, test loss: 0.8964388360259353, acc: 0.6435483870967742
epoch: 61, train loss: 0.7026783164735884, acc: 0.716930910774216, val loss: 0.8033563418927355, acc: 0.6873985938345052, test loss: 0.8287548818895893, acc: 0.6766129032258065
epoch: 62, train loss: 0.6702566032931976, acc: 0.7248582340006944, val loss: 0.8811478619964913, acc: 0.6619794483504597, test loss: 0.8825292592407554, acc: 0.6631720430107527
epoch: 63, train loss: 0.6733855343074686, acc: 0.7274042356208772, val loss: 0.8034821171706402, acc: 0.686046511627907, test loss: 0.8338250611418037, acc: 0.6776881720430108
epoch: 64, train loss: 0.6965670245290562, acc: 0.7201134127994445, val loss: 0.8502005463100369, acc: 0.6636019469983775, test loss: 0.8737247308095296, acc: 0.6548387096774193
epoch: 65, train loss: 0.6810845949213589, acc: 0.7215600046290939, val loss: 0.8789913589855088, acc: 0.6746890210924824, test loss: 0.8967729055753318, acc: 0.6623655913978495
epoch: 66, train loss: 0.6734146046398443, acc: 0.7282721907186668, val loss: 0.8801620813754522, acc: 0.6622498647917794, test loss: 0.8735764216351253, acc: 0.6543010752688172
epoch: 67, train loss: 0.6568067273446881, acc: 0.730123828260618, val loss: 0.7763485473964974, acc: 0.6995673336938886, test loss: 0.7990170130165675, acc: 0.6900537634408602
epoch: 68, train loss: 0.6537423112535625, acc: 0.734579331095938, val loss: 0.8104360400179775, acc: 0.6955110870740941, test loss: 0.8503258228302002, acc: 0.675
epoch: 69, train loss: 0.6592296109736455, acc: 0.7338271033445203, val loss: 0.7929580175534914, acc: 0.6949702541914549, test loss: 0.8176033978821129, acc: 0.6793010752688172
epoch: 70, train loss: 0.6844963555175416, acc: 0.7251475523666242, val loss: 0.8311909249565161, acc: 0.678745267712277, test loss: 0.8542981532312208, acc: 0.6650537634408602
epoch: 71, train loss: 0.6532739418936112, acc: 0.7341742853836362, val loss: 0.8341427056966052, acc: 0.6901027582477015, test loss: 0.8658970386751237, acc: 0.6798387096774193
epoch: 72, train loss: 0.6515133104767461, acc: 0.7343478764031941, val loss: 0.821051885915357, acc: 0.69118442401298, test loss: 0.8140231173525574, acc: 0.6836021505376344
epoch: 73, train loss: 0.6319062039816549, acc: 0.7419280175905566, val loss: 0.8355599022221089, acc: 0.6857760951865873, test loss: 0.8667785326639811, acc: 0.6766129032258065
epoch: 74, train loss: 0.6452724901928475, acc: 0.736546695984261, val loss: 0.9011666482045497, acc: 0.6760411032990806, test loss: 0.9260350406810801, acc: 0.6669354838709678
epoch: 75, train loss: 0.6299604904167411, acc: 0.7423909269760445, val loss: 0.786363031620461, acc: 0.6936181719848566, test loss: 0.8013222438032909, acc: 0.6811827956989247
epoch: 76, train loss: 0.637532317821881, acc: 0.7423909269760445, val loss: 0.8519128659662523, acc: 0.6671173607355327, test loss: 0.8715789282193748, acc: 0.6596774193548387
epoch: 77, train loss: 0.6351127573536332, acc: 0.7407707441268372, val loss: 0.8719334523699747, acc: 0.6625202812330989, test loss: 0.8954561469375446, acc: 0.6655913978494624
epoch: 78, train loss: 0.6091336151223922, acc: 0.747945839601898, val loss: 0.8489893171321772, acc: 0.6709031909140075, test loss: 0.8678035331028764, acc: 0.6674731182795699
Epoch    78: reducing learning rate of group 0 to 1.5000e-03.
epoch: 79, train loss: 0.5416915167358901, acc: 0.7759518574239093, val loss: 0.7676433494376002, acc: 0.7141698215251487, test loss: 0.7866953075573009, acc: 0.6978494623655914
epoch: 80, train loss: 0.5122721454581513, acc: 0.785441499826409, val loss: 0.7952502227719247, acc: 0.7122769064359114, test loss: 0.8120057018854285, acc: 0.7080645161290322
epoch: 81, train loss: 0.5046213465064726, acc: 0.7882189561393357, val loss: 0.8012286886903258, acc: 0.7087614926987561, test loss: 0.8183317394666775, acc: 0.6986559139784946
epoch: 82, train loss: 0.4974152152426002, acc: 0.7896076842957991, val loss: 0.820952873864388, acc: 0.6976744186046512, test loss: 0.8577249003994849, acc: 0.6860215053763441
epoch: 83, train loss: 0.49673025517838376, acc: 0.7900705936812868, val loss: 0.8679847630892269, acc: 0.6914548404542996, test loss: 0.8496247629965505, acc: 0.6967741935483871
epoch: 84, train loss: 0.4988277215756775, acc: 0.7892605022566832, val loss: 0.8098069310123821, acc: 0.7079502433747972, test loss: 0.8095762073352772, acc: 0.7077956989247312
epoch: 85, train loss: 0.5016632227416999, acc: 0.7885082745052656, val loss: 0.869498610303104, acc: 0.6841535965386696, test loss: 0.8827332137733377, acc: 0.681989247311828
epoch: 86, train loss: 0.4976801762094157, acc: 0.7872352736951742, val loss: 0.799768253943287, acc: 0.7060573282855598, test loss: 0.8014615648536272, acc: 0.7080645161290322
epoch: 87, train loss: 0.4794595698669627, acc: 0.7957990973266983, val loss: 0.8942033143350406, acc: 0.686046511627907, test loss: 0.865995515290127, acc: 0.6913978494623656
epoch: 88, train loss: 0.47656881227742715, acc: 0.7967249160976739, val loss: 0.8417845429956365, acc: 0.7003785830178475, test loss: 0.8386168541446809, acc: 0.7024193548387097
epoch: 89, train loss: 0.47674030520107274, acc: 0.8000810091424604, val loss: 0.8547630729128568, acc: 0.7020010816657652, test loss: 0.8675852032117947, acc: 0.6932795698924731
epoch: 90, train loss: 0.47764653707596316, acc: 0.7971299618099758, val loss: 0.95844537016506, acc: 0.6679286100594917, test loss: 0.9863955913051482, acc: 0.668010752688172
epoch: 91, train loss: 0.47705193917765937, acc: 0.7992709177178567, val loss: 0.821551056795342, acc: 0.7125473228772309, test loss: 0.831940145390008, acc: 0.7026881720430107
epoch: 92, train loss: 0.4596434122447462, acc: 0.8036685568799907, val loss: 0.8359093210902196, acc: 0.7076798269334775, test loss: 0.8459077189045567, acc: 0.6994623655913978
epoch: 93, train loss: 0.4515775131925418, acc: 0.8063881495197315, val loss: 0.8512205684294631, acc: 0.7117360735532721, test loss: 0.8560179879588465, acc: 0.7053763440860215
epoch: 94, train loss: 0.4523973321972615, acc: 0.8086448327739845, val loss: 0.8756760768209554, acc: 0.6952406706327745, test loss: 0.8667877469011532, acc: 0.692741935483871
epoch: 95, train loss: 0.4583053520509124, acc: 0.8057516491146858, val loss: 0.9274717447498543, acc: 0.6879394267171444, test loss: 0.9465358339330201, acc: 0.6768817204301075
epoch: 96, train loss: 0.46362222486065985, acc: 0.8049994213632682, val loss: 0.848837542882025, acc: 0.701460248783126, test loss: 0.8362174757065312, acc: 0.7075268817204301
epoch: 97, train loss: 0.44960281608447117, acc: 0.809512787871774, val loss: 0.8818237290245704, acc: 0.7003785830178475, test loss: 0.8990544139697988, acc: 0.6911290322580645
epoch: 98, train loss: 0.44365901700628946, acc: 0.8102071519500058, val loss: 0.9497445938456954, acc: 0.6990265008112493, test loss: 0.9633375552392776, acc: 0.689247311827957
epoch: 99, train loss: 0.4377547180006976, acc: 0.8115958801064692, val loss: 0.8502370368602664, acc: 0.7009194159004868, test loss: 0.8910590115413871, acc: 0.689247311827957
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3356102984831877, acc: 0.8127531535701886, val loss: 0.7302205514237325, acc: 0.7049756625202812, test loss: 0.7167983642188451, acc: 0.7075268817204301
epoch: 101, train loss: 0.3389636089115862, acc: 0.811769471126027, val loss: 0.723224271111001, acc: 0.7101135749053542, test loss: 0.7507653064625237, acc: 0.6997311827956989
epoch: 102, train loss: 0.33132120930352515, acc: 0.8126374262238167, val loss: 0.7749730750249488, acc: 0.6987560843699296, test loss: 0.7783828617424093, acc: 0.6879032258064516
epoch: 103, train loss: 0.31999610904369236, acc: 0.8161092466149751, val loss: 0.7394687858641373, acc: 0.7030827474310438, test loss: 0.7516952555666687, acc: 0.7008064516129032
epoch: 104, train loss: 0.3208513752051959, acc: 0.8204490221039231, val loss: 0.7650312467031443, acc: 0.6995673336938886, test loss: 0.7601660559254307, acc: 0.693010752688172
epoch: 105, train loss: 0.3202653900228597, acc: 0.8178451568105544, val loss: 0.7333508498479122, acc: 0.7087614926987561, test loss: 0.7300096240094913, acc: 0.693010752688172
epoch: 106, train loss: 0.335301822740548, acc: 0.8063881495197315, val loss: 0.7669567758164063, acc: 0.7041644131963224, test loss: 0.7872341730261362, acc: 0.6913978494623656
epoch: 107, train loss: 0.34356331296878173, acc: 0.8062724221733596, val loss: 0.7120176859324916, acc: 0.7063277447268794, test loss: 0.7186517092489427, acc: 0.6956989247311828
epoch: 108, train loss: 0.33859315113452365, acc: 0.8088762874667283, val loss: 0.7956057907504737, acc: 0.6838831800973499, test loss: 0.7910423219844859, acc: 0.6782258064516129
epoch: 109, train loss: 0.34617390118115976, acc: 0.8033792385140609, val loss: 0.7965866486532357, acc: 0.688209843158464, test loss: 0.7830074351320985, acc: 0.6943548387096774
epoch: 110, train loss: 0.3426828083065204, acc: 0.8099756972572619, val loss: 0.8101890391436959, acc: 0.6984856679286101, test loss: 0.8044963288050826, acc: 0.6884408602150538
epoch: 111, train loss: 0.3235762029931082, acc: 0.8158777919222312, val loss: 0.7918178839190965, acc: 0.6965927528393726, test loss: 0.825743176347466, acc: 0.6836021505376344
epoch: 112, train loss: 0.32467526678561115, acc: 0.8149519731512557, val loss: 0.7351848629115401, acc: 0.7028123309897242, test loss: 0.7471479421020836, acc: 0.6981182795698925
epoch: 113, train loss: 0.33248518319569204, acc: 0.8067931952320333, val loss: 0.7232522655010482, acc: 0.7017306652244456, test loss: 0.7448668033845963, acc: 0.7021505376344086
epoch: 114, train loss: 0.32386503306448894, acc: 0.8132160629556764, val loss: 0.7930015665315305, acc: 0.6919956733369389, test loss: 0.8198305868333385, acc: 0.6790322580645162
epoch: 115, train loss: 0.32479783944365687, acc: 0.8091077421594722, val loss: 0.7728795786043191, acc: 0.6884802595997837, test loss: 0.7805044204958024, acc: 0.692741935483871
epoch: 116, train loss: 0.3111292395088466, acc: 0.820796204143039, val loss: 0.784324493841457, acc: 0.7044348296376419, test loss: 0.781152569863104, acc: 0.6978494623655914
epoch: 117, train loss: 0.31542699314380546, acc: 0.8208540678162249, val loss: 0.8330273615340402, acc: 0.6730665224445647, test loss: 0.8493624482103573, acc: 0.6709677419354839
epoch: 118, train loss: 0.31230185218754636, acc: 0.8152412915171855, val loss: 0.7775388536355249, acc: 0.701460248783126, test loss: 0.7901637861805577, acc: 0.696505376344086
epoch: 119, train loss: 0.3147246469165313, acc: 0.8148941094780696, val loss: 0.8528710024107979, acc: 0.6822606814494321, test loss: 0.827874457451605, acc: 0.6739247311827957
epoch: 120, train loss: 0.32433593374906683, acc: 0.8115380164332832, val loss: 0.777864015315145, acc: 0.7036235803136831, test loss: 0.7661229190006051, acc: 0.6978494623655914
epoch: 121, train loss: 0.30085595878665194, acc: 0.8224163869922463, val loss: 0.7873013860280608, acc: 0.6971335857220119, test loss: 0.7927050303387385, acc: 0.6954301075268817
epoch: 122, train loss: 0.31755537876941003, acc: 0.8170929290591367, val loss: 0.7599300915257489, acc: 0.6995673336938886, test loss: 0.7852905299073907, acc: 0.6844086021505377
epoch: 123, train loss: 0.3030499834875558, acc: 0.8181344751764842, val loss: 0.8108070796731358, acc: 0.701460248783126, test loss: 0.8377679353119225, acc: 0.6938172043010753
epoch: 124, train loss: 0.30963301427723594, acc: 0.8176715657909964, val loss: 0.7967732606416396, acc: 0.6938885884261763, test loss: 0.7950348669482815, acc: 0.6905913978494623
epoch: 125, train loss: 0.2944646728058102, acc: 0.8239208424950816, val loss: 0.8032407228078888, acc: 0.6974040021633315, test loss: 0.8066594487877302, acc: 0.7005376344086022
epoch: 126, train loss: 0.3225275909317104, acc: 0.8165142923272769, val loss: 0.8018826172118447, acc: 0.6890210924824229, test loss: 0.7924743621580063, acc: 0.6879032258064516
epoch: 127, train loss: 0.32593393006088583, acc: 0.8128688809165606, val loss: 0.7441338516687174, acc: 0.6984856679286101, test loss: 0.7531139542979579, acc: 0.693010752688172
epoch: 128, train loss: 0.30580256650492266, acc: 0.818539520888786, val loss: 0.7365455775727705, acc: 0.6968631692806923, test loss: 0.7309425846222908, acc: 0.6978494623655914
epoch: 129, train loss: 0.296916963684105, acc: 0.8211433861821549, val loss: 0.7786109154774989, acc: 0.7022714981070849, test loss: 0.7809658004391578, acc: 0.6922043010752689
Epoch   129: reducing learning rate of group 0 to 7.5000e-04.
epoch: 130, train loss: 0.25784267115242543, acc: 0.838676079157505, val loss: 0.7680744722509719, acc: 0.7160627366143861, test loss: 0.7901893979759627, acc: 0.7051075268817204
epoch: 131, train loss: 0.23294379320454783, acc: 0.851637541951163, val loss: 0.7746878970049729, acc: 0.7233639805300163, test loss: 0.8014244776900097, acc: 0.7080645161290322
epoch: 132, train loss: 0.22868180152525877, acc: 0.8544149982640898, val loss: 0.7874573921757559, acc: 0.72065981611682, test loss: 0.7832836181886734, acc: 0.7123655913978495
epoch: 133, train loss: 0.21710376115726, acc: 0.860664274968175, val loss: 0.8357796982341744, acc: 0.7141698215251487, test loss: 0.8365333710947345, acc: 0.7120967741935483
epoch: 134, train loss: 0.20977532414564476, acc: 0.86274736720287, val loss: 0.8271543508996958, acc: 0.7171444023796647, test loss: 0.8182436409816947, acc: 0.7086021505376344
epoch: 135, train loss: 0.21807041491447893, acc: 0.8596227288508275, val loss: 0.8362565085718991, acc: 0.7128177393185505, test loss: 0.8468796822332567, acc: 0.7104838709677419
epoch: 136, train loss: 0.22812151286167728, acc: 0.8567295451915288, val loss: 0.8315912671447638, acc: 0.7128177393185505, test loss: 0.8425330259466683, acc: 0.7077956989247312
epoch: 137, train loss: 0.21668521931789192, acc: 0.8595648651776415, val loss: 0.8322440131152882, acc: 0.7122769064359114, test loss: 0.8418901540899789, acc: 0.7040322580645161
epoch: 138, train loss: 0.21910206464661064, acc: 0.8556301354009953, val loss: 0.8395710246765401, acc: 0.7125473228772309, test loss: 0.8387535638706658, acc: 0.7099462365591398
epoch: 139, train loss: 0.21909685976464444, acc: 0.8613007753732207, val loss: 0.8393937772902493, acc: 0.7109248242293131, test loss: 0.8557710883437947, acc: 0.7150537634408602
epoch: 140, train loss: 0.21342967254028955, acc: 0.8609535933341048, val loss: 0.8570138688600663, acc: 0.7103839913466738, test loss: 0.8715691961267943, acc: 0.7067204301075268
epoch: 141, train loss: 0.22367814779212639, acc: 0.8555722717278093, val loss: 0.8444616789428397, acc: 0.7082206598161168, test loss: 0.8461255709330241, acc: 0.707258064516129
epoch: 142, train loss: 0.22423608340180703, acc: 0.8555144080546233, val loss: 0.8584418933800325, acc: 0.7063277447268794, test loss: 0.8915340813257361, acc: 0.7010752688172043
epoch: 143, train loss: 0.2115190771425947, acc: 0.8630945492419859, val loss: 0.858769938390019, acc: 0.7047052460789616, test loss: 0.8386855094663559, acc: 0.7
epoch: 144, train loss: 0.21103569457561602, acc: 0.862631639856498, val loss: 0.8575339659024988, acc: 0.7111952406706328, test loss: 0.8661210142156129, acc: 0.7086021505376344
epoch: 145, train loss: 0.20790266702674823, acc: 0.8630945492419859, val loss: 0.8633439663360414, acc: 0.7182260681449432, test loss: 0.8573745507065967, acc: 0.7083333333333334
epoch: 146, train loss: 0.2079513060723779, acc: 0.8581182733479922, val loss: 0.891494228235124, acc: 0.7001081665765279, test loss: 0.8719641157375869, acc: 0.7026881720430107
epoch: 147, train loss: 0.20865767246083988, acc: 0.8597384561971994, val loss: 0.8495180579763674, acc: 0.7098431584640346, test loss: 0.8805791306239302, acc: 0.7086021505376344
epoch: 148, train loss: 0.21347850849818553, acc: 0.8593912741580836, val loss: 0.8706310441392122, acc: 0.7090319091400757, test loss: 0.8906616287846719, acc: 0.7043010752688172
epoch: 149, train loss: 0.2012041038956966, acc: 0.8645411410716353, val loss: 0.8790635291533317, acc: 0.7103839913466738, test loss: 0.8633837417889667, acc: 0.7134408602150538
epoch: 150, train loss: 0.19504580300565588, acc: 0.8680708251359797, val loss: 0.8889331996859957, acc: 0.714710654407788, test loss: 0.9175835850418255, acc: 0.7112903225806452
epoch: 151, train loss: 0.22855346299386275, acc: 0.8541256798981599, val loss: 0.8842621147471418, acc: 0.7063277447268794, test loss: 0.8780988206145584, acc: 0.7134408602150538
epoch: 152, train loss: 0.20755810432067942, acc: 0.8617058210855225, val loss: 0.8921737077753243, acc: 0.7076798269334775, test loss: 0.8885404561155585, acc: 0.7088709677419355
epoch: 153, train loss: 0.20783789509349282, acc: 0.8633260039347298, val loss: 0.8834742434673403, acc: 0.7098431584640346, test loss: 0.8886797689622449, acc: 0.7075268817204301
epoch: 154, train loss: 0.20694689502830294, acc: 0.8624001851637542, val loss: 0.8756032710851624, acc: 0.7128177393185505, test loss: 0.8836238215046545, acc: 0.7110215053763441
epoch: 155, train loss: 0.21022925376133322, acc: 0.8647725957643791, val loss: 0.9260326848924966, acc: 0.6925365062195782, test loss: 0.9093886662554997, acc: 0.6970430107526882
epoch: 156, train loss: 0.19791842740472718, acc: 0.8682444161555376, val loss: 0.8597442315938731, acc: 0.7125473228772309, test loss: 0.8826085300855739, acc: 0.7137096774193549
epoch: 157, train loss: 0.18813270977968255, acc: 0.8710797361416502, val loss: 0.9391251086157164, acc: 0.7017306652244456, test loss: 0.9355452773391559, acc: 0.7029569892473119
epoch: 158, train loss: 0.20928951427763004, acc: 0.8618794121050805, val loss: 0.8542529251073876, acc: 0.7187669010275824, test loss: 0.8509680753113121, acc: 0.7077956989247312
epoch: 159, train loss: 0.20659274640575545, acc: 0.8614743663927786, val loss: 0.9081240325698728, acc: 0.7087614926987561, test loss: 0.9178030639566401, acc: 0.6994623655913978
epoch: 160, train loss: 0.19261076725642584, acc: 0.8663927786135864, val loss: 0.9134562763412171, acc: 0.7109248242293131, test loss: 0.9249910190541257, acc: 0.6981182795698925
epoch: 161, train loss: 0.20011084500134177, acc: 0.8668556879990742, val loss: 0.9049051769879652, acc: 0.7128177393185505, test loss: 0.9290106260648338, acc: 0.7131720430107527
epoch: 162, train loss: 0.22700601380230886, acc: 0.8543571345909038, val loss: 0.9093477884584533, acc: 0.6968631692806923, test loss: 0.9148177962149343, acc: 0.6938172043010753
epoch: 163, train loss: 0.21497122504523253, acc: 0.8564402268255988, val loss: 0.8708447209559884, acc: 0.7106544077879935, test loss: 0.8391096812422557, acc: 0.714516129032258
epoch: 164, train loss: 0.1854809584485589, acc: 0.8720634185858118, val loss: 0.9323523261213122, acc: 0.7087614926987561, test loss: 0.9028897480298114, acc: 0.7198924731182795
epoch: 165, train loss: 0.18456698045998504, acc: 0.876056012035644, val loss: 0.9086293285765991, acc: 0.7047052460789616, test loss: 0.8983698726982199, acc: 0.7088709677419355
epoch: 166, train loss: 0.17727355453278534, acc: 0.8728735100104155, val loss: 0.9347492481580356, acc: 0.7065981611681991, test loss: 0.9367336160393172, acc: 0.7016129032258065
epoch: 167, train loss: 0.22341567722822062, acc: 0.8575975002893184, val loss: 0.8680510352017623, acc: 0.7103839913466738, test loss: 0.8800305304988738, acc: 0.7099462365591398
epoch: 168, train loss: 0.19151184633641508, acc: 0.869517416965629, val loss: 0.8799364413358896, acc: 0.7203893996755003, test loss: 0.8828860575152981, acc: 0.7188172043010753
epoch: 169, train loss: 0.18851473417399092, acc: 0.8711954634880222, val loss: 0.9311051781336123, acc: 0.7055164954029205, test loss: 0.9255297435227261, acc: 0.7137096774193549
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.14836109402954764, acc: 0.8682444161555376, val loss: 0.8654524871244373, acc: 0.7017306652244456, test loss: 0.8709271795006208, acc: 0.6989247311827957
epoch: 171, train loss: 0.16290472108151208, acc: 0.8596227288508275, val loss: 0.7527461183464418, acc: 0.7125473228772309, test loss: 0.7913691741164013, acc: 0.7008064516129032
epoch: 172, train loss: 0.1419988314926921, acc: 0.8698645990047448, val loss: 0.7612963611851389, acc: 0.7155219037317468, test loss: 0.7849407983082597, acc: 0.7129032258064516
epoch: 173, train loss: 0.1289765540130601, acc: 0.8768082397870617, val loss: 0.8375601890991029, acc: 0.7103839913466738, test loss: 0.8472261110941569, acc: 0.7016129032258065
epoch: 174, train loss: 0.13147034648770936, acc: 0.8758245573429001, val loss: 0.8437762753005412, acc: 0.7090319091400757, test loss: 0.8701758712850591, acc: 0.7075268817204301
epoch: 175, train loss: 0.1329461748146677, acc: 0.8742043744936928, val loss: 0.8168193556412676, acc: 0.7136289886425095, test loss: 0.8280813381236086, acc: 0.6989247311827957
epoch: 176, train loss: 0.13469787108761586, acc: 0.873799328781391, val loss: 0.7980962655813775, acc: 0.7190373174689021, test loss: 0.8202759219754127, acc: 0.7131720430107527
epoch: 177, train loss: 0.14090706281964943, acc: 0.8674921884041199, val loss: 0.7938349220545116, acc: 0.709572742022715, test loss: 0.8377955377742808, acc: 0.703494623655914
epoch: 178, train loss: 0.12863536610442336, acc: 0.8770975581529915, val loss: 0.8093995004167036, acc: 0.7079502433747972, test loss: 0.8223731061463715, acc: 0.714247311827957
epoch: 179, train loss: 0.13411551281014825, acc: 0.8720055549126259, val loss: 0.827830863295535, acc: 0.7087614926987561, test loss: 0.8185240658380653, acc: 0.7021505376344086
epoch: 180, train loss: 0.13372216798678108, acc: 0.8708482814489064, val loss: 0.8776208979171827, acc: 0.7022714981070849, test loss: 0.8604135646614978, acc: 0.6959677419354838
Epoch   180: reducing learning rate of group 0 to 3.7500e-04.
epoch: 181, train loss: 0.11592382780484145, acc: 0.8845041083207962, val loss: 0.8238100497512446, acc: 0.7130881557598702, test loss: 0.8538963312743812, acc: 0.7131720430107527
epoch: 182, train loss: 0.10337635168232116, acc: 0.8912741580835551, val loss: 0.8233413749156352, acc: 0.7182260681449432, test loss: 0.8514729345998456, acc: 0.710752688172043
epoch: 183, train loss: 0.09906595575545289, acc: 0.8921999768545307, val loss: 0.8574230318910177, acc: 0.7179556517036236, test loss: 0.8739894297815138, acc: 0.7102150537634409
epoch: 184, train loss: 0.09996839368623407, acc: 0.8946881148015277, val loss: 0.8772108378185975, acc: 0.7184964845862628, test loss: 0.8982750815729941, acc: 0.7126344086021505
epoch: 185, train loss: 0.11235569158460983, acc: 0.8883231107510705, val loss: 0.8950161180862418, acc: 0.7006489994591671, test loss: 0.9040304168578117, acc: 0.6978494623655914
epoch: 186, train loss: 0.10171205969737533, acc: 0.8933572503182502, val loss: 0.8869127016444023, acc: 0.7117360735532721, test loss: 0.8803920925304454, acc: 0.7091397849462365
epoch: 187, train loss: 0.1040690990130171, acc: 0.891505612776299, val loss: 0.8787155318995177, acc: 0.7114656571119524, test loss: 0.8830806168176795, acc: 0.7048387096774194
epoch: 188, train loss: 0.10649795218320952, acc: 0.8912741580835551, val loss: 0.8832751896137673, acc: 0.7160627366143861, test loss: 0.892206830875848, acc: 0.7056451612903226
epoch: 189, train loss: 0.10292487665743563, acc: 0.8899432936002778, val loss: 0.8770741209460311, acc: 0.7141698215251487, test loss: 0.8948428646210701, acc: 0.7120967741935483
epoch: 190, train loss: 0.09960997623279683, acc: 0.8953824788797593, val loss: 0.9029045125611606, acc: 0.7082206598161168, test loss: 0.9170616570339408, acc: 0.7056451612903226
epoch: 191, train loss: 0.09492421515881774, acc: 0.8964818886702928, val loss: 0.9242873258884692, acc: 0.7103839913466738, test loss: 0.9243899545361919, acc: 0.7118279569892473
epoch: 192, train loss: 0.09722422476706578, acc: 0.89364656868418, val loss: 0.8905440096341964, acc: 0.7103839913466738, test loss: 0.914950494868781, acc: 0.7045698924731183
epoch: 193, train loss: 0.09971903276593855, acc: 0.8930679319523204, val loss: 0.9224058747484982, acc: 0.7060573282855598, test loss: 0.9519844070557625, acc: 0.6997311827956989
epoch: 194, train loss: 0.09936405919658656, acc: 0.891621340122671, val loss: 0.9224855768287291, acc: 0.7106544077879935, test loss: 0.9610969194801905, acc: 0.7059139784946237
epoch: 195, train loss: 0.10302761790136016, acc: 0.8923157042009027, val loss: 0.8996533874307471, acc: 0.7098431584640346, test loss: 0.9037117983705254, acc: 0.7021505376344086
epoch: 196, train loss: 0.09852651268167913, acc: 0.8904640666589515, val loss: 0.9134994446747751, acc: 0.7087614926987561, test loss: 0.9406926857527866, acc: 0.7048387096774194
epoch: 197, train loss: 0.10092734962187905, acc: 0.891621340122671, val loss: 0.9419034217227917, acc: 0.7093023255813954, test loss: 0.9288482896743282, acc: 0.7059139784946237
epoch: 198, train loss: 0.10202207740841607, acc: 0.8912162944103692, val loss: 0.9417781234109898, acc: 0.7136289886425095, test loss: 0.9298080957064064, acc: 0.706989247311828
epoch: 199, train loss: 0.09670887520941385, acc: 0.8953824788797593, val loss: 0.9499521602354158, acc: 0.7101135749053542, test loss: 0.9354504185338174, acc: 0.7099462365591398
epoch: 200, train loss: 0.09776414083242554, acc: 0.89364656868418, val loss: 0.9383630312862365, acc: 0.7087614926987561, test loss: 0.9340277882032497, acc: 0.7080645161290322
best val acc 0.7233639805300163 at epoch 131.
****************************************************************
/opt/python/anaconda-2020.7/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
train report:
              precision    recall  f1-score   support

           0     0.9502    0.9800    0.9649      5337
           1     0.7837    0.7834    0.7835      2502
           2     0.9836    0.8877    0.9332       810
           3     0.8408    0.9357    0.8857      2100
           4     0.9510    0.8426    0.8935       737
           5     0.9026    0.9852    0.9421       677
           6     0.7914    0.9463    0.8620      1323
           7     0.7683    0.7466    0.7573      1164
           8     0.9584    0.8219    0.8849       421
           9     0.9707    0.9102    0.9395       401
          10     0.9278    0.9091    0.9184       396
          11     0.9367    0.7783    0.8502       627
          12     0.9366    0.8625    0.8980       291
          13     0.0000    0.0000    0.0000       261
          14     0.8895    0.7191    0.7953       235

    accuracy                         0.8831     17282
   macro avg     0.8394    0.8072    0.8206     17282
weighted avg     0.8724    0.8831    0.8759     17282

train confusion matrix:
[[9.79951283e-01 6.74536256e-03 0.00000000e+00 5.24639310e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.30953719e-03
  2.62319655e-03 1.87371182e-04 1.87371182e-04 0.00000000e+00
  7.49484729e-04 0.00000000e+00 0.00000000e+00]
 [7.99360512e-03 7.83373301e-01 0.00000000e+00 7.75379696e-02
  1.19904077e-03 0.00000000e+00 1.24700240e-01 3.99680256e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.99680256e-04
  7.99360512e-04 0.00000000e+00 0.00000000e+00]
 [1.23456790e-03 1.23456790e-02 8.87654321e-01 1.23456790e-03
  2.46913580e-03 8.02469136e-02 1.23456790e-03 4.93827160e-03
  0.00000000e+00 6.17283951e-03 0.00000000e+00 2.46913580e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [1.95238095e-02 4.19047619e-02 0.00000000e+00 9.35714286e-01
  0.00000000e+00 0.00000000e+00 1.90476190e-03 9.52380952e-04
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [1.35685210e-03 8.95522388e-02 0.00000000e+00 4.07055631e-03
  8.42605156e-01 0.00000000e+00 1.35685210e-03 4.07055631e-02
  0.00000000e+00 0.00000000e+00 5.42740841e-03 1.08548168e-02
  4.07055631e-03 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 2.95420975e-03 1.03397341e-02 0.00000000e+00
  0.00000000e+00 9.85228951e-01 1.47710487e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [1.51171580e-03 4.30839002e-02 0.00000000e+00 0.00000000e+00
  7.55857899e-04 5.29100529e-03 9.46334089e-01 7.55857899e-04
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  2.26757370e-03 0.00000000e+00 0.00000000e+00]
 [4.81099656e-02 1.05670103e-01 8.59106529e-04 2.92096220e-02
  1.11683849e-02 0.00000000e+00 6.01374570e-03 7.46563574e-01
  0.00000000e+00 0.00000000e+00 1.89003436e-02 1.63230241e-02
  1.71821306e-03 0.00000000e+00 1.54639175e-02]
 [1.66270784e-01 7.12589074e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.75059382e-03
  8.21852732e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [4.73815461e-02 1.74563591e-02 2.49376559e-03 0.00000000e+00
  4.98753117e-03 0.00000000e+00 2.49376559e-03 4.98753117e-03
  0.00000000e+00 9.10224439e-01 0.00000000e+00 4.98753117e-03
  4.98753117e-03 0.00000000e+00 0.00000000e+00]
 [2.52525253e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.80808081e-02
  0.00000000e+00 0.00000000e+00 9.09090909e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 7.57575758e-03]
 [1.59489633e-03 3.50877193e-02 3.18979266e-03 4.78468900e-03
  1.59489633e-02 0.00000000e+00 0.00000000e+00 1.54704944e-01
  0.00000000e+00 4.78468900e-03 1.59489633e-03 7.78309410e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [6.52920962e-02 4.46735395e-02 0.00000000e+00 0.00000000e+00
  3.43642612e-03 0.00000000e+00 1.03092784e-02 3.43642612e-03
  3.43642612e-03 3.43642612e-03 0.00000000e+00 3.43642612e-03
  8.62542955e-01 0.00000000e+00 0.00000000e+00]
 [9.96168582e-02 4.29118774e-01 0.00000000e+00 4.17624521e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.59770115e-02
  0.00000000e+00 3.83141762e-03 0.00000000e+00 0.00000000e+00
  3.83141762e-03 0.00000000e+00 0.00000000e+00]
 [3.40425532e-02 8.51063830e-03 4.25531915e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.34042553e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 7.19148936e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8322    0.8591    0.8455      1143
           1     0.5658    0.6175    0.5905       536
           2     0.8571    0.7283    0.7875       173
           3     0.6596    0.7622    0.7072       450
           4     0.8947    0.6456    0.7500       158
           5     0.8105    0.8552    0.8322       145
           6     0.7053    0.7951    0.7475       283
           7     0.5000    0.5261    0.5127       249
           8     0.7536    0.5778    0.6541        90
           9     0.6250    0.5294    0.5732        85
          10     0.8608    0.8095    0.8344        84
          11     0.8286    0.6493    0.7280       134
          12     0.6607    0.5968    0.6271        62
          13     0.0000    0.0000    0.0000        56
          14     0.5946    0.4400    0.5057        50

    accuracy                         0.7234      3698
   macro avg     0.6766    0.6261    0.6464      3698
weighted avg     0.7186    0.7234    0.7180      3698

validation confusion matrix:
[[0.85914261 0.03412073 0.00524934 0.03062117 0.         0.
  0.00612423 0.02624672 0.01312336 0.01137358 0.00174978 0.00087489
  0.00699913 0.         0.00437445]
 [0.07276119 0.61753731 0.00746269 0.13619403 0.00559701 0.00186567
  0.0988806  0.04291045 0.         0.         0.         0.00932836
  0.00373134 0.         0.00373134]
 [0.02312139 0.02890173 0.7283237  0.02312139 0.         0.10404624
  0.04624277 0.01156069 0.00578035 0.01734104 0.         0.01156069
  0.         0.         0.        ]
 [0.07555556 0.13333333 0.         0.76222222 0.         0.00222222
  0.         0.01333333 0.00222222 0.00222222 0.00444444 0.00222222
  0.         0.         0.00222222]
 [0.         0.17721519 0.         0.02531646 0.64556962 0.
  0.03797468 0.09493671 0.         0.         0.         0.00632911
  0.01265823 0.         0.        ]
 [0.02758621 0.0137931  0.04827586 0.00689655 0.         0.85517241
  0.04827586 0.         0.         0.         0.         0.
  0.         0.         0.        ]
 [0.02473498 0.09893993 0.01413428 0.00706714 0.00706714 0.02473498
  0.795053   0.01413428 0.         0.01060071 0.         0.
  0.00353357 0.         0.        ]
 [0.09638554 0.1686747  0.         0.11646586 0.01606426 0.
  0.02008032 0.52610442 0.         0.         0.01204819 0.02409639
  0.         0.         0.02008032]
 [0.27777778 0.03333333 0.         0.05555556 0.         0.
  0.         0.03333333 0.57777778 0.01111111 0.         0.
  0.01111111 0.         0.        ]
 [0.27058824 0.07058824 0.         0.         0.         0.01176471
  0.04705882 0.01176471 0.         0.52941176 0.         0.02352941
  0.03529412 0.         0.        ]
 [0.02380952 0.02380952 0.         0.02380952 0.         0.
  0.         0.0952381  0.         0.         0.80952381 0.
  0.         0.         0.02380952]
 [0.02238806 0.09701493 0.         0.02238806 0.01492537 0.
  0.02238806 0.14925373 0.         0.02238806 0.         0.64925373
  0.         0.         0.        ]
 [0.22580645 0.08064516 0.         0.         0.         0.01612903
  0.01612903 0.01612903 0.         0.0483871  0.         0.
  0.59677419 0.         0.        ]
 [0.17857143 0.35714286 0.         0.30357143 0.         0.
  0.         0.07142857 0.         0.         0.05357143 0.
  0.03571429 0.         0.        ]
 [0.18       0.02       0.         0.04       0.02       0.
  0.         0.28       0.         0.         0.02       0.
  0.         0.         0.44      ]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8205    0.8463    0.8332      1145
           1     0.5116    0.6164    0.5591       537
           2     0.8800    0.7543    0.8123       175
           3     0.6861    0.7561    0.7194       451
           4     0.8699    0.6730    0.7589       159
           5     0.8013    0.8562    0.8278       146
           6     0.6949    0.8099    0.7480       284
           7     0.4385    0.4280    0.4332       250
           8     0.7595    0.6593    0.7059        91
           9     0.6324    0.4943    0.5548        87
          10     0.8939    0.6860    0.7763        86
          11     0.8333    0.6250    0.7143       136
          12     0.6250    0.4688    0.5357        64
          13     0.0000    0.0000    0.0000        57
          14     0.5556    0.2885    0.3797        52

    accuracy                         0.7081      3720
   macro avg     0.6668    0.5975    0.6239      3720
weighted avg     0.7051    0.7081    0.7026      3720

test confusion matrix:
[[0.84628821 0.05676856 0.         0.02445415 0.00087336 0.00174672
  0.00436681 0.02620087 0.0139738  0.01222707 0.00262009 0.00174672
  0.00524017 0.         0.00349345]
 [0.06703911 0.61638734 0.0018622  0.12662942 0.01117318 0.00372439
  0.12290503 0.03538175 0.0018622  0.00558659 0.         0.0018622
  0.00372439 0.         0.0018622 ]
 [0.04       0.05714286 0.75428571 0.01142857 0.00571429 0.09142857
  0.02285714 0.         0.         0.01142857 0.         0.00571429
  0.         0.         0.        ]
 [0.07095344 0.13525499 0.00221729 0.75609756 0.00443459 0.
  0.         0.02660754 0.         0.         0.         0.00443459
  0.         0.         0.        ]
 [0.01257862 0.18867925 0.         0.05660377 0.67295597 0.
  0.01886792 0.05031447 0.         0.         0.         0.
  0.         0.         0.        ]
 [0.01369863 0.00684932 0.06849315 0.         0.         0.85616438
  0.05479452 0.         0.         0.         0.         0.
  0.         0.         0.        ]
 [0.00352113 0.10211268 0.01056338 0.00704225 0.00704225 0.02816901
  0.80985915 0.01408451 0.         0.00352113 0.         0.00352113
  0.01056338 0.         0.        ]
 [0.184      0.188      0.         0.092      0.012      0.004
  0.036      0.428      0.004      0.         0.008      0.024
  0.004      0.         0.016     ]
 [0.20879121 0.08791209 0.         0.         0.         0.
  0.01098901 0.01098901 0.65934066 0.         0.         0.
  0.01098901 0.01098901 0.        ]
 [0.24137931 0.12643678 0.         0.02298851 0.         0.01149425
  0.02298851 0.         0.01149425 0.49425287 0.         0.01149425
  0.05747126 0.         0.        ]
 [0.12790698 0.01162791 0.         0.01162791 0.         0.
  0.         0.13953488 0.         0.         0.68604651 0.
  0.         0.         0.02325581]
 [0.03676471 0.11029412 0.00735294 0.04411765 0.00735294 0.00735294
  0.00735294 0.13235294 0.         0.01470588 0.         0.625
  0.         0.         0.00735294]
 [0.265625   0.125      0.03125    0.         0.         0.
  0.015625   0.03125    0.         0.046875   0.         0.015625
  0.46875    0.         0.        ]
 [0.12280702 0.47368421 0.         0.1754386  0.         0.
  0.01754386 0.15789474 0.         0.         0.03508772 0.01754386
  0.         0.         0.        ]
 [0.11538462 0.05769231 0.         0.09615385 0.         0.
  0.         0.42307692 0.         0.         0.         0.01923077
  0.         0.         0.28846154]]
---------------------------------------
program finished.
