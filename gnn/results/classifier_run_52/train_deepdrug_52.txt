seed:  12
save trained model at:  ../trained_models/trained_classifier_model_52.pt
save loss at:  ./results/train_classifier_results_52.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['2hxsA00', '5cr2B00', '3lf2C00', '5f8eA00', '5yh3A00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['2fhiA00', '2yyeB00', '3nd6E00', '6cauA00', '3ijpA00']
model architecture:
DeepDruG(
  (embedding_net): JKMCNMMEmbeddingNet(
    (conv0): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=33, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b6e5a575880>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 1.9943713867637523, acc: 0.40020125488339053; test loss: 1.9104113162366867, acc: 0.43157645946584733
epoch: 2, train loss: 1.6867251968587011, acc: 0.47513910263999054; test loss: 1.6343497244411822, acc: 0.4904277948475538
epoch: 3, train loss: 1.590660240334901, acc: 0.5087013140760033; test loss: 1.5420832150485662, acc: 0.5187898841881352
epoch: 4, train loss: 1.5278418552131783, acc: 0.5329110926956316; test loss: 1.5863907824056251, acc: 0.4842826754904278
epoch: 5, train loss: 1.4528198590345802, acc: 0.5586006866343081; test loss: 1.472543669378073, acc: 0.5341526825809502
epoch: 6, train loss: 1.3831299793879719, acc: 0.5800876050668876; test loss: 1.3807665597347187, acc: 0.5800047270148901
epoch: 7, train loss: 1.3265527916812636, acc: 0.5964839588019415; test loss: 1.4375607573487352, acc: 0.5603876152209879
epoch: 8, train loss: 1.292066538631669, acc: 0.6073753995501362; test loss: 1.4247351055645656, acc: 0.5606239659654928
epoch: 9, train loss: 1.2267854073999676, acc: 0.6277376583402391; test loss: 1.2472815293280741, acc: 0.6175844953911604
epoch: 10, train loss: 1.2092022202460035, acc: 0.6303421333017639; test loss: 1.3853074104558827, acc: 0.5594422122429685
epoch: 11, train loss: 1.1581091366534517, acc: 0.6427725819817687; test loss: 1.244208849115987, acc: 0.6166390924131411
epoch: 12, train loss: 1.129267769197437, acc: 0.6541967562448207; test loss: 1.299495851534305, acc: 0.6062396596549279
epoch: 13, train loss: 1.077153733443604, acc: 0.674263051971114; test loss: 1.3495107698992785, acc: 0.5941857716851808
epoch: 14, train loss: 1.0533949928734165, acc: 0.6787024979282585; test loss: 1.3730566097585002, acc: 0.6067123611439376
epoch: 15, train loss: 1.0246539549233717, acc: 0.692198413637978; test loss: 1.1220455601885369, acc: 0.6662727487591585
epoch: 16, train loss: 1.0146566350756134, acc: 0.693027110216645; test loss: 1.1027871164918313, acc: 0.6620184353580714
epoch: 17, train loss: 1.0029925004510438, acc: 0.6953356221143602; test loss: 1.2187194917196924, acc: 0.6372016071850626
epoch: 18, train loss: 0.9783926856465575, acc: 0.701906002130934; test loss: 1.0863556408256652, acc: 0.6787993382179154
epoch: 19, train loss: 0.9470707078339631, acc: 0.7126790576536048; test loss: 1.068893893788932, acc: 0.6648546442921295
epoch: 20, train loss: 0.96342856266999, acc: 0.7106073162069374; test loss: 1.0350596595390895, acc: 0.6934530843772158
epoch: 21, train loss: 0.9137240503685875, acc: 0.7258790102995146; test loss: 1.1198846962513356, acc: 0.6676908532261877
epoch: 22, train loss: 0.9139972724998662, acc: 0.7249911211080857; test loss: 1.338486447398529, acc: 0.5849680926494919
epoch: 23, train loss: 0.8946352060538038, acc: 0.7272996330058009; test loss: 0.9672802854897254, acc: 0.7125974946821082
epoch: 24, train loss: 0.8813369804985554, acc: 0.7351130578903753; test loss: 0.9933929955021086, acc: 0.6998345544788466
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.6781809691410735, acc: 0.7390789629454244; test loss: 0.8030018150538762, acc: 0.6981800992673127
epoch: 26, train loss: 0.6588190258064397, acc: 0.7461820764768556; test loss: 0.7149824258199794, acc: 0.7225242259513117
epoch: 27, train loss: 0.6699351223951489, acc: 0.7403812004261868; test loss: 0.9251429563548035, acc: 0.6587095249350036
epoch: 28, train loss: 0.6523124394620595, acc: 0.7479578548597136; test loss: 0.7495590734921345, acc: 0.718978964783739
epoch: 29, train loss: 0.6484844436500995, acc: 0.749970403693619; test loss: 0.9050188563839816, acc: 0.6575277712124793
epoch: 30, train loss: 0.636503799160345, acc: 0.7537587309103824; test loss: 0.8415756502829949, acc: 0.6830536516190026
epoch: 31, train loss: 0.633004797807569, acc: 0.7532259973955251; test loss: 0.7940945368347312, acc: 0.6958165918222642
epoch: 32, train loss: 0.6108575313207284, acc: 0.7586125251568604; test loss: 1.3376477369882345, acc: 0.5275348617348145
epoch: 33, train loss: 0.6182895131092516, acc: 0.75890848822067; test loss: 0.6928861236437025, acc: 0.735759867643583
epoch: 34, train loss: 0.5986308159807864, acc: 0.7655972534627679; test loss: 1.0539592389264758, acc: 0.6100212715670055
epoch: 35, train loss: 0.6063954502368829, acc: 0.766307564815911; test loss: 0.7042960970149877, acc: 0.7315055542424959
epoch: 36, train loss: 0.5782997476161267, acc: 0.7767846572747721; test loss: 0.738044431941556, acc: 0.7159064051051761
epoch: 37, train loss: 0.6001505907702138, acc: 0.7688528471646738; test loss: 0.7121938885427541, acc: 0.7315055542424959
epoch: 38, train loss: 0.5682996948244897, acc: 0.7780868947555345; test loss: 0.8639205196906252, acc: 0.6785629874734106
epoch: 39, train loss: 0.5520358577527646, acc: 0.7851900082869658; test loss: 0.7131332022674369, acc: 0.7329236587095249
epoch: 40, train loss: 0.6154115771462653, acc: 0.7603883035397182; test loss: 1.1247629408181572, acc: 0.6171117939021508
epoch: 41, train loss: 0.5583141383100744, acc: 0.7831182668402983; test loss: 0.7022513811313861, acc: 0.7322146064760104
epoch: 42, train loss: 0.5627887640194709, acc: 0.7790339765597254; test loss: 0.8107742730169762, acc: 0.7239423304183408
epoch: 43, train loss: 0.5380738788883376, acc: 0.7883864093761098; test loss: 0.7934983187999829, acc: 0.6998345544788466
epoch: 44, train loss: 0.5758688531571681, acc: 0.7715757073517225; test loss: 0.8588155134719925, acc: 0.6851808083195462
Epoch    44: reducing learning rate of group 0 to 1.5000e-03.
epoch: 45, train loss: 0.4803275178632356, acc: 0.8093997869065941; test loss: 0.5987522822411396, acc: 0.7771212479319309
epoch: 46, train loss: 0.4054516533309643, acc: 0.837989818870605; test loss: 0.6174649989962775, acc: 0.7738123375088631
epoch: 47, train loss: 0.39241377462631744, acc: 0.8402983307683202; test loss: 0.7334124000042613, acc: 0.7641219569841645
epoch: 48, train loss: 0.3870776450512634, acc: 0.8451521250147982; test loss: 0.6110292320920908, acc: 0.7809028598440085
epoch: 49, train loss: 0.3898447115766772, acc: 0.8434355392447023; test loss: 0.6972150494939073, acc: 0.754195225714961
epoch: 50, train loss: 0.37980344034912206, acc: 0.8465727477210844; test loss: 0.6163686553627795, acc: 0.7757031434649019
epoch: 51, train loss: 0.3824958968962722, acc: 0.8460400142062271; test loss: 0.6843155107157812, acc: 0.7596312928385724
epoch: 52, train loss: 0.3769453066092801, acc: 0.8450337397892743; test loss: 0.6678996893621961, acc: 0.7714488300638147
epoch: 53, train loss: 0.40061505467874475, acc: 0.8391736711258435; test loss: 0.6246459182062152, acc: 0.7747577404868825
epoch: 54, train loss: 0.36696610962891557, acc: 0.8529655498993726; test loss: 0.7050415631643397, acc: 0.7414322855116994
epoch: 55, train loss: 0.3440367340674637, acc: 0.8616668639753758; test loss: 0.6363411374589113, acc: 0.7813755613330182
epoch: 56, train loss: 0.3483113160115862, acc: 0.8566946845033739; test loss: 0.6045607875859137, acc: 0.7948475537697943
epoch: 57, train loss: 0.358057454702938, acc: 0.8524328163845152; test loss: 0.7154268827372988, acc: 0.7530134719924367
epoch: 58, train loss: 0.3472088174847042, acc: 0.8575825736948028; test loss: 0.7200102016622083, acc: 0.7546679272039707
epoch: 59, train loss: 0.34300963820346475, acc: 0.8579377293713745; test loss: 0.8566756262843476, acc: 0.7185062632947293
epoch: 60, train loss: 0.3276653797884474, acc: 0.8674677400260448; test loss: 0.636758891185444, acc: 0.7875206806901441
epoch: 61, train loss: 0.3107142946389041, acc: 0.8716112229193796; test loss: 0.7211610426211915, acc: 0.7631765540061451
epoch: 62, train loss: 0.30569230547328946, acc: 0.8708417189534746; test loss: 0.6402225626526474, acc: 0.7813755613330182
epoch: 63, train loss: 0.2992349090139527, acc: 0.8740381200426187; test loss: 0.6645794202182006, acc: 0.7946112030252895
epoch: 64, train loss: 0.304508303910074, acc: 0.8717888007576654; test loss: 0.7462892153955638, acc: 0.7440321437012527
epoch: 65, train loss: 0.3047256971500718, acc: 0.8745116609447141; test loss: 0.705475289780489, acc: 0.7778303001654455
epoch: 66, train loss: 0.29995444626572204, acc: 0.8756363205871908; test loss: 0.6929922654988833, acc: 0.7823209643110376
epoch: 67, train loss: 0.3088671022143408, acc: 0.8730910382384278; test loss: 0.6587745251314095, acc: 0.7714488300638147
epoch: 68, train loss: 0.29253879048365083, acc: 0.8781816029359536; test loss: 0.7405472039556538, acc: 0.7530134719924367
epoch: 69, train loss: 0.28802058944282155, acc: 0.8789511069018586; test loss: 0.6202557245570981, acc: 0.7896478373906878
epoch: 70, train loss: 0.28751133580779104, acc: 0.8803125369953829; test loss: 0.7530880466867806, acc: 0.757267785393524
epoch: 71, train loss: 0.2788649738378266, acc: 0.8841008642121463; test loss: 0.6582961084430761, acc: 0.7934294493027653
epoch: 72, train loss: 0.3147833884376259, acc: 0.8710192967917604; test loss: 0.8248684308230807, acc: 0.7123611439376034
epoch: 73, train loss: 0.3222448548558507, acc: 0.866461465609092; test loss: 1.0626230997217656, acc: 0.6686362562042071
epoch: 74, train loss: 0.2926360612806377, acc: 0.8784183733870013; test loss: 0.7420308166743733, acc: 0.7549042779484756
epoch: 75, train loss: 0.25883204386836045, acc: 0.8918550964839588; test loss: 0.6671814735097487, acc: 0.7797211061214843
epoch: 76, train loss: 0.26179040056919367, acc: 0.8905528590031964; test loss: 0.6741914291580189, acc: 0.7856298747341054
epoch: 77, train loss: 0.25136498557726733, acc: 0.8948147271220551; test loss: 0.698272428341268, acc: 0.7830300165445521
epoch: 78, train loss: 0.2625336416751037, acc: 0.8925654078371019; test loss: 0.6746812509718417, acc: 0.7905932403687072
epoch: 79, train loss: 0.2594197012640195, acc: 0.8905528590031964; test loss: 0.6728524018393373, acc: 0.781611912077523
epoch: 80, train loss: 0.21892968511685854, acc: 0.9089617615721558; test loss: 0.7492251160634491, acc: 0.7584495391160482
epoch: 81, train loss: 0.24307932244527547, acc: 0.898247898662247; test loss: 0.7139356584871275, acc: 0.7948475537697943
epoch: 82, train loss: 0.23485067505109405, acc: 0.9034568485852965; test loss: 0.7246972177361687, acc: 0.7858662254786103
epoch: 83, train loss: 0.23434827489529444, acc: 0.899846099206819; test loss: 0.6905744959913058, acc: 0.7879933821791538
epoch: 84, train loss: 0.23796584786953298, acc: 0.8985438617260566; test loss: 0.7395266554536506, acc: 0.784684471756086
epoch: 85, train loss: 0.22963067852167937, acc: 0.9029833076832011; test loss: 0.7576544046458328, acc: 0.7686126211297566
epoch: 86, train loss: 0.2511625051202048, acc: 0.8925654078371019; test loss: 0.7400831090899626, acc: 0.7809028598440085
epoch: 87, train loss: 0.23400536982903358, acc: 0.8984846691132946; test loss: 0.7009296573608712, acc: 0.7792484046324746
epoch: 88, train loss: 0.21943212081528815, acc: 0.9045815082277732; test loss: 0.6963899364935763, acc: 0.7882297329236587
epoch: 89, train loss: 0.21171652973474406, acc: 0.9100272286018705; test loss: 0.6706078400484525, acc: 0.7988655164263767
epoch: 90, train loss: 0.2243940874897151, acc: 0.9020954184917722; test loss: 0.7357765076093374, acc: 0.7799574568659892
epoch: 91, train loss: 0.2196241548950607, acc: 0.9047590860660589; test loss: 0.6863680239744824, acc: 0.796974710470338
epoch: 92, train loss: 0.2083534524823782, acc: 0.9107375399550136; test loss: 0.6630347030885787, acc: 0.8019380761049397
epoch: 93, train loss: 0.20589734966181658, acc: 0.9100864212146325; test loss: 0.7101341435543557, acc: 0.7792484046324746
epoch: 94, train loss: 0.19836657680681333, acc: 0.9163016455546348; test loss: 0.7031172422392378, acc: 0.7858662254786103
epoch: 95, train loss: 0.20403899037550086, acc: 0.9126908961761572; test loss: 0.685052075754579, acc: 0.8061923895060269
epoch: 96, train loss: 0.23012397493415152, acc: 0.9028649224576772; test loss: 0.7408280601965792, acc: 0.7868116284566297
epoch: 97, train loss: 0.19887914597931677, acc: 0.9141115188824435; test loss: 0.6818530424298814, acc: 0.7967383597258332
epoch: 98, train loss: 0.1862412463426872, acc: 0.9189653131289215; test loss: 0.7473212837806635, acc: 0.7844481210115812
epoch: 99, train loss: 0.21200875805456482, acc: 0.9099088433763466; test loss: 0.6469435628721039, acc: 0.7981564641928622
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.140770172508663, acc: 0.9167159938439683; test loss: 0.5925561588894135, acc: 0.8047742850389978
epoch: 101, train loss: 0.11899780814846944, acc: 0.9279625902687345; test loss: 0.6243965542646087, acc: 0.7849208225005909
epoch: 102, train loss: 0.11948504270082735, acc: 0.9254765005327336; test loss: 0.6962403804521644, acc: 0.7601039943275821
epoch: 103, train loss: 0.11901428487768848, acc: 0.9297383686515923; test loss: 0.6284480005312518, acc: 0.7889387851571732
epoch: 104, train loss: 0.1257678369128296, acc: 0.9253581153072097; test loss: 0.5860466424580923, acc: 0.7979201134483573
epoch: 105, train loss: 0.12995241215289155, acc: 0.92014916538416; test loss: 0.586076961429202, acc: 0.784684471756086
epoch: 106, train loss: 0.13626170292456108, acc: 0.9173079199715876; test loss: 0.607359952127587, acc: 0.7882297329236587
Epoch   106: reducing learning rate of group 0 to 7.5000e-04.
epoch: 107, train loss: 0.08884498266615895, acc: 0.9425831656209305; test loss: 0.5854278714683314, acc: 0.8102103521626093
epoch: 108, train loss: 0.057873624007929725, acc: 0.9611104534154138; test loss: 0.6467138750562711, acc: 0.8076104939730561
epoch: 109, train loss: 0.04888654696852466, acc: 0.9651947436959868; test loss: 0.6796823407538082, acc: 0.8059560387615221
epoch: 110, train loss: 0.047402022405356306, acc: 0.9678584112702735; test loss: 0.6911885319971018, acc: 0.8035925313164737
epoch: 111, train loss: 0.04955471318902681, acc: 0.9667337516277968; test loss: 0.6973683101238863, acc: 0.8033561805719688
epoch: 112, train loss: 0.05711548794839345, acc: 0.9616431869302711; test loss: 0.7010007587569004, acc: 0.8019380761049397
epoch: 113, train loss: 0.11206427499576053, acc: 0.932402036225879; test loss: 0.6048303511204153, acc: 0.8021744268494446
epoch: 114, train loss: 0.07563842962172605, acc: 0.9500414348289333; test loss: 0.6748447612903553, acc: 0.8033561805719688
epoch: 115, train loss: 0.06623827208266168, acc: 0.9541257251095063; test loss: 0.713885209600403, acc: 0.787757031434649
epoch: 116, train loss: 0.057644698078658886, acc: 0.9589203267432225; test loss: 0.7233700117752699, acc: 0.8038288820609785
epoch: 117, train loss: 0.056436323313031715, acc: 0.9611104534154138; test loss: 0.6712812409843855, acc: 0.8080831954620658
epoch: 118, train loss: 0.0556158788812694, acc: 0.9625310761217; test loss: 0.6550345870513663, acc: 0.810683053651619
epoch: 119, train loss: 0.05565133236540031, acc: 0.9621167278323666; test loss: 0.6954042284349492, acc: 0.8038288820609785
epoch: 120, train loss: 0.047396824575322925, acc: 0.96898307091275; test loss: 0.7811968635932347, acc: 0.7818482628220279
epoch: 121, train loss: 0.06380609171188299, acc: 0.9579140523262697; test loss: 0.6448234668495919, acc: 0.8113921058851336
epoch: 122, train loss: 0.054017817962535866, acc: 0.9644844323428436; test loss: 0.6961067430631006, acc: 0.803119829827464
epoch: 123, train loss: 0.0630610907249434, acc: 0.9594530602580797; test loss: 0.7453962395980662, acc: 0.7884660836681635
epoch: 124, train loss: 0.058709653024950355, acc: 0.9602225642239849; test loss: 0.6714599004622415, acc: 0.8035925313164737
epoch: 125, train loss: 0.057392892252768064, acc: 0.9607552977388422; test loss: 0.6666421013164678, acc: 0.8057196880170172
epoch: 126, train loss: 0.0712344940957747, acc: 0.9548952290754114; test loss: 0.6636636885478687, acc: 0.7872843299456393
epoch: 127, train loss: 0.07507830021175899, acc: 0.9509885166331242; test loss: 0.6495236117798677, acc: 0.8066650909950366
epoch: 128, train loss: 0.06051077152922885, acc: 0.9591570971942701; test loss: 0.6532206257531841, acc: 0.804537934294493
epoch: 129, train loss: 0.056937979584324075, acc: 0.9620575352196046; test loss: 0.639171396437843, acc: 0.8125738596076577
epoch: 130, train loss: 0.04547774651032362, acc: 0.9699893453297028; test loss: 0.6922202744514377, acc: 0.8040652328054834
epoch: 131, train loss: 0.05556154198352721, acc: 0.9619983426068427; test loss: 0.6548099745477917, acc: 0.8102103521626093
epoch: 132, train loss: 0.052682680624384026, acc: 0.9625310761217; test loss: 0.6622792332765763, acc: 0.8196643819428031
epoch: 133, train loss: 0.04147952412576722, acc: 0.9721202793891323; test loss: 0.8067618247550364, acc: 0.783266367289057
epoch: 134, train loss: 0.12229712733654238, acc: 0.931573339647212; test loss: 0.6197601795421935, acc: 0.7853935239896006
epoch: 135, train loss: 0.07516616273980543, acc: 0.9532378359180774; test loss: 0.615719047231726, acc: 0.8050106357835027
epoch: 136, train loss: 0.08130596198271235, acc: 0.9470226115780751; test loss: 0.691894207911355, acc: 0.7950839045142992
epoch: 137, train loss: 0.05823198833454427, acc: 0.9608144903516042; test loss: 0.6806965776719373, acc: 0.8109194043961239
epoch: 138, train loss: 0.0436999104018252, acc: 0.9701077305552267; test loss: 0.7161076495732519, acc: 0.7991018671708816
epoch: 139, train loss: 0.03873527967603133, acc: 0.9728305907422754; test loss: 0.6871737355009316, acc: 0.813755613330182
epoch: 140, train loss: 0.07781587189699159, acc: 0.95270510240322; test loss: 0.7190629612945885, acc: 0.7780666509099504
epoch: 141, train loss: 0.09701865716729628, acc: 0.9418728542677873; test loss: 0.5858616611124128, acc: 0.8028834790829591
epoch: 142, train loss: 0.047705798386835946, acc: 0.9672072925298922; test loss: 0.6589926082849559, acc: 0.8158827700307256
epoch: 143, train loss: 0.0453997273829416, acc: 0.9688646856872263; test loss: 0.6581806125327561, acc: 0.8066650909950366
epoch: 144, train loss: 0.04760706476224193, acc: 0.96744406298094; test loss: 0.7156399821761764, acc: 0.7960293074923186
epoch: 145, train loss: 0.06067852463696262, acc: 0.9611104534154138; test loss: 0.6791174600829303, acc: 0.7995745686598913
epoch: 146, train loss: 0.05261720122238708, acc: 0.9644252397300817; test loss: 0.650726887836605, acc: 0.8061923895060269
epoch: 147, train loss: 0.07461289448368771, acc: 0.9522315615011246; test loss: 0.6500044299535677, acc: 0.8059560387615221
epoch: 148, train loss: 0.06463423649157182, acc: 0.9576772818752219; test loss: 0.6216902429683923, acc: 0.8059560387615221
epoch: 149, train loss: 0.043311079438126156, acc: 0.9686871078489404; test loss: 0.6955381453502153, acc: 0.8095012999290948
epoch: 150, train loss: 0.04494900311478654, acc: 0.9691014561382739; test loss: 0.6848945491450866, acc: 0.810683053651619
epoch: 151, train loss: 0.04122997005329522, acc: 0.971528353261513; test loss: 0.7129009329830324, acc: 0.7943748522807846
epoch: 152, train loss: 0.05342903535809124, acc: 0.9640700840535101; test loss: 0.6596128191967097, acc: 0.803119829827464
epoch: 153, train loss: 0.05693818556460824, acc: 0.9633005800876051; test loss: 0.6827625163591547, acc: 0.7993382179153864
epoch: 154, train loss: 0.0627980479015547, acc: 0.9577956671007458; test loss: 0.6606896741822436, acc: 0.8092649491845899
epoch: 155, train loss: 0.055042240093911454, acc: 0.9628862317982716; test loss: 0.6326060923693789, acc: 0.8078468447175609
epoch: 156, train loss: 0.051024881876123945, acc: 0.9648987806321772; test loss: 0.6268776639137277, acc: 0.8154100685417159
epoch: 157, train loss: 0.052276757034297044, acc: 0.9659642476618918; test loss: 0.6474769021061063, acc: 0.8043015835499882
Epoch   157: reducing learning rate of group 0 to 3.7500e-04.
epoch: 158, train loss: 0.03332106970101542, acc: 0.9773292293121818; test loss: 0.6413934436483435, acc: 0.8149373670527063
epoch: 159, train loss: 0.0191376528998049, acc: 0.9865040842902806; test loss: 0.6919868563230763, acc: 0.8170645237532498
epoch: 160, train loss: 0.014787022833002027, acc: 0.9900556410559962; test loss: 0.7067507991439145, acc: 0.8180099267312692
epoch: 161, train loss: 0.014593712444384874, acc: 0.990233218894282; test loss: 0.7145091203672895, acc: 0.8227369416213661
epoch: 162, train loss: 0.01419741950701599, acc: 0.9914762637622825; test loss: 0.740274972032075, acc: 0.8149373670527063
epoch: 163, train loss: 0.012316259898521413, acc: 0.9913578785367586; test loss: 0.7179425372437251, acc: 0.8298274639565114
epoch: 164, train loss: 0.01363687067145661, acc: 0.9915946489878064; test loss: 0.7411484117063893, acc: 0.8194280311982983
epoch: 165, train loss: 0.02005008749846412, acc: 0.9873919734817095; test loss: 0.7014950517502498, acc: 0.8173008744977547
epoch: 166, train loss: 0.03980643612525478, acc: 0.9747247543506571; test loss: 0.7043244222522881, acc: 0.8052469865280075
epoch: 167, train loss: 0.031443393489669395, acc: 0.9783946963418966; test loss: 0.6464355222949517, acc: 0.8189553297092886
epoch: 168, train loss: 0.019169162978265247, acc: 0.9883390552859003; test loss: 0.7174119991603627, acc: 0.8147010163082014
epoch: 169, train loss: 0.018799210195483095, acc: 0.9876879365455191; test loss: 0.6976888781319666, acc: 0.8217915386433468
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.01109941571803612, acc: 0.9888717888007577; test loss: 0.6311235163291277, acc: 0.8210824864098322
epoch: 171, train loss: 0.008286263262388854, acc: 0.9921865751154256; test loss: 0.6061350735218298, acc: 0.8251004490664146
epoch: 172, train loss: 0.013058621664033728, acc: 0.9890493666390434; test loss: 0.6149786183017558, acc: 0.8173008744977547
epoch: 173, train loss: 0.00957475835694492, acc: 0.9894637149283769; test loss: 0.6021148549611618, acc: 0.819900732687308
epoch: 174, train loss: 0.009138378682263514, acc: 0.9907659524091393; test loss: 0.6131252800705472, acc: 0.8173008744977547
epoch: 175, train loss: 0.006957997379967189, acc: 0.9935480052089499; test loss: 0.6143255841503806, acc: 0.8191916804537934
epoch: 176, train loss: 0.006830556596557325, acc: 0.9938439682727596; test loss: 0.6159811021587973, acc: 0.8154100685417159
epoch: 177, train loss: 0.007745183855352514, acc: 0.9921865751154256; test loss: 0.6393127092033716, acc: 0.8173008744977547
epoch: 178, train loss: 0.010245461625279555, acc: 0.989345329702853; test loss: 0.6235641733294428, acc: 0.8184826282202788
epoch: 179, train loss: 0.011067730704296216, acc: 0.9899372558304723; test loss: 0.6133569967828443, acc: 0.812101158118648
epoch: 180, train loss: 0.010051670196810475, acc: 0.9901740262815201; test loss: 0.6165144686945728, acc: 0.8177735759867644
epoch: 181, train loss: 0.011357367852407475, acc: 0.9890493666390434; test loss: 0.6195417779806629, acc: 0.8144646655636966
epoch: 182, train loss: 0.01455203263587192, acc: 0.9868000473540902; test loss: 0.604313256244123, acc: 0.8163554715197353
epoch: 183, train loss: 0.018435101056670147, acc: 0.9849058837457085; test loss: 0.601297418943282, acc: 0.8144646655636966
epoch: 184, train loss: 0.026356431324931522, acc: 0.9757310287676098; test loss: 0.5884614539636672, acc: 0.8184826282202788
epoch: 185, train loss: 0.017259846866700627, acc: 0.9833668758138985; test loss: 0.5861643987523554, acc: 0.8116284566296383
epoch: 186, train loss: 0.04963286225525772, acc: 0.9632413874748431; test loss: 0.5845091987856452, acc: 0.8092649491845899
epoch: 187, train loss: 0.022202332530224043, acc: 0.980407245175802; test loss: 0.5458604071445144, acc: 0.8189553297092886
epoch: 188, train loss: 0.014158495830075831, acc: 0.9863856990647567; test loss: 0.5924405169188343, acc: 0.8135192625856772
epoch: 189, train loss: 0.009620327662027833, acc: 0.9897004853794247; test loss: 0.5963688582512989, acc: 0.816828173008745
epoch: 190, train loss: 0.012560654253409687, acc: 0.9876287439327572; test loss: 0.6161180288620914, acc: 0.8189553297092886
epoch: 191, train loss: 0.01617061806862751, acc: 0.98526103942228; test loss: 0.6014714611250017, acc: 0.8147010163082014
epoch: 192, train loss: 0.014326023849260317, acc: 0.986977625192376; test loss: 0.6232852353691294, acc: 0.813755613330182
epoch: 193, train loss: 0.014539706272993608, acc: 0.9853794246478039; test loss: 0.6035669876367762, acc: 0.8165918222642401
epoch: 194, train loss: 0.033386017145934976, acc: 0.9736592873209423; test loss: 0.6023786497071009, acc: 0.8087922476955802
epoch: 195, train loss: 0.027877466161111863, acc: 0.9766781105718007; test loss: 0.5920969676926355, acc: 0.8128102103521626
epoch: 196, train loss: 0.019649126844788686, acc: 0.9815319048182787; test loss: 0.5980492695880765, acc: 0.8076104939730561
epoch: 197, train loss: 0.014224883516569264, acc: 0.9852018468095182; test loss: 0.6060504396596831, acc: 0.813755613330182
epoch: 198, train loss: 0.015048632655582499, acc: 0.9865040842902806; test loss: 0.6188372740140341, acc: 0.8118648073741432
epoch: 199, train loss: 0.01652970898495569, acc: 0.9834852610394222; test loss: 0.6169765710661638, acc: 0.8144646655636966
epoch: 200, train loss: 0.013473196558251216, acc: 0.9875103587072334; test loss: 0.612699359169358, acc: 0.8125738596076577
best test acc 0.8298274639565114 at epoch 163.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9997    0.9995    0.9996      6100
           1     0.9946    0.9978    0.9962       926
           2     0.9967    0.9996    0.9981      2400
           3     0.9988    1.0000    0.9994       843
           4     0.9949    1.0000    0.9974       774
           5     0.9947    0.9980    0.9964      1512
           6     0.9977    0.9880    0.9928      1330
           7     1.0000    1.0000    1.0000       481
           8     0.9978    0.9934    0.9956       458
           9     0.9826    1.0000    0.9912       452
          10     1.0000    0.9916    0.9958       717
          11     0.9970    1.0000    0.9985       333
          12     0.9966    0.9732    0.9848       299
          13     1.0000    1.0000    1.0000       269

    accuracy                         0.9975     16894
   macro avg     0.9965    0.9958    0.9961     16894
weighted avg     0.9975    0.9975    0.9975     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8921    0.9108    0.9014      1525
           1     0.8616    0.8319    0.8465       232
           2     0.8474    0.8686    0.8578       601
           3     0.8439    0.8199    0.8317       211
           4     0.8571    0.8969    0.8766       194
           5     0.8676    0.8492    0.8583       378
           6     0.6154    0.6727    0.6428       333
           7     0.8713    0.7273    0.7928       121
           8     0.6789    0.6435    0.6607       115
           9     0.9619    0.8860    0.9224       114
          10     0.8581    0.7389    0.7940       180
          11     0.7260    0.6310    0.6752        84
          12     0.2255    0.3067    0.2599        75
          13     0.9149    0.6324    0.7478        68

    accuracy                         0.8298      4231
   macro avg     0.7873    0.7440    0.7620      4231
weighted avg     0.8354    0.8298    0.8314      4231

---------------------------------------
program finished.
