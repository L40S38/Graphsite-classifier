seed:  666
number of classes (from original clusters): 10
how to merge clusters:  [[0, 9], [1, 5], 2, [3, 8], 4, 6, 7]
whether to further subcluster data according to chemical reaction: False
positive training pair sampling threshold:  13500
negative training pair sampling threshold:  4500
positive validation pair sampling threshold:  3900
negative validation pair sampling threshold:  1300
number of epochs to train: 60
batch size: 256
similar margin of contrastive loss: 0.15
dissimilar margin of contrastive loss: 1.8
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['x', 'y', 'z', 'charge', 'hydrophobicity', 'binding_probability', 'sasa', 'sequence_entropy']
number of classes after merging:  7
number of pockets in training set:  10527
number of pockets in validation set:  2254
number of pockets in test set:  2263
number of train positive pairs: 94500
number of train negative pairs: 94500
number of validation positive pairs: 27300
number of validation negative pairs: 27300
model architecture:
SiameseNet(
  (embedding_net): EmbeddingNet(
    (set2set): Set2Set(32, 64)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.15, dissimilar_margin=1.8, normalize=True, mean=True)
epoch: 1, train loss: 0.5471504574730283, validation loss: 0.528146862966237.
epoch: 2, train loss: 0.4490194837782118, validation loss: 0.5023182742499607.
epoch: 3, train loss: 0.3990230889295144, validation loss: 0.48459710117661475.
epoch: 4, train loss: 0.3673757480843357, validation loss: 0.47722581549005194.
epoch: 5, train loss: 0.34371940277745483, validation loss: 0.4690687478592981.
epoch: 6, train loss: 0.3242897359050771, validation loss: 0.44388281685965403.
epoch: 7, train loss: 0.30558201778754984, validation loss: 0.4394850087427831.
epoch: 8, train loss: 0.29448507644007443, validation loss: 0.4498965570079538.
epoch: 9, train loss: 0.2823277523303158, validation loss: 0.5350906922616365.
epoch: 10, train loss: 0.2731411728228211, validation loss: 0.4726653390402322.
epoch: 11, train loss: 0.26344977404074693, validation loss: 0.47868734953604336.
epoch: 12, train loss: 0.26059539415470506, validation loss: 0.478051515795809.
epoch: 13, train loss: 0.2535743707727503, validation loss: 0.4563252379868057.
epoch: 14, train loss: 0.2473749379152974, validation loss: 0.4402554254217462.
epoch: 15, train loss: 0.24355058448024527, validation loss: 0.4291023867820209.
epoch: 16, train loss: 0.2389531863177264, validation loss: 0.4458754382290683.
epoch: 17, train loss: 0.2386584568931943, validation loss: 0.4735853470463456.
epoch: 18, train loss: 0.2373886444858773, validation loss: 0.45014812441535923.
epoch: 19, train loss: 0.23018978615412636, validation loss: 0.4599120328015897.
epoch: 20, train loss: 0.23163269573796993, validation loss: 0.4606377436040522.
epoch: 21, train loss: 0.22640190847084005, validation loss: 0.4371128912286444.
epoch: 22, train loss: 0.22624059307764446, validation loss: 0.4706278077848665.
epoch: 23, train loss: 0.2243210978835979, validation loss: 0.4514327854813237.
epoch: 24, train loss: 0.22020569945643187, validation loss: 0.4770379145416148.
epoch: 25, train loss: 0.21942648416347604, validation loss: 0.4521916251828819.
epoch: 26, train loss: 0.2173305474437734, validation loss: 0.4567422391730787.
epoch: 27, train loss: 0.21875096796429347, validation loss: 0.45863428514082355.
epoch: 28, train loss: 0.21506284275761356, validation loss: 0.5153862111297719.
epoch: 29, train loss: 0.2179481562659854, validation loss: 0.4757648936121455.
epoch: 30, train loss: 0.2161766686010613, validation loss: 0.4501295680790157.
epoch: 31, train loss: 0.21166979534411556, validation loss: 0.47773700406699826.
epoch: 32, train loss: 0.2108069859257451, validation loss: 0.47253284705864207.
epoch: 33, train loss: 0.20984393356969117, validation loss: 0.4791357878800277.
epoch: 34, train loss: 0.20800449571155366, validation loss: 0.4632050104106302.
epoch: 35, train loss: 0.20780441169133262, validation loss: 0.48262424021850137.
epoch: 36, train loss: 0.20418523243495396, validation loss: 0.4883756808046893.
epoch: 37, train loss: 0.2078713114804061, validation loss: 0.45192064124585946.
epoch: 38, train loss: 0.20896050422910661, validation loss: 0.4657178230425377.
epoch: 39, train loss: 0.2074060247108419, validation loss: 0.46365287529243215.
epoch: 40, train loss: 0.20250839599730477, validation loss: 0.5023611283913637.
epoch: 41, train loss: 0.20296098456811654, validation loss: 0.4408311085124592.
epoch: 42, train loss: 0.20547825705311284, validation loss: 0.475191166916173.
epoch: 43, train loss: 0.20157038745173703, validation loss: 0.4888592649466825.
epoch: 44, train loss: 0.20384279404493866, validation loss: 0.4760978180322892.
epoch: 45, train loss: 0.20104911140159323, validation loss: 0.46920059818924564.
epoch: 46, train loss: 0.2000368738729487, validation loss: 0.4432653308351398.
epoch: 47, train loss: 0.20113013263480373, validation loss: 0.45686649867466517.
epoch: 48, train loss: 0.19619234489763857, validation loss: 0.472264881343632.
epoch: 49, train loss: 0.20275384267171223, validation loss: 0.4573224765651829.
epoch: 50, train loss: 0.19342639290340363, validation loss: 0.45260484548715446.
epoch: 51, train loss: 0.20020077185656027, validation loss: 0.45758338299426404.
epoch: 52, train loss: 0.20051918039876948, validation loss: 0.4758696642026796.
epoch: 53, train loss: 0.19911757969730115, validation loss: 0.4603205167330228.
epoch: 54, train loss: 0.19448490069152186, validation loss: 0.47581149118723887.
epoch: 55, train loss: 0.19995700338656308, validation loss: 0.5025362021757133.
epoch: 56, train loss: 0.19871644055401838, validation loss: 0.4570703064915025.
epoch: 57, train loss: 0.19651673906316203, validation loss: 0.4672208467420641.
epoch: 58, train loss: 0.1919251125073307, validation loss: 0.44198283310774916.
epoch: 59, train loss: 0.19680569221859887, validation loss: 0.4258195210551168.
epoch: 60, train loss: 0.1907950050111801, validation loss: 0.44330161307757593.
best validation loss 0.4258195210551168 at epoch 59.
