seed:  666
number of classes (from original clusters): 14
how to merge clusters:  [[0, 9], [1, 5], 2, [3, 8], 4, 6, 7, 10, 11, 12, 13]
whether to further subcluster data according to chemical reaction: False
positive training pair sampling threshold:  13000
negative training pair sampling threshold:  3500
positive validation pair sampling threshold:  3400
negative validation pair sampling threshold:  900
number of epochs to train: 60
learning rate decay to half at epoch 35.
batch size: 256
similar margin of contrastive loss: 0.0
dissimilar margin of contrastive loss: 2.0
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of classes after merging:  11
number of pockets in training set:  12472
number of pockets in validation set:  2670
number of pockets in test set:  2684
number of train positive pairs: 143000
number of train negative pairs: 192500
number of validation positive pairs: 37400
number of validation negative pairs: 49500
model architecture:
ResidualSiameseNet(
  (embedding_net): ResidualEmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (rb_2): ResidualBlock(
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=32, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_3): ResidualBlock(
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=32, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_4): ResidualBlock(
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=32, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_5): ResidualBlock(
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=32, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_6): ResidualBlock(
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=32, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_7): ResidualBlock(
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=32, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_8): ResidualBlock(
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=32, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (bn_8): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(32, 64)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.7660037460668254, validation loss: 0.7553747275893515.
epoch: 2, train loss: 0.6892840616194928, validation loss: 0.8009519907574933.
epoch: 3, train loss: 0.6530181418133209, validation loss: 0.751820252553503.
epoch: 4, train loss: 0.6241940105682692, validation loss: 0.683819173762384.
epoch: 5, train loss: 0.6049204823114478, validation loss: 0.7032528112343458.
epoch: 6, train loss: 0.5878743267713112, validation loss: 0.7937601856367772.
epoch: 7, train loss: 0.5728202380252844, validation loss: 0.6534524785470908.
epoch: 8, train loss: 0.5596546680888191, validation loss: 0.6662539808056023.
epoch: 9, train loss: 0.5481510525517244, validation loss: 0.6522684265417663.
epoch: 10, train loss: 0.5397413171577737, validation loss: 0.673445942728374.
epoch: 11, train loss: 0.5312604947040405, validation loss: 0.659086239971692.
epoch: 12, train loss: 0.5227994544175627, validation loss: 0.7369916036625589.
epoch: 13, train loss: 0.5191976780671121, validation loss: 0.6873565261191137.
epoch: 14, train loss: 0.5139840524960559, validation loss: 0.6672034785333245.
epoch: 15, train loss: 0.5082256112247927, validation loss: 0.663192382039903.
epoch: 16, train loss: 0.505911212699424, validation loss: 0.6861144933492048.
epoch: 17, train loss: 0.4991737681359008, validation loss: 0.6667709085444724.
epoch: 18, train loss: 0.5000864275095183, validation loss: 0.6519529523948256.
epoch: 19, train loss: 0.49125514087023214, validation loss: 0.6590389740727713.
epoch: 20, train loss: 0.48839372305720824, validation loss: 0.7005508180537076.
epoch: 21, train loss: 0.49868743675902777, validation loss: 0.6645863633764901.
epoch: 22, train loss: 0.48519688199493816, validation loss: 0.6546237005271077.
epoch: 23, train loss: 0.479821388983691, validation loss: 0.6974631815428564.
epoch: 24, train loss: 0.47845153544805447, validation loss: 0.7448936846944899.
epoch: 25, train loss: 0.4821975530632917, validation loss: 0.6612098024848965.
epoch: 26, train loss: 0.4703133200860059, validation loss: 0.6730579064548085.
epoch: 27, train loss: 0.47203418664605357, validation loss: 0.6392792708525312.
epoch: 28, train loss: 0.4729030046534076, validation loss: 0.661614569520237.
epoch: 29, train loss: 0.4684276494851944, validation loss: 0.6394316054542813.
epoch: 30, train loss: 0.4827807658120168, validation loss: 0.6609169036197992.
epoch: 31, train loss: 0.4741194131001275, validation loss: 0.6533344496188147.
epoch: 32, train loss: 0.4658940273688554, validation loss: 0.6468099173138687.
epoch: 33, train loss: 0.46455363657648624, validation loss: 0.6579652064140153.
epoch: 34, train loss: 0.4645189074816185, validation loss: 0.675508493438826.
epoch: 35, train loss: 0.4140195705681018, validation loss: 0.6342806817198513.
epoch: 36, train loss: 0.4092951477323666, validation loss: 0.655878217760795.
epoch: 37, train loss: 0.4044710262656745, validation loss: 0.6448887295388243.
epoch: 38, train loss: 0.4004448207622136, validation loss: 0.6554942430643272.
epoch: 39, train loss: 0.40059482016186776, validation loss: 0.6539537961408924.
epoch: 40, train loss: 0.400037219711281, validation loss: 0.653541828326718.
epoch: 41, train loss: 0.39739082870824505, validation loss: 0.6365882469677678.
epoch: 42, train loss: 0.39253290247455264, validation loss: 0.6508251972439922.
epoch: 43, train loss: 0.3927656946082762, validation loss: 0.6537519787260523.
epoch: 44, train loss: 0.39198729415870814, validation loss: 0.6440591339357155.
epoch: 45, train loss: 0.3918732250230678, validation loss: 0.6750146480863196.
epoch: 46, train loss: 0.39068879131197753, validation loss: 0.6595124539462003.
epoch: 47, train loss: 0.38725572986858614, validation loss: 0.6587442622387779.
epoch: 48, train loss: 0.3897943717626808, validation loss: 0.6773318743074722.
epoch: 49, train loss: 0.387784113581241, validation loss: 0.6608577153213554.
epoch: 50, train loss: 0.3867249970755883, validation loss: 0.6751441398945447.
epoch: 51, train loss: 0.38345674383444867, validation loss: 0.6477267247102341.
epoch: 52, train loss: 0.3831735430971876, validation loss: 0.658504334549635.
epoch: 53, train loss: 0.38194440888861075, validation loss: 0.6632634068742583.
epoch: 54, train loss: 0.38219514401658994, validation loss: 0.6922185838565453.
epoch: 55, train loss: 0.3824264455644633, validation loss: 0.6710038263915735.
epoch: 56, train loss: 0.39355139460329147, validation loss: 0.6790521754713684.
epoch: 57, train loss: 0.3757894951810425, validation loss: 0.6553151594892882.
epoch: 58, train loss: 0.3814977193681742, validation loss: 0.6606904684578179.
epoch: 59, train loss: 0.3806496251386253, validation loss: 0.6606437707688098.
epoch: 60, train loss: 0.38613443236486034, validation loss: 0.6569677795831573.
best validation loss 0.6342806817198513 at epoch 35.
