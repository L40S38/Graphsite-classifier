seed:  666
number of classes (from original clusters): 10
how to merge clusters:  [[0, 9], [1, 5], 2, [3, 8], 4, 6, 7]
whether to further subcluster data according to chemical reaction: False
positive training pair sampling threshold:  13500
negative training pair sampling threshold:  4500
positive validation pair sampling threshold:  3900
negative validation pair sampling threshold:  1300
number of epochs to train: 60
learning rate decay to half at epoch 40.
batch size: 256
similar margin of contrastive loss: 0.0
dissimilar margin of contrastive loss: 2.0
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of classes after merging:  7
number of pockets in training set:  10527
number of pockets in validation set:  2254
number of pockets in test set:  2263
number of train positive pairs: 94500
number of train negative pairs: 94500
number of validation positive pairs: 27300
number of validation negative pairs: 27300
model architecture:
SiameseNet(
  (embedding_net): EmbeddingNet(
    (set2set): Set2Set(32, 64)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.9191520176640263, validation loss: 0.8053157658280033.
epoch: 2, train loss: 0.7549299356773417, validation loss: 0.746837905492538.
epoch: 3, train loss: 0.6643850300299428, validation loss: 0.6805545292669164.
epoch: 4, train loss: 0.5993104443827634, validation loss: 0.7198081696339143.
epoch: 5, train loss: 0.5514409672570607, validation loss: 0.6815594035277873.
epoch: 6, train loss: 0.5160214188187211, validation loss: 0.6685532507704292.
epoch: 7, train loss: 0.4946181616001028, validation loss: 0.6436424462175194.
epoch: 8, train loss: 0.4750613402109298, validation loss: 0.6445940062414596.
epoch: 9, train loss: 0.4567266818657123, validation loss: 0.6321027374267578.
epoch: 10, train loss: 0.4381029200074534, validation loss: 0.6343394649072445.
epoch: 11, train loss: 0.424705110055429, validation loss: 0.6360626511346726.
epoch: 12, train loss: 0.42041720568944535, validation loss: 0.6099848645947356.
epoch: 13, train loss: 0.4048047252836682, validation loss: 0.6193250524779379.
epoch: 14, train loss: 0.39334529206614016, validation loss: 0.6762388401241093.
epoch: 15, train loss: 0.39652336209413236, validation loss: 0.6554095850235376.
epoch: 16, train loss: 0.3903548414038603, validation loss: 0.5943849307888157.
epoch: 17, train loss: 0.38173240540519593, validation loss: 0.6206519926860655.
epoch: 18, train loss: 0.378560911027212, validation loss: 0.6190601510966653.
epoch: 19, train loss: 0.3751113840738932, validation loss: 0.6166107390127776.
epoch: 20, train loss: 0.37142167962291256, validation loss: 0.6296382725195133.
epoch: 21, train loss: 0.365709977266019, validation loss: 0.6472853813590584.
epoch: 22, train loss: 0.3618614005436973, validation loss: 0.6311052715036022.
epoch: 23, train loss: 0.3625303531222873, validation loss: 0.663529786050538.
epoch: 24, train loss: 0.3601315117891503, validation loss: 0.6106695176568223.
epoch: 25, train loss: 0.3580216739795826, validation loss: 0.6375650192093063.
epoch: 26, train loss: 0.35648403167724607, validation loss: 0.6256181008475168.
epoch: 27, train loss: 0.35539793016544724, validation loss: 0.6187021265274439.
epoch: 28, train loss: 0.3521480544963211, validation loss: 0.6236207801343757.
epoch: 29, train loss: 0.355934653791801, validation loss: 0.6308779666886661.
epoch: 30, train loss: 0.34833170027959914, validation loss: 0.6365143297356127.
epoch: 31, train loss: 0.34850397340078204, validation loss: 0.5979305870978386.
epoch: 32, train loss: 0.34844764612591456, validation loss: 0.619725092514094.
epoch: 33, train loss: 0.34415738847520616, validation loss: 0.6098254942282653.
epoch: 34, train loss: 0.3489320487370567, validation loss: 0.6377906902718457.
epoch: 35, train loss: 0.3470587699930504, validation loss: 0.6371724379542983.
epoch: 36, train loss: 0.34293550105826565, validation loss: 0.611889164963048.
epoch: 37, train loss: 0.34510995317893056, validation loss: 0.6440012249230466.
epoch: 38, train loss: 0.3380927275602149, validation loss: 0.6571560252542461.
epoch: 39, train loss: 0.3434267158911972, validation loss: 0.6462833547068166.
epoch: 40, train loss: 0.2869949696656888, validation loss: 0.6273876239091922.
epoch: 41, train loss: 0.28889982415254783, validation loss: 0.6325833071195163.
epoch: 42, train loss: 0.2897533248618797, validation loss: 0.6665320777194403.
epoch: 43, train loss: 0.2987590807354639, validation loss: 0.677324710328937.
epoch: 44, train loss: 0.2876363090838074, validation loss: 0.6211454779935843.
epoch: 45, train loss: 0.2861712338886564, validation loss: 0.6325558561108487.
epoch: 46, train loss: 0.2824288909548805, validation loss: 0.6456058685246842.
epoch: 47, train loss: 0.28266641106176627, validation loss: 0.6364143980815734.
epoch: 48, train loss: 0.27887026101571544, validation loss: 0.6396469437595689.
epoch: 49, train loss: 0.2841584834951572, validation loss: 0.6637451809055203.
epoch: 50, train loss: 0.28101307853819835, validation loss: 0.653917232415615.
epoch: 51, train loss: 0.2852431100108636, validation loss: 0.6493124132540636.
epoch: 52, train loss: 0.2791679245479523, validation loss: 0.6411315465235448.
epoch: 53, train loss: 0.28303722020305655, validation loss: 0.6297825232704917.
epoch: 54, train loss: 0.27616605861603266, validation loss: 0.6510128826099437.
epoch: 55, train loss: 0.28051296254314445, validation loss: 0.6142648425818363.
epoch: 56, train loss: 0.2831172431259559, validation loss: 0.646807869712075.
epoch: 57, train loss: 0.28016101966333135, validation loss: 0.6889776331863123.
epoch: 58, train loss: 0.28075001116152165, validation loss: 0.6174526394854535.
epoch: 59, train loss: 0.2699875905677755, validation loss: 0.6242725098438752.
epoch: 60, train loss: 0.2782336500460509, validation loss: 0.6495042654184194.
best validation loss 0.5943849307888157 at epoch 16.
