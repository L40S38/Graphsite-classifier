seed:  666
number of classes (from original clusters): 10
how to merge clusters:  [[0, 9], [1, 5], 2, [3, 8], 4, 6, 7]
whether to further subcluster data according to chemical reaction: False
positive training pair sampling threshold:  13500
negative training pair sampling threshold:  4500
positive validation pair sampling threshold:  3900
negative validation pair sampling threshold:  1300
number of epochs to train: 60
batch size: 256
margin of contrastive loss: 2.0
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['x', 'y', 'z', 'charge', 'hydrophobicity', 'binding_probability', 'sasa', 'sequence_entropy']
number of classes after merging:  7
number of pockets in training set:  10527
number of pockets in validation set:  2254
number of pockets in test set:  2263
number of train positive pairs: 94500
number of train negative pairs: 94500
number of validation positive pairs: 27300
number of validation negative pairs: 27300
model architecture:
SiameseNet(
  (embedding_net): EmbeddingNet(
    (set2set): Set2Set(32, 64)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.810373701872649, validation loss: 0.7581780516795623.
epoch: 2, train loss: 0.661567552435335, validation loss: 0.7093381821628892.
epoch: 3, train loss: 0.5945521342363307, validation loss: 0.7005538507258936.
epoch: 4, train loss: 0.5507855808318607, validation loss: 0.6622320279970274.
epoch: 5, train loss: 0.5221387654460927, validation loss: 0.6402091739640567.
epoch: 6, train loss: 0.49526698892583293, validation loss: 0.6180589245527218.
epoch: 7, train loss: 0.47252449907575333, validation loss: 0.6202557250082275.
epoch: 8, train loss: 0.45782654946947854, validation loss: 0.6129090407654479.
epoch: 9, train loss: 0.4451875907050239, validation loss: 0.6274480597527472.
epoch: 10, train loss: 0.42801349454203613, validation loss: 0.632725464957101.
epoch: 11, train loss: 0.42377960390767094, validation loss: 0.6216143339108198.
epoch: 12, train loss: 0.4206628931439112, validation loss: 0.6101925416045136.
epoch: 13, train loss: 0.4116561620399435, validation loss: 0.6215617018479568.
epoch: 14, train loss: 0.40979044959658667, validation loss: 0.6621239088568496.
epoch: 15, train loss: 0.4005652012497029, validation loss: 0.6177149309430804.
epoch: 16, train loss: 0.3950133243540607, validation loss: 0.6212427302769252.
epoch: 17, train loss: 0.3870464257941675, validation loss: 0.6140641619783618.
epoch: 18, train loss: 0.3889281513678334, validation loss: 0.6370956789792239.
epoch: 19, train loss: 0.38290324167725903, validation loss: 0.6474898058154207.
epoch: 20, train loss: 0.37879504015079885, validation loss: 0.6813572838192894.
epoch: 21, train loss: 0.37675922107191945, validation loss: 0.6226431489602113.
epoch: 22, train loss: 0.3796470498786402, validation loss: 0.6252583662319533.
epoch: 23, train loss: 0.37650166974748883, validation loss: 0.6313938842326293.
epoch: 24, train loss: 0.3698976583329458, validation loss: 0.6201918298071557.
epoch: 25, train loss: 0.3674149189499951, validation loss: 0.6237925305209318.
epoch: 26, train loss: 0.36146527305481924, validation loss: 0.6087718779176146.
epoch: 27, train loss: 0.367818718440949, validation loss: 0.6331595105740614.
epoch: 28, train loss: 0.3628799610844365, validation loss: 0.6377584666996212.
epoch: 29, train loss: 0.3578300725946981, validation loss: 0.6524185147128262.
epoch: 30, train loss: 0.353717975717373, validation loss: 0.61380797794887.
epoch: 31, train loss: 0.36440406993078805, validation loss: 0.6406647891090029.
epoch: 32, train loss: 0.35807922924384866, validation loss: 0.6589063257350153.
epoch: 33, train loss: 0.3552543856504733, validation loss: 0.6513377911354596.
epoch: 34, train loss: 0.35394735285844753, validation loss: 0.6515684509277344.
epoch: 35, train loss: 0.3513850900761034, validation loss: 0.6112272205282917.
epoch: 36, train loss: 0.35345709797692676, validation loss: 0.6289039231744005.
epoch: 37, train loss: 0.35104413951893965, validation loss: 0.6298350587781969.
epoch: 38, train loss: 0.3426893695447811, validation loss: 0.6277404667780949.
epoch: 39, train loss: 0.3537899437758027, validation loss: 0.6378454960134876.
epoch: 40, train loss: 0.34764473807117924, validation loss: 0.6271885921492245.
epoch: 41, train loss: 0.3449447769891648, validation loss: 0.5954290971301851.
epoch: 42, train loss: 0.33866096926492356, validation loss: 0.6329398545764742.
epoch: 43, train loss: 0.3406257537155555, validation loss: 0.6513364533364991.
epoch: 44, train loss: 0.3388367082908671, validation loss: 0.6404179671570495.
epoch: 45, train loss: 0.33831496788711146, validation loss: 0.6085859042995578.
epoch: 46, train loss: 0.34101627095540366, validation loss: 0.6434590762351459.
epoch: 47, train loss: 0.33723976686265733, validation loss: 0.6181404365288032.
epoch: 48, train loss: 0.33355473588004947, validation loss: 0.6362451501643701.
epoch: 49, train loss: 0.33971294108395855, validation loss: 0.6500239465175531.
epoch: 50, train loss: 0.33809364163434064, validation loss: 0.6280079466432006.
epoch: 51, train loss: 0.33392084105557235, validation loss: 0.6160057052500518.
epoch: 52, train loss: 0.34486260405040925, validation loss: 0.6309935117728545.
epoch: 53, train loss: 0.34384343239113135, validation loss: 0.6063640456147247.
epoch: 54, train loss: 0.3342010953186681, validation loss: 0.6175517729874496.
epoch: 55, train loss: 0.3290416151379782, validation loss: 0.6240835873111263.
epoch: 56, train loss: 0.33205692426489775, validation loss: 0.6230019210983109.
epoch: 57, train loss: 0.3335838880993071, validation loss: 0.5971930901384179.
epoch: 58, train loss: 0.3352115879967099, validation loss: 0.6832292731428321.
epoch: 59, train loss: 0.32954317718586595, validation loss: 0.6279954961979346.
epoch: 60, train loss: 0.32564789865887356, validation loss: 0.6171860460833315.
best validation loss 0.5954290971301851 at epoch 41.
