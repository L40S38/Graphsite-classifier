seed:  666
number of classes (from original clusters): 14
how to merge clusters:  [[0, 9, 12], [1, 5, 11], 2, [3, 8, 13], 4, 6, 7, 10]
whether to further subcluster data according to chemical reaction: False
positive training pair sampling threshold:  16000
negative training pair sampling threshold:  4600
number of epochs to train: 55
learning rate decay to half at epoch 25.
batch size: 256
similar margin of contrastive loss: 0.0
dissimilar margin of contrastive loss: 2.0
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of classes after merging:  8
number of pockets in training set:  12475
number of pockets in validation set:  2670
number of pockets in test set:  2681
number of train positive pairs: 128000
number of train negative pairs: 128800
model architecture:
ResidualSiameseNet(
  (embedding_net): ResidualEmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=48, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=48, out_features=48, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (rb_2): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_3): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_4): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_5): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_6): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_7): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_8): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (bn_8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(48, 96)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.7764501156910929, train acc: 0.7621226718047527， validation acc: 0.670045045045045.
epoch: 2, train loss: 0.6711169545301395, train acc: 0.7687861271676301， validation acc: 0.6711711711711712.
epoch: 3, train loss: 0.6234973629092873, train acc: 0.7769749518304432， validation acc: 0.6794294294294294.
epoch: 4, train loss: 0.5927796436915888, train acc: 0.7886962106615286， validation acc: 0.6831831831831832.
epoch: 5, train loss: 0.5642602351892775, train acc: 0.7823538856775851， validation acc: 0.6869369369369369.
epoch: 6, train loss: 0.5349706734155197, train acc: 0.7821130378933847， validation acc: 0.6888138138138138.
epoch: 7, train loss: 0.5159216172227236, train acc: 0.7869299935773925， validation acc: 0.6880630630630631.
epoch: 8, train loss: 0.49690215749532635, train acc: 0.7883750802825947， validation acc: 0.6858108108108109.
epoch: 9, train loss: 0.48540268446426155, train acc: 0.7968047527296083， validation acc: 0.6783033033033034.
epoch: 10, train loss: 0.4721098627702469, train acc: 0.8022639691714836， validation acc: 0.6824324324324325.
epoch: 11, train loss: 0.4585501929309881, train acc: 0.800417469492614， validation acc: 0.6809309309309309.
epoch: 12, train loss: 0.45224568417510513, train acc: 0.7965639049454079， validation acc: 0.6936936936936937.
epoch: 13, train loss: 0.4506256202745289, train acc: 0.7956807964033398， validation acc: 0.6824324324324325.
epoch: 14, train loss: 0.4372677447119977, train acc: 0.7952793834296724， validation acc: 0.676051051051051.
epoch: 15, train loss: 0.43240349918139687, train acc: 0.7878131021194605， validation acc: 0.6621621621621622.
epoch: 16, train loss: 0.4242586706583374, train acc: 0.7976075786769429， validation acc: 0.676051051051051.
epoch: 17, train loss: 0.41351469954968983, train acc: 0.8059569685292228， validation acc: 0.6861861861861862.
epoch: 18, train loss: 0.4145124053063794, train acc: 0.7957610789980732， validation acc: 0.6835585585585585.
epoch: 19, train loss: 0.42371775365692804, train acc: 0.8039499036608864， validation acc: 0.68506006006006.
epoch: 20, train loss: 0.41285373964042305, train acc: 0.8029865125240848， validation acc: 0.6677927927927928.
epoch: 21, train loss: 0.4029517631590181, train acc: 0.8021034039820167， validation acc: 0.6861861861861862.
epoch: 22, train loss: 0.40139963557044295, train acc: 0.8029865125240848， validation acc: 0.6843093093093093.
epoch: 23, train loss: 0.39895041415253163, train acc: 0.8017822736030829， validation acc: 0.6805555555555556.
epoch: 24, train loss: 0.3898809328703123, train acc: 0.8115767501605652， validation acc: 0.6933183183183184.
epoch: 25, train loss: 0.3287851308810748, train acc: 0.8206486833654464， validation acc: 0.6854354354354354.
epoch: 26, train loss: 0.33031819339110474, train acc: 0.8243416827231856， validation acc: 0.6824324324324325.
epoch: 27, train loss: 0.32856955090044443, train acc: 0.8175176621708413， validation acc: 0.6974474474474475.
epoch: 28, train loss: 0.32446265596467017, train acc: 0.8275529865125241， validation acc: 0.6963213213213213.
epoch: 29, train loss: 0.32620674671042377, train acc: 0.8277938342967245， validation acc: 0.6831831831831832.
epoch: 30, train loss: 0.31579462372253986, train acc: 0.8309248554913294， validation acc: 0.6970720720720721.
epoch: 31, train loss: 0.32520147689035006, train acc: 0.8164739884393064， validation acc: 0.6835585585585585.
epoch: 32, train loss: 0.32063769370224615, train acc: 0.8270712909441233， validation acc: 0.6921921921921922.
epoch: 33, train loss: 0.3172495757010867, train acc: 0.8157514450867052， validation acc: 0.6779279279279279.
epoch: 34, train loss: 0.31794804706751745, train acc: 0.8327713551701991， validation acc: 0.6955705705705706.
epoch: 35, train loss: 0.44815995902658623, train acc: 0.8201669877970456， validation acc: 0.6831831831831832.
epoch: 36, train loss: 0.3277003252766214, train acc: 0.8387122671804753， validation acc: 0.6944444444444444.
epoch: 37, train loss: 0.31125665529494717, train acc: 0.8267501605651895， validation acc: 0.6880630630630631.
epoch: 38, train loss: 0.31703064058428615, train acc: 0.8328516377649325， validation acc: 0.6985735735735735.
epoch: 39, train loss: 0.34130758327115734, train acc: 0.8072414900449583， validation acc: 0.670045045045045.
epoch: 40, train loss: 0.3239380892192092, train acc: 0.8313262684649968， validation acc: 0.6895645645645646.
epoch: 41, train loss: 0.3167391009939794, train acc: 0.8292389210019268， validation acc: 0.6798048048048048.
epoch: 42, train loss: 0.32007904214651045, train acc: 0.8241811175337187， validation acc: 0.6876876876876877.
epoch: 43, train loss: 0.3167393734596229, train acc: 0.8315671162491972， validation acc: 0.683933933933934.
epoch: 44, train loss: 0.3146057449174447, train acc: 0.8313262684649968， validation acc: 0.6936936936936937.
epoch: 45, train loss: 0.3228629316793424, train acc: 0.8292389210019268， validation acc: 0.68506006006006.
epoch: 46, train loss: 0.33016179227383335, train acc: 0.8243416827231856， validation acc: 0.6816816816816816.
epoch: 47, train loss: 0.33136809839266484, train acc: 0.8327713551701991， validation acc: 0.6876876876876877.
epoch: 48, train loss: 0.3161892653848523, train acc: 0.8230571612074502， validation acc: 0.6805555555555556.
epoch: 49, train loss: 0.3132272078166498, train acc: 0.8387925497752088， validation acc: 0.7019519519519519.
epoch: 50, train loss: 0.3131848229797458, train acc: 0.8363037893384714， validation acc: 0.6918168168168168.
epoch: 51, train loss: 0.30891619902162165, train acc: 0.8311657032755299， validation acc: 0.6884384384384384.
epoch: 52, train loss: 0.3071031161037932, train acc: 0.8295600513808606， validation acc: 0.6884384384384384.
epoch: 53, train loss: 0.3054003802266819, train acc: 0.8299614643545279， validation acc: 0.6805555555555556.
epoch: 54, train loss: 0.31266308276452753, train acc: 0.8413615928066795， validation acc: 0.7008258258258259.
epoch: 55, train loss: 0.3065278948578879, train acc: 0.8390333975594091， validation acc: 0.6940690690690691.
best validation loss 0.7019519519519519 at epoch 49.
