seed:  666
save trained model at:  ../trained_models/trained_model_67.pt
save loss at:  ./results/train_results_67.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
positive training pair sampling threshold:  14000
negative training pair sampling threshold:  3000
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs to train: 40
learning rate decay to half at epoch 20.
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['1lolA00', '2h5lA01', '1dqaB01', '4nz9A00', '3w8dA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['6adiA00', '3rogA00', '2ydbA00', '4bivB00', '4hluC00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['4bv2B02', '1khzB00', '5uqlA00', '3drpA00', '4o29A00']
number of train positive pairs: 252000
number of train negative pairs: 459000
number of epochs to train for hard pairs:  120
learning rate decay at epoch for hard pairs:  50
begin to select hard pairs at epoch 1
batch size for hard pairs:  128
number of hardest positive pairs for each mini-batch:  192
number of hardest negative pairs for each mini-batch:  256

*******************************************************
             train by random pairs
*******************************************************
model architecture:
SiameseNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(80, 160)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.9254340974402663, train acc: 0.6585212674850128, validation acc: 0.5388310648518815.
epoch: 2, train loss: 0.7584122920941703, train acc: 0.7072223808164431, validation acc: 0.5871363757672805.
epoch: 3, train loss: 0.6924972603069579, train acc: 0.716186126177562, validation acc: 0.5916733386709367.
epoch: 4, train loss: 0.6483422730522317, train acc: 0.7332572081073365, validation acc: 0.6188951160928743.
epoch: 5, train loss: 0.616565516272007, train acc: 0.73788181558664, validation acc: 0.6207632772884975.
epoch: 6, train loss: 0.5973178256500287, train acc: 0.7335426777048244, validation acc: 0.6282359220709901.
epoch: 7, train loss: 0.5820654883082909, train acc: 0.7362260919212104, validation acc: 0.6196957566052842.
epoch: 8, train loss: 0.5724614710854746, train acc: 0.74564658863831, validation acc: 0.6317053642914331.
epoch: 9, train loss: 0.5631873664480389, train acc: 0.7459320582357979, validation acc: 0.6287696824125968.
epoch: 10, train loss: 0.556710029752278, train acc: 0.7472452183842421, validation acc: 0.622097678142514.
epoch: 11, train loss: 0.5528714487475517, train acc: 0.7478161575792178, validation acc: 0.6253002401921537.
epoch: 12, train loss: 0.5508343070415169, train acc: 0.7466171852697687, validation acc: 0.6210301574593008.
epoch: 13, train loss: 0.5451062229870073, train acc: 0.7414787325149872, validation acc: 0.622097678142514.
epoch: 14, train loss: 0.5430528386112004, train acc: 0.7518698258635456, validation acc: 0.6282359220709901.
epoch: 15, train loss: 0.5425559217128405, train acc: 0.7478732514987154, validation acc: 0.6180944755804644.
epoch: 16, train loss: 0.5354814300644415, train acc: 0.7474165001427348, validation acc: 0.6215639178009074.
epoch: 17, train loss: 0.5357271695894866, train acc: 0.7403939480445333, validation acc: 0.6202295169468909.
epoch: 18, train loss: 0.5323782083374539, train acc: 0.7554667427918926, validation acc: 0.6274352815585802.
epoch: 19, train loss: 0.5314297585467246, train acc: 0.7482729089351984, validation acc: 0.6242327195089404.
epoch: 20, train loss: 0.4908056742672176, train acc: 0.7666000570939195, validation acc: 0.6266346410461703.
epoch: 21, train loss: 0.48560881660558003, train acc: 0.7629460462460748, validation acc: 0.6207632772884975.
epoch: 22, train loss: 0.484250065488319, train acc: 0.7620325435341136, validation acc: 0.6188951160928743.
epoch: 23, train loss: 0.4808209772016093, train acc: 0.7493005994861547, validation acc: 0.6175607152388578.
epoch: 24, train loss: 0.4788966412336347, train acc: 0.7616899800171282, validation acc: 0.615158793701628.
epoch: 25, train loss: 0.47813921014970867, train acc: 0.7636311732800457, validation acc: 0.6183613557512677.
epoch: 26, train loss: 0.4751423272778046, train acc: 0.7494147873251499, validation acc: 0.6226314384841206.
epoch: 27, train loss: 0.47522043859640084, train acc: 0.7670568084499001, validation acc: 0.6218307979717107.
epoch: 28, train loss: 0.4719307085941277, train acc: 0.7578075934912932, validation acc: 0.6188951160928743.
epoch: 29, train loss: 0.47109089057589715, train acc: 0.7655152726234656, validation acc: 0.6258340005337604.
epoch: 30, train loss: 0.4708740787640235, train acc: 0.770139880102769, validation acc: 0.622898318654924.
epoch: 31, train loss: 0.4676995504086866, train acc: 0.7459320582357979, validation acc: 0.5956765412329864.
epoch: 32, train loss: 0.46802184338576347, train acc: 0.7695118469882958, validation acc: 0.618628235922071.
epoch: 33, train loss: 0.4653827498230753, train acc: 0.7642592063945189, validation acc: 0.6170269548972511.
epoch: 34, train loss: 0.46644672568706186, train acc: 0.7614045104196403, validation acc: 0.6111555911395783.
epoch: 35, train loss: 0.47903126991583156, train acc: 0.7294319155009992, validation acc: 0.6138243928476115.
epoch: 36, train loss: 0.4815335367688314, train acc: 0.7516414501855553, validation acc: 0.6188951160928743.
epoch: 37, train loss: 0.4748089218354259, train acc: 0.7625463888095918, validation acc: 0.6253002401921537.
epoch: 38, train loss: 0.4654823637370822, train acc: 0.7586069083642593, validation acc: 0.622898318654924.
epoch: 39, train loss: 0.4668062846938937, train acc: 0.7679703111618612, validation acc: 0.6199626367760875.
epoch: 40, train loss: 0.46209509791335307, train acc: 0.758892377961747, validation acc: 0.617827595409661.
best validation acc 0.6317053642914331 at epoch 8.


*******************************************************
             train by hard pairs
*******************************************************
model architecture:
SelectiveSiameseNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(80, 160)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
SelectiveContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, num_pos_pair=192, num_neg_pair=256)
epoch: 1, train loss: 1.4893323348333583, train acc: 0.765857836140451, validation acc: 0.6583933813717641.
epoch: 2, train loss: 1.3362906750434649, train acc: 0.7734513274336283, validation acc: 0.6784093941820123.
epoch: 3, train loss: 1.299306142819008, train acc: 0.7659720239794462, validation acc: 0.6626634641046171.
epoch: 4, train loss: 1.2719029269111883, train acc: 0.7677419354838709, validation acc: 0.6607953029089939.
epoch: 5, train loss: 1.262680759500283, train acc: 0.7584927205252641, validation acc: 0.6437149719775821.
epoch: 6, train loss: 1.2593749332490054, train acc: 0.7628889523265773, validation acc: 0.6514544969308781.
epoch: 7, train loss: 1.2442983284666989, train acc: 0.7399371966885526, validation acc: 0.611689351481185.
epoch: 8, train loss: 1.241282450244959, train acc: 0.748330002854696, validation acc: 0.6274352815585802.
epoch: 9, train loss: 1.2373384279965784, train acc: 0.7613474165001427, validation acc: 0.6375767280491059.
epoch: 10, train loss: 1.2377097456127786, train acc: 0.7398801027690551, validation acc: 0.614358153189218.
epoch: 11, train loss: 1.2354491333765951, train acc: 0.7522694833000285, validation acc: 0.6335735254870564.
epoch: 12, train loss: 1.2305715260471535, train acc: 0.7525549528975164, validation acc: 0.636776087536696.
epoch: 13, train loss: 1.2310947001070902, train acc: 0.7330288324293462, validation acc: 0.611689351481185.
epoch: 14, train loss: 1.2302150595961487, train acc: 0.7438766771338853, validation acc: 0.6234320789965305.
epoch: 15, train loss: 1.2255359338457037, train acc: 0.7246931201827005, validation acc: 0.5996797437950361.
epoch: 16, train loss: 1.2269231593901124, train acc: 0.7293748215815016, validation acc: 0.6018147851614625.
epoch: 17, train loss: 1.2240931100611523, train acc: 0.735198401370254, validation acc: 0.6226314384841206.
epoch: 18, train loss: 1.2223833733407814, train acc: 0.7154439052240936, validation acc: 0.5986122231118228.
epoch: 19, train loss: 1.2251127641975401, train acc: 0.7395375392520697, validation acc: 0.619428876434481.
epoch: 20, train loss: 1.2227251274670041, train acc: 0.7282329431915501, validation acc: 0.6068855084067254.
epoch: 21, train loss: 1.2189753396466074, train acc: 0.7312018270054239, validation acc: 0.6122231118227915.
epoch: 22, train loss: 1.2213752127490352, train acc: 0.7135598058806737, validation acc: 0.5820656525220176.
epoch: 23, train loss: 1.2207437259924854, train acc: 0.7249785897801884, validation acc: 0.5900720576461169.
epoch: 24, train loss: 1.2186249178230284, train acc: 0.7284613188695404, validation acc: 0.6028823058446757.
epoch: 25, train loss: 1.217456898944811, train acc: 0.7294319155009992, validation acc: 0.6031491860154791.
epoch: 26, train loss: 1.216080134228942, train acc: 0.7268055952041108, validation acc: 0.6007472644782492.
epoch: 27, train loss: 1.21624425477127, train acc: 0.7001998287182415, validation acc: 0.5753936482519348.
epoch: 28, train loss: 1.2127231954623003, train acc: 0.7164715957750499, validation acc: 0.5922070990125433.
epoch: 29, train loss: 1.2137820392193983, train acc: 0.7310876391664288, validation acc: 0.6079530290899386.
epoch: 30, train loss: 1.2137138079500271, train acc: 0.7295461033399943, validation acc: 0.5967440619161997.
epoch: 31, train loss: 1.2145599428136737, train acc: 0.7317156722809021, validation acc: 0.6060848678943155.
epoch: 32, train loss: 1.2137303380289572, train acc: 0.6806166143305737, validation acc: 0.5468374699759808.
epoch: 33, train loss: 1.2163842379801133, train acc: 0.6911789894376249, validation acc: 0.5668534827862289.
epoch: 34, train loss: 1.2117369563441442, train acc: 0.7313160148444191, validation acc: 0.5983453429410195.
epoch: 35, train loss: 1.2101879672337883, train acc: 0.7202397944618898, validation acc: 0.5908726981585268.
epoch: 36, train loss: 1.2097137841828658, train acc: 0.7131601484441907, validation acc: 0.5831331732052308.
epoch: 37, train loss: 1.2138269212970663, train acc: 0.7298886668569797, validation acc: 0.6050173472111022.
epoch: 38, train loss: 1.2095188170664997, train acc: 0.7253782472166714, validation acc: 0.6052842273819056.
epoch: 39, train loss: 1.2086289435411695, train acc: 0.7307450756494434, validation acc: 0.6103549506271684.
epoch: 40, train loss: 1.2127144464470865, train acc: 0.7071652868969455, validation acc: 0.5716573258606885.
epoch: 41, train loss: 1.2108680332371717, train acc: 0.7216100485298316, validation acc: 0.5895382973045102.
epoch: 42, train loss: 1.2100803218402063, train acc: 0.730802169568941, validation acc: 0.5999466239658393.
epoch: 43, train loss: 1.2107351528224832, train acc: 0.7293748215815016, validation acc: 0.5916733386709367.
epoch: 44, train loss: 1.2115721120108076, train acc: 0.7178418498429917, validation acc: 0.5911395783293302.
epoch: 45, train loss: 1.212322951035635, train acc: 0.7125321153297174, validation acc: 0.5868694955964772.
epoch: 46, train loss: 1.2121880591937326, train acc: 0.7217242363688268, validation acc: 0.5852682145716573.
epoch: 47, train loss: 1.2118218981229087, train acc: 0.7120753639737368, validation acc: 0.578062449959968.
epoch: 48, train loss: 1.2109942925573818, train acc: 0.7204110762203826, validation acc: 0.5940752602081665.
epoch: 49, train loss: 1.2120544363656254, train acc: 0.7116186126177562, validation acc: 0.5735254870563117.
epoch: 50, train loss: 1.1999913201360026, train acc: 0.7289751641450185, validation acc: 0.6036829463570856.
epoch: 51, train loss: 1.195853880362698, train acc: 0.7317156722809021, validation acc: 0.5999466239658393.
epoch: 52, train loss: 1.1925508538873315, train acc: 0.7344561804167856, validation acc: 0.6202295169468909.
epoch: 53, train loss: 1.1926343958815981, train acc: 0.7353125892092492, validation acc: 0.607419268748332.
epoch: 54, train loss: 1.1924524876269065, train acc: 0.7238938053097345, validation acc: 0.5916733386709367.
epoch: 55, train loss: 1.192163548311253, train acc: 0.7346274621752783, validation acc: 0.615158793701628.
epoch: 56, train loss: 1.1913969963174704, train acc: 0.7402797602055381, validation acc: 0.615158793701628.
epoch: 57, train loss: 1.1921420460488072, train acc: 0.7361689980017129, validation acc: 0.6079530290899386.
epoch: 58, train loss: 1.1919894128436508, train acc: 0.7325149871538681, validation acc: 0.611689351481185.
epoch: 59, train loss: 1.1917095908531012, train acc: 0.7389666000570939, validation acc: 0.6023485455030692.
epoch: 60, train loss: 1.1905482214168086, train acc: 0.7369112189551813, validation acc: 0.6042167066986923.
epoch: 61, train loss: 1.188977452777011, train acc: 0.7305166999714531, validation acc: 0.6044835868694955.
epoch: 62, train loss: 1.1892619458887783, train acc: 0.736283185840708, validation acc: 0.6132906325060048.
epoch: 63, train loss: 1.1891766411799312, train acc: 0.7333143020268341, validation acc: 0.6135575126768081.
epoch: 64, train loss: 1.1873063391422496, train acc: 0.7400513845275478, validation acc: 0.6087536696023486.
epoch: 65, train loss: 1.187724755523675, train acc: 0.7445047102483585, validation acc: 0.6242327195089404.
epoch: 66, train loss: 1.1876959901074313, train acc: 0.7447901798458464, validation acc: 0.6034160661862824.
epoch: 67, train loss: 1.187595872305872, train acc: 0.7333713959463317, validation acc: 0.6087536696023486.
epoch: 68, train loss: 1.188322049879826, train acc: 0.7482729089351984, validation acc: 0.6271684013877769.
epoch: 69, train loss: 1.1867115809514481, train acc: 0.7423351413074507, validation acc: 0.6138243928476115.
epoch: 70, train loss: 1.1869262609235582, train acc: 0.7433628318584071, validation acc: 0.6269015212169736.
epoch: 71, train loss: 1.186433697760713, train acc: 0.7515843562660577, validation acc: 0.6285028022417934.
epoch: 72, train loss: 1.1858468137456786, train acc: 0.7394804453325721, validation acc: 0.6140912730184147.
epoch: 73, train loss: 1.1849492919964912, train acc: 0.7540964887239509, validation acc: 0.6261008807045636.
epoch: 74, train loss: 1.1859400292543663, train acc: 0.7417642021124751, validation acc: 0.619428876434481.
epoch: 75, train loss: 1.1864418651359825, train acc: 0.7498715386811304, validation acc: 0.6250333600213505.
epoch: 76, train loss: 1.1857444728421762, train acc: 0.7339994290608051, validation acc: 0.614358153189218.
epoch: 77, train loss: 1.1862177477402294, train acc: 0.7455894947188124, validation acc: 0.6277021617293835.
epoch: 78, train loss: 1.1865292460366479, train acc: 0.7333713959463317, validation acc: 0.6164931945556446.
epoch: 79, train loss: 1.185327019653188, train acc: 0.7462746217527834, validation acc: 0.6239658393381372.
epoch: 80, train loss: 1.1847969079177998, train acc: 0.7481016271767057, validation acc: 0.6148919135308246.
epoch: 81, train loss: 1.1850945734879437, train acc: 0.7324578932343705, validation acc: 0.6068855084067254.
epoch: 82, train loss: 1.1850163208789126, train acc: 0.7479874393377105, validation acc: 0.6269015212169736.
epoch: 83, train loss: 1.184654334331702, train acc: 0.7437053953753925, validation acc: 0.6285028022417934.
epoch: 84, train loss: 1.184794407548861, train acc: 0.7531258920924921, validation acc: 0.6378436082199093.
epoch: 85, train loss: 1.1836164184391538, train acc: 0.7351413074507565, validation acc: 0.6122231118227915.
epoch: 86, train loss: 1.1847686424098116, train acc: 0.7452469312018271, validation acc: 0.6130237523352015.
epoch: 87, train loss: 1.1847365324087014, train acc: 0.7501570082786183, validation acc: 0.6386442487323192.
epoch: 88, train loss: 1.1843530858098412, train acc: 0.7425064230659435, validation acc: 0.6247664798505471.
epoch: 89, train loss: 1.1840853754210352, train acc: 0.7463888095917784, validation acc: 0.6212970376301041.
epoch: 90, train loss: 1.1822699890087058, train acc: 0.7454182129603197, validation acc: 0.622898318654924.
epoch: 91, train loss: 1.1847525729251596, train acc: 0.7490722238081644, validation acc: 0.6269015212169736.
epoch: 92, train loss: 1.1842228787455908, train acc: 0.7425635169854411, validation acc: 0.6162263143848412.
epoch: 93, train loss: 1.182518816894346, train acc: 0.7489580359691693, validation acc: 0.6298372030958099.
epoch: 94, train loss: 1.1830407126292823, train acc: 0.7512417927490722, validation acc: 0.6271684013877769.
epoch: 95, train loss: 1.1841329231932614, train acc: 0.7614045104196403, validation acc: 0.6343741659994663.
epoch: 96, train loss: 1.1835058555346452, train acc: 0.7494147873251499, validation acc: 0.6301040832666133.
epoch: 97, train loss: 1.181743652449691, train acc: 0.7549528975164145, validation acc: 0.6317053642914331.
epoch: 98, train loss: 1.1838222612499751, train acc: 0.7523265772195261, validation acc: 0.6399786495863358.
epoch: 99, train loss: 1.183776590202863, train acc: 0.7431915500999143, validation acc: 0.6338404056578596.
epoch: 100, train loss: 1.1838024025186848, train acc: 0.7514130745075649, validation acc: 0.6319722444622364.
epoch: 101, train loss: 1.1829944756678166, train acc: 0.754210676562946, validation acc: 0.6383773685615158.
epoch: 102, train loss: 1.184303844432868, train acc: 0.7414787325149872, validation acc: 0.6175607152388578.
epoch: 103, train loss: 1.1836428599855238, train acc: 0.7522694833000285, validation acc: 0.640245529757139.
epoch: 104, train loss: 1.1814478507758894, train acc: 0.7622609192121039, validation acc: 0.6471844141980251.
epoch: 105, train loss: 1.1814850281417228, train acc: 0.7558664002283757, validation acc: 0.6343741659994663.
epoch: 106, train loss: 1.1831935076424915, train acc: 0.7442763345703682, validation acc: 0.6250333600213505.
epoch: 107, train loss: 1.1825624318204595, train acc: 0.7394233514130745, validation acc: 0.607419268748332.
epoch: 108, train loss: 1.1818633914433532, train acc: 0.7465600913502712, validation acc: 0.6239658393381372.
epoch: 109, train loss: 1.182285211173695, train acc: 0.7617470739366258, validation acc: 0.6517213771016813.
epoch: 110, train loss: 1.1817497345288526, train acc: 0.7536968312874679, validation acc: 0.625567120362957.
epoch: 111, train loss: 1.1810260283556437, train acc: 0.7579788752497859, validation acc: 0.6338404056578596.
epoch: 112, train loss: 1.1825107645641881, train acc: 0.7508421353125893, validation acc: 0.6373098478783027.
epoch: 113, train loss: 1.183188760790139, train acc: 0.7628318584070797, validation acc: 0.6471844141980251.
epoch: 114, train loss: 1.181512263002766, train acc: 0.7495860690836426, validation acc: 0.6421136909527622.
epoch: 115, train loss: 1.1809489659255383, train acc: 0.751812731944048, validation acc: 0.6365092073658927.
epoch: 116, train loss: 1.1821921417474694, train acc: 0.7639166428775336, validation acc: 0.6511876167600748.
epoch: 117, train loss: 1.181676000958023, train acc: 0.7552383671139024, validation acc: 0.6301040832666133.
epoch: 118, train loss: 1.1807770283161676, train acc: 0.7635740793605481, validation acc: 0.6365092073658927.
epoch: 119, train loss: 1.1818285234632966, train acc: 0.7550670853554097, validation acc: 0.6426474512943688.
epoch: 120, train loss: 1.1816289491626544, train acc: 0.754610333999429, validation acc: 0.6383773685615158.
best validation acc 0.6784093941820123 at epoch 2.

*******************************************************
             k-nearest neighbor for testing
*******************************************************
train accuracy: 0.7734513274336283, validation accuracy: 0.6784093941820123, test accuracy: 0.6696712619300106
train report:
              precision    recall  f1-score   support

           0     0.7622    0.9445    0.8436      5337
           1     0.6701    0.7234    0.6958      2502
           2     0.9017    0.8605    0.8806       810
           3     0.7377    0.7185    0.7280      1840
           4     0.8717    0.8114    0.8405       737
           5     0.8770    0.8951    0.8860       677
           6     0.7917    0.7929    0.7923      1323
           7     0.7504    0.5204    0.6146       907
           8     0.8350    0.6128    0.7068       421
           9     0.8571    0.5087    0.6385       401
          10     0.8538    0.8258    0.8395       396
          11     0.9254    0.9337    0.9295       332
          12     0.8100    0.6068    0.6938       295
          13     0.8909    0.5052    0.6447       291
          14     0.7292    0.1341    0.2265       261
          15     0.8489    0.4777    0.6114       494
          16     0.7752    0.3906    0.5195       256
          17     0.7879    0.6638    0.7206       235

    accuracy                         0.7735     17515
   macro avg     0.8153    0.6626    0.7118     17515
weighted avg     0.7781    0.7735    0.7634     17515

validation report:
              precision    recall  f1-score   support

           0     0.6961    0.8758    0.7757      1143
           1     0.5525    0.6082    0.5790       536
           2     0.8118    0.7977    0.8047       173
           3     0.6397    0.6218    0.6306       394
           4     0.8060    0.6835    0.7397       158
           5     0.7871    0.8414    0.8133       145
           6     0.7183    0.7208    0.7196       283
           7     0.5379    0.4021    0.4602       194
           8     0.7188    0.5111    0.5974        90
           9     0.6739    0.3647    0.4733        85
          10     0.7887    0.6667    0.7226        84
          11     0.9143    0.9014    0.9078        71
          12     0.7143    0.4762    0.5714        63
          13     0.5789    0.3548    0.4400        62
          14     0.2500    0.0357    0.0625        56
          15     0.7959    0.3714    0.5065       105
          16     0.3000    0.1091    0.1600        55
          17     0.6000    0.4800    0.5333        50

    accuracy                         0.6784      3747
   macro avg     0.6602    0.5457    0.5832      3747
weighted avg     0.6703    0.6784    0.6637      3747

test report: 
              precision    recall  f1-score   support

           0     0.6792    0.8987    0.7737      1145
           1     0.5281    0.5251    0.5266       537
           2     0.8155    0.7829    0.7988       175
           3     0.6125    0.6203    0.6164       395
           4     0.7905    0.7358    0.7622       159
           5     0.7829    0.8151    0.7987       146
           6     0.7032    0.7007    0.7019       284
           7     0.5891    0.3897    0.4691       195
           8     0.6957    0.5275    0.6000        91
           9     0.7619    0.3678    0.4961        87
          10     0.7564    0.6860    0.7195        86
          11     0.8571    0.8333    0.8451        72
          12     0.5349    0.3594    0.4299        64
          13     0.8462    0.3438    0.4889        64
          14     0.4286    0.0526    0.0938        57
          15     0.7143    0.2804    0.4027       107
          16     0.6400    0.2857    0.3951        56
          17     0.7073    0.5577    0.6237        52

    accuracy                         0.6697      3772
   macro avg     0.6913    0.5424    0.5857      3772
weighted avg     0.6677    0.6697    0.6529      3772

generating embeddings for train...
embedding path:  ../embeddings/run_67/train_embedding.npy
label path:  ../embeddings/run_67/train_label.npy
shape of generated embedding: (17515, 160)
shape of label: (17515,)
generating embeddings for val...
embedding path:  ../embeddings/run_67/val_embedding.npy
label path:  ../embeddings/run_67/val_label.npy
shape of generated embedding: (3747, 160)
shape of label: (3747,)
generating embeddings for test...
embedding path:  ../embeddings/run_67/test_embedding.npy
label path:  ../embeddings/run_67/test_label.npy
shape of generated embedding: (3772, 160)
shape of label: (3772,)

program finished.
