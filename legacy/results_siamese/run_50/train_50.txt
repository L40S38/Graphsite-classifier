seed:  666
number of classes (from original clusters): 14
how to merge clusters:  [[0, 9, 12], [1, 5, 11], 2, [3, 8], 4, 6, 7, 10, 13]
whether to further subcluster data according to chemical reaction: False
positive training pair sampling threshold:  16000
negative training pair sampling threshold:  4600
number of epochs to train: 50
learning rate decay to half at epoch 20.
batch size: 256
similar margin of contrastive loss: 0.0
dissimilar margin of contrastive loss: 2.0
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of classes after merging:  9
number of pockets in training set:  12474
number of pockets in validation set:  2670
number of pockets in test set:  2682
number of train positive pairs: 144000
number of train negative pairs: 165600
model architecture:
ResidualSiameseNet(
  (embedding_net): ResidualEmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=48, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=48, out_features=48, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (rb_2): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_3): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_4): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_5): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_6): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_7): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_8): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (bn_8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(48, 96)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.7555065879033209, train acc: 0.7764932562620424, validation acc: 0.6768018018018018.
epoch: 2, train loss: 0.6641270843032718, train acc: 0.7849229287090559, validation acc: 0.674924924924925.
epoch: 3, train loss: 0.6231991154525323, train acc: 0.7951188182402055, validation acc: 0.6951951951951952.
epoch: 4, train loss: 0.5946551236933824, train acc: 0.7985709698137444, validation acc: 0.7015765765765766.
epoch: 5, train loss: 0.565683165532982, train acc: 0.8060372511239563, validation acc: 0.7008258258258259.
epoch: 6, train loss: 0.5450625148300052, train acc: 0.7817116249197174, validation acc: 0.6820570570570571.
epoch: 7, train loss: 0.5221366935059698, train acc: 0.8127007064868337, validation acc: 0.6918168168168168.
epoch: 8, train loss: 0.5069345408447029, train acc: 0.8034682080924855, validation acc: 0.6914414414414415.
epoch: 9, train loss: 0.4921131379474965, train acc: 0.8065992292870906, validation acc: 0.6903153153153153.
epoch: 10, train loss: 0.479817990172433, train acc: 0.8043513166345536, validation acc: 0.68993993993994.
epoch: 11, train loss: 0.4648256604492818, train acc: 0.809971098265896, validation acc: 0.6891891891891891.
epoch: 12, train loss: 0.4532561989964133, train acc: 0.8293192035966602, validation acc: 0.6985735735735735.
epoch: 13, train loss: 0.44293458938598634, train acc: 0.8178387925497752, validation acc: 0.6918168168168168.
epoch: 14, train loss: 0.43632791642378776, train acc: 0.8250642260757868, validation acc: 0.6884384384384384.
epoch: 15, train loss: 0.4277676834185302, train acc: 0.8344572896596018, validation acc: 0.6933183183183184.
epoch: 16, train loss: 0.4199774963664762, train acc: 0.8084457289659602, validation acc: 0.6591591591591591.
epoch: 17, train loss: 0.4133327805965138, train acc: 0.8314868336544637, validation acc: 0.691066066066066.
epoch: 18, train loss: 0.4142066410715266, train acc: 0.8241811175337187, validation acc: 0.6918168168168168.
epoch: 19, train loss: 0.4063769956830244, train acc: 0.8216923570969814, validation acc: 0.68506006006006.
epoch: 20, train loss: 0.34690739365511164, train acc: 0.8591040462427746, validation acc: 0.6974474474474475.
epoch: 21, train loss: 0.33843067938043164, train acc: 0.8555716120745023, validation acc: 0.6929429429429429.
epoch: 22, train loss: 0.3372770289857258, train acc: 0.8629576107899807, validation acc: 0.6993243243243243.
epoch: 23, train loss: 0.33366985728882387, train acc: 0.8684971098265896, validation acc: 0.7053303303303303.
epoch: 24, train loss: 0.32922662047452705, train acc: 0.8555716120745023, validation acc: 0.6925675675675675.
epoch: 25, train loss: 0.33254781980539166, train acc: 0.8547687861271677, validation acc: 0.6914414414414415.
