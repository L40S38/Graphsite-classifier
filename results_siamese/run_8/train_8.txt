number of classes: 60
number of epochs to train: 50
batch size: 256
number of workers to load data:  36
device:  cuda
number of gpus:  2
number of pockets in training set:  22527
number of pockets in validation set:  4806
number of pockets in test set:  4890
number of train positive pairs: 180000
number of train negative pairs: 177000
number of validation positive pairs: 47172
number of validation negative pairs: 44250
model architecture:
SiameseNet(
  (embedding_net): EmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=5, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=5, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=5, out_features=5, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(margin=2.0, normalize=False, mean=True)
train loss: 967.6457101510526, validation loss: 44.241896033127965.
train loss: 19.661685685358126, validation loss: 11.22605210584014.
train loss: 6.763885133203672, validation loss: 4.893598562151598.
train loss: 3.1913795421611026, validation loss: 3.9023293120497358.
train loss: 2.094269446279488, validation loss: 2.5685935380440004.
train loss: 1.602236231272294, validation loss: 1.5198684718753803.
train loss: 1.3026254192533948, validation loss: 1.5749019078608977.
train loss: 1.165941338632621, validation loss: 1.6335904859390253.
train loss: 1.057928183846781, validation loss: 1.591975280757713.
train loss: 0.967188581108713, validation loss: 1.0796553392755146.
train loss: 0.891625264784869, validation loss: 1.0178265245399276.
train loss: 0.8540748862899652, validation loss: 0.9202971001428301.
train loss: 0.8194135859299775, validation loss: 0.8339637345260638.
train loss: 0.8032310750411005, validation loss: 0.8698081226653062.
train loss: 0.7801987641919561, validation loss: 0.8580614025582216.
train loss: 0.7666239545378698, validation loss: 0.7821550336924851.
train loss: 0.7501155718635111, validation loss: 0.7932028564632048.
train loss: 0.7502616918654669, validation loss: 0.784921819574187.
train loss: 0.7303210194478182, validation loss: 0.7992477477077988.
train loss: 0.7198599516710982, validation loss: 0.7711506564606048.
train loss: 0.7103847828926493, validation loss: 0.7727070854914551.
train loss: 0.7082843701018005, validation loss: 0.8153261251162299.
train loss: 0.6978532389579367, validation loss: 0.7489383216239194.
train loss: 0.6891278279622396, validation loss: 0.772304745295864.
train loss: 0.6855638000317315, validation loss: 0.7437579510538505.
train loss: 0.6798621621011686, validation loss: 0.7489272214398784.
train loss: 0.6761786385330499, validation loss: 0.7552001169339112.
train loss: 0.6748450091279188, validation loss: 0.7577170308397608.
train loss: 0.6705243797836518, validation loss: 0.7382628572357212.
train loss: 0.6682894505519493, validation loss: 0.7450662248936957.
train loss: 0.6660047174020975, validation loss: 0.7316488387957041.
train loss: 0.6635665278928954, validation loss: 0.7551062839434034.
train loss: 0.6735363323307839, validation loss: 0.7400899524358666.
train loss: 0.6624312934341217, validation loss: 1.4560722883000459.
train loss: 0.7298283237061914, validation loss: 0.7373173737822718.
train loss: 0.6736645766399821, validation loss: 0.7516631457522469.
train loss: 0.6644683818656857, validation loss: 0.7470749980241118.
train loss: 0.6597647568304665, validation loss: 0.761190145169217.
train loss: 0.6617336912181865, validation loss: 0.7353139282595089.
train loss: 0.6600358128053467, validation loss: 0.7438234527548501.
train loss: 0.6558626774806602, validation loss: 0.7255494730966656.
train loss: 0.656064998979328, validation loss: 0.7634735275995926.
train loss: 0.6532784468706916, validation loss: 0.7426254273462367.
train loss: 0.6520320367893251, validation loss: 0.8114355665230568.
train loss: 0.6511588009959843, validation loss: 0.7486446938328297.
train loss: 0.6485525857193464, validation loss: 0.732895342262767.
train loss: 0.6491216788412142, validation loss: 0.7443969009538612.
train loss: 0.646980785594267, validation loss: 0.7253429515238016.
train loss: 0.6510367211093422, validation loss: 0.7285972260236505.
train loss: 0.6450374475473784, validation loss: 0.7283583510845482.
best validation loss 0.7253429515238016 at epoch 48.
