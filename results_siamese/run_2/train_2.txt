number of classes: 60
max number of data of each class: 5000
batch size: 256
number of workers to load data:  36
device:  cuda
number of gpus:  2
number of pockets in training set:  18872
number of pockets in validation set:  6274
number of pockets in test set:  6348
number of train positive pairs: 180000
number of train negative pairs: 177000
number of validation positive pairs: 24000
number of validation negative pairs: 26550
model architecture:
SiameseNet(
  (embedding_net): EmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=5, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=5, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=5, out_features=5, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0002
)
loss function:
ContrastiveLoss(margin=2.0, normalize=True, mean=True)
number of epochs to train: 100
train loss: 0.8318599980885909, validation loss: 0.797397979223174.
train loss: 0.7614334510354435, validation loss: 0.750437278558899.
train loss: 0.7213610067073705, validation loss: 0.7295314727692647.
train loss: 0.6925602653781239, validation loss: 0.7149074140146862.
train loss: 0.6705123368378113, validation loss: 0.704384175195892.
train loss: 0.6541414484617089, validation loss: 0.7024854852844299.
train loss: 0.6397014373009946, validation loss: 0.6989159760819462.
train loss: 0.6279604347506825, validation loss: 0.7018785751750278.
train loss: 0.6170017503583465, validation loss: 0.6957283697684842.
train loss: 0.6082723754113462, validation loss: 0.7025456637751337.
train loss: 0.6008667262315083, validation loss: 0.6961779021462159.
train loss: 0.5941083770239053, validation loss: 0.6898588397500541.
train loss: 0.5877331362385042, validation loss: 0.6840635109136414.
train loss: 0.5809844666595887, validation loss: 0.6880018321413668.
train loss: 0.5770801752768979, validation loss: 0.7072070058941488.
train loss: 0.5725048576590059, validation loss: 0.690670424705679.
train loss: 0.5681912152159447, validation loss: 0.6802015385689061.
train loss: 0.5652394146531904, validation loss: 0.6844612651338209.
train loss: 0.5620243401407193, validation loss: 0.6891810950516004.
train loss: 0.5587269677768568, validation loss: 0.6867827056069756.
train loss: 0.5556359523057269, validation loss: 0.687429585291771.
train loss: 0.5536441974159049, validation loss: 0.6815984491636916.
train loss: 0.5504376549386845, validation loss: 0.68895964459307.
train loss: 0.5480645667324547, validation loss: 0.6885297660186194.
train loss: 0.546332890037729, validation loss: 0.6904864637788043.
train loss: 0.543780095161844, validation loss: 0.6921558646514319.
train loss: 0.5418473758857791, validation loss: 0.6887057597748014.
train loss: 0.5397966060478147, validation loss: 0.688728935022854.
train loss: 0.5374197527033274, validation loss: 0.6973555431875348.
train loss: 0.5357040385398544, validation loss: 0.689283959816047.
train loss: 0.5334874026514903, validation loss: 0.6857329344254219.
train loss: 0.5325164361520975, validation loss: 0.6896601977852992.
train loss: 0.5295675079388444, validation loss: 0.6873732470051117.
train loss: 0.530145052677443, validation loss: 0.6927215868971586.
train loss: 0.5279873817507961, validation loss: 0.6943704742824053.
train loss: 0.5273858347660353, validation loss: 0.690353716841791.
train loss: 0.5257866621044169, validation loss: 0.696153380321348.
train loss: 0.5256686637515113, validation loss: 0.6896873173774999.
train loss: 0.5243667171488957, validation loss: 0.6900531401289912.
train loss: 0.5231978796309784, validation loss: 0.6916091786128948.
train loss: 0.5220941768400476, validation loss: 0.6972824887540763.
train loss: 0.521644263259503, validation loss: 0.6913966395286613.
train loss: 0.5210127936184239, validation loss: 0.6877574536543336.
train loss: 0.5187009726345372, validation loss: 0.6978259841813296.
train loss: 0.518289818194734, validation loss: 0.6904729436766853.
train loss: 0.5176848907470704, validation loss: 0.6908474592591841.
train loss: 0.5173253411258302, validation loss: 0.6942554121460807.
train loss: 0.5163001420050443, validation loss: 0.7062195328317221.
train loss: 0.5145559170933999, validation loss: 0.6872819644312949.
train loss: 0.513992937510087, validation loss: 0.6957296731799811.
train loss: 0.5136553954009583, validation loss: 0.6897764721727985.
train loss: 0.5124834384437369, validation loss: 0.6922889511757387.
train loss: 0.5130763455772934, validation loss: 0.6894215608892997.
train loss: 0.5116741286419353, validation loss: 0.690994600471474.
train loss: 0.5114173976780654, validation loss: 0.6957284071985503.
train loss: 0.5113968941611068, validation loss: 0.6903782948117582.
train loss: 0.5106311128760587, validation loss: 0.7029475630732838.
train loss: 0.5105939942047376, validation loss: 0.7038629452245997.
train loss: 0.5092292581766593, validation loss: 0.7049079959918436.
train loss: 0.5079917638228387, validation loss: 0.7024014994605241.
train loss: 0.5080943022655839, validation loss: 0.6986568118132187.
train loss: 0.5078087172521597, validation loss: 0.7015109690619977.
train loss: 0.507777229309082, validation loss: 0.7004520782259405.
train loss: 0.5062987377049208, validation loss: 0.6980263606070529.
train loss: 0.5060658193422632, validation loss: 0.6988074122880734.
train loss: 0.5058330977837913, validation loss: 0.6988933592663321.
train loss: 0.5056223981413854, validation loss: 0.7054100173965288.
train loss: 0.5047712747269318, validation loss: 0.7027772082773089.
train loss: 0.504562614440918, validation loss: 0.6986516422381152.
train loss: 0.5037203041098031, validation loss: 0.7054089630157139.
train loss: 0.5035291360593309, validation loss: 0.6957304100089445.
train loss: 0.5037700109829087, validation loss: 0.6981470327254691.
train loss: 0.5035082171784729, validation loss: 0.6960156292259753.
train loss: 0.5027445846044717, validation loss: 0.7044332906872064.
train loss: 0.5033094808541092, validation loss: 0.6952971455227845.
train loss: 0.5016607816039014, validation loss: 0.6998519803885772.
train loss: 0.5014591740199498, validation loss: 0.6978121987483386.
train loss: 0.5023562091432032, validation loss: 0.699713028512534.
train loss: 0.5018112694662826, validation loss: 0.7051285442046666.
train loss: 0.500809740873278, validation loss: 0.6975172899738618.
train loss: 0.5001692139441226, validation loss: 0.6969580995765331.
train loss: 0.5003304704724908, validation loss: 0.6955427942172239.
train loss: 0.49919482520180924, validation loss: 0.7049146528083657.
train loss: 0.4993588743877678, validation loss: 0.7000106980562446.
train loss: 0.4994232637635132, validation loss: 0.7057925412020509.
train loss: 0.49917803298992935, validation loss: 0.6961297492716834.
train loss: 0.499372374505222, validation loss: 0.6939177826863486.
train loss: 0.4980622306845101, validation loss: 0.6991417125849059.
train loss: 0.49796876872086726, validation loss: 0.704087963613629.
train loss: 0.4984220294685257, validation loss: 0.6982152478136011.
train loss: 0.4973981984670089, validation loss: 0.6998879428339052.
train loss: 0.4967652826816762, validation loss: 0.703403532818682.
train loss: 0.49754338172036394, validation loss: 0.7084613287649334.
train loss: 0.4964532005459655, validation loss: 0.6999461191179255.
train loss: 0.4969881143008961, validation loss: 0.7023529922077847.
train loss: 0.49633645255892883, validation loss: 0.6969506841974608.
train loss: 0.49580338118523776, validation loss: 0.6926736954293784.
train loss: 0.4964706668586624, validation loss: 0.6984837264335474.
train loss: 0.4961662286924047, validation loss: 0.7015267011602837.
train loss: 0.49540753124675163, validation loss: 0.7013381417686696.
best validation loss 0.6802015385689061 at epoch 17.
