save trained model at:  ../trained_models/trained_model_62.pt
save loss at:  ./results/train_results_62.json
number of classes (from original clusters): 19
how to merge clusters:  [[0, 9, 12], [1, 5, 11], 2, [3, 8, 13], 4, 6, 7, [10, 16], 15, 17, 18]
positive training pair sampling threshold:  16000
negative training pair sampling threshold:  4800
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs to train: 40
learning rate decay to half at epoch 25.
number of workers to load data:  36
device:  cuda
number of classes after merging:  11
number of pockets in training set:  14097
number of pockets in validation set:  3016
number of pockets in test set:  3031
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['6brxA00', '4c0lA00', '3rs8A02', '4nk4F00', '5fogD00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['4d86A00', '4u00A00', '1pujA00', '3nt5A00', '4j1nB01']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['4k28A00', '6hwlB00', '3upqA01', '2p2bA00', '1h5rB02']
number of train positive pairs: 176000
number of train negative pairs: 264000
number of epochs to train for hard pairs:  80
learning rate decay at epoch for hard pairs:  40
begin to select hard pairs at epoch 1
batch size for hard pairs:  128
number of hardest positive pairs for each mini-batch:  192
number of hardest negative pairs for each mini-batch:  256

*******************************************************
             train by random pairs
*******************************************************
model architecture:
JKSiameseNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 1.0013556576468727, train acc: 0.5124494573313471, validation acc: 0.34714854111405835.
epoch: 2, train loss: 0.8841608108520508, train acc: 0.6890118464921614, validation acc: 0.5543766578249337.
epoch: 3, train loss: 0.7030609084736217, train acc: 0.7627154713768887, validation acc: 0.649867374005305.
epoch: 4, train loss: 0.6206098858399824, train acc: 0.7645598354259772, validation acc: 0.6604774535809018.
epoch: 5, train loss: 0.5742661369497126, train acc: 0.7803078669220401, validation acc: 0.6889920424403183.
epoch: 6, train loss: 0.539068288525668, train acc: 0.7864793927786053, validation acc: 0.6833554376657824.
epoch: 7, train loss: 0.5187036637392911, train acc: 0.7987515074129248, validation acc: 0.6883289124668435.
epoch: 8, train loss: 0.5017669676520607, train acc: 0.801234305171313, validation acc: 0.6949602122015915.
epoch: 9, train loss: 0.4855387553648515, train acc: 0.8039299141661347, validation acc: 0.6999336870026526.
epoch: 10, train loss: 0.47353008672540836, train acc: 0.78094630063134, validation acc: 0.6760610079575596.
epoch: 11, train loss: 0.4648472582210194, train acc: 0.8052777186635455, validation acc: 0.6866710875331565.
epoch: 12, train loss: 0.4592305522918701, train acc: 0.8015889905653685, validation acc: 0.6946286472148541.
epoch: 13, train loss: 0.4537677580573342, train acc: 0.8103851883379443, validation acc: 0.7175066312997348.
epoch: 14, train loss: 0.4497677759690718, train acc: 0.805632404057601, validation acc: 0.6840185676392573.
epoch: 15, train loss: 0.4448926699378274, train acc: 0.819252323189331, validation acc: 0.6982758620689655.
epoch: 16, train loss: 0.44040687456997957, train acc: 0.8202454422926864, validation acc: 0.6992705570291777.
epoch: 17, train loss: 0.4400392208446156, train acc: 0.817691707455487, validation acc: 0.7005968169761273.
epoch: 18, train loss: 0.43514082280939276, train acc: 0.8264879052280627, validation acc: 0.7049071618037135.
epoch: 19, train loss: 0.42895318962443957, train acc: 0.809392069234589, validation acc: 0.6863395225464191.
epoch: 20, train loss: 0.42726458220048386, train acc: 0.8197488827410088, validation acc: 0.705238726790451.
epoch: 21, train loss: 0.42580097699598835, train acc: 0.8212385613960417, validation acc: 0.7178381962864722.
epoch: 22, train loss: 0.42258518974997783, train acc: 0.8267007164644959, validation acc: 0.7095490716180372.
epoch: 23, train loss: 0.4233785401951183, train acc: 0.8248563524154076, validation acc: 0.7141909814323607.
epoch: 24, train loss: 0.42078705787658693, train acc: 0.8326594310846279, validation acc: 0.7105437665782494.
epoch: 25, train loss: 0.3733515288439664, train acc: 0.8490458962899908, validation acc: 0.7198275862068966.
epoch: 26, train loss: 0.37125493465770376, train acc: 0.8475562176349578, validation acc: 0.7148541114058355.
epoch: 27, train loss: 0.3681373682022095, train acc: 0.8483365255018799, validation acc: 0.7108753315649867.
epoch: 28, train loss: 0.36593765820589935, train acc: 0.8413846917783926, validation acc: 0.7029177718832891.
epoch: 29, train loss: 0.3646229705463756, train acc: 0.8451443569553806, validation acc: 0.7158488063660478.
epoch: 30, train loss: 0.36195924106944694, train acc: 0.851528694048379, validation acc: 0.7095490716180372.
epoch: 31, train loss: 0.3595256433660334, train acc: 0.8500390153933461, validation acc: 0.7125331564986738.
epoch: 32, train loss: 0.3597841733065518, train acc: 0.844718734482514, validation acc: 0.7035809018567639.
epoch: 33, train loss: 0.3591162882371382, train acc: 0.8501808895509683, validation acc: 0.7092175066312998.
epoch: 34, train loss: 0.3561221212387085, train acc: 0.8496133929204795, validation acc: 0.7165119363395226.
epoch: 35, train loss: 0.35603770015889946, train acc: 0.8574164715896999, validation acc: 0.71684350132626.
epoch: 36, train loss: 0.35774327817396684, train acc: 0.8557139816982336, validation acc: 0.7198275862068966.
epoch: 37, train loss: 0.3539863735545765, train acc: 0.8547917996736895, validation acc: 0.7244694960212201.
epoch: 38, train loss: 0.3565424471768466, train acc: 0.8615308221607434, validation acc: 0.7204907161803713.
epoch: 39, train loss: 0.3598963929263028, train acc: 0.8540824288855785, validation acc: 0.726790450928382.
epoch: 40, train loss: 0.35208677380301734, train acc: 0.8435837412215365, validation acc: 0.7211538461538461.
best validation acc 0.726790450928382 at epoch 39.


*******************************************************
             train by random pairs
*******************************************************
model architecture:
SelectiveSiameseNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
SelectiveContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, num_pos_pair=192, num_neg_pair=256)
epoch: 1, train loss: 1.616988575418731, train acc: 0.7690998084698872, validation acc: 0.661472148541114.
epoch: 2, train loss: 1.408254142745502, train acc: 0.7886075051429382, validation acc: 0.669761273209549.
epoch: 3, train loss: 1.3586637346359165, train acc: 0.7981130737036248, validation acc: 0.6982758620689655.
epoch: 4, train loss: 1.3338666962520043, train acc: 0.8076186422643116, validation acc: 0.7062334217506632.
epoch: 5, train loss: 1.309152304499486, train acc: 0.8103142512591331, validation acc: 0.7032493368700266.
epoch: 6, train loss: 1.2915901656277202, train acc: 0.817691707455487, validation acc: 0.7151856763925729.
epoch: 7, train loss: 1.2799929927512643, train acc: 0.8286160175923956, validation acc: 0.7165119363395226.
epoch: 8, train loss: 1.2704279501636986, train acc: 0.8313116265872171, validation acc: 0.713527851458886.
epoch: 9, train loss: 1.262892609114315, train acc: 0.8342200468184721, validation acc: 0.7261273209549072.
epoch: 10, train loss: 1.25487214961419, train acc: 0.8320209973753281, validation acc: 0.7274535809018567.
epoch: 11, train loss: 1.248310841958559, train acc: 0.8399659502021707, validation acc: 0.7403846153846154.
epoch: 12, train loss: 1.2464423360789674, train acc: 0.8464212243739803, validation acc: 0.7410477453580901.
epoch: 13, train loss: 1.2438917534130416, train acc: 0.8332269277151166, validation acc: 0.7340848806366048.
epoch: 14, train loss: 1.238144263451531, train acc: 0.8339362985032276, validation acc: 0.7287798408488063.
epoch: 15, train loss: 1.2372121135379663, train acc: 0.8291835142228843, validation acc: 0.7224801061007957.
epoch: 16, train loss: 1.2429188535374212, train acc: 0.8291835142228843, validation acc: 0.7357427055702918.
epoch: 17, train loss: 1.2398556733433632, train acc: 0.8294672625381286, validation acc: 0.7257957559681698.
epoch: 18, train loss: 1.2324062289650646, train acc: 0.8391147052564375, validation acc: 0.736737400530504.
epoch: 19, train loss: 1.2297038867374788, train acc: 0.8304603816414841, validation acc: 0.730106100795756.
epoch: 20, train loss: 1.2276765027500456, train acc: 0.8307441299567284, validation acc: 0.7354111405835544.
epoch: 21, train loss: 1.2266950437109791, train acc: 0.8346456692913385, validation acc: 0.7324270557029178.
epoch: 22, train loss: 1.2287823730069387, train acc: 0.8381925232318933, validation acc: 0.7420424403183024.
epoch: 23, train loss: 1.2273491783585202, train acc: 0.8373412782861602, validation acc: 0.7407161803713528.
epoch: 24, train loss: 1.2260850715650762, train acc: 0.8360644108675604, validation acc: 0.7360742705570292.
epoch: 25, train loss: 1.2214002104466168, train acc: 0.8386181457047599, validation acc: 0.7453580901856764.
epoch: 26, train loss: 1.2268766239904807, train acc: 0.8323047456905724, validation acc: 0.7155172413793104.
epoch: 27, train loss: 1.2223866433561823, train acc: 0.8392565794140597, validation acc: 0.7496684350132626.
epoch: 28, train loss: 1.2245576170486456, train acc: 0.8281194580407178, validation acc: 0.7314323607427056.
epoch: 29, train loss: 1.2210043895231038, train acc: 0.8352841030006384, validation acc: 0.7513262599469496.
epoch: 30, train loss: 1.221387981800685, train acc: 0.8381925232318933, validation acc: 0.7360742705570292.
epoch: 31, train loss: 1.222144888246939, train acc: 0.8318081861388948, validation acc: 0.735079575596817.
epoch: 32, train loss: 1.2193559029802161, train acc: 0.836490033340427, validation acc: 0.7311007957559682.
epoch: 33, train loss: 1.220054261379248, train acc: 0.8329431793998723, validation acc: 0.7370689655172413.
epoch: 34, train loss: 1.2223939230622358, train acc: 0.8338653614244166, validation acc: 0.736737400530504.
epoch: 35, train loss: 1.2222669195207514, train acc: 0.8403206355962262, validation acc: 0.7602785145888594.
epoch: 36, train loss: 1.2322257138190311, train acc: 0.8291125771440732, validation acc: 0.7407161803713528.
epoch: 37, train loss: 1.2274926092179557, train acc: 0.8280485209619068, validation acc: 0.7324270557029178.
epoch: 38, train loss: 1.2229105060142085, train acc: 0.8315953749024615, validation acc: 0.736737400530504.
epoch: 39, train loss: 1.2246174548735158, train acc: 0.8343619209760942, validation acc: 0.7208222811671088.
epoch: 40, train loss: 1.2127337837619243, train acc: 0.8439384266155919, validation acc: 0.7493368700265252.
epoch: 41, train loss: 1.2029433227873336, train acc: 0.8464921614527914, validation acc: 0.75.
epoch: 42, train loss: 1.2013987090586005, train acc: 0.8429453075122366, validation acc: 0.7390583554376657.
epoch: 43, train loss: 1.1988038374071335, train acc: 0.8449315457189472, validation acc: 0.75.
epoch: 44, train loss: 1.195827079628077, train acc: 0.8503227637085905, validation acc: 0.741710875331565.
epoch: 45, train loss: 1.1969647571233741, train acc: 0.8408171951479038, validation acc: 0.7430371352785146.
epoch: 46, train loss: 1.1939454862416403, train acc: 0.8466340356104136, validation acc: 0.7420424403183024.
epoch: 47, train loss: 1.1948941733447882, train acc: 0.8479109030290133, validation acc: 0.7503315649867374.
epoch: 48, train loss: 1.1913891155746523, train acc: 0.8533021210186564, validation acc: 0.7446949602122016.
epoch: 49, train loss: 1.192479092118277, train acc: 0.847769028871391, validation acc: 0.7529840848806366.
epoch: 50, train loss: 1.192514367537065, train acc: 0.8516705682060013, validation acc: 0.7443633952254642.
epoch: 51, train loss: 1.190239399447031, train acc: 0.8476271547137689, validation acc: 0.7423740053050398.
epoch: 52, train loss: 1.188653377349758, train acc: 0.8501099524721571, validation acc: 0.745026525198939.
epoch: 53, train loss: 1.1901444430712318, train acc: 0.8506065120238349, validation acc: 0.7473474801061007.
epoch: 54, train loss: 1.1891404532166179, train acc: 0.8580549052989997, validation acc: 0.7586206896551724.
epoch: 55, train loss: 1.1901208322098102, train acc: 0.8490458962899908, validation acc: 0.7453580901856764.
epoch: 56, train loss: 1.1857224272895077, train acc: 0.8508902603390792, validation acc: 0.7427055702917772.
epoch: 57, train loss: 1.1882792015315788, train acc: 0.8560686670922891, validation acc: 0.7456896551724138.
epoch: 58, train loss: 1.1886756139538364, train acc: 0.8541533659643896, validation acc: 0.7460212201591512.
epoch: 59, train loss: 1.1861038590304493, train acc: 0.8547917996736895, validation acc: 0.7470159151193634.
epoch: 60, train loss: 1.1879002006143948, train acc: 0.859757395190466, validation acc: 0.763262599469496.
epoch: 61, train loss: 1.1850377433250878, train acc: 0.8552883592253671, validation acc: 0.7536472148541115.
epoch: 62, train loss: 1.1860183106944813, train acc: 0.8555721075406115, validation acc: 0.7609416445623343.
epoch: 63, train loss: 1.1866707285481088, train acc: 0.8561396041711002, validation acc: 0.7589522546419099.
epoch: 64, train loss: 1.1840402609927483, train acc: 0.8579130311413776, validation acc: 0.7639257294429708.
epoch: 65, train loss: 1.1851581736018648, train acc: 0.855997730013478, validation acc: 0.7609416445623343.
epoch: 66, train loss: 1.1806458320084787, train acc: 0.8524508760729234, validation acc: 0.7493368700265252.
epoch: 67, train loss: 1.1830434617861656, train acc: 0.8589061502447329, validation acc: 0.7523209549071618.
epoch: 68, train loss: 1.1829228493750779, train acc: 0.855217422146556, validation acc: 0.7476790450928382.
epoch: 69, train loss: 1.1819205387974108, train acc: 0.8626658154217209, validation acc: 0.7569628647214854.
epoch: 70, train loss: 1.183249733073148, train acc: 0.8586933390082996, validation acc: 0.756631299734748.
epoch: 71, train loss: 1.1811626898686363, train acc: 0.863446123288643, validation acc: 0.7728779840848806.
epoch: 72, train loss: 1.1793626049681551, train acc: 0.8601120805845215, validation acc: 0.7539787798408488.
epoch: 73, train loss: 1.1799631343491361, train acc: 0.860466765978577, validation acc: 0.7556366047745358.
epoch: 74, train loss: 1.178435456843479, train acc: 0.8616017592395545, validation acc: 0.7672413793103449.
epoch: 75, train loss: 1.1773450569817119, train acc: 0.8657870468894091, validation acc: 0.7645888594164456.
epoch: 76, train loss: 1.177943764945969, train acc: 0.8613889480031213, validation acc: 0.7606100795755968.
epoch: 77, train loss: 1.1762259556279795, train acc: 0.8627367525005321, validation acc: 0.7579575596816976.
epoch: 78, train loss: 1.177676984688641, train acc: 0.8677023480173086, validation acc: 0.7675729442970822.
epoch: 79, train loss: 1.1769266156003906, train acc: 0.8693339008299639, validation acc: 0.7606100795755968.
epoch: 80, train loss: 1.1779747195069061, train acc: 0.8640845569979428, validation acc: 0.7543103448275862.
best validation acc 0.7728779840848806 at epoch 71.

*******************************************************
             k-nearest neighbor for testing
*******************************************************
train accuracy: 0.8640845569979428, validation accuracy: 0.7543103448275862, test accuracy: 0.7472781260310128
train report:
              precision    recall  f1-score   support

           0     0.8597    0.9633    0.9086      5074
           1     0.7794    0.7645    0.7719      2200
           2     0.9650    0.9543    0.9597       810
           3     0.8264    0.8022    0.8141      1840
           4     0.9276    0.8697    0.8978       737
           5     0.9610    0.9823    0.9715       677
           6     0.8693    0.8186    0.8432       634
           7     0.8607    0.5722    0.6874       907
           8     0.9357    0.8646    0.8988       421
           9     0.9128    0.7307    0.8116       401
          10     0.9678    0.9116    0.9389       396

    accuracy                         0.8641     14097
   macro avg     0.8969    0.8395    0.8639     14097
weighted avg     0.8646    0.8641    0.8609     14097

validation report:
              precision    recall  f1-score   support

           0     0.7787    0.9227    0.8446      1087
           1     0.6039    0.5987    0.6013       471
           2     0.8434    0.8092    0.8260       173
           3     0.7135    0.6193    0.6630       394
           4     0.8533    0.8101    0.8312       158
           5     0.8264    0.8207    0.8235       145
           6     0.7519    0.7185    0.7348       135
           7     0.7321    0.4227    0.5359       194
           8     0.8391    0.8111    0.8249        90
           9     0.7170    0.4471    0.5507        85
          10     0.8846    0.8214    0.8519        84

    accuracy                         0.7543      3016
   macro avg     0.7767    0.7092    0.7353      3016
weighted avg     0.7516    0.7543    0.7467      3016

test report: 
              precision    recall  f1-score   support

           0     0.8050    0.9182    0.8579      1088
           1     0.5791    0.5742    0.5766       472
           2     0.8068    0.8114    0.8091       175
           3     0.6946    0.6506    0.6719       395
           4     0.8462    0.7610    0.8013       159
           5     0.8582    0.8288    0.8432       146
           6     0.6596    0.6788    0.6691       137
           7     0.6212    0.4205    0.5015       195
           8     0.8023    0.7582    0.7797        91
           9     0.8182    0.5172    0.6338        87
          10     0.8333    0.7558    0.7927        86

    accuracy                         0.7473      3031
   macro avg     0.7568    0.6977    0.7215      3031
weighted avg     0.7430    0.7473    0.7413      3031

generating embeddings for train...
embedding path:  ../embeddings/run_62/train_embedding.npy
label path:  ../embeddings/run_62/train_label.npy
shape of generated embedding: (14097, 128)
shape of label: (14097,)
generating embeddings for val...
embedding path:  ../embeddings/run_62/val_embedding.npy
label path:  ../embeddings/run_62/val_label.npy
shape of generated embedding: (3016, 128)
shape of label: (3016,)
generating embeddings for test...
embedding path:  ../embeddings/run_62/test_embedding.npy
label path:  ../embeddings/run_62/test_label.npy
shape of generated embedding: (3031, 128)
shape of label: (3031,)
