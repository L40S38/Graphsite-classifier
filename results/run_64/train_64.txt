seed:  666
save trained model at:  ../trained_models/trained_model_64.pt
save loss at:  ./results/train_results_64.json
how to merge clusters:  [[0, 9, 12], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23]
positive training pair sampling threshold:  14000
negative training pair sampling threshold:  4000
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs to train: 50
learning rate decay to half at epoch 25.
number of workers to load data:  36
device:  cuda
number of classes after merging:  13
number of pockets in training set:  15715
number of pockets in validation set:  3363
number of pockets in test set:  3379
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['6brxA00', '4c0lA00', '3rs8A02', '4nk4F00', '5fogD00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['4d86A00', '4u00A00', '1pujA00', '3nt5A00', '4j1nB01']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['4k28A00', '6hwlB00', '3upqA01', '2p2bA00', '1h5rB02']
number of train positive pairs: 182000
number of train negative pairs: 312000
number of epochs to train for hard pairs:  100
learning rate decay at epoch for hard pairs:  40
begin to select hard pairs at epoch 1
batch size for hard pairs:  128
number of hardest positive pairs for each mini-batch:  192
number of hardest negative pairs for each mini-batch:  256

*******************************************************
             train by random pairs
*******************************************************
model architecture:
SiameseNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.9580714486620204, train acc: 0.5860642698059179, validation acc: 0.43651501635444545.
epoch: 2, train loss: 0.7798246947686199, train acc: 0.7432389436843779, validation acc: 0.6390127862027951.
epoch: 3, train loss: 0.6387230357737677, train acc: 0.7616926503340757, validation acc: 0.655961938745168.
epoch: 4, train loss: 0.5729431886557143, train acc: 0.7721921730830417, validation acc: 0.6729110912875409.
epoch: 5, train loss: 0.540166359750848, train acc: 0.7764556156538339, validation acc: 0.6767766874814154.
epoch: 6, train loss: 0.5308591554000792, train acc: 0.7939548202354438, validation acc: 0.6859946476360392.
epoch: 7, train loss: 0.5099468540083542, train acc: 0.7888005090677697, validation acc: 0.6862920011894142.
epoch: 8, train loss: 0.49825688243974076, train acc: 0.7840279987273305, validation acc: 0.688076122509664.
epoch: 9, train loss: 0.48643848212238266, train acc: 0.7782373528475979, validation acc: 0.6824264049955397.
epoch: 10, train loss: 0.4803056180081387, train acc: 0.7909640470887687, validation acc: 0.6898602438299137.
epoch: 11, train loss: 0.4706270295888306, train acc: 0.7875914731148584, validation acc: 0.6743978590544157.
epoch: 12, train loss: 0.46536768958829194, train acc: 0.7937639198218263, validation acc: 0.6865893547427891.
epoch: 13, train loss: 0.4560619462047994, train acc: 0.8027998727330576, validation acc: 0.7056199821587869.
epoch: 14, train loss: 0.45288867309508535, train acc: 0.7922367165128857, validation acc: 0.6856972940826643.
epoch: 15, train loss: 0.45056304713878553, train acc: 0.8029907731466751, validation acc: 0.688076122509664.
epoch: 16, train loss: 0.4436586507264419, train acc: 0.793000318167356, validation acc: 0.6741005055010407.
epoch: 17, train loss: 0.44388614061486864, train acc: 0.7961183582564428, validation acc: 0.6862920011894142.
epoch: 18, train loss: 0.4420803037944593, train acc: 0.8083359847279669, validation acc: 0.6990782039845376.
epoch: 19, train loss: 0.43872832107543946, train acc: 0.7949729557747375, validation acc: 0.6853999405292893.
epoch: 20, train loss: 0.4356805358917607, train acc: 0.8097995545657015, validation acc: 0.6972940826642878.
epoch: 21, train loss: 0.43269292712501184, train acc: 0.7957365574292078, validation acc: 0.6901575973832887.
epoch: 22, train loss: 0.43274128004993023, train acc: 0.7919185491568566, validation acc: 0.6824264049955397.
epoch: 23, train loss: 0.4315533660857784, train acc: 0.7965637925548839, validation acc: 0.6752899197145406.
epoch: 24, train loss: 0.42789591610866035, train acc: 0.8087814190264079, validation acc: 0.7059173357121618.
epoch: 25, train loss: 0.38012724508061585, train acc: 0.8276169265033407, validation acc: 0.7017543859649122.
epoch: 26, train loss: 0.37539546271567403, train acc: 0.8250715876551066, validation acc: 0.6975914362176628.
epoch: 27, train loss: 0.3809207324904469, train acc: 0.820935412026726, validation acc: 0.6964020220041629.
epoch: 28, train loss: 0.36772985975751993, train acc: 0.8372892141266306, validation acc: 0.7035385072851621.
epoch: 29, train loss: 0.3693705137306862, train acc: 0.8384982500795418, validation acc: 0.7085935176925364.
epoch: 30, train loss: 0.36609230189767444, train acc: 0.8310531339484569, validation acc: 0.6975914362176628.
epoch: 31, train loss: 0.3655433371231141, train acc: 0.8315622017181037, validation acc: 0.7074041034790366.
epoch: 32, train loss: 0.363766798644896, train acc: 0.8315622017181037, validation acc: 0.7032411537317871.
epoch: 33, train loss: 0.3606876749509742, train acc: 0.8307349665924276, validation acc: 0.7029438001784122.
epoch: 34, train loss: 0.3583428854691355, train acc: 0.8357620108176901, validation acc: 0.711864406779661.
epoch: 35, train loss: 0.36177201120476976, train acc: 0.829271396754693, validation acc: 0.704133214391912.
epoch: 36, train loss: 0.3603885426386165, train acc: 0.8321985364301623, validation acc: 0.704133214391912.
epoch: 37, train loss: 0.35709042179246664, train acc: 0.8377346484250716, validation acc: 0.7047279214986619.
epoch: 38, train loss: 0.35531188438199307, train acc: 0.8351256761056316, validation acc: 0.7020517395182873.
epoch: 39, train loss: 0.3590125474350655, train acc: 0.8419980909958639, validation acc: 0.696104668450788.
epoch: 40, train loss: 0.3531090592449976, train acc: 0.8344257079223671, validation acc: 0.7032411537317871.
epoch: 41, train loss: 0.3530723868767742, train acc: 0.8377346484250716, validation acc: 0.6966993755575379.
epoch: 42, train loss: 0.35269489811885696, train acc: 0.8354438434616608, validation acc: 0.7112696996729111.
epoch: 43, train loss: 0.35254856690318, train acc: 0.841934457524658, validation acc: 0.7062146892655368.
epoch: 44, train loss: 0.3490468699806615, train acc: 0.8453706649697741, validation acc: 0.7053226286054118.
epoch: 45, train loss: 0.3499148235861589, train acc: 0.8369074132993954, validation acc: 0.7008623253047874.
epoch: 46, train loss: 0.34944343488129526, train acc: 0.8342348075087496, validation acc: 0.696104668450788.
epoch: 47, train loss: 0.34694853826669547, train acc: 0.8431434934775692, validation acc: 0.7002676181980375.
epoch: 48, train loss: 0.34614966882867854, train acc: 0.8421889914094813, validation acc: 0.7082961641391614.
epoch: 49, train loss: 0.34719210169865533, train acc: 0.8428889595927458, validation acc: 0.7088908712459114.
epoch: 50, train loss: 0.3481529584413598, train acc: 0.8383709831371301, validation acc: 0.7014570324115373.
best validation acc 0.711864406779661 at epoch 34.


*******************************************************
             train by random pairs
*******************************************************
model architecture:
SelectiveSiameseNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
SelectiveContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, num_pos_pair=192, num_neg_pair=256)
epoch: 1, train loss: 1.6265622661256123, train acc: 0.7789373210308622, validation acc: 0.6645851917930419.
epoch: 2, train loss: 1.387590004558636, train acc: 0.8008272351256761, validation acc: 0.6895628902765388.
epoch: 3, train loss: 1.3321347969762858, train acc: 0.8115176582882596, validation acc: 0.7068093963722867.
epoch: 4, train loss: 1.3044988983619623, train acc: 0.817053770283169, validation acc: 0.7148379423134107.
epoch: 5, train loss: 1.2894517018804101, train acc: 0.8179446388800509, validation acc: 0.7160273565269105.
epoch: 6, train loss: 1.2728381102391089, train acc: 0.825771555838371, validation acc: 0.7341659232827832.
epoch: 7, train loss: 1.2649651870339769, train acc: 0.8246897868278714, validation acc: 0.7311923877490336.
epoch: 8, train loss: 1.2596759488349343, train acc: 0.8148265987909641, validation acc: 0.7091882247992863.
epoch: 9, train loss: 1.2525612934117396, train acc: 0.8169901368119631, validation acc: 0.71959559916741.
epoch: 10, train loss: 1.247284048969894, train acc: 0.8281896277441935, validation acc: 0.7291109128754089.
epoch: 11, train loss: 1.2452213461850468, train acc: 0.820489977728285, validation acc: 0.7187035385072852.
epoch: 12, train loss: 1.2458075745133008, train acc: 0.8155902004454343, validation acc: 0.7088908712459114.
epoch: 13, train loss: 1.24645682435478, train acc: 0.8129812281259943, validation acc: 0.7172167707404103.
epoch: 14, train loss: 1.241569623983497, train acc: 0.820935412026726, validation acc: 0.7249479631281593.
epoch: 15, train loss: 1.2410747678337437, train acc: 0.8113903913458479, validation acc: 0.7109723461195361.
epoch: 16, train loss: 1.2389806047025145, train acc: 0.8210626789691378, validation acc: 0.7181088314005353.
epoch: 17, train loss: 1.238831441556938, train acc: 0.8123448934139357, validation acc: 0.7044305679452869.
epoch: 18, train loss: 1.2427101535142755, train acc: 0.8169901368119631, validation acc: 0.7071067499256616.
epoch: 19, train loss: 1.2417802468945747, train acc: 0.8145720649061406, validation acc: 0.7082961641391614.
epoch: 20, train loss: 1.2387485138191505, train acc: 0.8127903277123767, validation acc: 0.7097829319060363.
epoch: 21, train loss: 1.2397996536506934, train acc: 0.8059179128221444, validation acc: 0.7181088314005353.
epoch: 22, train loss: 1.2434332012675768, train acc: 0.8072542157174674, validation acc: 0.7133511745465358.
epoch: 23, train loss: 1.2347736562222356, train acc: 0.8169901368119631, validation acc: 0.72019030627416.
epoch: 24, train loss: 1.236172831194828, train acc: 0.8120903595291123, validation acc: 0.7074041034790366.
epoch: 25, train loss: 1.2381817440823042, train acc: 0.8076360165447025, validation acc: 0.7008623253047874.
epoch: 26, train loss: 1.2373423180101484, train acc: 0.8064269805917913, validation acc: 0.6975914362176628.
epoch: 27, train loss: 1.2405238591428027, train acc: 0.8093541202672606, validation acc: 0.696104668450788.
epoch: 28, train loss: 1.2390305631799952, train acc: 0.8071905822462615, validation acc: 0.6922390722569135.
epoch: 29, train loss: 1.2384150421452795, train acc: 0.8068724148902323, validation acc: 0.7056199821587869.
epoch: 30, train loss: 1.2367179167770521, train acc: 0.8129812281259943, validation acc: 0.7059173357121618.
epoch: 31, train loss: 1.233718992764038, train acc: 0.8007636016544702, validation acc: 0.6987808504311627.
epoch: 32, train loss: 1.2364948253316528, train acc: 0.8028635062042634, validation acc: 0.7097829319060363.
epoch: 33, train loss: 1.2367047172943852, train acc: 0.8022271714922049, validation acc: 0.6889681831697889.
epoch: 34, train loss: 1.2360846159570251, train acc: 0.7984091632198537, validation acc: 0.6925364258102884.
epoch: 35, train loss: 1.2381566017373846, train acc: 0.8057270124085268, validation acc: 0.7050252750520368.
epoch: 36, train loss: 1.2355406803581888, train acc: 0.7980273623926185, validation acc: 0.6978887897710377.
epoch: 37, train loss: 1.2376726923631138, train acc: 0.8011454024817054, validation acc: 0.6966993755575379.
epoch: 38, train loss: 1.2365217712964611, train acc: 0.8132357620108177, validation acc: 0.719892952720785.
epoch: 39, train loss: 1.2326508809226593, train acc: 0.8052815781100859, validation acc: 0.703835860838537.
epoch: 40, train loss: 1.2237070963373633, train acc: 0.8218899140948138, validation acc: 0.7219744275944098.
epoch: 41, train loss: 1.2161076605092767, train acc: 0.8166719694559338, validation acc: 0.7243532560214094.
epoch: 42, train loss: 1.2129517539483619, train acc: 0.8157811008590519, validation acc: 0.7192982456140351.
epoch: 43, train loss: 1.211040990507133, train acc: 0.8287623289850461, validation acc: 0.7181088314005353.
epoch: 44, train loss: 1.2090194988129555, train acc: 0.8179446388800509, validation acc: 0.7071067499256616.
epoch: 45, train loss: 1.2093468546110004, train acc: 0.8238625517021954, validation acc: 0.7225691347011597.
epoch: 46, train loss: 1.2075432584731067, train acc: 0.8195354756601972, validation acc: 0.7151352958667856.
epoch: 47, train loss: 1.207957450013603, train acc: 0.8202354438434617, validation acc: 0.7172167707404103.
epoch: 48, train loss: 1.204772359322108, train acc: 0.8164810690423162, validation acc: 0.7157300029735355.
epoch: 49, train loss: 1.2029866239167228, train acc: 0.8265987909640471, validation acc: 0.7267320844484092.
epoch: 50, train loss: 1.2037494816955798, train acc: 0.8183900731784919, validation acc: 0.7145405887600357.
epoch: 51, train loss: 1.2040730671476683, train acc: 0.8199809099586383, validation acc: 0.7234611953612846.
epoch: 52, train loss: 1.202746922785021, train acc: 0.8172446706967865, validation acc: 0.7085935176925364.
epoch: 53, train loss: 1.2019942510234052, train acc: 0.829271396754693, validation acc: 0.7353553374962831.
epoch: 54, train loss: 1.2017111539537753, train acc: 0.8199172764874324, validation acc: 0.7401129943502824.
epoch: 55, train loss: 1.2005912300895099, train acc: 0.8295895641107223, validation acc: 0.7282188522152839.
epoch: 56, train loss: 1.2020831712773674, train acc: 0.8238625517021954, validation acc: 0.7237585489146595.
epoch: 57, train loss: 1.2015659070529963, train acc: 0.8249443207126949, validation acc: 0.7219744275944098.
epoch: 58, train loss: 1.2021986533604856, train acc: 0.8122812599427299, validation acc: 0.7210823669342848.
epoch: 59, train loss: 1.2008705691733839, train acc: 0.821826280623608, validation acc: 0.71959559916741.
epoch: 60, train loss: 1.1992137038874353, train acc: 0.8274260260897232, validation acc: 0.7311923877490336.
epoch: 61, train loss: 1.199838590803716, train acc: 0.8296531975819281, validation acc: 0.7264347308950342.
epoch: 62, train loss: 1.198460315961244, train acc: 0.8257079223671652, validation acc: 0.7395182872435325.
epoch: 63, train loss: 1.2012791683289117, train acc: 0.8286350620426344, validation acc: 0.727921498661909.
epoch: 64, train loss: 1.2015178898541112, train acc: 0.8243716194718422, validation acc: 0.7222717811477847.
epoch: 65, train loss: 1.1975721332593674, train acc: 0.8274260260897232, validation acc: 0.7243532560214094.
epoch: 66, train loss: 1.1964589275277706, train acc: 0.8234807508749602, validation acc: 0.7344632768361582.
epoch: 67, train loss: 1.1972008035022392, train acc: 0.8283168946866052, validation acc: 0.736544751709783.
epoch: 68, train loss: 1.1956206044217683, train acc: 0.831880369074133, validation acc: 0.727921498661909.
epoch: 69, train loss: 1.1954847246143385, train acc: 0.8306713331212218, validation acc: 0.7317870948557835.
epoch: 70, train loss: 1.1955175889341167, train acc: 0.8310531339484569, validation acc: 0.7300029735355338.
epoch: 71, train loss: 1.1958105637430387, train acc: 0.8307349665924276, validation acc: 0.7338685697294083.
epoch: 72, train loss: 1.1973328795184628, train acc: 0.8262169901368119, validation acc: 0.7300029735355338.
epoch: 73, train loss: 1.1950443156338102, train acc: 0.8291441298122812, validation acc: 0.7300029735355338.
epoch: 74, train loss: 1.1952418699349352, train acc: 0.8264078905504295, validation acc: 0.7332738626226584.
epoch: 75, train loss: 1.1959380744827446, train acc: 0.8321985364301623, validation acc: 0.735950044603033.
epoch: 76, train loss: 1.1946410473601792, train acc: 0.8236080178173719, validation acc: 0.7317870948557835.
epoch: 77, train loss: 1.194158714517402, train acc: 0.827489659560929, validation acc: 0.7291109128754089.
epoch: 78, train loss: 1.1929142948329978, train acc: 0.8234171174037543, validation acc: 0.727921498661909.
epoch: 79, train loss: 1.195430055203856, train acc: 0.827107858733694, validation acc: 0.7216770740410348.
epoch: 80, train loss: 1.1947578207207452, train acc: 0.8281896277441935, validation acc: 0.735950044603033.
epoch: 81, train loss: 1.194214753971318, train acc: 0.8252624880687242, validation acc: 0.7362473981564079.
epoch: 82, train loss: 1.1925209796716478, train acc: 0.8280623608017817, validation acc: 0.736544751709783.
epoch: 83, train loss: 1.1924732959558275, train acc: 0.8310531339484569, validation acc: 0.7410050550104074.
epoch: 84, train loss: 1.192630671364228, train acc: 0.8322621699013681, validation acc: 0.7347606303895332.
epoch: 85, train loss: 1.1920227293138128, train acc: 0.8362074451161311, validation acc: 0.7332738626226584.
epoch: 86, train loss: 1.1908021842054974, train acc: 0.8363983455297487, validation acc: 0.743681236990782.
epoch: 87, train loss: 1.1911926860882034, train acc: 0.8351893095768375, validation acc: 0.7418971156705323.
epoch: 88, train loss: 1.1917629518230202, train acc: 0.8306713331212218, validation acc: 0.736544751709783.
epoch: 89, train loss: 1.1917560519226926, train acc: 0.8344257079223671, validation acc: 0.7383288730300327.
epoch: 90, train loss: 1.19182730290608, train acc: 0.8309258670060452, validation acc: 0.7288135593220338.
epoch: 91, train loss: 1.1919136044939713, train acc: 0.8279987273305759, validation acc: 0.7305976806422837.
epoch: 92, train loss: 1.1907789041308314, train acc: 0.8314985682468978, validation acc: 0.7222717811477847.
epoch: 93, train loss: 1.1895788146701216, train acc: 0.8288895959274578, validation acc: 0.735652691049658.
epoch: 94, train loss: 1.1907909194577906, train acc: 0.8346166083359847, validation acc: 0.7383288730300327.
epoch: 95, train loss: 1.1903540215013593, train acc: 0.8344257079223671, validation acc: 0.7192982456140351.
epoch: 96, train loss: 1.1896436587070722, train acc: 0.8292077632834871, validation acc: 0.7255426702349093.
epoch: 97, train loss: 1.1870484716858918, train acc: 0.8298440979955457, validation acc: 0.7380315194766578.
epoch: 98, train loss: 1.1880710970189097, train acc: 0.8324530703149857, validation acc: 0.7344632768361582.
epoch: 99, train loss: 1.1908269399911962, train acc: 0.831434934775692, validation acc: 0.7368421052631579.
epoch: 100, train loss: 1.1891773306278293, train acc: 0.838434616608336, validation acc: 0.7457627118644068.
best validation acc 0.7457627118644068 at epoch 100.

*******************************************************
             k-nearest neighbor for testing
*******************************************************
train accuracy: 0.838434616608336, validation accuracy: 0.7457627118644068, test accuracy: 0.7413435927789287
train report:
              precision    recall  f1-score   support

           0     0.8139    0.9620    0.8818      5074
           1     0.7883    0.7186    0.7518      2502
           2     0.9194    0.8877    0.9033       810
           3     0.8121    0.8152    0.8137      1840
           4     0.9478    0.8128    0.8751       737
           5     0.8837    0.9424    0.9121       677
           6     0.8319    0.8639    0.8476      1323
           7     0.8617    0.5700    0.6861       907
           8     0.9351    0.7530    0.8342       421
           9     0.9478    0.5436    0.6910       401
          10     0.8911    0.9091    0.9000       396
          11     0.9805    0.9066    0.9421       332
          12     0.9250    0.6271    0.7475       295

    accuracy                         0.8384     15715
   macro avg     0.8876    0.7932    0.8297     15715
weighted avg     0.8428    0.8384    0.8341     15715

validation report:
              precision    recall  f1-score   support

           0     0.7329    0.9439    0.8251      1087
           1     0.6650    0.5037    0.5732       536
           2     0.8253    0.7919    0.8083       173
           3     0.6912    0.7157    0.7032       394
           4     0.8779    0.7278    0.7958       158
           5     0.7531    0.8414    0.7948       145
           6     0.7641    0.7668    0.7654       283
           7     0.7075    0.3866    0.5000       194
           8     0.8816    0.7444    0.8072        90
           9     0.8780    0.4235    0.5714        85
          10     0.8000    0.8095    0.8047        84
          11     0.9848    0.9155    0.9489        71
          12     0.8750    0.4444    0.5895        63

    accuracy                         0.7458      3363
   macro avg     0.8028    0.6935    0.7298      3363
weighted avg     0.7481    0.7458    0.7342      3363

test report: 
              precision    recall  f1-score   support

           0     0.7437    0.9228    0.8236      1088
           1     0.6517    0.5121    0.5735       537
           2     0.8580    0.7943    0.8249       175
           3     0.7098    0.7367    0.7230       395
           4     0.8258    0.6855    0.7491       159
           5     0.8050    0.8767    0.8393       146
           6     0.6984    0.7500    0.7233       284
           7     0.6364    0.3949    0.4873       195
           8     0.7910    0.5824    0.6709        91
           9     0.8780    0.4138    0.5625        87
          10     0.7835    0.8837    0.8306        86
          11     0.9315    0.9444    0.9379        72
          12     0.9000    0.5625    0.6923        64

    accuracy                         0.7413      3379
   macro avg     0.7856    0.6969    0.7260      3379
weighted avg     0.7402    0.7413    0.7308      3379

generating embeddings for train...
embedding path:  ../embeddings/run_64/train_embedding.npy
label path:  ../embeddings/run_64/train_label.npy
shape of generated embedding: (15715, 128)
shape of label: (15715,)
generating embeddings for val...
embedding path:  ../embeddings/run_64/val_embedding.npy
label path:  ../embeddings/run_64/val_label.npy
shape of generated embedding: (3363, 128)
shape of label: (3363,)
generating embeddings for test...
embedding path:  ../embeddings/run_64/test_embedding.npy
label path:  ../embeddings/run_64/test_label.npy
shape of generated embedding: (3379, 128)
shape of label: (3379,)
