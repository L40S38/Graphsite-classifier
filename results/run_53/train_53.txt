seed:  666
using pretrained model: ../trained_models/trained_model_49.pt
number of classes: 14
how to merge clusters:  [[0, 9, 12], [1, 5, 11], 2, [3, 8, 13], 4, 6, 7, 10]
whether to further subcluster data according to chemical reaction: False
number of epochs to train: 60
learning rate decay to half at epoch 30.
begin to select hard pairs at epoch 1
batch size: 128
number of hardest positive pairs for each mini-batch:  192
number of hardest negative pairs for each mini-batch:  256
number of workers to load data:  36
similar margin of contrastive loss: 0.0
dissimilar margin of contrastive loss: 2.0
device:  cuda
number of classes after merging:  8
number of pockets in training set:  12475
number of pockets in validation set:  2670
number of pockets in test set:  2681
model architecture:
SelectiveSiameseNet(
  (embedding_net): ResidualEmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=48, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=48, out_features=48, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (rb_2): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_3): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_4): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_5): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_6): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_7): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_8): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (bn_8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(48, 96)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
SelectiveContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, num_pos_pair=192, num_neg_pair=256)
epoch: 1, train loss: 1.7073711639651494, train acc: 0.8098421391752577, validation acc: 0.737890625.
epoch: 2, train loss: 1.4424270326504827, train acc: 0.8381926546391752, validation acc: 0.76796875.
epoch: 3, train loss: 1.3789048595175653, train acc: 0.8419780927835051, validation acc: 0.769921875.
epoch: 4, train loss: 1.3402299164672313, train acc: 0.8530927835051546, validation acc: 0.779296875.
epoch: 5, train loss: 1.3181695292027604, train acc: 0.8580057989690721, validation acc: 0.7859375.
epoch: 6, train loss: 1.3035679018023438, train acc: 0.8501932989690721, validation acc: 0.776953125.
epoch: 7, train loss: 1.2920132961469828, train acc: 0.8505960051546392, validation acc: 0.77578125.
epoch: 8, train loss: 1.2856125206589173, train acc: 0.8617912371134021, validation acc: 0.7734375.
epoch: 9, train loss: 1.2804343640716507, train acc: 0.8604220360824743, validation acc: 0.785546875.
epoch: 10, train loss: 1.2770314518587227, train acc: 0.8652545103092784, validation acc: 0.79140625.
epoch: 11, train loss: 1.2736722176485806, train acc: 0.8574420103092784, validation acc: 0.790625.
epoch: 12, train loss: 1.2679623841185985, train acc: 0.8656572164948454, validation acc: 0.784375.
epoch: 13, train loss: 1.265962043110506, train acc: 0.8708923969072165, validation acc: 0.789453125.
epoch: 14, train loss: 1.2647681229244452, train acc: 0.8690399484536082, validation acc: 0.79375.
epoch: 15, train loss: 1.2628829363343994, train acc: 0.8679123711340206, validation acc: 0.788671875.
epoch: 16, train loss: 1.261036938876572, train acc: 0.8614690721649485, validation acc: 0.791015625.
epoch: 17, train loss: 1.2581742164488219, train acc: 0.8693621134020618, validation acc: 0.7921875.
epoch: 18, train loss: 1.2579800236383138, train acc: 0.8749194587628866, validation acc: 0.790625.
epoch: 19, train loss: 1.2516708626838426, train acc: 0.8685567010309279, validation acc: 0.793359375.
epoch: 20, train loss: 1.253868837595337, train acc: 0.8645296391752577, validation acc: 0.792578125.
epoch: 21, train loss: 1.247559095166393, train acc: 0.8733086340206185, validation acc: 0.794921875.
epoch: 22, train loss: 1.2504078804656578, train acc: 0.857764175257732, validation acc: 0.78984375.
epoch: 23, train loss: 1.253688263085524, train acc: 0.8696842783505154, validation acc: 0.791796875.
epoch: 24, train loss: 1.2485725869135231, train acc: 0.858972293814433, validation acc: 0.7875.
epoch: 25, train loss: 1.2476358006383252, train acc: 0.8682345360824743, validation acc: 0.79453125.
epoch: 26, train loss: 1.2590476332427125, train acc: 0.8547841494845361, validation acc: 0.775.
epoch: 27, train loss: 1.2498125320681768, train acc: 0.8569587628865979, validation acc: 0.78671875.
epoch: 28, train loss: 1.2483182311584742, train acc: 0.8646907216494846, validation acc: 0.776953125.
epoch: 29, train loss: 1.2472569700193334, train acc: 0.8546230670103093, validation acc: 0.782421875.
epoch: 30, train loss: 1.2265167643641162, train acc: 0.8849871134020618, validation acc: 0.805859375.
epoch: 31, train loss: 1.2141233705456023, train acc: 0.8869201030927835, validation acc: 0.792578125.
epoch: 32, train loss: 1.2103976894373043, train acc: 0.8874033505154639, validation acc: 0.8015625.
epoch: 33, train loss: 1.2109635287075577, train acc: 0.8887725515463918, validation acc: 0.8078125.
epoch: 34, train loss: 1.207498648731979, train acc: 0.8860341494845361, validation acc: 0.80703125.
epoch: 35, train loss: 1.2074287664556713, train acc: 0.8916720360824743, validation acc: 0.803125.
epoch: 36, train loss: 1.2051577476759898, train acc: 0.8813627577319587, validation acc: 0.80390625.
epoch: 37, train loss: 1.2094735944745116, train acc: 0.8876449742268041, validation acc: 0.8046875.
epoch: 38, train loss: 1.2038625301422883, train acc: 0.8992429123711341, validation acc: 0.816796875.
epoch: 39, train loss: 1.2024110968228818, train acc: 0.8913498711340206, validation acc: 0.810546875.
epoch: 40, train loss: 1.2058226075544625, train acc: 0.8870811855670103, validation acc: 0.79140625.
epoch: 41, train loss: 1.1973950409924334, train acc: 0.8953769329896907, validation acc: 0.8078125.
epoch: 42, train loss: 1.2003425884668184, train acc: 0.8919136597938144, validation acc: 0.801953125.
epoch: 43, train loss: 1.1971938114980887, train acc: 0.8952963917525774, validation acc: 0.814453125.
epoch: 44, train loss: 1.203565864394445, train acc: 0.8973099226804123, validation acc: 0.80546875.
epoch: 45, train loss: 1.1942168756858589, train acc: 0.8919942010309279, validation acc: 0.796484375.
epoch: 46, train loss: 1.1998007595978009, train acc: 0.8954574742268041, validation acc: 0.814453125.
epoch: 47, train loss: 1.1948925686865737, train acc: 0.8945715206185567, validation acc: 0.8140625.
epoch: 48, train loss: 1.1969810367859517, train acc: 0.8925579896907216, validation acc: 0.809375.
epoch: 49, train loss: 1.1959596501885528, train acc: 0.8985985824742269, validation acc: 0.816015625.
epoch: 50, train loss: 1.1896542695204182, train acc: 0.898034793814433, validation acc: 0.81171875.
epoch: 51, train loss: 1.193045329626249, train acc: 0.8998872422680413, validation acc: 0.813671875.
epoch: 52, train loss: 1.1877198830268871, train acc: 0.9048807989690721, validation acc: 0.821875.
epoch: 53, train loss: 1.1917374938155836, train acc: 0.8920747422680413, validation acc: 0.811328125.
epoch: 54, train loss: 1.1928325324416686, train acc: 0.8960212628865979, validation acc: 0.814453125.
epoch: 55, train loss: 1.1872981707723222, train acc: 0.9039948453608248, validation acc: 0.808203125.
epoch: 56, train loss: 1.1900181672007768, train acc: 0.9029478092783505, validation acc: 0.8109375.
epoch: 57, train loss: 1.1856394186286758, train acc: 0.9023034793814433, validation acc: 0.803125.
epoch: 58, train loss: 1.18770595292104, train acc: 0.9021423969072165, validation acc: 0.812109375.
epoch: 59, train loss: 1.1832189693366657, train acc: 0.9023034793814433, validation acc: 0.809765625.
epoch: 60, train loss: 1.183420898285811, train acc: 0.8940077319587629, validation acc: 0.805859375.
best validation acc 0.821875 at epoch 52.
