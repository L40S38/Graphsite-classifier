seed:  16
save trained model at:  ../trained_models/trained_classifier_model_56.pt
save loss at:  ./results/train_classifier_results_56.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['5hr5A02', '5n2sA00', '1cjaB00', '4fxsA00', '3ngtE00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['4bnfF02', '1ztfA00', '2zroA00', '4at8C00', '3kycB00']
model architecture:
DeepDruG(
  (embedding_net): JKMCNMMEmbeddingNet(
    (conv0): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=33, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b4745ed1880>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0168347194997236, acc: 0.39505149757310287; test loss: 1.815380837443421, acc: 0.4341763176554006
epoch: 2, train loss: 1.7265411808855706, acc: 0.46543151414703443; test loss: 1.6007493314267445, acc: 0.4989364216497282
epoch: 3, train loss: 1.605278874637531, acc: 0.508168580561146; test loss: 1.511092863249683, acc: 0.525171354289766
epoch: 4, train loss: 1.5170135012713921, acc: 0.5355747602699183; test loss: 1.4892529989353902, acc: 0.5284802647128338
epoch: 5, train loss: 1.469899401081036, acc: 0.5532733514857345; test loss: 1.5173773488883457, acc: 0.5322618766249114
epoch: 6, train loss: 1.4022229656998724, acc: 0.5754113886586953; test loss: 1.892226607431562, acc: 0.506972346962893
epoch: 7, train loss: 1.3434128537324246, acc: 0.5893216526577483; test loss: 1.625111578993413, acc: 0.4963365634601749
epoch: 8, train loss: 1.3195942603047253, acc: 0.5974902332188943; test loss: 1.6273505439771825, acc: 0.5173717797211062
epoch: 9, train loss: 1.2673643907315777, acc: 0.6129395051497573; test loss: 1.5636312079694474, acc: 0.5098085558969511
epoch: 10, train loss: 1.2572216716224207, acc: 0.6247780277021427; test loss: 1.3849921227628472, acc: 0.5672417868116284
epoch: 11, train loss: 1.1910729135043863, acc: 0.6452586717177696; test loss: 1.319883955142031, acc: 0.5930040179626566
epoch: 12, train loss: 1.1494077188214762, acc: 0.6559133420149166; test loss: 1.2629525910537311, acc: 0.6190025998581895
epoch: 13, train loss: 1.1234579999107883, acc: 0.6657985083461584; test loss: 1.4212373413675587, acc: 0.5795320255258805
epoch: 14, train loss: 1.106836314510139, acc: 0.665324967444063; test loss: 1.3256563558783607, acc: 0.6069487118884425
epoch: 15, train loss: 1.073357194023498, acc: 0.6804190836983545; test loss: 1.6993762504629029, acc: 0.5594422122429685
epoch: 16, train loss: 1.0446006672134256, acc: 0.6878773529063573; test loss: 1.2541501059990858, acc: 0.6019853462538407
epoch: 17, train loss: 1.0206682268255376, acc: 0.6944477329229313; test loss: 1.1442551198114996, acc: 0.6447648310092177
epoch: 18, train loss: 1.0048896025747636, acc: 0.6997750680715047; test loss: 1.6008893227470145, acc: 0.4826282202788939
epoch: 19, train loss: 0.993118248480346, acc: 0.7041553214158873; test loss: 1.1148745615807472, acc: 0.6535098085558969
epoch: 20, train loss: 0.9820398472252154, acc: 0.7038001657393157; test loss: 1.1962823009017622, acc: 0.6284566296383833
epoch: 21, train loss: 0.9494337734631904, acc: 0.7144548360364626; test loss: 1.2046097975424124, acc: 0.6364925549515481
epoch: 22, train loss: 0.9332807159669387, acc: 0.7192494376701788; test loss: 1.2097684350370994, acc: 0.6454738832427322
epoch: 23, train loss: 0.942942560728138, acc: 0.718183970640464; test loss: 1.6210875595695047, acc: 0.6166390924131411
epoch: 24, train loss: 0.9199293582301401, acc: 0.7268852847164674; test loss: 1.1207355252114686, acc: 0.6568187189789648
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.7096267287316542, acc: 0.7339883982478986; test loss: 0.7739244476048779, acc: 0.6993618529898369
epoch: 26, train loss: 0.7063873704825828, acc: 0.73126553806085; test loss: 0.7405461240564218, acc: 0.7170881588277003
epoch: 27, train loss: 0.681985083756199, acc: 0.7389605777199005; test loss: 0.9159481528802167, acc: 0.6613093831245569
epoch: 28, train loss: 0.6738427215050392, acc: 0.7430448680004735; test loss: 0.771293339845391, acc: 0.7097612857480501
epoch: 29, train loss: 0.6631978131037739, acc: 0.7473067361193323; test loss: 0.8415544300151474, acc: 0.6773812337508863
epoch: 30, train loss: 0.6561131974120132, acc: 0.745116609447141; test loss: 0.7706308096076715, acc: 0.6988891515008272
epoch: 31, train loss: 0.6433790538231127, acc: 0.7512726411743814; test loss: 0.72717249030003, acc: 0.7123611439376034
epoch: 32, train loss: 0.6672167382156184, acc: 0.7478394696341897; test loss: 1.1968162253629342, acc: 0.5833136374379579
epoch: 33, train loss: 0.6541182528773299, acc: 0.7471291582810465; test loss: 0.7047295387798211, acc: 0.7291420467974474
epoch: 34, train loss: 0.63660584166302, acc: 0.7583165620930508; test loss: 0.9022107873789048, acc: 0.6655636965256441
epoch: 35, train loss: 0.6430162694505845, acc: 0.7529892269444773; test loss: 1.0091811945691398, acc: 0.6480737414322855
epoch: 36, train loss: 0.6120249695279435, acc: 0.7623416597608619; test loss: 0.7703040166826458, acc: 0.7128338454266131
epoch: 37, train loss: 0.59434365270772, acc: 0.7689712323901977; test loss: 0.7976886786571549, acc: 0.6995982037343418
epoch: 38, train loss: 0.5908615490776894, acc: 0.7724044039303894; test loss: 0.9539860837765547, acc: 0.6440557787757032
epoch: 39, train loss: 0.5829789003239759, acc: 0.7751864567302; test loss: 0.7413592070334066, acc: 0.7050342708579532
epoch: 40, train loss: 0.5867105985204327, acc: 0.7721676334793418; test loss: 0.7655685532884772, acc: 0.726778539352399
epoch: 41, train loss: 0.5686409933843373, acc: 0.7779685095300106; test loss: 0.7048512905434609, acc: 0.7315055542424959
epoch: 42, train loss: 0.5576788410288605, acc: 0.7829998816147745; test loss: 0.7890128495422162, acc: 0.7154337036161664
epoch: 43, train loss: 0.5343641397994524, acc: 0.7899846099206819; test loss: 1.1156131274535057, acc: 0.6149846372016072
epoch: 44, train loss: 0.5548238840994503, acc: 0.7843613117082988; test loss: 0.642037496361657, acc: 0.7494682108248641
epoch: 45, train loss: 0.5298913441262358, acc: 0.7911092695631585; test loss: 0.7180215023111097, acc: 0.7473410541243205
epoch: 46, train loss: 0.5337499773359813, acc: 0.7915828104652539; test loss: 1.0366695105282868, acc: 0.6194753013471992
epoch: 47, train loss: 0.5266142053562917, acc: 0.7943648632650645; test loss: 0.7308468157674257, acc: 0.7265421886078941
epoch: 48, train loss: 0.5171728883958702, acc: 0.8001065467029714; test loss: 0.7570282282544152, acc: 0.711415740959584
epoch: 49, train loss: 0.529002134646412, acc: 0.7934177814608737; test loss: 0.7438649544054328, acc: 0.7140155991491374
epoch: 50, train loss: 0.5172624664855904, acc: 0.7964366047117319; test loss: 0.6804728025636108, acc: 0.74048688253368
epoch: 51, train loss: 0.4936687416461314, acc: 0.8068545045578311; test loss: 0.9690547283026547, acc: 0.6511463011108485
epoch: 52, train loss: 0.4930390048001636, acc: 0.8048419557239257; test loss: 0.7221744081929513, acc: 0.7255967856298747
epoch: 53, train loss: 0.4859532688240391, acc: 0.8078607789747839; test loss: 0.9031898397780626, acc: 0.6984164500118175
epoch: 54, train loss: 0.478378797037525, acc: 0.8129513436723097; test loss: 0.9649143641622205, acc: 0.6740723233278185
epoch: 55, train loss: 0.4870313297041185, acc: 0.8068545045578311; test loss: 0.8772911851047145, acc: 0.6828173008744978
Epoch    55: reducing learning rate of group 0 to 1.5000e-03.
epoch: 56, train loss: 0.39959606136657, acc: 0.8384633597727004; test loss: 0.5767603087892941, acc: 0.7858662254786103
epoch: 57, train loss: 0.3637766392714768, acc: 0.8525512016100391; test loss: 0.6184328313528632, acc: 0.7697943748522807
epoch: 58, train loss: 0.34778152749681523, acc: 0.8600686634308038; test loss: 0.6136775058202218, acc: 0.7804301583549988
epoch: 59, train loss: 0.32871807986241736, acc: 0.8638569906475672; test loss: 0.5913279553277925, acc: 0.7870479792011345
epoch: 60, train loss: 0.32390631758081456, acc: 0.8649816502900438; test loss: 0.626705601209164, acc: 0.7721578822973293
epoch: 61, train loss: 0.3238662169666083, acc: 0.8665798508346159; test loss: 0.6096026704993774, acc: 0.7915386433467265
epoch: 62, train loss: 0.3325191595236253, acc: 0.8642121463241388; test loss: 0.7167046044177688, acc: 0.7504136138028835
epoch: 63, train loss: 0.3281291105542025, acc: 0.8642121463241388; test loss: 0.6199776450952608, acc: 0.7783030016544552
epoch: 64, train loss: 0.3186974975361942, acc: 0.8695394814727122; test loss: 0.6249901010082338, acc: 0.7757031434649019
epoch: 65, train loss: 0.3020716311452628, acc: 0.875399550136143; test loss: 0.6317627540972464, acc: 0.7768848971874261
epoch: 66, train loss: 0.3153488655915638, acc: 0.870486563276903; test loss: 0.6720602930479651, acc: 0.7669581659182226
epoch: 67, train loss: 0.32313280716596643, acc: 0.8642713389369007; test loss: 0.6116471881422414, acc: 0.7773575986764358
epoch: 68, train loss: 0.28406499563497495, acc: 0.8807860778974784; test loss: 0.7162263661066541, acc: 0.7749940912313874
epoch: 69, train loss: 0.29234553828555, acc: 0.8758730910382384; test loss: 0.7718573397875564, acc: 0.7489955093358545
epoch: 70, train loss: 0.28805348401988656, acc: 0.8795430330294779; test loss: 0.6594267518659541, acc: 0.793902150791775
epoch: 71, train loss: 0.2701585376856806, acc: 0.8847519829525275; test loss: 0.6395571138863867, acc: 0.784684471756086
epoch: 72, train loss: 0.25792430518709925, acc: 0.8912631703563395; test loss: 0.6498019497492048, acc: 0.7955566060033089
epoch: 73, train loss: 0.24935477014489466, acc: 0.8960577719900557; test loss: 0.6365704727184122, acc: 0.7844481210115812
epoch: 74, train loss: 0.3149211755148318, acc: 0.8674677400260448; test loss: 0.6811482231263657, acc: 0.7565587331600094
epoch: 75, train loss: 0.2720904836107658, acc: 0.8816147744761453; test loss: 0.7012534799995053, acc: 0.7667218151737178
epoch: 76, train loss: 0.2623742033994394, acc: 0.8882443471054813; test loss: 0.6737008102958407, acc: 0.7830300165445521
epoch: 77, train loss: 0.2669940245205148, acc: 0.8846927903397656; test loss: 0.7057109392159826, acc: 0.7697943748522807
epoch: 78, train loss: 0.2424199708547905, acc: 0.894933112347579; test loss: 0.7494528855423194, acc: 0.7794847553769795
epoch: 79, train loss: 0.2453166044850709, acc: 0.894163608381674; test loss: 0.7821846409797443, acc: 0.7553769794374853
epoch: 80, train loss: 0.23480223795782537, acc: 0.8969456611814846; test loss: 0.6922425640809905, acc: 0.7749940912313874
epoch: 81, train loss: 0.26994558404264496, acc: 0.8864093761098615; test loss: 0.6806837984417325, acc: 0.7901205388796975
epoch: 82, train loss: 0.24725500938348183, acc: 0.8939860305433882; test loss: 0.6389613187614555, acc: 0.7922476955802411
epoch: 83, train loss: 0.22722270211567183, acc: 0.9011483366875814; test loss: 0.6732054449542142, acc: 0.783266367289057
epoch: 84, train loss: 0.23874565012725665, acc: 0.8980111282111992; test loss: 0.6946919041022542, acc: 0.7669581659182226
epoch: 85, train loss: 0.23774401554239138, acc: 0.8962945424411034; test loss: 0.8030550623582459, acc: 0.7575041361380288
epoch: 86, train loss: 0.217017871590215, acc: 0.9050550491298686; test loss: 0.6414905782288002, acc: 0.7924840463247459
epoch: 87, train loss: 0.3492083292861542, acc: 0.8571090327927074; test loss: 0.6761198059037853, acc: 0.7813755613330182
epoch: 88, train loss: 0.24938917442110145, acc: 0.8943411862199597; test loss: 0.66488876764838, acc: 0.7797211061214843
epoch: 89, train loss: 0.20163181215979806, acc: 0.9134604001420623; test loss: 0.7453112321608851, acc: 0.7757031434649019
epoch: 90, train loss: 0.22198691245111557, acc: 0.9051142417426306; test loss: 0.8376762319811408, acc: 0.7456865989127865
epoch: 91, train loss: 0.2342255010475869, acc: 0.8980111282111992; test loss: 0.65601734457442, acc: 0.780193807610494
epoch: 92, train loss: 0.20931248113158413, acc: 0.9105599621167279; test loss: 0.6509052796218446, acc: 0.7965020089813283
epoch: 93, train loss: 0.19765295307389572, acc: 0.9155321415887298; test loss: 0.6858562984468807, acc: 0.7913022926022217
epoch: 94, train loss: 0.19179450217345573, acc: 0.9148810228483485; test loss: 0.649887511505356, acc: 0.796974710470338
epoch: 95, train loss: 0.1939927467167187, acc: 0.9160056824908251; test loss: 0.7197216807795709, acc: 0.7858662254786103
epoch: 96, train loss: 0.19437992540854623, acc: 0.912809281401681; test loss: 0.721794605536879, acc: 0.7856298747341054
epoch: 97, train loss: 0.19947601409018836, acc: 0.9145850597845389; test loss: 0.7694616868010764, acc: 0.7754667927203971
epoch: 98, train loss: 0.19445312928310918, acc: 0.9147626376228247; test loss: 0.688437008992968, acc: 0.7934294493027653
epoch: 99, train loss: 0.17328986186303513, acc: 0.9237007221498756; test loss: 0.7388807266098307, acc: 0.7785393523989601
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.13453928657194503, acc: 0.9200307801586362; test loss: 0.5780841078044844, acc: 0.7995745686598913
epoch: 101, train loss: 0.11416060046266491, acc: 0.9303302947792116; test loss: 0.6465593300032519, acc: 0.7865752777121248
epoch: 102, train loss: 0.11318441129258586, acc: 0.9296199834260684; test loss: 0.7560267265849969, acc: 0.7461593004017962
epoch: 103, train loss: 0.12520336931035747, acc: 0.9233455664733041; test loss: 0.6485163794992889, acc: 0.7754667927203971
epoch: 104, train loss: 0.1269723812526777, acc: 0.9206818988990174; test loss: 0.5737942354923105, acc: 0.7792484046324746
epoch: 105, train loss: 0.12213356758615485, acc: 0.9238782999881615; test loss: 0.6247276904255581, acc: 0.7804301583549988
epoch: 106, train loss: 0.12239679846239186, acc: 0.9254173079199716; test loss: 0.6061963062707878, acc: 0.7870479792011345
epoch: 107, train loss: 0.11234992763001748, acc: 0.9306262578430212; test loss: 0.5818708439409747, acc: 0.7922476955802411
epoch: 108, train loss: 0.13081658387679496, acc: 0.9181366165502546; test loss: 0.5809389182338476, acc: 0.787757031434649
epoch: 109, train loss: 0.12595028727205884, acc: 0.9206818988990174; test loss: 0.6538492288186009, acc: 0.774048688253368
epoch: 110, train loss: 0.1103316097349066, acc: 0.9313365691961644; test loss: 0.5431314752164447, acc: 0.8054833372725124
epoch: 111, train loss: 0.11943922714113385, acc: 0.926719545400734; test loss: 0.5697598738524965, acc: 0.7870479792011345
epoch: 112, train loss: 0.13786470688411126, acc: 0.9138747484313957; test loss: 0.6306088055259256, acc: 0.751122666036398
epoch: 113, train loss: 0.12449241578543667, acc: 0.9212738250266367; test loss: 0.5937140617097869, acc: 0.7941385015362799
epoch: 114, train loss: 0.1326885110214291, acc: 0.9173079199715876; test loss: 0.6598921593298898, acc: 0.7690853226187663
epoch: 115, train loss: 0.11891085371539144, acc: 0.9231679886350184; test loss: 0.6742017199260014, acc: 0.7676672181517372
epoch: 116, train loss: 0.13934747962146818, acc: 0.9125133183378714; test loss: 0.5652021312510762, acc: 0.7950839045142992
epoch: 117, train loss: 0.12326322123540623, acc: 0.9217473659287321; test loss: 0.6056578882308019, acc: 0.7811392105885133
epoch: 118, train loss: 0.1335149885504658, acc: 0.9190245057416835; test loss: 0.5493790186541888, acc: 0.7943748522807846
epoch: 119, train loss: 0.11679608052978152, acc: 0.925950041434829; test loss: 0.6315908529774795, acc: 0.7811392105885133
epoch: 120, train loss: 0.13248735081130575, acc: 0.9188469279033976; test loss: 0.5491540755427664, acc: 0.7957929567478138
epoch: 121, train loss: 0.12882518338404902, acc: 0.9172487273588256; test loss: 0.6238824601602792, acc: 0.7754667927203971
Epoch   121: reducing learning rate of group 0 to 7.5000e-04.
epoch: 122, train loss: 0.08784600922634322, acc: 0.9404522315615012; test loss: 0.6248719115202039, acc: 0.7920113448357362
epoch: 123, train loss: 0.05867670606625522, acc: 0.9571445483603647; test loss: 0.6098020805925998, acc: 0.8043015835499882
epoch: 124, train loss: 0.05161997875589244, acc: 0.9619983426068427; test loss: 0.6365361423307603, acc: 0.8076104939730561
epoch: 125, train loss: 0.04118922206196007, acc: 0.9689238782999882; test loss: 0.6572984387423236, acc: 0.8118648073741432
epoch: 126, train loss: 0.04045489937114662, acc: 0.9696933822658932; test loss: 0.653397759656426, acc: 0.8069014417395415
epoch: 127, train loss: 0.04777380491428168, acc: 0.9648987806321772; test loss: 0.646043578999681, acc: 0.7998109194043961
epoch: 128, train loss: 0.04653266124489943, acc: 0.9665561737895111; test loss: 0.6790500725451104, acc: 0.7986291656818719
epoch: 129, train loss: 0.044171942244477354, acc: 0.9670297146916065; test loss: 0.6748061418758727, acc: 0.8035925313164737
epoch: 130, train loss: 0.06810183629148371, acc: 0.956019888717888; test loss: 0.671083441244967, acc: 0.7837390687780666
epoch: 131, train loss: 0.05597632613844091, acc: 0.9592162898070321; test loss: 0.6553813030296566, acc: 0.7988655164263767
epoch: 132, train loss: 0.04409673847646188, acc: 0.9668521368533207; test loss: 0.6538066089477305, acc: 0.8087922476955802
epoch: 133, train loss: 0.042220117052155046, acc: 0.96744406298094; test loss: 0.6565774287266868, acc: 0.8043015835499882
epoch: 134, train loss: 0.038717256251881785, acc: 0.9718835089380845; test loss: 0.715058095290063, acc: 0.7905932403687072
epoch: 135, train loss: 0.05486332660785043, acc: 0.9612288386409377; test loss: 0.7094912910202213, acc: 0.7891751359016781
epoch: 136, train loss: 0.05174779976687587, acc: 0.9642476618917959; test loss: 0.6953258367893235, acc: 0.7889387851571732
epoch: 137, train loss: 0.04271968604025014, acc: 0.9714099680359891; test loss: 0.7125669484779756, acc: 0.7839754195225715
epoch: 138, train loss: 0.049598466305495914, acc: 0.9665561737895111; test loss: 0.6747406186067982, acc: 0.800047270148901
epoch: 139, train loss: 0.057386577115912926, acc: 0.9624718835089381; test loss: 0.7195574416812219, acc: 0.7820846135665327
epoch: 140, train loss: 0.05636720267946203, acc: 0.9624126908961762; test loss: 0.7243283766433438, acc: 0.7780666509099504
epoch: 141, train loss: 0.0596991779633961, acc: 0.9569077779093169; test loss: 0.7000716947207521, acc: 0.7785393523989601
epoch: 142, train loss: 0.06650222829192774, acc: 0.9561974665561738; test loss: 0.6606417010507243, acc: 0.784684471756086
epoch: 143, train loss: 0.07352899072899739, acc: 0.9502190126672191; test loss: 0.7220263611873085, acc: 0.7792484046324746
epoch: 144, train loss: 0.08415546704922058, acc: 0.9463714928376938; test loss: 0.6573845570996365, acc: 0.7764121956984165
epoch: 145, train loss: 0.06409988022403094, acc: 0.956019888717888; test loss: 0.6258596355147014, acc: 0.8026471283384543
epoch: 146, train loss: 0.04739291441457165, acc: 0.9638925062152244; test loss: 0.6735774046758816, acc: 0.8012290238714252
epoch: 147, train loss: 0.055030403715859676, acc: 0.9617615721557949; test loss: 0.6633644943624996, acc: 0.7929567478137556
epoch: 148, train loss: 0.053821975135289524, acc: 0.9607552977388422; test loss: 0.8782750296835019, acc: 0.7407232332781848
epoch: 149, train loss: 0.059875617349085704, acc: 0.959334675032556; test loss: 0.6732429328524906, acc: 0.787757031434649
epoch: 150, train loss: 0.037019060162788345, acc: 0.9728305907422754; test loss: 0.6756883706361287, acc: 0.8028834790829591
epoch: 151, train loss: 0.0341493913973734, acc: 0.9748431395761809; test loss: 0.6823555017244559, acc: 0.8078468447175609
epoch: 152, train loss: 0.0540055899990089, acc: 0.9652539363087487; test loss: 0.6764582530231066, acc: 0.7884660836681635
epoch: 153, train loss: 0.06449614803277982, acc: 0.9554871552030306; test loss: 0.6624308840317299, acc: 0.7903568896242024
epoch: 154, train loss: 0.05856478512789902, acc: 0.959275482419794; test loss: 0.619649873307286, acc: 0.8083195462065705
epoch: 155, train loss: 0.042220020231211894, acc: 0.9701669231679886; test loss: 0.6766613115517137, acc: 0.7865752777121248
epoch: 156, train loss: 0.044085043721404656, acc: 0.9685687226234166; test loss: 0.6890641709812085, acc: 0.8026471283384543
epoch: 157, train loss: 0.04651857078220063, acc: 0.9672072925298922; test loss: 0.7153685000793107, acc: 0.7780666509099504
epoch: 158, train loss: 0.055690349301170484, acc: 0.9627086539599858; test loss: 0.6442128007391107, acc: 0.7974474119593477
epoch: 159, train loss: 0.058897016611207345, acc: 0.9598082159346514; test loss: 0.6869168780819797, acc: 0.7856298747341054
epoch: 160, train loss: 0.06050449489556651, acc: 0.9585651710666508; test loss: 0.6225418379278055, acc: 0.7948475537697943
epoch: 161, train loss: 0.053223555832854305, acc: 0.9630638096365574; test loss: 0.7302814684024631, acc: 0.7742850389978728
epoch: 162, train loss: 0.050208106050886346, acc: 0.9642476618917959; test loss: 0.7087308065853511, acc: 0.7924840463247459
epoch: 163, train loss: 0.040294003900133026, acc: 0.9709364271338937; test loss: 0.6798544152356073, acc: 0.8057196880170172
epoch: 164, train loss: 0.05381371637088978, acc: 0.964188469279034; test loss: 0.6757139413809274, acc: 0.7929567478137556
epoch: 165, train loss: 0.048992734664824636, acc: 0.9650171658577009; test loss: 0.719214490840013, acc: 0.7787757031434649
epoch: 166, train loss: 0.06796589685947751, acc: 0.9567302000710312; test loss: 0.6445028534230823, acc: 0.7927203970692508
epoch: 167, train loss: 0.05049578242010092, acc: 0.9660234402746537; test loss: 0.6699915427007564, acc: 0.790829591113212
epoch: 168, train loss: 0.05548615619492584, acc: 0.9617023795430331; test loss: 0.632521556247016, acc: 0.7884660836681635
epoch: 169, train loss: 0.09697382234639355, acc: 0.9392091866935006; test loss: 0.6245839650172822, acc: 0.7868116284566297
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.03952531410134007, acc: 0.9605185272877945; test loss: 0.5262393311557125, acc: 0.8083195462065705
epoch: 171, train loss: 0.02506625142803598, acc: 0.973067361193323; test loss: 0.5576939088913375, acc: 0.8102103521626093
epoch: 172, train loss: 0.021255435832982637, acc: 0.9754350657038001; test loss: 0.5565963905772202, acc: 0.8050106357835027
Epoch   172: reducing learning rate of group 0 to 3.7500e-04.
epoch: 173, train loss: 0.0161092324930801, acc: 0.9824197940097076; test loss: 0.5466455262030194, acc: 0.8099740014181045
epoch: 174, train loss: 0.01208660538623982, acc: 0.9871552030306617; test loss: 0.557888676668842, acc: 0.813755613330182
epoch: 175, train loss: 0.011734823873494804, acc: 0.9867408547413283; test loss: 0.5590716807787595, acc: 0.8090285984400851
epoch: 176, train loss: 0.010113538470277123, acc: 0.9898780632177104; test loss: 0.5679727883182274, acc: 0.8125738596076577
epoch: 177, train loss: 0.01317072660888587, acc: 0.987806321771043; test loss: 0.5679329585814921, acc: 0.8059560387615221
epoch: 178, train loss: 0.011473608866139472, acc: 0.9880430922220906; test loss: 0.5689621580827605, acc: 0.8102103521626093
epoch: 179, train loss: 0.010117078202297994, acc: 0.9895229075411389; test loss: 0.5764804475275345, acc: 0.803119829827464
epoch: 180, train loss: 0.009916968615550331, acc: 0.9900556410559962; test loss: 0.5899777441153259, acc: 0.8043015835499882
epoch: 181, train loss: 0.04074959529291896, acc: 0.968272759559607; test loss: 0.5794486600269073, acc: 0.7924840463247459
epoch: 182, train loss: 0.02259528363588591, acc: 0.9777435776015153; test loss: 0.5734606500326728, acc: 0.7962656582368235
epoch: 183, train loss: 0.015362659274150937, acc: 0.9844323428436131; test loss: 0.5712131798281452, acc: 0.804537934294493
epoch: 184, train loss: 0.010879393003248555, acc: 0.9886350183497099; test loss: 0.5771054914729642, acc: 0.8116284566296383
epoch: 185, train loss: 0.010110021231675502, acc: 0.9886350183497099; test loss: 0.5818458842036572, acc: 0.8080831954620658
epoch: 186, train loss: 0.009364981244073312, acc: 0.9911803006984728; test loss: 0.5913755121424924, acc: 0.8109194043961239
epoch: 187, train loss: 0.008509956504671097, acc: 0.9911211080857109; test loss: 0.5980761817825289, acc: 0.8109194043961239
epoch: 188, train loss: 0.009226643227711558, acc: 0.9914170711495205; test loss: 0.6020444952773868, acc: 0.8061923895060269
epoch: 189, train loss: 0.00869933310556022, acc: 0.9917722268260921; test loss: 0.6057079555689091, acc: 0.8095012999290948
epoch: 190, train loss: 0.007990108359359427, acc: 0.9926009234047591; test loss: 0.6180074843524112, acc: 0.804537934294493
epoch: 191, train loss: 0.00883547941601402, acc: 0.9912986859239967; test loss: 0.6199477019532018, acc: 0.8066650909950366
epoch: 192, train loss: 0.013360283127056997, acc: 0.9865632769030425; test loss: 0.5884886422187717, acc: 0.8087922476955802
epoch: 193, train loss: 0.012371401647724342, acc: 0.9865632769030425; test loss: 0.623483966017299, acc: 0.8005199716379107
epoch: 194, train loss: 0.020546678747071966, acc: 0.9814727122055168; test loss: 0.6087868833789812, acc: 0.7960293074923186
epoch: 195, train loss: 0.019228859838088145, acc: 0.9796969338226589; test loss: 0.6060196113654046, acc: 0.8026471283384543
epoch: 196, train loss: 0.02136181656994932, acc: 0.9799928968864686; test loss: 0.7043143685222997, acc: 0.7754667927203971
epoch: 197, train loss: 0.025078182959426567, acc: 0.9764413401207529; test loss: 0.5722863715017865, acc: 0.800047270148901
epoch: 198, train loss: 0.013452423207119603, acc: 0.987747129158281; test loss: 0.5940882945224146, acc: 0.8026471283384543
epoch: 199, train loss: 0.012588238403023814, acc: 0.9872143956434237; test loss: 0.5932162563109392, acc: 0.8061923895060269
epoch: 200, train loss: 0.01288083901096762, acc: 0.9871552030306617; test loss: 0.6466358475469861, acc: 0.7922476955802411
best test acc 0.813755613330182 at epoch 174.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9992    0.9998    0.9995      6100
           1     0.9978    0.9860    0.9919       926
           2     0.9917    0.9962    0.9940      2400
           3     0.9988    1.0000    0.9994       843
           4     0.9822    1.0000    0.9910       774
           5     0.9960    0.9980    0.9970      1512
           6     0.9939    0.9872    0.9906      1330
           7     0.9979    1.0000    0.9990       481
           8     1.0000    0.9978    0.9989       458
           9     0.9847    1.0000    0.9923       452
          10     1.0000    0.9958    0.9979       717
          11     0.9970    1.0000    0.9985       333
          12     0.9754    0.9264    0.9503       299
          13     0.9925    0.9814    0.9869       269

    accuracy                         0.9956     16894
   macro avg     0.9934    0.9906    0.9919     16894
weighted avg     0.9956    0.9956    0.9956     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8865    0.9010    0.8937      1525
           1     0.8618    0.8060    0.8330       232
           2     0.8833    0.7937    0.8361       601
           3     0.8619    0.7393    0.7959       211
           4     0.8550    0.8814    0.8680       194
           5     0.8837    0.8439    0.8633       378
           6     0.5033    0.6847    0.5802       333
           7     0.8596    0.8099    0.8340       121
           8     0.6493    0.7565    0.6988       115
           9     0.8692    0.8158    0.8416       114
          10     0.8333    0.7222    0.7738       180
          11     0.7857    0.6548    0.7143        84
          12     0.2209    0.2533    0.2360        75
          13     0.7903    0.7206    0.7538        68

    accuracy                         0.8138      4231
   macro avg     0.7674    0.7417    0.7516      4231
weighted avg     0.8263    0.8138    0.8178      4231

---------------------------------------
program finished.
