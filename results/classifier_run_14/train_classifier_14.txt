seed:  666
save trained model at:  ../trained_models/trained_classifier_model_14.pt
save loss at:  ./results/train_classifier_results_14.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  128
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): PNAEmbeddingNet(
    (convs): ModuleList(
      (0): PNAConv(11, 64, towers=4, edge_dim=1)
      (1): PNAConv(64, 64, towers=4, edge_dim=1)
      (2): PNAConv(64, 64, towers=4, edge_dim=1)
      (3): PNAConv(64, 64, towers=4, edge_dim=1)
    )
    (batch_norms): ModuleList(
      (0): BatchNorm(64)
      (1): BatchNorm(64)
      (2): BatchNorm(64)
      (3): BatchNorm(64)
    )
    (set2set): Set2Set(64, 128)
  )
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2add85ef85e0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.1792833642690073, acc: 0.3620325435341136, val loss: 2.2028548230735465, acc: 0.35895382973045104, test loss: 2.2285031690830137, acc: 0.3470307529162248
epoch: 2, train loss: 1.989989867136882, acc: 0.4005709391949757, val loss: 2.4165865856391573, acc: 0.27328529490258874, test loss: 2.4141153687510486, acc: 0.2722693531283139
epoch: 3, train loss: 1.922002019808696, acc: 0.41552954610334, val loss: 2.0910313144696056, acc: 0.3808380037363224, test loss: 2.07726304730833, acc: 0.39183457051961823
epoch: 4, train loss: 1.8562806960075133, acc: 0.4338566942620611, val loss: 1.9740368702712618, acc: 0.39178009073925807, test loss: 1.9623310957834803, acc: 0.41039236479321317
epoch: 5, train loss: 1.8085426250662493, acc: 0.443505566657151, val loss: 2.096342831253528, acc: 0.3677608753669602, test loss: 2.07720515078224, acc: 0.36770943796394484
epoch: 6, train loss: 1.7619803714983606, acc: 0.4536682843277191, val loss: 2.492135010371375, acc: 0.33600213504136645, test loss: 2.4724338436430107, acc: 0.3465005302226935
epoch: 7, train loss: 1.7207675075925761, acc: 0.47382243791036255, val loss: 1.8816039835194571, acc: 0.4448892447291166, test loss: 1.8869065277649768, acc: 0.4342523860021209
epoch: 8, train loss: 1.6813730131650768, acc: 0.4902083928061661, val loss: 1.6124711303924733, acc: 0.503069121964238, test loss: 1.605840217270755, acc: 0.513255567338282
epoch: 9, train loss: 1.6418080078544801, acc: 0.5036254638880959, val loss: 1.683969710456679, acc: 0.4782492660795303, test loss: 1.667813793093377, acc: 0.4782608695652174
epoch: 10, train loss: 1.608660604951724, acc: 0.5198972309449044, val loss: 2.0702695945182863, acc: 0.44542300507072324, test loss: 2.0656865849095465, acc: 0.4369034994697773
epoch: 11, train loss: 1.5756055608804382, acc: 0.5243505566657151, val loss: 1.8235604521876372, acc: 0.4544969308780358, test loss: 1.8418618482344982, acc: 0.45254506892895013
epoch: 12, train loss: 1.5725671209201382, acc: 0.5286896945475307, val loss: 1.8881826011346186, acc: 0.4206031491860155, test loss: 1.8616635179974748, acc: 0.4233828207847296
epoch: 13, train loss: 1.5234345293575922, acc: 0.5469026548672566, val loss: 1.5348014952438431, acc: 0.5361622631438484, test loss: 1.5201958080364564, acc: 0.5405620360551432
epoch: 14, train loss: 1.5073831550264372, acc: 0.5477019697402227, val loss: 1.576233900035576, acc: 0.5161462503336002, test loss: 1.5677714555159858, acc: 0.5214740190880169
epoch: 15, train loss: 1.4854006631762988, acc: 0.5536397373679703, val loss: 1.6407478645320634, acc: 0.5028022417934348, test loss: 1.6306256380718307, acc: 0.514846235418876
epoch: 16, train loss: 1.4667759892330012, acc: 0.5612903225806452, val loss: 1.598109156092167, acc: 0.5326928209234054, test loss: 1.604831232877956, acc: 0.5389713679745494
epoch: 17, train loss: 1.452570936423249, acc: 0.5664858692549244, val loss: 1.5843400241916963, acc: 0.5300240192153723, test loss: 1.5704848422478264, acc: 0.5304878048780488
epoch: 18, train loss: 1.42769052466426, acc: 0.572423636882672, val loss: 2.1462424729135727, acc: 0.37710168134507605, test loss: 2.149287228366856, acc: 0.3753976670201485
epoch: 19, train loss: 1.4142724793262902, acc: 0.5739651727091065, val loss: 1.4766259880615673, acc: 0.5527088337336535, test loss: 1.4528212704056773, acc: 0.5580593849416755
epoch: 20, train loss: 1.4138587274246477, acc: 0.5808735369683129, val loss: 1.5981113616708187, acc: 0.5012009607686149, test loss: 1.5939240091552411, acc: 0.5087486744432662
epoch: 21, train loss: 1.4014970877359092, acc: 0.581387382243791, val loss: 1.5129129721191301, acc: 0.5281558580197492, test loss: 1.4818797116567828, acc: 0.5418875927889714
epoch: 22, train loss: 1.3750668666786785, acc: 0.5874393377105338, val loss: 1.7289742321404324, acc: 0.5060048038430744, test loss: 1.7091023532608534, acc: 0.514846235418876
epoch: 23, train loss: 1.3742186990969867, acc: 0.591435912075364, val loss: 1.5323938647683284, acc: 0.526287696824126, test loss: 1.494548868399789, acc: 0.546659597030753
epoch: 24, train loss: 1.3482482663161952, acc: 0.5921210391093349, val loss: 1.9543998760002468, acc: 0.4985321590605818, test loss: 1.9499019003875182, acc: 0.5164369034994698
epoch 25, gamma increased to 1.
epoch: 25, train loss: 1.1164003816155954, acc: 0.595717956037682, val loss: 1.729340038778052, acc: 0.4174005871363758, test loss: 1.7015012228349866, acc: 0.4135737009544008
epoch: 26, train loss: 1.0990024246626366, acc: 0.6028546959748787, val loss: 1.7285015556950363, acc: 0.3325326928209234, test loss: 1.7011373616984136, acc: 0.3430540827147402
epoch: 27, train loss: 1.0839014802903337, acc: 0.6080502426491579, val loss: 1.2243054870735781, acc: 0.5663197224446224, test loss: 1.1866495407019542, acc: 0.5782078472958643
epoch: 28, train loss: 1.0851668644046701, acc: 0.6078789608906652, val loss: 1.4185188643799358, acc: 0.515879370162797, test loss: 1.3924705456322097, acc: 0.5270413573700954
epoch: 29, train loss: 1.0744563587046065, acc: 0.6126177562089637, val loss: 1.1448827625943527, acc: 0.5868694955964772, test loss: 1.150670756970079, acc: 0.5975609756097561
epoch: 30, train loss: 1.0518578006443282, acc: 0.6180987724807308, val loss: 1.4474503561309537, acc: 0.5236188951160928, test loss: 1.433605009020323, acc: 0.5204135737009544
epoch: 31, train loss: 1.041280100701572, acc: 0.6217527833285755, val loss: 1.4111890713946482, acc: 0.4993327995729917, test loss: 1.402653158911971, acc: 0.5045068928950159
epoch: 32, train loss: 1.0371691283313949, acc: 0.6218669711675706, val loss: 1.1394249295946564, acc: 0.5852682145716573, test loss: 1.1452189940536515, acc: 0.5898727465535525
epoch: 33, train loss: 1.0461613741800373, acc: 0.6214102198115901, val loss: 1.2856746120773572, acc: 0.5540432345876701, test loss: 1.287625385367352, acc: 0.5405620360551432
epoch: 34, train loss: 1.012622732112927, acc: 0.6295746502997431, val loss: 1.3098211786032423, acc: 0.5350947424606352, test loss: 1.3273668516731465, acc: 0.5235949098621421
epoch: 35, train loss: 1.0028394838077763, acc: 0.6355695118469883, val loss: 1.277516711448586, acc: 0.5732586068855084, test loss: 1.2655239074885023, acc: 0.577677624602333
epoch: 36, train loss: 0.9921448358494386, acc: 0.6374536111904082, val loss: 1.5707778571795425, acc: 0.5308246597277823, test loss: 1.5836735712255678, acc: 0.5291622481442205
epoch: 37, train loss: 1.0001464121620076, acc: 0.6372252355124179, val loss: 1.0813645990428715, acc: 0.6068855084067254, test loss: 1.0499291902887986, acc: 0.6224814422057264
epoch: 38, train loss: 0.9777819378254178, acc: 0.642306594347702, val loss: 1.4633360134941245, acc: 0.49426207632772884, test loss: 1.4288398536872258, acc: 0.5082184517497349
epoch: 39, train loss: 0.9744898508114369, acc: 0.6432200970596631, val loss: 1.4143928125568286, acc: 0.5788630904723779, test loss: 1.437639224314361, acc: 0.5715800636267232
epoch: 40, train loss: 0.959637244949673, acc: 0.6473308592634884, val loss: 1.186538722122514, acc: 0.588470776621297, test loss: 1.1623361070234475, acc: 0.5988865323435844
epoch: 41, train loss: 0.9633013379461521, acc: 0.6450471024835855, val loss: 1.4471122332181745, acc: 0.5070723245262877, test loss: 1.4058094034771398, acc: 0.5087486744432662
epoch: 42, train loss: 0.9526029236752953, acc: 0.6484156437339423, val loss: 1.186158173810268, acc: 0.55564451561249, test loss: 1.195032295214915, acc: 0.5487804878048781
epoch: 43, train loss: 0.945323090977986, acc: 0.6533828147302312, val loss: 1.243211064301779, acc: 0.544435548438751, test loss: 1.2435607197175982, acc: 0.5516967126193001
epoch: 44, train loss: 0.9561071311784342, acc: 0.6498429917213817, val loss: 1.1093827389639983, acc: 0.5946090205497732, test loss: 1.1042063832915094, acc: 0.5919936373276776
epoch: 45, train loss: 0.9332843027964273, acc: 0.6568084499000857, val loss: 1.2073332823083214, acc: 0.5788630904723779, test loss: 1.2204931088242272, acc: 0.5819194061505832
epoch: 46, train loss: 0.9150617186185329, acc: 0.6564658863831002, val loss: 1.393591829576841, acc: 0.5286896183613558, test loss: 1.4340503445864987, acc: 0.5212089077412513
epoch: 47, train loss: 0.9088292252803033, acc: 0.6606908364259206, val loss: 1.3070489453544036, acc: 0.5340272217774219, test loss: 1.3548403615536897, acc: 0.528897136797455
epoch: 48, train loss: 0.9070806758933976, acc: 0.6614330573793891, val loss: 1.5134553775044164, acc: 0.5257539364825193, test loss: 1.4889299429613359, acc: 0.5405620360551432
Epoch    48: reducing learning rate of group 0 to 1.5000e-03.
epoch: 49, train loss: 0.8336650901816758, acc: 0.6825578075934913, val loss: 0.9852203739335259, acc: 0.6319722444622364, test loss: 0.9746529504830612, acc: 0.6436903499469777
epoch: 50, train loss: 0.8049091871588563, acc: 0.6898087353696831, val loss: 1.0197298259203167, acc: 0.6375767280491059, test loss: 1.0113787562571552, acc: 0.6474019088016967
epoch: 51, train loss: 0.7943004479008065, acc: 0.6950613759634598, val loss: 1.0743762413469797, acc: 0.6399786495863358, test loss: 1.053477522929931, acc: 0.6405090137857901
epoch: 52, train loss: 0.7801299532481408, acc: 0.7025977733371396, val loss: 1.039239280599895, acc: 0.6191619962636776, test loss: 1.0252055540317442, acc: 0.6373276776246023
epoch: 53, train loss: 0.7651361096221789, acc: 0.7019697402226663, val loss: 1.1338124329292332, acc: 0.6311716039498265, test loss: 1.1457891699365188, acc: 0.6314952279957582
epoch: 54, train loss: 0.7656343158537614, acc: 0.7043676848415644, val loss: 0.9743906611088025, acc: 0.6511876167600748, test loss: 0.9693399830815143, acc: 0.6585365853658537
epoch: 55, train loss: 0.7545451688691612, acc: 0.7097345132743362, val loss: 1.1305625930543832, acc: 0.6138243928476115, test loss: 1.1463097140493221, acc: 0.6243372216330859
epoch: 56, train loss: 0.7537600698520074, acc: 0.7096203254353411, val loss: 0.9980176783638826, acc: 0.641046170269549, test loss: 1.0037669189914287, acc: 0.6230116648992576
epoch: 57, train loss: 0.7443219066006505, acc: 0.709106480159863, val loss: 1.1920614240644836, acc: 0.5970109420870029, test loss: 1.1716176710098445, acc: 0.5994167550371156
epoch: 58, train loss: 0.7592123596762849, acc: 0.7052811875535255, val loss: 1.4600273581673566, acc: 0.5214838537496664, test loss: 1.4497558792474288, acc: 0.5172322375397667
epoch: 59, train loss: 0.7329011433198729, acc: 0.7191550099914359, val loss: 1.4040657013042277, acc: 0.5489725113424072, test loss: 1.4238275900119688, acc: 0.5477200424178155
epoch: 60, train loss: 0.7287516035932899, acc: 0.7181273194404796, val loss: 1.1040327357456847, acc: 0.6199626367760875, test loss: 1.0959233984841017, acc: 0.6238069989395546
epoch: 61, train loss: 0.7127773445060245, acc: 0.7193262917499287, val loss: 1.2084038644782504, acc: 0.6050173472111022, test loss: 1.2235340515856779, acc: 0.6102863202545069
epoch: 62, train loss: 0.7187661712727069, acc: 0.7197830431059092, val loss: 1.0560246722870648, acc: 0.6357085668534828, test loss: 1.0290338891940556, acc: 0.6423647932131495
epoch: 63, train loss: 0.7175964227173283, acc: 0.7193262917499287, val loss: 1.0976742478348525, acc: 0.6202295169468909, test loss: 1.091989349712117, acc: 0.6317603393425238
epoch: 64, train loss: 0.7059617648354742, acc: 0.7234370539537539, val loss: 1.2743199774510772, acc: 0.5756605284227382, test loss: 1.2508067161887726, acc: 0.5742311770943797
epoch: 65, train loss: 0.702987039092606, acc: 0.7214387667713389, val loss: 1.0116087558716813, acc: 0.6450493728315986, test loss: 1.005868643237829, acc: 0.6431601272534464
epoch: 66, train loss: 0.6944895797377479, acc: 0.7288609763060234, val loss: 1.3878584423477502, acc: 0.5847344542300507, test loss: 1.3940546740150654, acc: 0.5853658536585366
epoch: 67, train loss: 0.6992362101248867, acc: 0.7245789323437054, val loss: 1.2837522781337456, acc: 0.5476381104883907, test loss: 1.2860490298599858, acc: 0.545068928950159
epoch: 68, train loss: 0.6893991129826451, acc: 0.7251498715386812, val loss: 1.0118020048197474, acc: 0.6599946623965839, test loss: 0.9938711530962526, acc: 0.6646341463414634
epoch: 69, train loss: 0.6871165041374949, acc: 0.7288609763060234, val loss: 1.119230368501955, acc: 0.6413130504403523, test loss: 1.1442350557474923, acc: 0.6394485683987274
epoch: 70, train loss: 0.6729948101845463, acc: 0.7340565229803026, val loss: 1.202618279583078, acc: 0.6135575126768081, test loss: 1.1925147498779054, acc: 0.6150583244962884
epoch: 71, train loss: 0.669646109766937, acc: 0.7334855837853269, val loss: 1.2454684897998762, acc: 0.6191619962636776, test loss: 1.243764042980724, acc: 0.6248674443266172
epoch: 72, train loss: 0.6648554222841723, acc: 0.7363402797602056, val loss: 0.9593503496504988, acc: 0.6586602615425674, test loss: 0.9590565005390413, acc: 0.6651643690349947
epoch: 73, train loss: 0.6591261242615506, acc: 0.7391949757350842, val loss: 1.0085465769975193, acc: 0.6365092073658927, test loss: 1.0054520697254896, acc: 0.6423647932131495
epoch: 74, train loss: 0.6413239174490821, acc: 0.7397659149300599, val loss: 1.0312515633692256, acc: 0.6517213771016813, test loss: 1.0506000640157171, acc: 0.6534994697773064
epoch: 75, train loss: 0.6504013543056822, acc: 0.7410219811590065, val loss: 0.9765949792979016, acc: 0.6506538564184681, test loss: 0.9898103250552084, acc: 0.6540296924708378
epoch: 76, train loss: 0.640152768004189, acc: 0.7437053953753925, val loss: 1.2331014089912677, acc: 0.6223645583133173, test loss: 1.2141259798695148, acc: 0.6288441145281018
epoch: 77, train loss: 0.6503800178778842, acc: 0.7406223237225236, val loss: 1.1033089788303014, acc: 0.622097678142514, test loss: 1.1031697188303553, acc: 0.634676564156946
epoch: 78, train loss: 0.6333286621810162, acc: 0.7486154724521838, val loss: 1.271028766989676, acc: 0.6087536696023486, test loss: 1.2880398276501976, acc: 0.6052492046659597
epoch: 79, train loss: 0.630787523998181, acc: 0.7437624892948901, val loss: 1.0898486901385835, acc: 0.6285028022417934, test loss: 1.0825796768243088, acc: 0.61983032873807
epoch: 80, train loss: 0.6247415494381139, acc: 0.7449614616043392, val loss: 1.4472425963867814, acc: 0.5689885241526554, test loss: 1.437063610313553, acc: 0.5813891834570519
epoch: 81, train loss: 0.6231101747362946, acc: 0.7454182129603197, val loss: 1.0693521285839707, acc: 0.641046170269549, test loss: 1.0616340544924123, acc: 0.6439554612937434
epoch: 82, train loss: 0.6331697822195647, acc: 0.7450756494433343, val loss: 1.6891375742882515, acc: 0.4328796370429677, test loss: 1.6918317058053527, acc: 0.43213149522799577
epoch: 83, train loss: 0.6394516155714993, acc: 0.7423922352269483, val loss: 1.4130249269046178, acc: 0.5473712303175874, test loss: 1.4173168266312568, acc: 0.5559384941675504
epoch: 84, train loss: 0.6059527508435915, acc: 0.7541535826434485, val loss: 1.0366289023561481, acc: 0.6607953029089939, test loss: 1.0432919897178576, acc: 0.6534994697773064
epoch: 85, train loss: 0.6308666066479554, acc: 0.7482729089351984, val loss: 1.106393681547309, acc: 0.6365092073658927, test loss: 1.1111588490729873, acc: 0.6288441145281018
epoch: 86, train loss: 0.6189541402048224, acc: 0.7507279474735941, val loss: 1.3096619167740198, acc: 0.5858019749132639, test loss: 1.2936853512979387, acc: 0.591728525980912
epoch: 87, train loss: 0.6236089105984499, acc: 0.7485012846131887, val loss: 1.220211031438829, acc: 0.6247664798505471, test loss: 1.2460365168995216, acc: 0.6261930010604454
epoch: 88, train loss: 0.6191165538675405, acc: 0.7490151298886668, val loss: 1.064693400056196, acc: 0.644515612489992, test loss: 1.0985999260322918, acc: 0.6299045599151644
epoch: 89, train loss: 0.6135461513736811, acc: 0.749129317727662, val loss: 1.3900524795548517, acc: 0.5729917267147051, test loss: 1.3700051216659466, acc: 0.5837751855779427
epoch: 90, train loss: 0.6140611546229472, acc: 0.7509563231515843, val loss: 1.262903877853361, acc: 0.5983453429410195, test loss: 1.2725259145269465, acc: 0.5975609756097561
epoch: 91, train loss: 0.6034445009808727, acc: 0.7556951184698829, val loss: 1.1440117620041061, acc: 0.636776087536696, test loss: 1.0914206042143093, acc: 0.6545599151643691
epoch: 92, train loss: 0.5956401285361536, acc: 0.7571224664573223, val loss: 1.5601989859608227, acc: 0.5505737923672271, test loss: 1.5921405385685876, acc: 0.5408271474019088
epoch: 93, train loss: 0.6066214419282439, acc: 0.7540964887239509, val loss: 1.2400486458769981, acc: 0.607419268748332, test loss: 1.222719959590746, acc: 0.6134676564156946
epoch: 94, train loss: 0.5901661730207377, acc: 0.756894090779332, val loss: 1.6919988765696192, acc: 0.5492393915132106, test loss: 1.677599747638561, acc: 0.573170731707317
epoch: 95, train loss: 0.595043848628083, acc: 0.7562089637453612, val loss: 1.1394886589381166, acc: 0.6482519348812383, test loss: 1.1155885107823167, acc: 0.6580063626723224
epoch: 96, train loss: 0.5951448388757142, acc: 0.7548958035969169, val loss: 1.0147092506540023, acc: 0.6599946623965839, test loss: 1.0228773637389328, acc: 0.6625132555673383
epoch: 97, train loss: 0.5797636884129324, acc: 0.7621467313731087, val loss: 1.7882791075796836, acc: 0.49879903923138513, test loss: 1.7698969760097856, acc: 0.49787910922587486
epoch: 98, train loss: 0.5945069471019491, acc: 0.7591778475592349, val loss: 1.3009816594909025, acc: 0.5983453429410195, test loss: 1.271549209051648, acc: 0.6033934252386002
epoch: 99, train loss: 0.5778862820041204, acc: 0.7606051955466743, val loss: 1.2770880542502647, acc: 0.6359754470242861, test loss: 1.275823888444951, acc: 0.6367974549310711
Epoch    99: reducing learning rate of group 0 to 7.5000e-04.
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3905184922747839, acc: 0.7882957465029974, val loss: 0.9641809621937663, acc: 0.6677341873498799, test loss: 0.9465333769455442, acc: 0.6784199363732768
epoch: 101, train loss: 0.35473356829552594, acc: 0.799200685127034, val loss: 0.9547513619975978, acc: 0.6725380304243395, test loss: 0.9461585292128465, acc: 0.676829268292683
epoch: 102, train loss: 0.36598152972070824, acc: 0.799600342563517, val loss: 0.9683877827804949, acc: 0.6613290632506005, test loss: 0.9560866500009029, acc: 0.6688759278897137
epoch: 103, train loss: 0.3537959589418465, acc: 0.799600342563517, val loss: 0.9931523259048116, acc: 0.6589271417133707, test loss: 0.9907747861057172, acc: 0.6561505832449629
epoch: 104, train loss: 0.3521875060645982, acc: 0.7972023979446189, val loss: 1.0536763845903891, acc: 0.6645316253002402, test loss: 1.0407420951766402, acc: 0.6617179215270413
epoch: 105, train loss: 0.35561116050557684, acc: 0.7976591493005994, val loss: 1.009143288306691, acc: 0.6629303442754203, test loss: 0.9889238452102321, acc: 0.6622481442205727
epoch: 106, train loss: 0.34139532750244317, acc: 0.8024550385383956, val loss: 0.9931040728031746, acc: 0.6599946623965839, test loss: 0.9661959661279478, acc: 0.6603923647932132
epoch: 107, train loss: 0.3362561231455121, acc: 0.8057093919497573, val loss: 1.00936412677658, acc: 0.6578596210301575, test loss: 0.9959256198980143, acc: 0.6558854718981972
epoch: 108, train loss: 0.34322653084253196, acc: 0.8013702540679417, val loss: 1.1210353761346683, acc: 0.6413130504403523, test loss: 1.1092921420852806, acc: 0.6362672322375398
epoch: 109, train loss: 0.3322594064662976, acc: 0.8083928061661433, val loss: 1.0441285816548442, acc: 0.6690685882038965, test loss: 1.0293879825009522, acc: 0.6694061505832449
epoch: 110, train loss: 0.34398794409516126, acc: 0.8, val loss: 1.1168270225616528, acc: 0.6503869762476648, test loss: 1.131803650618358, acc: 0.6399787910922587
epoch: 111, train loss: 0.3407600694661681, acc: 0.8005709391949757, val loss: 1.0378323609841038, acc: 0.6562583400053376, test loss: 1.038547927476189, acc: 0.6572110286320254
epoch: 112, train loss: 0.340090024122469, acc: 0.8018270054239224, val loss: 1.3755507542987109, acc: 0.5946090205497732, test loss: 1.364350014501513, acc: 0.5946447507953341
epoch: 113, train loss: 0.3375558643052825, acc: 0.8032543534113616, val loss: 1.1738222984749698, acc: 0.6415799306111556, test loss: 1.1632687673841782, acc: 0.6519088016967126
epoch: 114, train loss: 0.330783540240159, acc: 0.8013702540679417, val loss: 1.2543870740996892, acc: 0.6269015212169736, test loss: 1.2477685649220536, acc: 0.6349416755037116
epoch: 115, train loss: 0.3627263067893699, acc: 0.7901798458464173, val loss: 1.1820258586477337, acc: 0.6269015212169736, test loss: 1.1456001444560726, acc: 0.6304347826086957
epoch: 116, train loss: 0.3233336691154673, acc: 0.8105623751070511, val loss: 1.1902090827719447, acc: 0.640245529757139, test loss: 1.1712080476645574, acc: 0.6468716861081655
epoch: 117, train loss: 0.3244877660066919, acc: 0.8059377676277476, val loss: 1.0350170302206527, acc: 0.661862823592207, test loss: 1.0543601280305444, acc: 0.6601272534464475
epoch: 118, train loss: 0.3245558250215848, acc: 0.8027976020553811, val loss: 1.3446779718136896, acc: 0.5914064585001334, test loss: 1.3070494783518802, acc: 0.6047189819724285
epoch: 119, train loss: 0.32463574092149255, acc: 0.8055381101912646, val loss: 1.1591253279367064, acc: 0.6242327195089404, test loss: 1.1374169857352814, acc: 0.626458112407211
epoch: 120, train loss: 0.3208933765977374, acc: 0.8064516129032258, val loss: 1.0995983322493261, acc: 0.6519882572724847, test loss: 1.0862216400822047, acc: 0.6540296924708378
epoch: 121, train loss: 0.31518476640807197, acc: 0.8070796460176991, val loss: 1.1275854770552485, acc: 0.6442487323191887, test loss: 1.1312003641340918, acc: 0.647136797454931
epoch: 122, train loss: 0.3194147748774268, acc: 0.8114187838995147, val loss: 1.2591268885189735, acc: 0.615158793701628, test loss: 1.2069646905108196, acc: 0.6267232237539767
epoch: 123, train loss: 0.31160979479032347, acc: 0.8121610048529831, val loss: 1.2868804948184214, acc: 0.6196957566052842, test loss: 1.2302806612542678, acc: 0.6341463414634146
epoch: 124, train loss: 0.31015823046854146, acc: 0.8125035683699686, val loss: 1.353766081840285, acc: 0.5871363757672805, test loss: 1.3164244188610088, acc: 0.5874867444326617
epoch: 125, train loss: 0.3080508193730151, acc: 0.8127890379674565, val loss: 1.0720404150456087, acc: 0.6615959434214038, test loss: 1.0282223265092674, acc: 0.6651643690349947
epoch: 126, train loss: 0.31648941084449034, acc: 0.8082215244076506, val loss: 1.0651206860136344, acc: 0.6586602615425674, test loss: 1.0383417586127368, acc: 0.6648992576882291
epoch: 127, train loss: 0.30926523285187074, acc: 0.8117042534970026, val loss: 1.1920494802608341, acc: 0.6397117694155324, test loss: 1.1439017453097589, acc: 0.637592788971368
epoch: 128, train loss: 0.30612510847017627, acc: 0.8118755352554953, val loss: 1.1170283978800473, acc: 0.6490525753936482, test loss: 1.0918134640534887, acc: 0.6593319194061505
epoch: 129, train loss: 0.31566387846440613, acc: 0.8059948615472452, val loss: 1.1084126962290657, acc: 0.6482519348812383, test loss: 1.0305225134907194, acc: 0.6630434782608695
epoch: 130, train loss: 0.29832760758683097, acc: 0.8135312589209249, val loss: 1.1024741418271946, acc: 0.6554576994929276, test loss: 1.0891720351080627, acc: 0.662778366914104
epoch: 131, train loss: 0.31040126729821466, acc: 0.8113616899800171, val loss: 1.3553182975368434, acc: 0.5825994128636243, test loss: 1.3163164854049683, acc: 0.5930540827147401
epoch: 132, train loss: 0.3037063598037277, acc: 0.8164430488153012, val loss: 1.4205427209886194, acc: 0.5852682145716573, test loss: 1.3985393178804302, acc: 0.5994167550371156
epoch: 133, train loss: 0.30619580050961753, acc: 0.8121039109334856, val loss: 1.1356437623993951, acc: 0.6386442487323192, test loss: 1.1032197343961812, acc: 0.6431601272534464
epoch: 134, train loss: 0.29320690474049416, acc: 0.8184413359977163, val loss: 1.1035316286960029, acc: 0.6477181745396317, test loss: 1.0722963337175162, acc: 0.6598621420996819
epoch: 135, train loss: 0.2967881160908143, acc: 0.8164430488153012, val loss: 1.2062680105988681, acc: 0.6474512943688284, test loss: 1.217454519908982, acc: 0.6521739130434783
epoch: 136, train loss: 0.2919603566884518, acc: 0.815129888666857, val loss: 1.2856399438716521, acc: 0.6301040832666133, test loss: 1.2525525185361521, acc: 0.6418345705196182
epoch: 137, train loss: 0.29351621075184253, acc: 0.8144447616328861, val loss: 1.2410491912690105, acc: 0.6351748065118762, test loss: 1.2329418719889378, acc: 0.6420996818663839
epoch: 138, train loss: 0.29782560040295347, acc: 0.8133599771624322, val loss: 1.1591958129822746, acc: 0.6434480918067788, test loss: 1.1395764697268052, acc: 0.6460763520678685
epoch: 139, train loss: 0.2877794521989191, acc: 0.8168998001712817, val loss: 1.4263252403566224, acc: 0.5935414998665599, test loss: 1.3485500304342455, acc: 0.6121420996818664
epoch: 140, train loss: 0.29495779779037, acc: 0.8187268055952042, val loss: 1.2456060979154417, acc: 0.6146250333600214, test loss: 1.1840455384168493, acc: 0.6378579003181336
epoch: 141, train loss: 0.2873932470645423, acc: 0.8180416785612332, val loss: 1.4686798701325765, acc: 0.6207632772884975, test loss: 1.4731732198567558, acc: 0.6277836691410392
epoch: 142, train loss: 0.2967714396946913, acc: 0.8157579217813303, val loss: 1.6637881375453554, acc: 0.5468374699759808, test loss: 1.6360591461650893, acc: 0.5498409331919406
epoch: 143, train loss: 0.29260903419748635, acc: 0.8159862974593206, val loss: 1.197892221349508, acc: 0.6418468107819589, test loss: 1.1099486075428613, acc: 0.6553552492046659
epoch: 144, train loss: 0.29085539177574843, acc: 0.8187268055952042, val loss: 1.127368344508523, acc: 0.6586602615425674, test loss: 1.074192289077338, acc: 0.6643690349946978
epoch: 145, train loss: 0.2868822148109619, acc: 0.8175278332857551, val loss: 1.203290120741511, acc: 0.6394448892447291, test loss: 1.173290175066773, acc: 0.6420996818663839
epoch: 146, train loss: 0.2901214297179185, acc: 0.8178703968027405, val loss: 1.2437013708689324, acc: 0.6274352815585802, test loss: 1.1882046434558209, acc: 0.6280487804878049
epoch: 147, train loss: 0.27817231097665135, acc: 0.823693976591493, val loss: 1.1851797456387554, acc: 0.6514544969308781, test loss: 1.1694083653730147, acc: 0.6574761399787911
epoch: 148, train loss: 0.28649597383893083, acc: 0.8180987724807308, val loss: 1.2286666443674348, acc: 0.6269015212169736, test loss: 1.1826443548293533, acc: 0.6397136797454931
epoch: 149, train loss: 0.27709249640205336, acc: 0.8246645732229517, val loss: 1.6786360726026908, acc: 0.5639178009073926, test loss: 1.589954445496091, acc: 0.5795334040296924
epoch: 150, train loss: 0.29806104055003646, acc: 0.8151869825863546, val loss: 1.4621420527955262, acc: 0.5959434214037898, test loss: 1.39944011889485, acc: 0.6015376458112407
Epoch   150: reducing learning rate of group 0 to 3.7500e-04.
epoch: 151, train loss: 0.2292250605317616, acc: 0.8423065943477019, val loss: 1.1719075023316687, acc: 0.6645316253002402, test loss: 1.1068417505140902, acc: 0.6744432661717922
epoch: 152, train loss: 0.21729248061525186, acc: 0.8485869254924351, val loss: 1.2226708170443112, acc: 0.6594609020549773, test loss: 1.168032134957177, acc: 0.6683457051961824
epoch: 153, train loss: 0.21534678270195404, acc: 0.8489294890094204, val loss: 1.2224179921164207, acc: 0.6685348278622898, test loss: 1.1511240495983135, acc: 0.6717921527041357
epoch: 154, train loss: 0.212889495410308, acc: 0.8496146160433914, val loss: 1.3075766264676476, acc: 0.6543901788097144, test loss: 1.2400071416148957, acc: 0.6609225874867445
epoch: 155, train loss: 0.20971766214880505, acc: 0.8532686268912361, val loss: 1.3833339518472485, acc: 0.647985054710435, test loss: 1.3040775474030544, acc: 0.6564156945917285
epoch: 156, train loss: 0.2005501587944578, acc: 0.8562946046246075, val loss: 1.4017768842810658, acc: 0.6394448892447291, test loss: 1.3919391644721573, acc: 0.6431601272534464
epoch: 157, train loss: 0.20361626072335576, acc: 0.8561804167856123, val loss: 1.3410862257934297, acc: 0.6589271417133707, test loss: 1.2805260844599784, acc: 0.6606574761399788
epoch: 158, train loss: 0.20094387739462885, acc: 0.8549814444761633, val loss: 1.2749948485043325, acc: 0.6535895382973045, test loss: 1.2212910669858736, acc: 0.6588016967126193
epoch: 159, train loss: 0.19842726737813815, acc: 0.8577219526120469, val loss: 1.3441518963512689, acc: 0.6549239391513211, test loss: 1.2868570495132172, acc: 0.662778366914104
epoch: 160, train loss: 0.1979751242779338, acc: 0.8576648586925493, val loss: 1.416973338274756, acc: 0.6503869762476648, test loss: 1.307079377381698, acc: 0.6553552492046659
epoch: 161, train loss: 0.21415977925359947, acc: 0.8494433342848987, val loss: 1.3627120585068722, acc: 0.6405124099279423, test loss: 1.3240094609629691, acc: 0.6423647932131495
epoch: 162, train loss: 0.20633465244179278, acc: 0.8548101627176705, val loss: 1.3241519870394098, acc: 0.6626634641046171, test loss: 1.286543209757557, acc: 0.6664899257688229
epoch: 163, train loss: 0.19384658174653616, acc: 0.860119897230945, val loss: 1.3605810889950873, acc: 0.6639978649586336, test loss: 1.3027201952888756, acc: 0.6725874867444327
epoch: 164, train loss: 0.20604136617327296, acc: 0.8549243505566657, val loss: 1.3438954152900568, acc: 0.6554576994929276, test loss: 1.303058615924192, acc: 0.6633085896076352
epoch: 165, train loss: 0.19286232307455453, acc: 0.8586925492435056, val loss: 1.3494798578451181, acc: 0.6589271417133707, test loss: 1.2934617364141114, acc: 0.6699363732767762
epoch: 166, train loss: 0.21513706819775646, acc: 0.8505281187553525, val loss: 1.3485693583209768, acc: 0.6450493728315986, test loss: 1.2606842313060578, acc: 0.6548250265111347
epoch: 167, train loss: 0.18963652152420282, acc: 0.8589780188409935, val loss: 1.462206320855533, acc: 0.6461168935148118, test loss: 1.398714970429907, acc: 0.651643690349947
epoch: 168, train loss: 0.19794684712661528, acc: 0.8576648586925493, val loss: 1.4344096346667585, acc: 0.6421136909527622, test loss: 1.3910265220437803, acc: 0.6532343584305408
epoch: 169, train loss: 0.19402915353166555, acc: 0.857436483014559, val loss: 1.5781104243975626, acc: 0.63063784360822, test loss: 1.5115145137681687, acc: 0.6468716861081655
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.14135619609688746, acc: 0.8588067370825007, val loss: 1.2451158546911483, acc: 0.6653322658126501, test loss: 1.224033342060077, acc: 0.6643690349946978
epoch: 171, train loss: 0.14143351483712563, acc: 0.8589209249214959, val loss: 1.327787167808995, acc: 0.6327728849746463, test loss: 1.2579261131529338, acc: 0.651643690349947
epoch: 172, train loss: 0.13596934283116596, acc: 0.8596631458749643, val loss: 1.2850777472945765, acc: 0.6442487323191887, test loss: 1.2075311700208062, acc: 0.6556203605514316
epoch: 173, train loss: 0.1351597057936738, acc: 0.8598344276334571, val loss: 1.1855481643182995, acc: 0.6602615425673872, test loss: 1.1487277952256916, acc: 0.6694061505832449
epoch: 174, train loss: 0.13837307149210964, acc: 0.8590922066799885, val loss: 1.2542357897166732, acc: 0.6527888977848946, test loss: 1.2158146577068503, acc: 0.6643690349946978
epoch: 175, train loss: 0.12917469010552507, acc: 0.8620039965743649, val loss: 1.2589487789279783, acc: 0.6506538564184681, test loss: 1.2248338950148063, acc: 0.6633085896076352
epoch: 176, train loss: 0.14243469831860478, acc: 0.8554952897516415, val loss: 1.2685082581700533, acc: 0.6365092073658927, test loss: 1.2382059206006888, acc: 0.6503181336161188
epoch: 177, train loss: 0.14055324835570376, acc: 0.8556665715101341, val loss: 1.2194647520168131, acc: 0.6562583400053376, test loss: 1.1749806778301994, acc: 0.665694591728526
epoch: 178, train loss: 0.1447877469937052, acc: 0.8521838424207822, val loss: 1.1828267613124108, acc: 0.6514544969308781, test loss: 1.125171177966218, acc: 0.6617179215270413
epoch: 179, train loss: 0.13423516197763372, acc: 0.8605195546674279, val loss: 1.2908943022032373, acc: 0.6535895382973045, test loss: 1.2238593182407027, acc: 0.6630434782608695
epoch: 180, train loss: 0.1369979776363355, acc: 0.8581787039680274, val loss: 1.2585536448008097, acc: 0.6535895382973045, test loss: 1.1904760061872852, acc: 0.6667550371155886
epoch: 181, train loss: 0.13943571177697542, acc: 0.8557236654296317, val loss: 1.531605969596488, acc: 0.6076861489191353, test loss: 1.4808410589414336, acc: 0.616914103923648
epoch: 182, train loss: 0.13886889716939452, acc: 0.8570368255780759, val loss: 1.2921409782550162, acc: 0.6381104883907126, test loss: 1.2354046264357177, acc: 0.6500530222693531
epoch: 183, train loss: 0.14074154145252082, acc: 0.8549243505566657, val loss: 1.1835581164758364, acc: 0.6650653856418468, test loss: 1.138208388144492, acc: 0.6707317073170732
epoch: 184, train loss: 0.14171603345850553, acc: 0.8566942620610905, val loss: 1.27424022230047, acc: 0.6375767280491059, test loss: 1.2075243891233605, acc: 0.6436903499469777
epoch: 185, train loss: 0.136378616556555, acc: 0.8577219526120469, val loss: 1.1781532326347834, acc: 0.6650653856418468, test loss: 1.1428272147699479, acc: 0.6731177094379639
epoch: 186, train loss: 0.12665525197838157, acc: 0.8644590351127605, val loss: 1.2532190196126627, acc: 0.652255137443288, test loss: 1.2267361941292076, acc: 0.6572110286320254
epoch: 187, train loss: 0.12761736518862654, acc: 0.8593776762774764, val loss: 1.3718530114949912, acc: 0.6381104883907126, test loss: 1.2812325175721218, acc: 0.6566808059384942
epoch: 188, train loss: 0.13254340092454542, acc: 0.8604624607479303, val loss: 1.3181813723378606, acc: 0.644515612489992, test loss: 1.2842041139005982, acc: 0.6503181336161188
epoch: 189, train loss: 0.137408412810465, acc: 0.8561804167856123, val loss: 1.2334944226756426, acc: 0.6439818521483853, test loss: 1.1906344068897365, acc: 0.6646341463414634
epoch: 190, train loss: 0.145227555247875, acc: 0.853154439052241, val loss: 1.2981929193982322, acc: 0.6477181745396317, test loss: 1.2030870302610912, acc: 0.6609225874867445
epoch: 191, train loss: 0.1415787431922702, acc: 0.8563516985441051, val loss: 1.211033373188266, acc: 0.6586602615425674, test loss: 1.1339842512144895, acc: 0.6707317073170732
epoch: 192, train loss: 0.12676409161706806, acc: 0.8612046817013989, val loss: 1.2937899432501732, acc: 0.6359754470242861, test loss: 1.2210974574468294, acc: 0.6503181336161188
epoch: 193, train loss: 0.12463109066213773, acc: 0.8616043391378818, val loss: 1.2796651260166254, acc: 0.6415799306111556, test loss: 1.2463306119055155, acc: 0.6484623541887593
epoch: 194, train loss: 0.14530606658721426, acc: 0.8541250356836997, val loss: 1.3691688491911898, acc: 0.6279690419001868, test loss: 1.3030291721145775, acc: 0.6450159066808059
epoch: 195, train loss: 0.12935071201953008, acc: 0.8630316871253212, val loss: 1.1634583281363555, acc: 0.6631972244462236, test loss: 1.1367372763624626, acc: 0.6638388123011665
epoch: 196, train loss: 0.12651828844360785, acc: 0.8628033114473309, val loss: 1.3015011651120822, acc: 0.6354416866826795, test loss: 1.2883697577524034, acc: 0.6500530222693531
epoch: 197, train loss: 0.13230766162654858, acc: 0.8571510134170711, val loss: 1.226828966740134, acc: 0.6482519348812383, test loss: 1.1656401706019994, acc: 0.6648992576882291
epoch: 198, train loss: 0.1225940056223887, acc: 0.8639451898372823, val loss: 1.3682312653928177, acc: 0.6343741659994663, test loss: 1.2968097307017095, acc: 0.644220572640509
epoch: 199, train loss: 0.13935255886757678, acc: 0.8564658863831002, val loss: 1.3617820663391065, acc: 0.6335735254870564, test loss: 1.3097561434748823, acc: 0.6439554612937434
epoch: 200, train loss: 0.1273848135859123, acc: 0.8624036540108478, val loss: 1.2730493116035186, acc: 0.6506538564184681, test loss: 1.229771226992157, acc: 0.6574761399787911
best val acc 0.6725380304243395 at epoch 101.
****************************************************************
/opt/python/anaconda-2020.7/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
train report:
              precision    recall  f1-score   support

           0     0.9262    0.9803    0.9525      5337
           1     0.7900    0.7730    0.7814      2502
           2     0.9249    0.8210    0.8698       810
           3     0.7893    0.8793    0.8319      1840
           4     0.9220    0.7693    0.8388       737
           5     0.8193    0.9911    0.8971       677
           6     0.7090    0.9191    0.8005      1323
           7     0.5232    0.6714    0.5881       907
           8     0.9272    0.7862    0.8509       421
           9     0.8990    0.8653    0.8818       401
          10     0.8674    0.9747    0.9180       396
          11     0.9968    0.9458    0.9706       332
          12     0.4706    0.4068    0.4364       295
          13     0.7386    0.7766    0.7571       291
          14     0.0000    0.0000    0.0000       261
          15     0.0000    0.0000    0.0000       494
          16     0.0000    0.0000    0.0000       256
          17     0.5055    0.5830    0.5415       235

    accuracy                         0.8206     17515
   macro avg     0.6561    0.6746    0.6620     17515
weighted avg     0.7802    0.8206    0.7974     17515

train confusion matrix:
[[9.80326026e-01 1.12422709e-03 7.49484729e-04 1.87371182e-04
  0.00000000e+00 1.87371182e-04 0.00000000e+00 5.43376429e-03
  4.68427956e-03 2.43582537e-03 5.62113547e-04 0.00000000e+00
  0.00000000e+00 3.93479483e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 3.74742365e-04]
 [1.83852918e-02 7.72981615e-01 2.39808153e-03 3.67705835e-02
  7.59392486e-03 1.59872102e-03 1.23900879e-01 1.95843325e-02
  3.99680256e-04 2.39808153e-03 0.00000000e+00 3.99680256e-04
  1.23900879e-02 0.00000000e+00 0.00000000e+00 3.99680256e-04
  0.00000000e+00 7.99360512e-04]
 [1.23456790e-03 6.17283951e-03 8.20987654e-01 1.23456790e-03
  0.00000000e+00 1.32098765e-01 9.87654321e-03 0.00000000e+00
  0.00000000e+00 4.93827160e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.34567901e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.93478261e-02 7.77173913e-02 0.00000000e+00 8.79347826e-01
  0.00000000e+00 1.63043478e-03 5.43478261e-04 1.14130435e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [5.42740841e-03 6.64857531e-02 0.00000000e+00 9.49796472e-03
  7.69335142e-01 0.00000000e+00 4.07055631e-03 5.29172320e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  8.81953867e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 4.07055631e-03]
 [0.00000000e+00 0.00000000e+00 2.95420975e-03 0.00000000e+00
  0.00000000e+00 9.91137371e-01 5.90841950e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [7.55857899e-04 3.02343159e-02 1.28495843e-02 0.00000000e+00
  0.00000000e+00 1.81405896e-02 9.19123205e-01 1.51171580e-03
  0.00000000e+00 5.29100529e-03 0.00000000e+00 0.00000000e+00
  4.53514739e-03 7.55857899e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [8.59977949e-02 5.62293275e-02 2.20507166e-03 6.61521499e-03
  7.71775083e-03 0.00000000e+00 8.82028666e-03 6.71444322e-01
  0.00000000e+00 1.10253583e-03 2.53583241e-02 0.00000000e+00
  2.97684675e-02 6.61521499e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.81256891e-02]
 [1.97149644e-01 4.75059382e-03 0.00000000e+00 2.37529691e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  7.86223278e-01 9.50118765e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [5.48628429e-02 7.48129676e-03 2.49376559e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.49376559e-03 0.00000000e+00
  0.00000000e+00 8.65336658e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 4.48877805e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.05050505e-03
  0.00000000e+00 0.00000000e+00 9.74747475e-01 0.00000000e+00
  0.00000000e+00 2.52525253e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.76767677e-02]
 [1.20481928e-02 3.31325301e-02 3.01204819e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.01204819e-03 0.00000000e+00
  0.00000000e+00 3.01204819e-03 0.00000000e+00 9.45783133e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [8.81355932e-02 2.03389831e-02 3.38983051e-03 0.00000000e+00
  6.44067797e-02 0.00000000e+00 1.01694915e-02 3.79661017e-01
  0.00000000e+00 0.00000000e+00 3.38983051e-03 0.00000000e+00
  4.06779661e-01 1.35593220e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.01694915e-02]
 [1.51202749e-01 0.00000000e+00 2.40549828e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.03092784e-02
  0.00000000e+00 1.03092784e-02 2.06185567e-02 0.00000000e+00
  6.87285223e-03 7.76632302e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.11111111e-01 5.70881226e-01 0.00000000e+00 1.68582375e-01
  7.66283525e-03 0.00000000e+00 3.83141762e-03 1.30268199e-01
  0.00000000e+00 0.00000000e+00 3.83141762e-03 0.00000000e+00
  3.83141762e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 9.31174089e-02 6.07287449e-03 5.64777328e-01
  0.00000000e+00 1.61943320e-02 3.17813765e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  2.02429150e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [8.20312500e-02 3.90625000e-03 0.00000000e+00 3.90625000e-03
  3.90625000e-03 0.00000000e+00 7.81250000e-03 7.57812500e-01
  0.00000000e+00 0.00000000e+00 1.95312500e-02 0.00000000e+00
  7.81250000e-03 3.90625000e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.09375000e-01]
 [1.70212766e-02 8.51063830e-03 4.25531915e-03 0.00000000e+00
  0.00000000e+00 4.25531915e-03 0.00000000e+00 2.97872340e-01
  0.00000000e+00 0.00000000e+00 8.51063830e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.82978723e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8083    0.8740    0.8398      1143
           1     0.5853    0.5504    0.5673       536
           2     0.7484    0.6705    0.7073       173
           3     0.6239    0.7284    0.6721       394
           4     0.8783    0.6392    0.7399       158
           5     0.7381    0.8552    0.7923       145
           6     0.6213    0.8057    0.7015       283
           7     0.3245    0.4433    0.3747       194
           8     0.7429    0.5778    0.6500        90
           9     0.4938    0.4706    0.4819        85
          10     0.6509    0.8214    0.7263        84
          11     0.9434    0.7042    0.8065        71
          12     0.3143    0.1746    0.2245        63
          13     0.5429    0.6129    0.5758        62
          14     0.0000    0.0000    0.0000        56
          15     0.0000    0.0000    0.0000       105
          16     0.0000    0.0000    0.0000        55
          17     0.3934    0.4800    0.4324        50

    accuracy                         0.6725      3747
   macro avg     0.5228    0.5227    0.5163      3747
weighted avg     0.6408    0.6725    0.6521      3747

validation confusion matrix:
[[0.87401575 0.02887139 0.00437445 0.01924759 0.00087489 0.
  0.00087489 0.02449694 0.01049869 0.01224847 0.00699913 0.
  0.00087489 0.01224847 0.         0.         0.         0.00437445]
 [0.09141791 0.55037313 0.01679104 0.12126866 0.00932836 0.00932836
  0.10447761 0.05597015 0.00746269 0.01119403 0.00373134 0.
  0.00559701 0.00746269 0.         0.00186567 0.         0.00373134]
 [0.04624277 0.02890173 0.67052023 0.01156069 0.         0.13872832
  0.06358382 0.00578035 0.         0.02890173 0.00578035 0.
  0.         0.         0.         0.         0.         0.        ]
 [0.08375635 0.09898477 0.00253807 0.7284264  0.00253807 0.
  0.01015228 0.06091371 0.         0.00253807 0.         0.00253807
  0.         0.00507614 0.         0.         0.         0.00253807]
 [0.02531646 0.16455696 0.         0.02531646 0.63924051 0.
  0.03797468 0.05063291 0.         0.00632911 0.00632911 0.
  0.03164557 0.00632911 0.         0.         0.         0.00632911]
 [0.         0.0137931  0.06206897 0.00689655 0.         0.85517241
  0.06206897 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.02473498 0.07067138 0.01060071 0.00353357 0.         0.02826855
  0.80565371 0.         0.00353357 0.02473498 0.         0.
  0.02120141 0.00706714 0.         0.         0.         0.        ]
 [0.15979381 0.09278351 0.         0.07731959 0.01030928 0.
  0.02061856 0.44329897 0.         0.         0.06701031 0.
  0.03092784 0.00515464 0.         0.         0.         0.09278351]
 [0.31111111 0.04444444 0.         0.         0.         0.
  0.         0.04444444 0.57777778 0.02222222 0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.17647059 0.04705882 0.08235294 0.01176471 0.         0.04705882
  0.07058824 0.01176471 0.         0.47058824 0.         0.01176471
  0.         0.07058824 0.         0.         0.         0.        ]
 [0.04761905 0.         0.         0.01190476 0.01190476 0.
  0.         0.05952381 0.         0.         0.82142857 0.
  0.         0.         0.         0.         0.         0.04761905]
 [0.04225352 0.15492958 0.01408451 0.04225352 0.         0.
  0.02816901 0.         0.         0.01408451 0.         0.70422535
  0.         0.         0.         0.         0.         0.        ]
 [0.19047619 0.15873016 0.         0.03174603 0.03174603 0.
  0.         0.3968254  0.         0.         0.01587302 0.
  0.17460317 0.         0.         0.         0.         0.        ]
 [0.17741935 0.01612903 0.0483871  0.01612903 0.         0.
  0.         0.01612903 0.         0.03225806 0.06451613 0.
  0.01612903 0.61290323 0.         0.         0.         0.        ]
 [0.17857143 0.32142857 0.         0.19642857 0.         0.
  0.01785714 0.19642857 0.         0.01785714 0.         0.
  0.01785714 0.01785714 0.         0.         0.         0.03571429]
 [0.07619048 0.07619048 0.00952381 0.38095238 0.         0.02857143
  0.36190476 0.02857143 0.00952381 0.00952381 0.         0.
  0.         0.00952381 0.         0.         0.         0.00952381]
 [0.16363636 0.16363636 0.         0.07272727 0.03636364 0.
  0.01818182 0.41818182 0.         0.         0.03636364 0.01818182
  0.01818182 0.         0.         0.         0.         0.05454545]
 [0.1        0.02       0.         0.         0.         0.
  0.         0.3        0.         0.         0.1        0.
  0.         0.         0.         0.         0.         0.48      ]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8057    0.8655    0.8345      1145
           1     0.6371    0.6015    0.6188       537
           2     0.7862    0.7143    0.7485       175
           3     0.6364    0.7089    0.6707       395
           4     0.8390    0.6226    0.7148       159
           5     0.7151    0.8767    0.7877       146
           6     0.6281    0.8028    0.7048       284
           7     0.3529    0.5231    0.4215       195
           8     0.7200    0.5934    0.6506        91
           9     0.6125    0.5632    0.5868        87
          10     0.6701    0.7558    0.7104        86
          11     0.9643    0.7500    0.8437        72
          12     0.2000    0.1719    0.1849        64
          13     0.4167    0.4688    0.4412        64
          14     0.0000    0.0000    0.0000        57
          15     1.0000    0.0093    0.0185       107
          16     0.0000    0.0000    0.0000        56
          17     0.2549    0.2500    0.2524        52

    accuracy                         0.6768      3772
   macro avg     0.5688    0.5154    0.5105      3772
weighted avg     0.6745    0.6768    0.6580      3772

test confusion matrix:
[[8.65502183e-01 2.44541485e-02 6.98689956e-03 1.83406114e-02
  8.73362445e-04 8.73362445e-04 2.62008734e-03 2.62008734e-02
  1.31004367e-02 7.86026201e-03 3.49344978e-03 1.74672489e-03
  2.62008734e-03 1.65938865e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 8.73362445e-03]
 [7.44878957e-02 6.01489758e-01 9.31098696e-03 1.04283054e-01
  9.31098696e-03 1.11731844e-02 1.21042831e-01 2.79329609e-02
  5.58659218e-03 1.48975791e-02 1.86219739e-03 0.00000000e+00
  1.30353818e-02 3.72439479e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.86219739e-03]
 [4.00000000e-02 1.14285714e-02 7.14285714e-01 5.71428571e-03
  0.00000000e+00 1.31428571e-01 1.14285714e-02 5.71428571e-03
  5.71428571e-03 3.42857143e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 4.00000000e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [8.35443038e-02 9.36708861e-02 1.26582278e-02 7.08860759e-01
  5.06329114e-03 5.06329114e-03 5.06329114e-03 7.34177215e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  5.06329114e-03 2.53164557e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.06329114e-03]
 [3.77358491e-02 8.80503145e-02 0.00000000e+00 3.14465409e-02
  6.22641509e-01 0.00000000e+00 3.14465409e-02 1.06918239e-01
  0.00000000e+00 0.00000000e+00 6.28930818e-03 0.00000000e+00
  6.91823899e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 6.28930818e-03]
 [6.84931507e-03 6.84931507e-03 1.36986301e-02 6.84931507e-03
  0.00000000e+00 8.76712329e-01 8.21917808e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 6.84931507e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.52112676e-02 5.98591549e-02 1.40845070e-02 7.04225352e-03
  7.04225352e-03 3.52112676e-02 8.02816901e-01 7.04225352e-03
  0.00000000e+00 7.04225352e-03 7.04225352e-03 0.00000000e+00
  1.05633803e-02 7.04225352e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.17948718e-01 7.17948718e-02 5.12820513e-03 6.15384615e-02
  5.12820513e-03 1.02564103e-02 5.12820513e-02 5.23076923e-01
  5.12820513e-03 0.00000000e+00 3.58974359e-02 0.00000000e+00
  5.64102564e-02 5.12820513e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.12820513e-02]
 [3.62637363e-01 0.00000000e+00 0.00000000e+00 1.09890110e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.09890110e-02
  5.93406593e-01 1.09890110e-02 1.09890110e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.06896552e-01 6.89655172e-02 1.14942529e-02 0.00000000e+00
  1.14942529e-02 2.29885057e-02 4.59770115e-02 1.14942529e-02
  0.00000000e+00 5.63218391e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.74712644e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.97674419e-02 0.00000000e+00 0.00000000e+00 2.32558140e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.32558140e-02
  0.00000000e+00 0.00000000e+00 7.55813953e-01 0.00000000e+00
  0.00000000e+00 3.48837209e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.30232558e-02]
 [6.94444444e-02 8.33333333e-02 4.16666667e-02 2.77777778e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.77777778e-02 0.00000000e+00 7.50000000e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.25000000e-01 2.03125000e-01 0.00000000e+00 0.00000000e+00
  3.12500000e-02 0.00000000e+00 4.68750000e-02 3.90625000e-01
  0.00000000e+00 0.00000000e+00 1.56250000e-02 0.00000000e+00
  1.71875000e-01 1.56250000e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.96875000e-01 1.56250000e-02 3.12500000e-02 1.56250000e-02
  1.56250000e-02 0.00000000e+00 4.68750000e-02 1.56250000e-02
  0.00000000e+00 0.00000000e+00 4.68750000e-02 0.00000000e+00
  3.12500000e-02 4.68750000e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.56250000e-02]
 [1.05263158e-01 4.38596491e-01 0.00000000e+00 2.10526316e-01
  1.75438596e-02 0.00000000e+00 0.00000000e+00 1.40350877e-01
  1.75438596e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.50877193e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 3.50877193e-02]
 [9.34579439e-02 1.68224299e-01 2.80373832e-02 3.55140187e-01
  1.86915888e-02 4.67289720e-02 2.42990654e-01 9.34579439e-03
  0.00000000e+00 2.80373832e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.34579439e-03
  0.00000000e+00 0.00000000e+00]
 [1.42857143e-01 1.78571429e-02 0.00000000e+00 8.92857143e-02
  1.78571429e-02 0.00000000e+00 0.00000000e+00 5.71428571e-01
  0.00000000e+00 0.00000000e+00 7.14285714e-02 0.00000000e+00
  3.57142857e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.35714286e-02]
 [1.15384615e-01 1.92307692e-02 0.00000000e+00 1.92307692e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.23076923e-01
  0.00000000e+00 0.00000000e+00 1.53846154e-01 0.00000000e+00
  1.92307692e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.50000000e-01]]
---------------------------------------
program finished.
