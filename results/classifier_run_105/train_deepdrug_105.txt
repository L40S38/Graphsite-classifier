seed:  5
save trained model at:  ../trained_models/trained_classifier_model_105.pt
save loss at:  ./results/train_classifier_results_105.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['6brxA00', '6ab1A01', '5i7vB01', '5aagA00', '3zf6A00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['3u9cB00', '4rzqA01', '6cp3L00', '4k4yI00', '5n1kA01']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b3a8e756730>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0079660973944606, acc: 0.3999052918195809; test loss: 1.786790413823823, acc: 0.4433939966910896
epoch: 2, train loss: 1.7209098804091594, acc: 0.46738487036817805; test loss: 1.6477895449645354, acc: 0.48782793665800045
epoch: 3, train loss: 1.6329552969086363, acc: 0.498342606842666; test loss: 1.5439681474324576, acc: 0.5291893169463484
epoch: 4, train loss: 1.5866842489207555, acc: 0.5181129395051498; test loss: 1.5222813872912342, acc: 0.5469156227842118
epoch: 5, train loss: 1.5163551322296411, acc: 0.5414348289333492; test loss: 1.465075656275072, acc: 0.5568423540534153
epoch: 6, train loss: 1.4916915829201147, acc: 0.5473540902095418; test loss: 1.4622776845870247, acc: 0.5615693689435122
epoch: 7, train loss: 1.457570405913568, acc: 0.5523854622943056; test loss: 1.4619211021875045, acc: 0.5596785629874734
epoch: 8, train loss: 1.4313289281728354, acc: 0.5697880904463123; test loss: 1.4034599694369447, acc: 0.5665327345781139
epoch: 9, train loss: 1.3925128000875162, acc: 0.5762400852373624; test loss: 1.3983320319830062, acc: 0.5653509808555897
epoch: 10, train loss: 1.3966853911740915, acc: 0.5758849295607908; test loss: 1.4301697429092577, acc: 0.5568423540534153
epoch: 11, train loss: 1.3736507415461008, acc: 0.5809754942583165; test loss: 1.3887496934492687, acc: 0.5672417868116284
epoch: 12, train loss: 1.3339798805966834, acc: 0.5925772463596544; test loss: 1.2987108423872722, acc: 0.5951311746632002
epoch: 13, train loss: 1.30801670754804, acc: 0.5995619746655617; test loss: 1.3477709902513397, acc: 0.5804774285038998
epoch: 14, train loss: 1.296126992459465, acc: 0.6092103705457559; test loss: 1.3066062421219424, acc: 0.5948948239186953
epoch: 15, train loss: 1.2917508525341463, acc: 0.6083816739670889; test loss: 1.3874747959385807, acc: 0.584731741904987
epoch: 16, train loss: 1.2774695355722547, acc: 0.6105718006392802; test loss: 1.2688386867976702, acc: 0.6104939730560152
epoch: 17, train loss: 1.2546036392467974, acc: 0.619746655617379; test loss: 1.3172399528875782, acc: 0.5963129283857244
epoch: 18, train loss: 1.243351729858609, acc: 0.621285663549189; test loss: 1.2722571833819483, acc: 0.6071850626329472
epoch: 19, train loss: 1.2281766400134562, acc: 0.6262578430211909; test loss: 1.2715777768730356, acc: 0.603167099976365
epoch: 20, train loss: 1.2217617632581508, acc: 0.6298093997869066; test loss: 1.2826102930049585, acc: 0.6019853462538407
epoch: 21, train loss: 1.2036082398415853, acc: 0.635195927548242; test loss: 1.2511829464916246, acc: 0.6161663909241314
epoch: 22, train loss: 1.2094115685665834, acc: 0.6320587190718598; test loss: 1.1606802180201639, acc: 0.645710233987237
epoch: 23, train loss: 1.1887219118707855, acc: 0.6357286610630993; test loss: 1.2026237601282692, acc: 0.6310564878279367
epoch: 24, train loss: 1.1824101305465184, acc: 0.6426541967562448; test loss: 1.2992341700977423, acc: 0.603167099976365
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9448383089719022, acc: 0.643364508109388; test loss: 0.9657401992366884, acc: 0.6357835027180335
epoch: 26, train loss: 0.9192529296866533, acc: 0.6502900438025334; test loss: 0.9967383749517136, acc: 0.6291656818718979
epoch: 27, train loss: 0.9227421543111854, acc: 0.6483958801941517; test loss: 1.002389582695056, acc: 0.6239659654927913
epoch: 28, train loss: 0.9114563227532474, acc: 0.6527761335385344; test loss: 1.1630846097153205, acc: 0.5672417868116284
epoch: 29, train loss: 0.905391654051506, acc: 0.6553214158872973; test loss: 0.9632598136057952, acc: 0.6383833609075868
epoch: 30, train loss: 0.8937892339081965, acc: 0.6601160175210133; test loss: 0.9290925296196502, acc: 0.6494918458993146
epoch: 31, train loss: 0.9023923693894916, acc: 0.6544927193086303; test loss: 0.950822766186131, acc: 0.6414559205861499
epoch: 32, train loss: 0.8927294257774683, acc: 0.6583402391381555; test loss: 0.9609266848914647, acc: 0.6400378161191208
epoch: 33, train loss: 0.8762339682879358, acc: 0.6621877589676808; test loss: 0.9797585289463088, acc: 0.6298747341054124
epoch: 34, train loss: 0.8808775289363235, acc: 0.6582218539126317; test loss: 0.9808463376905585, acc: 0.6317655400614512
epoch: 35, train loss: 0.8665449702769352, acc: 0.6665680123120634; test loss: 0.939973074616171, acc: 0.647364689198771
epoch: 36, train loss: 0.8516815268557297, acc: 0.6703563395288268; test loss: 1.0658468598875417, acc: 0.6069487118884425
epoch: 37, train loss: 0.8511431325437389, acc: 0.6716585770095892; test loss: 0.9137746008595967, acc: 0.6565823682344599
epoch: 38, train loss: 0.8333961311086079, acc: 0.6791760388303539; test loss: 0.9697590205607981, acc: 0.6414559205861499
epoch: 39, train loss: 0.8399040302722796, acc: 0.6747957854859713; test loss: 0.9270857950385032, acc: 0.6367289056960529
epoch: 40, train loss: 0.8337129223300138, acc: 0.6752693263880668; test loss: 1.0206564561437699, acc: 0.6225478610257622
epoch: 41, train loss: 0.8269032909204879, acc: 0.6781697644134012; test loss: 1.0218317679439248, acc: 0.6133301819900733
epoch: 42, train loss: 0.8151638334681187, acc: 0.6863383449745472; test loss: 1.0790126137743603, acc: 0.6133301819900733
epoch: 43, train loss: 0.81366135816652, acc: 0.6838522552385462; test loss: 1.0458337814355398, acc: 0.6041125029543843
epoch: 44, train loss: 0.8063440912218931, acc: 0.686456730200071; test loss: 0.9783462122861051, acc: 0.6270385251713543
epoch: 45, train loss: 0.7978236954762863, acc: 0.6861607671362614; test loss: 0.9062380020400362, acc: 0.6589458756795084
epoch: 46, train loss: 0.7938975803920104, acc: 0.6915472948975968; test loss: 0.8986742389988206, acc: 0.6674545024816828
epoch: 47, train loss: 0.7869903945465603, acc: 0.6958091630164556; test loss: 1.067277785706706, acc: 0.6050579059324037
epoch: 48, train loss: 0.7830966626015784, acc: 0.6939741920208358; test loss: 0.8807122388987134, acc: 0.67170881588277
epoch: 49, train loss: 0.7746679538648567, acc: 0.6928495323783592; test loss: 0.9150609212179777, acc: 0.6615457338690617
epoch: 50, train loss: 0.7511380369866179, acc: 0.7083579969219841; test loss: 1.0894121054187504, acc: 0.5842590404159773
epoch: 51, train loss: 0.7779163361315447, acc: 0.6961051260802652; test loss: 0.9945827876667807, acc: 0.6171117939021508
epoch: 52, train loss: 0.7458957486834571, acc: 0.7033858174499822; test loss: 0.9853075887260356, acc: 0.6301110848499173
epoch: 53, train loss: 0.7589558509661486, acc: 0.6998934532970286; test loss: 0.888299247272413, acc: 0.6551642637674309
epoch: 54, train loss: 0.751205738379403, acc: 0.708476382147508; test loss: 0.9451324544147914, acc: 0.6367289056960529
epoch: 55, train loss: 0.7357878366546319, acc: 0.7061678702497928; test loss: 0.9763568854732058, acc: 0.6376743086740724
epoch: 56, train loss: 0.7315995089116146, acc: 0.7107848940452232; test loss: 0.8607328174188046, acc: 0.6702907114157409
epoch: 57, train loss: 0.7250333805683733, acc: 0.7099561974665561; test loss: 0.8671143559298315, acc: 0.6646182935476247
epoch: 58, train loss: 0.7077122180083948, acc: 0.7194862081212264; test loss: 0.8811907552427446, acc: 0.6669818009926731
epoch: 59, train loss: 0.7319381036207466, acc: 0.7103705457558896; test loss: 0.8577196849939305, acc: 0.6806901441739541
epoch: 60, train loss: 0.723373633560322, acc: 0.7124422872025571; test loss: 0.9921401117293723, acc: 0.6272748759158592
epoch: 61, train loss: 0.7175104880143838, acc: 0.7179472001894164; test loss: 0.851627340950151, acc: 0.674781375561333
epoch: 62, train loss: 0.7153520663631051, acc: 0.7179472001894164; test loss: 0.9022828230928851, acc: 0.654927913022926
epoch: 63, train loss: 0.6952947563951194, acc: 0.7233929205635137; test loss: 0.8293854475753918, acc: 0.67950839045143
epoch: 64, train loss: 0.7026070250242127, acc: 0.7217947200189416; test loss: 0.8989221357730116, acc: 0.6648546442921295
epoch: 65, train loss: 0.7021176969848385, acc: 0.7221498756955133; test loss: 1.013903646428544, acc: 0.6258567714488301
epoch: 66, train loss: 0.6905755412865069, acc: 0.7249911211080857; test loss: 0.8470483277599684, acc: 0.6920349799101867
epoch: 67, train loss: 0.6882416266752613, acc: 0.7260565881378004; test loss: 0.9020940140611706, acc: 0.6676908532261877
epoch: 68, train loss: 0.7044348784546972, acc: 0.718243163253226; test loss: 0.8229822438536906, acc: 0.6882533679981092
epoch: 69, train loss: 0.6897881402127684, acc: 0.7275364034568486; test loss: 0.9552041548866663, acc: 0.6348380997400142
epoch: 70, train loss: 0.6666961021785752, acc: 0.7346395169882799; test loss: 0.8472644809965433, acc: 0.676199480028362
epoch: 71, train loss: 0.6684687443777021, acc: 0.7328045459926601; test loss: 0.881703832137672, acc: 0.6750177263058379
epoch: 72, train loss: 0.6704602750699393, acc: 0.7316798863501836; test loss: 0.786803806794504, acc: 0.7017253604348853
epoch: 73, train loss: 0.6744506980902318, acc: 0.7287202557120871; test loss: 0.820329304424422, acc: 0.6882533679981092
epoch: 74, train loss: 0.676561495517434, acc: 0.728838640937611; test loss: 0.8180162801271464, acc: 0.6865989127865753
epoch: 75, train loss: 0.6641894659657894, acc: 0.7340475908606606; test loss: 0.9610517372675919, acc: 0.6468919877097613
epoch: 76, train loss: 0.6480389570461669, acc: 0.7421569788090446; test loss: 0.9083314021973147, acc: 0.6606003308910423
epoch: 77, train loss: 0.6536809900702563, acc: 0.7370664141115189; test loss: 0.973844955052588, acc: 0.6310564878279367
epoch: 78, train loss: 0.6591352608918833, acc: 0.7335740499585651; test loss: 0.8150856771020567, acc: 0.680926494918459
epoch: 79, train loss: 0.6511079138575947, acc: 0.7377175328519001; test loss: 0.8599450331268454, acc: 0.6759631292838573
epoch: 80, train loss: 0.6484340935878475, acc: 0.7386646146560909; test loss: 0.7624125063658831, acc: 0.7118884424485937
epoch: 81, train loss: 0.6380328188580959, acc: 0.741032319166568; test loss: 1.0397404843372984, acc: 0.6277475774048689
epoch: 82, train loss: 0.6408765301544087, acc: 0.741861015745235; test loss: 0.8465210663965199, acc: 0.6851808083195462
epoch: 83, train loss: 0.6342341966695075, acc: 0.7445246833195217; test loss: 0.8277849771260257, acc: 0.6910895769321673
epoch: 84, train loss: 0.631845343616766, acc: 0.7471883508938084; test loss: 0.8522881258466626, acc: 0.691562278421177
epoch: 85, train loss: 0.6272701609201004, acc: 0.748431395761809; test loss: 0.8271014418226696, acc: 0.6894351217206334
epoch: 86, train loss: 0.62157235133432, acc: 0.7496152480170475; test loss: 0.8128235446620681, acc: 0.6943984873552351
epoch: 87, train loss: 0.650997582886986, acc: 0.7418018231324731; test loss: 0.8698893019956383, acc: 0.6714724651382652
epoch: 88, train loss: 0.6080759433905754, acc: 0.7522789155913342; test loss: 0.8383593904693142, acc: 0.6981800992673127
epoch: 89, train loss: 0.5977501944280791, acc: 0.7566591689357168; test loss: 0.9482631222966771, acc: 0.6561096667454502
epoch: 90, train loss: 0.6258512236344627, acc: 0.7488457440511425; test loss: 0.829831254930255, acc: 0.6870716142755849
epoch: 91, train loss: 0.5958156921855935, acc: 0.7596187995738132; test loss: 0.8103011224152095, acc: 0.7024344126683999
Epoch    91: reducing learning rate of group 0 to 1.5000e-03.
epoch: 92, train loss: 0.5502927796691356, acc: 0.7741209897004854; test loss: 0.7451315209026512, acc: 0.7194516662727488
epoch: 93, train loss: 0.5152345812502762, acc: 0.789866224695158; test loss: 0.7557473115057732, acc: 0.7194516662727488
epoch: 94, train loss: 0.5055277604626045, acc: 0.7901621877589676; test loss: 0.7400011789204239, acc: 0.7326873079650201
epoch: 95, train loss: 0.4895751350579691, acc: 0.8005800876050669; test loss: 0.7723504591991196, acc: 0.725124084140865
epoch: 96, train loss: 0.49402668193640675, acc: 0.7948975967799219; test loss: 0.7688890758966109, acc: 0.7182699125502245
epoch: 97, train loss: 0.49345484743234574, acc: 0.795548715520303; test loss: 0.7290752311035748, acc: 0.7331600094540298
epoch: 98, train loss: 0.4910807381780441, acc: 0.7945424411033503; test loss: 0.7378723704203509, acc: 0.734341763176554
epoch: 99, train loss: 0.48440909403321925, acc: 0.798034805256304; test loss: 0.7655074050369569, acc: 0.7260694871188844
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.38097373296399645, acc: 0.8001065467029714; test loss: 0.6257980801596537, acc: 0.7303238005199716
epoch: 101, train loss: 0.37323809104712596, acc: 0.802947792115544; test loss: 0.6515959175842869, acc: 0.7149610021271567
epoch: 102, train loss: 0.3709532458097452, acc: 0.8052563040132591; test loss: 0.5968974556265867, acc: 0.7307965020089813
epoch: 103, train loss: 0.37049069314422417, acc: 0.8005800876050669; test loss: 0.5901632434057869, acc: 0.7385960765776413
epoch: 104, train loss: 0.3653772912978618, acc: 0.8024150586006866; test loss: 0.6360296269337693, acc: 0.7215788229732923
epoch: 105, train loss: 0.35839430804194483, acc: 0.8067953119450693; test loss: 0.5931317924016251, acc: 0.7461593004017962
epoch: 106, train loss: 0.3636339723259454, acc: 0.8035989108559252; test loss: 0.6175724825546889, acc: 0.7338690616875443
epoch: 107, train loss: 0.36247690371808267, acc: 0.8010536285071623; test loss: 0.6197423427219342, acc: 0.7352871661545733
epoch: 108, train loss: 0.35841482376380546, acc: 0.8038356813069729; test loss: 0.6191934409419911, acc: 0.7329236587095249
epoch: 109, train loss: 0.36743508832009003, acc: 0.8015271694092577; test loss: 0.6260698341472412, acc: 0.7383597258331364
epoch: 110, train loss: 0.3643627447624439, acc: 0.8004617023795431; test loss: 0.6337395165722243, acc: 0.7296147482864571
epoch: 111, train loss: 0.35688099364945347, acc: 0.8040724517580206; test loss: 0.6151272928255497, acc: 0.7319782557315055
epoch: 112, train loss: 0.3619432031219212, acc: 0.7991002722860187; test loss: 0.641002099357917, acc: 0.7312692034979911
epoch: 113, train loss: 0.3693043499341784, acc: 0.8000473540902096; test loss: 0.6474731067652061, acc: 0.7248877333963601
epoch: 114, train loss: 0.35443618776841435, acc: 0.8048419557239257; test loss: 0.6240888362628465, acc: 0.74048688253368
epoch: 115, train loss: 0.3580173571441136, acc: 0.8019415176985912; test loss: 0.6670880034245432, acc: 0.7211061214842827
epoch: 116, train loss: 0.3529545089239423, acc: 0.8050787261749733; test loss: 0.6251957281293443, acc: 0.7274875915859135
epoch: 117, train loss: 0.3403642108571082, acc: 0.809518172132118; test loss: 0.6414643825016189, acc: 0.723705979673836
epoch: 118, train loss: 0.3413145507463015, acc: 0.8116491061915473; test loss: 0.6227598236514952, acc: 0.7364689198770976
epoch: 119, train loss: 0.3505996472809856, acc: 0.8065585414940215; test loss: 0.6941767175607269, acc: 0.7055069723469629
epoch: 120, train loss: 0.34968731529870456, acc: 0.80774239374926; test loss: 0.6489193504286612, acc: 0.7255967856298747
epoch: 121, train loss: 0.34211632133419306, acc: 0.8105244465490706; test loss: 0.6182405108736074, acc: 0.735759867643583
epoch: 122, train loss: 0.34411044880470837, acc: 0.8136616550254528; test loss: 0.6408419407335361, acc: 0.7248877333963601
epoch: 123, train loss: 0.3287088639308281, acc: 0.8157925890848822; test loss: 0.627380119320124, acc: 0.7307965020089813
epoch: 124, train loss: 0.3414715169392578, acc: 0.8099917130342134; test loss: 0.6673104832905513, acc: 0.723705979673836
epoch: 125, train loss: 0.3363617391957183, acc: 0.8108204096128803; test loss: 0.6461619475974847, acc: 0.7265421886078941
epoch: 126, train loss: 0.3340016042211006, acc: 0.8111163726766899; test loss: 0.6656058275848944, acc: 0.7102339872370598
epoch: 127, train loss: 0.3318862990047631, acc: 0.8085710903279271; test loss: 0.6545696902066589, acc: 0.7336327109430395
epoch: 128, train loss: 0.33226010225588887, acc: 0.8154966260210725; test loss: 0.6615108784364769, acc: 0.7073977783030017
epoch: 129, train loss: 0.3260486585805723, acc: 0.812714573221262; test loss: 0.6393630092686667, acc: 0.7234696289293311
epoch: 130, train loss: 0.3304920158182162, acc: 0.8134840771871671; test loss: 0.6710086330627049, acc: 0.7248877333963601
epoch: 131, train loss: 0.3294007608251796, acc: 0.8157925890848822; test loss: 0.6214961214455497, acc: 0.734341763176554
epoch: 132, train loss: 0.318993496534259, acc: 0.8146679294424056; test loss: 0.6586187338327522, acc: 0.7225242259513117
epoch: 133, train loss: 0.31419093934222625, acc: 0.8206463833313602; test loss: 0.659836490310357, acc: 0.7324509572205152
epoch: 134, train loss: 0.31254200336918175, acc: 0.8170356339528827; test loss: 0.6784367072218784, acc: 0.7177972110612149
epoch: 135, train loss: 0.31310659137169566, acc: 0.8203504202675506; test loss: 0.7406402624856493, acc: 0.6939257858662254
epoch: 136, train loss: 0.29783774898068305, acc: 0.8268024150586006; test loss: 0.6494165485673299, acc: 0.7383597258331364
epoch: 137, train loss: 0.3115748423570089, acc: 0.8196993015271694; test loss: 0.6242021963236265, acc: 0.7369416213661073
epoch: 138, train loss: 0.3220097774639064, acc: 0.8149046998934533; test loss: 0.6804322609803378, acc: 0.7182699125502245
epoch: 139, train loss: 0.3205056565180747, acc: 0.8144311589913579; test loss: 0.6305948462899879, acc: 0.7371779721106122
epoch: 140, train loss: 0.3168437500119675, acc: 0.818397064046407; test loss: 0.7026355037224882, acc: 0.7092885842590404
epoch: 141, train loss: 0.29499148114116797, acc: 0.8255001775778383; test loss: 0.6955132092237191, acc: 0.7286693453084377
epoch: 142, train loss: 0.30182149237156536, acc: 0.8246714809991713; test loss: 0.7023592728921578, acc: 0.7192153155282439
Epoch   142: reducing learning rate of group 0 to 7.5000e-04.
epoch: 143, train loss: 0.2625634830506353, acc: 0.8427252278915591; test loss: 0.6234515668426781, acc: 0.7518317182699126
epoch: 144, train loss: 0.23390126418676463, acc: 0.8533207055759441; test loss: 0.6463599565096201, acc: 0.7494682108248641
epoch: 145, train loss: 0.2301069246098969, acc: 0.853439090801468; test loss: 0.6618559624224063, acc: 0.7508863152918932
epoch: 146, train loss: 0.22164993701678867, acc: 0.8606605895584231; test loss: 0.7560393083351945, acc: 0.71756086031671
epoch: 147, train loss: 0.22899564424146204, acc: 0.8555700248608974; test loss: 0.6761166231763529, acc: 0.7426140392342235
epoch: 148, train loss: 0.22303533879020193, acc: 0.8571682254054694; test loss: 0.6650545189099457, acc: 0.7463956511463011
epoch: 149, train loss: 0.2101505727503756, acc: 0.8659287320942346; test loss: 0.6701678251774985, acc: 0.7508863152918932
epoch: 150, train loss: 0.22027437568303607, acc: 0.8592991594648988; test loss: 0.6848874474157821, acc: 0.7478137556133302
epoch: 151, train loss: 0.2145475410837894, acc: 0.8609565526222327; test loss: 0.7036751339379791, acc: 0.7440321437012527
epoch: 152, train loss: 0.2095845814931459, acc: 0.8642713389369007; test loss: 0.722299652047767, acc: 0.7411959347671945
epoch: 153, train loss: 0.21563728575496485, acc: 0.8659879247069966; test loss: 0.7368971227162613, acc: 0.7393051288111557
epoch: 154, train loss: 0.2082742614775448, acc: 0.8642121463241388; test loss: 0.7135623357054247, acc: 0.738832427322146
epoch: 155, train loss: 0.21102600928113294, acc: 0.8633834497454718; test loss: 0.6616932942607504, acc: 0.7478137556133302
epoch: 156, train loss: 0.20733733813381908, acc: 0.8641529537113768; test loss: 0.6983602581731931, acc: 0.7478137556133302
epoch: 157, train loss: 0.19961777806451395, acc: 0.8700722149875696; test loss: 0.7271383952598599, acc: 0.7322146064760104
epoch: 158, train loss: 0.20859168801905206, acc: 0.8651000355155677; test loss: 0.69737604672507, acc: 0.7397778303001654
epoch: 159, train loss: 0.2093436044109521, acc: 0.8641529537113768; test loss: 0.7091210548039786, acc: 0.7445048451902624
epoch: 160, train loss: 0.2104416592958426, acc: 0.8655735764176631; test loss: 0.7055617972937241, acc: 0.7454502481682818
epoch: 161, train loss: 0.20275571654608396, acc: 0.8652184207410916; test loss: 0.6975660467790222, acc: 0.7463956511463011
epoch: 162, train loss: 0.20263122803638203, acc: 0.8678820883153783; test loss: 0.7463878101559853, acc: 0.7362325691325927
epoch: 163, train loss: 0.19265064289298328, acc: 0.8703089854386172; test loss: 0.7084781122782415, acc: 0.7475774048688253
epoch: 164, train loss: 0.19227905355890473, acc: 0.8719071859831893; test loss: 0.7309133951276844, acc: 0.735759867643583
epoch: 165, train loss: 0.19305496388982143, acc: 0.870545755889665; test loss: 0.7111636270053334, acc: 0.7383597258331364
epoch: 166, train loss: 0.19042505184877578, acc: 0.8727950751746182; test loss: 0.7486150791052807, acc: 0.7440321437012527
epoch: 167, train loss: 0.1988707779109217, acc: 0.8684740144429975; test loss: 0.7463477125575824, acc: 0.7397778303001654
epoch: 168, train loss: 0.19108116040713014, acc: 0.8723807268852847; test loss: 0.747414050387367, acc: 0.7456865989127865
epoch: 169, train loss: 0.1896231439503972, acc: 0.871374452468332; test loss: 0.7515298326199739, acc: 0.7322146064760104
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.15298333101841605, acc: 0.8690659405706168; test loss: 0.6312892293287659, acc: 0.7381233750886316
epoch: 171, train loss: 0.12780525987919117, acc: 0.8796614182550018; test loss: 0.6616522903223743, acc: 0.7390687780666509
epoch: 172, train loss: 0.13046907670324026, acc: 0.8803717296081449; test loss: 0.6600951646637111, acc: 0.7341054124320492
epoch: 173, train loss: 0.13475888401068808, acc: 0.8764058245530958; test loss: 0.6345647050821029, acc: 0.7442684944457575
epoch: 174, train loss: 0.15065079185297126, acc: 0.8651000355155677; test loss: 0.6262089963275979, acc: 0.7352871661545733
epoch: 175, train loss: 0.14359258181620904, acc: 0.8693619036344264; test loss: 0.6426753887074181, acc: 0.7341054124320492
epoch: 176, train loss: 0.13399934306753108, acc: 0.8759914762637623; test loss: 0.6709976796420914, acc: 0.7326873079650201
epoch: 177, train loss: 0.1368193844232077, acc: 0.873801349591571; test loss: 0.646431799138925, acc: 0.7437957929567478
epoch: 178, train loss: 0.13411274234105725, acc: 0.8756363205871908; test loss: 0.6677630929286472, acc: 0.7352871661545733
epoch: 179, train loss: 0.13163407300052607, acc: 0.8769385580679531; test loss: 0.6630125279348811, acc: 0.7371779721106122
epoch: 180, train loss: 0.14067757936734568, acc: 0.872084763821475; test loss: 0.6652238960385576, acc: 0.718978964783739
epoch: 181, train loss: 0.1379734261125252, acc: 0.8743932757191902; test loss: 0.6381039087497943, acc: 0.7378870243441267
epoch: 182, train loss: 0.13235137550149484, acc: 0.873801349591571; test loss: 0.6569467296106712, acc: 0.7416686362562042
epoch: 183, train loss: 0.1303252442379922, acc: 0.8752219722978573; test loss: 0.6324962003099752, acc: 0.7428503899787284
epoch: 184, train loss: 0.12570331415100408, acc: 0.8791878773529064; test loss: 0.6458331130689775, acc: 0.7371779721106122
epoch: 185, train loss: 0.13064374585849964, acc: 0.8764650171658577; test loss: 0.6955401708093046, acc: 0.7260694871188844
epoch: 186, train loss: 0.1671857636603178, acc: 0.8563987214395643; test loss: 0.7165317169967991, acc: 0.7104703379815647
epoch: 187, train loss: 0.1642896420111441, acc: 0.8552148691843258; test loss: 0.6342593560796966, acc: 0.7352871661545733
epoch: 188, train loss: 0.13952338592327598, acc: 0.8669941991239494; test loss: 0.6421129740431246, acc: 0.7305601512644765
epoch: 189, train loss: 0.1436142568191269, acc: 0.8678228957026163; test loss: 0.6020632098397868, acc: 0.7471047033798156
epoch: 190, train loss: 0.13251880357445267, acc: 0.872084763821475; test loss: 0.6474446004458675, acc: 0.7400141810446703
epoch: 191, train loss: 0.1304089856088493, acc: 0.8738605422043328; test loss: 0.6856634088003222, acc: 0.7362325691325927
epoch: 192, train loss: 0.12777077425255837, acc: 0.875399550136143; test loss: 0.6485491984844546, acc: 0.7497045615693689
epoch: 193, train loss: 0.1297980570519367, acc: 0.8730910382384278; test loss: 0.6512906260029471, acc: 0.7352871661545733
Epoch   193: reducing learning rate of group 0 to 3.7500e-04.
epoch: 194, train loss: 0.1129605340408308, acc: 0.8854622943056707; test loss: 0.6451182076058132, acc: 0.7485228078468447
epoch: 195, train loss: 0.09473735728079827, acc: 0.8979519355984373; test loss: 0.6958737461540441, acc: 0.7411959347671945
epoch: 196, train loss: 0.09163997703979068, acc: 0.8999644844323429; test loss: 0.6831699168251018, acc: 0.7452138974237769
epoch: 197, train loss: 0.09193371429137963, acc: 0.899017402628152; test loss: 0.6905851275383849, acc: 0.7494682108248641
epoch: 198, train loss: 0.08661681977890646, acc: 0.9038120042618681; test loss: 0.6942156258555795, acc: 0.7471047033798156
epoch: 199, train loss: 0.08630618565221136, acc: 0.9036344264235824; test loss: 0.7154018325294003, acc: 0.7475774048688253
epoch: 200, train loss: 0.0857816710397804, acc: 0.9044631230022493; test loss: 0.7010863428278133, acc: 0.7508863152918932
best test acc 0.7518317182699126 at epoch 143.
****************************************************************
/opt/python/anaconda-2020.7/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
train report:
              precision    recall  f1-score   support

           0     0.9139    0.9484    0.9308      6100
           1     0.9492    0.8877    0.9174       926
           2     0.8427    0.9150    0.8773      2400
           3     0.9129    0.8826    0.8975       843
           4     0.8901    0.9522    0.9201       774
           5     0.9263    0.9392    0.9327      1512
           6     0.7348    0.7707    0.7523      1330
           7     0.8946    0.7235    0.8000       481
           8     0.8288    0.8559    0.8421       458
           9     0.8516    0.9646    0.9046       452
          10     0.9201    0.8675    0.8930       717
          11     0.8646    0.7477    0.8019       333
          12     0.0000    0.0000    0.0000       299
          13     0.9235    0.6283    0.7478       269

    accuracy                         0.8846     16894
   macro avg     0.8181    0.7917    0.8013     16894
weighted avg     0.8703    0.8846    0.8761     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8152    0.8820    0.8472      1525
           1     0.8206    0.7888    0.8044       232
           2     0.7179    0.7537    0.7354       601
           3     0.7929    0.7441    0.7677       211
           4     0.8235    0.8660    0.8442       194
           5     0.7818    0.7487    0.7649       378
           6     0.5078    0.5886    0.5452       333
           7     0.7901    0.5289    0.6337       121
           8     0.5812    0.5913    0.5862       115
           9     0.7520    0.8246    0.7866       114
          10     0.7707    0.6722    0.7181       180
          11     0.4412    0.3571    0.3947        84
          12     0.0000    0.0000    0.0000        75
          13     0.6552    0.2794    0.3918        68

    accuracy                         0.7518      4231
   macro avg     0.6607    0.6161    0.6300      4231
weighted avg     0.7386    0.7518    0.7421      4231

---------------------------------------
program finished.
