save trained model at:  ../trained_models/trained_model_63.pt
save loss at:  ./results/train_results_63.json
how to merge clusters:  [[0, 9, 12], [1, 5, 11], 2, [3, 8, 13], 4, 6, 7, [10, 16], 15, 17, 18]
positive training pair sampling threshold:  16000
negative training pair sampling threshold:  4800
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs to train: 50
learning rate decay to half at epoch 25.
number of workers to load data:  36
device:  cuda
number of classes after merging:  11
number of pockets in training set:  14097
number of pockets in validation set:  3016
number of pockets in test set:  3031
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['6brxA00', '4c0lA00', '3rs8A02', '4nk4F00', '5fogD00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['4d86A00', '4u00A00', '1pujA00', '3nt5A00', '4j1nB01']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['4k28A00', '6hwlB00', '3upqA01', '2p2bA00', '1h5rB02']
number of train positive pairs: 176000
number of train negative pairs: 264000
number of epochs to train for hard pairs:  100
learning rate decay at epoch for hard pairs:  40
begin to select hard pairs at epoch 1
batch size for hard pairs:  128
number of hardest positive pairs for each mini-batch:  192
number of hardest negative pairs for each mini-batch:  256

*******************************************************
             train by random pairs
*******************************************************
model architecture:
SiameseNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 1.005223093518344, train acc: 0.5603319855288359, validation acc: 0.4041777188328912.
epoch: 2, train loss: 0.9442841207677668, train acc: 0.6740441228630205, validation acc: 0.5397877984084881.
epoch: 3, train loss: 0.7128047867861661, train acc: 0.7796694332127403, validation acc: 0.6906498673740054.
epoch: 4, train loss: 0.5957576117775657, train acc: 0.7887493793005604, validation acc: 0.705238726790451.
epoch: 5, train loss: 0.5431273941386829, train acc: 0.7880400085124495, validation acc: 0.6913129973474801.
epoch: 6, train loss: 0.517518024895408, train acc: 0.8031496062992126, validation acc: 0.6956233421750663.
epoch: 7, train loss: 0.49234388725974343, train acc: 0.8166985883521316, validation acc: 0.6976127320954907.
epoch: 8, train loss: 0.47416357701041484, train acc: 0.8041427254025679, validation acc: 0.6962864721485411.
epoch: 9, train loss: 0.46481341079365124, train acc: 0.8112364332836773, validation acc: 0.6979442970822282.
epoch: 10, train loss: 0.45283238435225054, train acc: 0.8153507838547208, validation acc: 0.6992705570291777.
epoch: 11, train loss: 0.4452523729324341, train acc: 0.8125842377810881, validation acc: 0.7025862068965517.
epoch: 12, train loss: 0.4402341770172119, train acc: 0.8062708377669008, validation acc: 0.6850132625994695.
epoch: 13, train loss: 0.435889950058677, train acc: 0.8179754557707314, validation acc: 0.7029177718832891.
epoch: 14, train loss: 0.4412083632555875, train acc: 0.8261332198340072, validation acc: 0.7022546419098143.
epoch: 15, train loss: 0.42641901789578524, train acc: 0.8226573029722636, validation acc: 0.7012599469496021.
epoch: 16, train loss: 0.4275858673095703, train acc: 0.8170532737461871, validation acc: 0.7035809018567639.
epoch: 17, train loss: 0.4222748209173029, train acc: 0.8298928850109952, validation acc: 0.7128647214854111.
epoch: 18, train loss: 0.42525556914589624, train acc: 0.8318791232177059, validation acc: 0.7082228116710876.
epoch: 19, train loss: 0.42148732098666103, train acc: 0.8277647726466624, validation acc: 0.7039124668435013.
epoch: 20, train loss: 0.412167301160639, train acc: 0.8254238490458963, validation acc: 0.6946286472148541.
epoch: 21, train loss: 0.41803329682783646, train acc: 0.8168404625097538, validation acc: 0.6946286472148541.
epoch: 22, train loss: 0.41452826564095235, train acc: 0.8311697524295949, validation acc: 0.7115384615384616.
epoch: 23, train loss: 0.41798320666226474, train acc: 0.8315244378236504, validation acc: 0.7078912466843501.
epoch: 24, train loss: 0.415619828414917, train acc: 0.8370575299709158, validation acc: 0.7072281167108754.
epoch: 25, train loss: 0.3655924390446056, train acc: 0.8494005816840462, validation acc: 0.7178381962864722.
epoch: 26, train loss: 0.3655173376776955, train acc: 0.859686458111655, validation acc: 0.7171750663129973.
epoch: 27, train loss: 0.36210414674932306, train acc: 0.8336525501879832, validation acc: 0.7082228116710876.
epoch: 28, train loss: 0.36245704879760743, train acc: 0.8540114918067674, validation acc: 0.7181697612732095.
epoch: 29, train loss: 0.3619918717471036, train acc: 0.8515996311271902, validation acc: 0.7092175066312998.
epoch: 30, train loss: 0.355843394574252, train acc: 0.8545789884372561, validation acc: 0.7178381962864722.
epoch: 31, train loss: 0.3613022954940796, train acc: 0.8482655884230688, validation acc: 0.7082228116710876.
epoch: 32, train loss: 0.35836688948544587, train acc: 0.8472724693197135, validation acc: 0.7055702917771883.
epoch: 33, train loss: 0.35698238594748755, train acc: 0.8539405547279563, validation acc: 0.7194960212201591.
epoch: 34, train loss: 0.3603990514235063, train acc: 0.8524508760729234, validation acc: 0.7208222811671088.
epoch: 35, train loss: 0.3535345687346025, train acc: 0.8558558558558559, validation acc: 0.7241379310344828.
epoch: 36, train loss: 0.3515513409527865, train acc: 0.8579130311413776, validation acc: 0.7330901856763926.
epoch: 37, train loss: 0.35218366089734165, train acc: 0.8547208625948783, validation acc: 0.7228116710875332.
epoch: 38, train loss: 0.35036707155054264, train acc: 0.8551464850677449, validation acc: 0.7191644562334217.
epoch: 39, train loss: 0.3507351698615334, train acc: 0.8555721075406115, validation acc: 0.7098806366047745.
epoch: 40, train loss: 0.34819494106986304, train acc: 0.859757395190466, validation acc: 0.7112068965517241.
epoch: 41, train loss: 0.3474844236373901, train acc: 0.8513158828119458, validation acc: 0.7122015915119363.
epoch: 42, train loss: 0.35027654661698776, train acc: 0.8633042491310208, validation acc: 0.7294429708222812.
epoch: 43, train loss: 0.345389839033647, train acc: 0.8546499255160672, validation acc: 0.7075596816976127.
epoch: 44, train loss: 0.3503940130667253, train acc: 0.8569199120380223, validation acc: 0.7204907161803713.
epoch: 45, train loss: 0.34523252155997536, train acc: 0.867205788465631, validation acc: 0.726790450928382.
epoch: 46, train loss: 0.34429411288174716, train acc: 0.8584095906930552, validation acc: 0.7224801061007957.
epoch: 47, train loss: 0.34479656187404284, train acc: 0.852947435624601, validation acc: 0.7175066312997348.
epoch: 48, train loss: 0.34542692316228696, train acc: 0.851528694048379, validation acc: 0.7112068965517241.
epoch: 49, train loss: 0.3421122446753762, train acc: 0.8521671277576789, validation acc: 0.7082228116710876.
epoch: 50, train loss: 0.34137481262900615, train acc: 0.8551464850677449, validation acc: 0.7115384615384616.
best validation acc 0.7330901856763926 at epoch 36.


*******************************************************
             train by random pairs
*******************************************************
model architecture:
SelectiveSiameseNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
SelectiveContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, num_pos_pair=192, num_neg_pair=256)
epoch: 1, train loss: 1.698861148336966, train acc: 0.7712279208342201, validation acc: 0.65815649867374.
epoch: 2, train loss: 1.4009579212421168, train acc: 0.7893168759310492, validation acc: 0.6856763925729443.
epoch: 3, train loss: 1.3490349793085807, train acc: 0.8043555366390012, validation acc: 0.7055702917771883.
epoch: 4, train loss: 1.3182321536209718, train acc: 0.810952684968433, validation acc: 0.7045755968169761.
epoch: 5, train loss: 1.2982847505202086, train acc: 0.8189685748740867, validation acc: 0.7228116710875332.
epoch: 6, train loss: 1.2845698404288406, train acc: 0.8243597928637298, validation acc: 0.7344164456233422.
epoch: 7, train loss: 1.2713374305555556, train acc: 0.8314535007448394, validation acc: 0.725132625994695.
epoch: 8, train loss: 1.2640725788816753, train acc: 0.8360644108675604, validation acc: 0.7228116710875332.
epoch: 9, train loss: 1.2531730307974043, train acc: 0.8388309569411931, validation acc: 0.7360742705570292.
epoch: 10, train loss: 1.2488196544609944, train acc: 0.8415265659360147, validation acc: 0.7377320954907162.
epoch: 11, train loss: 1.2415200261528658, train acc: 0.8511740086543236, validation acc: 0.7476790450928382.
epoch: 12, train loss: 1.24020959679746, train acc: 0.8383343973895155, validation acc: 0.736737400530504.
epoch: 13, train loss: 1.234188418119738, train acc: 0.8452152940341917, validation acc: 0.7420424403183024.
epoch: 14, train loss: 1.2293380434415475, train acc: 0.8494715187628573, validation acc: 0.7453580901856764.
epoch: 15, train loss: 1.227107610523995, train acc: 0.8373412782861602, validation acc: 0.7446949602122016.
epoch: 16, train loss: 1.2277282428405785, train acc: 0.8479109030290133, validation acc: 0.751657824933687.
epoch: 17, train loss: 1.2241234430080665, train acc: 0.8408171951479038, validation acc: 0.735079575596817.
epoch: 18, train loss: 1.2242081142781445, train acc: 0.8443640490884585, validation acc: 0.7364058355437666.
epoch: 19, train loss: 1.2213572398882488, train acc: 0.8425906221181811, validation acc: 0.7403846153846154.
epoch: 20, train loss: 1.2247666463081683, train acc: 0.846917783925658, validation acc: 0.738395225464191.
epoch: 21, train loss: 1.2196490809589773, train acc: 0.8449315457189472, validation acc: 0.7490053050397878.
epoch: 22, train loss: 1.218611885391947, train acc: 0.8439384266155919, validation acc: 0.7430371352785146.
epoch: 23, train loss: 1.2243518166591862, train acc: 0.845428105270625, validation acc: 0.7387267904509284.
epoch: 24, train loss: 1.218087903173514, train acc: 0.8422359367241257, validation acc: 0.7423740053050398.
epoch: 25, train loss: 1.2162778972435688, train acc: 0.84315811874867, validation acc: 0.7539787798408488.
epoch: 26, train loss: 1.2106182224916902, train acc: 0.8495424558416684, validation acc: 0.7446949602122016.
epoch: 27, train loss: 1.2150016880089618, train acc: 0.8384762715471377, validation acc: 0.7311007957559682.
epoch: 28, train loss: 1.2150246006087984, train acc: 0.8432999929062921, validation acc: 0.7347480106100795.
epoch: 29, train loss: 1.2131385624332491, train acc: 0.8389018940200043, validation acc: 0.7287798408488063.
epoch: 30, train loss: 1.217110983231369, train acc: 0.8467759097680357, validation acc: 0.7390583554376657.
epoch: 31, train loss: 1.2152644939661297, train acc: 0.8393275164928709, validation acc: 0.735079575596817.
epoch: 32, train loss: 1.2122941975975063, train acc: 0.8440803007732142, validation acc: 0.745026525198939.
epoch: 33, train loss: 1.2148407699499801, train acc: 0.8445768603248919, validation acc: 0.7493368700265252.
epoch: 34, train loss: 1.2095736855697239, train acc: 0.8494005816840462, validation acc: 0.7513262599469496.
epoch: 35, train loss: 1.2083521378380717, train acc: 0.8451443569553806, validation acc: 0.7539787798408488.
epoch: 36, train loss: 1.209283768640064, train acc: 0.8461374760587359, validation acc: 0.735079575596817.
epoch: 37, train loss: 1.2103391119878237, train acc: 0.8411718805419592, validation acc: 0.7423740053050398.
epoch: 38, train loss: 1.2150226394170232, train acc: 0.8390437681776265, validation acc: 0.7403846153846154.
epoch: 39, train loss: 1.2140846401611936, train acc: 0.8493296446052352, validation acc: 0.7413793103448276.
epoch: 40, train loss: 1.198283505908309, train acc: 0.8550046109101227, validation acc: 0.7539787798408488.
epoch: 41, train loss: 1.1916230357205309, train acc: 0.8611761367666879, validation acc: 0.7539787798408488.
epoch: 42, train loss: 1.1907158746623783, train acc: 0.8608923884514436, validation acc: 0.751657824933687.
epoch: 43, train loss: 1.188824508661027, train acc: 0.8691210895935305, validation acc: 0.7536472148541115.
epoch: 44, train loss: 1.1857512636337144, train acc: 0.8632333120522097, validation acc: 0.75.
epoch: 45, train loss: 1.1842903840449495, train acc: 0.8598283322692771, validation acc: 0.7553050397877984.
epoch: 46, train loss: 1.1844100626847363, train acc: 0.8689792154359084, validation acc: 0.7635941644562334.
epoch: 47, train loss: 1.1849253930383201, train acc: 0.86195644463361, validation acc: 0.7592838196286472.
epoch: 48, train loss: 1.183643210532759, train acc: 0.8661417322834646, validation acc: 0.7589522546419099.
epoch: 49, train loss: 1.1804102499333806, train acc: 0.863517060367454, validation acc: 0.7509946949602122.
epoch: 50, train loss: 1.1830914976178846, train acc: 0.8670639143080088, validation acc: 0.761604774535809.
epoch: 51, train loss: 1.1813218482389656, train acc: 0.8647229907072427, validation acc: 0.758289124668435.
epoch: 52, train loss: 1.1812246150863994, train acc: 0.8702560828545081, validation acc: 0.7635941644562334.
epoch: 53, train loss: 1.180667210923752, train acc: 0.8655742356529759, validation acc: 0.7562997347480106.
epoch: 54, train loss: 1.1812177957243761, train acc: 0.8645811165496204, validation acc: 0.7586206896551724.
epoch: 55, train loss: 1.180956121209791, train acc: 0.8689082783570973, validation acc: 0.7622679045092838.
epoch: 56, train loss: 1.1766782915057459, train acc: 0.8656451727317869, validation acc: 0.7679045092838196.
epoch: 57, train loss: 1.1770334693419402, train acc: 0.8674895367808754, validation acc: 0.7572944297082228.
epoch: 58, train loss: 1.1797744205438792, train acc: 0.8680570334113641, validation acc: 0.7546419098143236.
epoch: 59, train loss: 1.17873515611479, train acc: 0.8667801659927644, validation acc: 0.7599469496021221.
epoch: 60, train loss: 1.1758776668022068, train acc: 0.8721004469035966, validation acc: 0.761604774535809.
epoch: 61, train loss: 1.1756024933430544, train acc: 0.8722423210612187, validation acc: 0.758289124668435.
epoch: 62, train loss: 1.1788970215104073, train acc: 0.8690501525147194, validation acc: 0.7503315649867374.
epoch: 63, train loss: 1.1759439797036089, train acc: 0.8723132581400298, validation acc: 0.761604774535809.
epoch: 64, train loss: 1.1752628307032804, train acc: 0.8677732850961197, validation acc: 0.7546419098143236.
epoch: 65, train loss: 1.173756111401472, train acc: 0.8727388806128964, validation acc: 0.7635941644562334.
epoch: 66, train loss: 1.1720874023003456, train acc: 0.8767822941051288, validation acc: 0.764920424403183.
epoch: 67, train loss: 1.1720785069203807, train acc: 0.8713201390366745, validation acc: 0.7569628647214854.
epoch: 68, train loss: 1.172696134228727, train acc: 0.8727388806128964, validation acc: 0.751657824933687.
epoch: 69, train loss: 1.1718970172828005, train acc: 0.8759310491593956, validation acc: 0.7652519893899205.
epoch: 70, train loss: 1.1748591845685785, train acc: 0.8695467120663971, validation acc: 0.7440318302387268.
epoch: 71, train loss: 1.1736424857999983, train acc: 0.8761438603958289, validation acc: 0.7645888594164456.
epoch: 72, train loss: 1.17300931793848, train acc: 0.8768532311839399, validation acc: 0.7655835543766578.
epoch: 73, train loss: 1.1711699750678926, train acc: 0.8710363907214301, validation acc: 0.7509946949602122.
epoch: 74, train loss: 1.176758269797705, train acc: 0.8745832446619848, validation acc: 0.761604774535809.
epoch: 75, train loss: 1.1701458364293196, train acc: 0.8753635525289069, validation acc: 0.7635941644562334.
epoch: 76, train loss: 1.1678238826011544, train acc: 0.8776335390508618, validation acc: 0.7622679045092838.
epoch: 77, train loss: 1.169808378586077, train acc: 0.8708235794849968, validation acc: 0.76657824933687.
epoch: 78, train loss: 1.168687014269201, train acc: 0.8801163368092502, validation acc: 0.761604774535809.
epoch: 79, train loss: 1.1707162227232404, train acc: 0.8757182379229623, validation acc: 0.7682360742705571.
epoch: 80, train loss: 1.1675370732703074, train acc: 0.8811803929914166, validation acc: 0.7655835543766578.
epoch: 81, train loss: 1.1670125415920787, train acc: 0.8767822941051288, validation acc: 0.7476790450928382.
epoch: 82, train loss: 1.1693227671553343, train acc: 0.8805419592821168, validation acc: 0.7695623342175066.
epoch: 83, train loss: 1.1671132072393438, train acc: 0.8773497907356175, validation acc: 0.7556366047745358.
epoch: 84, train loss: 1.1681087290326695, train acc: 0.8771369794991842, validation acc: 0.7526525198938993.
epoch: 85, train loss: 1.1629004377302692, train acc: 0.879194154784706, validation acc: 0.763262599469496.
epoch: 86, train loss: 1.164420072966293, train acc: 0.8760729233170178, validation acc: 0.761604774535809.
epoch: 87, train loss: 1.1647154270886342, train acc: 0.8840888132226715, validation acc: 0.7572944297082228.
epoch: 88, train loss: 1.166898964573918, train acc: 0.8786975952330283, validation acc: 0.7592838196286472.
epoch: 89, train loss: 1.1640064878357992, train acc: 0.8823863233312053, validation acc: 0.771551724137931.
epoch: 90, train loss: 1.166223989499914, train acc: 0.8790522806270837, validation acc: 0.7639257294429708.
epoch: 91, train loss: 1.1644867911000485, train acc: 0.8787685323118394, validation acc: 0.7506631299734748.
epoch: 92, train loss: 1.1644550711678943, train acc: 0.8862169255870044, validation acc: 0.7688992042440318.
epoch: 93, train loss: 1.163651455157901, train acc: 0.8728807547705185, validation acc: 0.7652519893899205.
epoch: 94, train loss: 1.16166780092121, train acc: 0.8816769525430943, validation acc: 0.7596153846153846.
epoch: 95, train loss: 1.1608864199831161, train acc: 0.8821735120947719, validation acc: 0.7625994694960212.
epoch: 96, train loss: 1.1626432221810548, train acc: 0.8854366177200823, validation acc: 0.7559681697612732.
epoch: 97, train loss: 1.1679331994664035, train acc: 0.8779882244449173, validation acc: 0.7609416445623343.
epoch: 98, train loss: 1.1601944273125293, train acc: 0.879974462651628, validation acc: 0.7519893899204244.
epoch: 99, train loss: 1.1568883222174382, train acc: 0.8828828828828829, validation acc: 0.751657824933687.
epoch: 100, train loss: 1.162330717192609, train acc: 0.8765694828686955, validation acc: 0.743368700265252.
best validation acc 0.771551724137931 at epoch 89.

*******************************************************
             k-nearest neighbor for testing
*******************************************************
train accuracy: 0.8765694828686955, validation accuracy: 0.743368700265252, test accuracy: 0.745298581326295
train report:
              precision    recall  f1-score   support

           0     0.8843    0.9606    0.9208      5074
           1     0.7959    0.7514    0.7730      2200
           2     0.9596    0.9667    0.9631       810
           3     0.8050    0.8413    0.8227      1840
           4     0.9490    0.9091    0.9286       737
           5     0.9751    0.9823    0.9787       677
           6     0.8864    0.8738    0.8801       634
           7     0.8710    0.6251    0.7279       907
           8     0.9684    0.8741    0.9189       421
           9     0.9108    0.7382    0.8154       401
          10     0.9475    0.9571    0.9523       396

    accuracy                         0.8766     14097
   macro avg     0.9048    0.8618    0.8801     14097
weighted avg     0.8765    0.8766    0.8741     14097

validation report:
              precision    recall  f1-score   support

           0     0.7843    0.8997    0.8380      1087
           1     0.6200    0.5648    0.5911       471
           2     0.7910    0.8092    0.8000       173
           3     0.6390    0.6827    0.6601       394
           4     0.8333    0.7911    0.8117       158
           5     0.8712    0.7931    0.8303       145
           6     0.7164    0.7111    0.7138       135
           7     0.6610    0.4021    0.5000       194
           8     0.8421    0.7111    0.7711        90
           9     0.7759    0.5294    0.6294        85
          10     0.8919    0.7857    0.8354        84

    accuracy                         0.7434      3016
   macro avg     0.7660    0.6982    0.7255      3016
weighted avg     0.7403    0.7434    0.7370      3016

test report: 
              precision    recall  f1-score   support

           0     0.7810    0.9044    0.8382      1088
           1     0.6250    0.5932    0.6087       472
           2     0.8380    0.8571    0.8475       175
           3     0.6667    0.7038    0.6847       395
           4     0.8321    0.6855    0.7517       159
           5     0.8662    0.8425    0.8542       146
           6     0.7087    0.6569    0.6818       137
           7     0.6378    0.4154    0.5031       195
           8     0.8684    0.7253    0.7904        91
           9     0.6977    0.3448    0.4615        87
          10     0.8395    0.7907    0.8144        86

    accuracy                         0.7453      3031
   macro avg     0.7601    0.6836    0.7124      3031
weighted avg     0.7413    0.7453    0.7377      3031

generating embeddings for train...
embedding path:  ../embeddings/run_63/train_embedding.npy
label path:  ../embeddings/run_63/train_label.npy
shape of generated embedding: (14097, 128)
shape of label: (14097,)
generating embeddings for val...
embedding path:  ../embeddings/run_63/val_embedding.npy
label path:  ../embeddings/run_63/val_label.npy
shape of generated embedding: (3016, 128)
shape of label: (3016,)
generating embeddings for test...
embedding path:  ../embeddings/run_63/test_embedding.npy
label path:  ../embeddings/run_63/test_label.npy
shape of generated embedding: (3031, 128)
shape of label: (3031,)
