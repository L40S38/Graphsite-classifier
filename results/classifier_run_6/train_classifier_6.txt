seed:  666
save trained model at:  ../trained_models/trained_classifier_model_6.pt
save loss at:  ./results/train_classifier_results_6.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  100
learning rate decay at epoch:  50
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(80, 160)
  )
  (fc1): Linear(in_features=160, out_features=80, bias=True)
  (fc2): Linear(in_features=80, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [30, 70]
loss function:
FocalLoss(gamma=0, alpha=1, reduction=mean)
begin training...
epoch: 1, train loss: 2.3347462200825806, acc: 0.3237225235512418, val loss: 2.102137724303996, acc: 0.3637576728049106, test loss: 2.116582997909659, acc: 0.3571049840933192
epoch: 2, train loss: 2.0957629954239794, acc: 0.37893234370539536, val loss: 2.245895461460925, acc: 0.36509207365892715, test loss: 2.266670414650554, acc: 0.36187698833510074
epoch: 3, train loss: 1.8998615024500223, acc: 0.42711961176134744, val loss: 2.061733586520299, acc: 0.39418201227648786, test loss: 2.050037309195558, acc: 0.40084835630965004
epoch: 4, train loss: 1.7912815779779354, acc: 0.4518412789037968, val loss: 1.8068473732309211, acc: 0.44382172404590337, test loss: 1.7893535408210148, acc: 0.45652173913043476
epoch: 5, train loss: 1.7341422052748232, acc: 0.47553525549528974, val loss: 2.264320541707108, acc: 0.33947157726180943, test loss: 2.268042854135135, acc: 0.3348356309650053
epoch: 6, train loss: 1.690005059886108, acc: 0.490493862403654, val loss: 1.6883046452700694, acc: 0.4809180677875634, test loss: 1.6614770808376664, acc: 0.492576882290562
epoch: 7, train loss: 1.6160985840547775, acc: 0.5117898943762489, val loss: 1.75715767099662, acc: 0.46277021617293834, test loss: 1.730427981434293, acc: 0.4745493107104984
epoch: 8, train loss: 1.5798587711613552, acc: 0.5203539823008849, val loss: 1.6887995958773652, acc: 0.4675740592473979, test loss: 1.6840215346213996, acc: 0.47481442205726404
epoch: 9, train loss: 1.5523826419235194, acc: 0.5276620039965744, val loss: 1.7837896456488107, acc: 0.48385374966639977, test loss: 1.7760109208674344, acc: 0.49072110286320253
epoch: 10, train loss: 1.5139965313963981, acc: 0.5393091635740793, val loss: 1.6619714106245935, acc: 0.4795836669335468, test loss: 1.6375881523241547, acc: 0.4923117709437964
epoch: 11, train loss: 1.494790367497535, acc: 0.542449329146446, val loss: 1.521557909410923, acc: 0.5358953829730451, test loss: 1.51870171818981, acc: 0.5307529162248145
epoch: 12, train loss: 1.471174612842966, acc: 0.5515272623465601, val loss: 1.4651543397982343, acc: 0.5503069121964238, test loss: 1.4658868391212956, acc: 0.5461293743372216
epoch: 13, train loss: 1.4466242157524734, acc: 0.5567799029403369, val loss: 1.6176307960481748, acc: 0.5161462503336002, test loss: 1.5957152995173272, acc: 0.517762460233298
epoch: 14, train loss: 1.41911147822321, acc: 0.5655723665429632, val loss: 1.4334546361632559, acc: 0.5564451561248999, test loss: 1.4356538152138476, acc: 0.563626723223754
epoch: 15, train loss: 1.392253791744832, acc: 0.5733371395946332, val loss: 1.396267813048046, acc: 0.5687216439818521, test loss: 1.4097390751317855, acc: 0.5641569459172853
epoch: 16, train loss: 1.3822082080604212, acc: 0.578989437624893, val loss: 1.4316282673839063, acc: 0.5631171603949826, test loss: 1.4221614269284908, acc: 0.5567338282078473
epoch: 17, train loss: 1.3603148452697262, acc: 0.5865258349985727, val loss: 1.372451629580133, acc: 0.5753936482519348, test loss: 1.356838853083638, acc: 0.5851007423117709
epoch: 18, train loss: 1.3524157523019906, acc: 0.5881815586640022, val loss: 1.387106386222361, acc: 0.5751267680811316, test loss: 1.383789991144158, acc: 0.5697242841993637
epoch: 19, train loss: 1.3459649392138064, acc: 0.5872680559520411, val loss: 1.4208084896847124, acc: 0.5604483586869495, test loss: 1.3983316105471435, acc: 0.5750265111346765
epoch: 20, train loss: 1.3209466956187887, acc: 0.6011418783899515, val loss: 1.4376975131537522, acc: 0.5612489991993594, test loss: 1.4363337930920015, acc: 0.559119830328738
epoch: 21, train loss: 1.2808534252279458, acc: 0.6082786183271481, val loss: 1.3587525575549755, acc: 0.5740592473979184, test loss: 1.3442157232622327, acc: 0.5893425238600212
epoch: 22, train loss: 1.282949083810393, acc: 0.6086782757636312, val loss: 1.3967838632223732, acc: 0.5644515612489992, test loss: 1.3519495547386646, acc: 0.5827147401908802
epoch: 23, train loss: 1.281496980283521, acc: 0.6117613474165001, val loss: 1.4221752011238304, acc: 0.5831331732052308, test loss: 1.4287370624117481, acc: 0.584305408271474
epoch: 24, train loss: 1.247925801274438, acc: 0.6226662860405366, val loss: 1.4634098392631139, acc: 0.5425673872431278, test loss: 1.459953124550974, acc: 0.5501060445387063
epoch: 25, train loss: 1.2319935569142464, acc: 0.6272908935198401, val loss: 1.2688211342923827, acc: 0.6082199092607419, test loss: 1.2637645588952688, acc: 0.6158536585365854
epoch: 26, train loss: 1.219657924891267, acc: 0.6280902083928062, val loss: 1.4418052860282407, acc: 0.5503069121964238, test loss: 1.4373784816151334, acc: 0.5596500530222693
epoch: 27, train loss: 1.2306104705975527, acc: 0.6262632029688838, val loss: 1.3069954132251114, acc: 0.5855350947424607, test loss: 1.3131264636524156, acc: 0.5882820784729587
epoch: 28, train loss: 1.1941234366266924, acc: 0.6360262632029688, val loss: 1.3403024315229568, acc: 0.5842006938884441, test loss: 1.325577182961926, acc: 0.5927889713679746
epoch: 29, train loss: 1.1952213691977682, acc: 0.6382529260633742, val loss: 1.3375089781933986, acc: 0.5954096610621831, test loss: 1.3184482065263508, acc: 0.5967656415694592
epoch 30, gamma increased to 1.
epoch: 30, train loss: 0.967078829111115, acc: 0.6424207821866971, val loss: 1.0778675719455366, acc: 0.5930077395249533, test loss: 1.0668366830144176, acc: 0.6036585365853658
epoch: 31, train loss: 0.9459606818495361, acc: 0.6456180416785612, val loss: 1.3080006565830056, acc: 0.5369629036562583, test loss: 1.2952101855111147, acc: 0.5416224814422057
epoch: 32, train loss: 0.9365974814933332, acc: 0.649272052526406, val loss: 0.9561895921384935, acc: 0.6418468107819589, test loss: 0.9511239963523404, acc: 0.6394485683987274
epoch: 33, train loss: 0.9347643999348291, acc: 0.6467599200685127, val loss: 1.0348567294539979, acc: 0.618628235922071, test loss: 1.0251493775073444, acc: 0.6224814422057264
epoch: 34, train loss: 0.9470895883018141, acc: 0.6417356551527262, val loss: 1.0271061041974245, acc: 0.6132906325060048, test loss: 1.0222565058306696, acc: 0.6142629904559915
epoch: 35, train loss: 0.912068068674895, acc: 0.6558949471881245, val loss: 1.0160372805716293, acc: 0.6258340005337604, test loss: 1.005184765711569, acc: 0.626458112407211
epoch: 36, train loss: 0.9046990556782257, acc: 0.6597202397944619, val loss: 1.1653870971863958, acc: 0.5922070990125433, test loss: 1.1709131264105075, acc: 0.584305408271474
epoch: 37, train loss: 0.9045384863870606, acc: 0.6574935769340565, val loss: 1.2942658775101543, acc: 0.5310915398985855, test loss: 1.281694701216001, acc: 0.532608695652174
epoch: 38, train loss: 0.8831032252264064, acc: 0.664801598629746, val loss: 0.9867221354674682, acc: 0.6327728849746463, test loss: 0.9770336082858979, acc: 0.6391834570519618
epoch: 39, train loss: 0.8780319861333914, acc: 0.671881244647445, val loss: 0.9917635672691189, acc: 0.622898318654924, test loss: 0.9835197892699742, acc: 0.6285790031813362
epoch: 40, train loss: 0.8777001621312766, acc: 0.6670853554096489, val loss: 1.0150491946978604, acc: 0.6269015212169736, test loss: 0.9951724884745172, acc: 0.6336161187698833
epoch: 41, train loss: 0.8605470917921151, acc: 0.6730231230373965, val loss: 0.9927930656292612, acc: 0.6399786495863358, test loss: 0.9954131395414804, acc: 0.6352067868504772
epoch: 42, train loss: 0.8373391227997136, acc: 0.6795889237796174, val loss: 1.0655155199064648, acc: 0.618628235922071, test loss: 1.0767305147609934, acc: 0.6139978791092259
epoch: 43, train loss: 0.8527566794766517, acc: 0.6757065372537825, val loss: 1.2636881669998679, acc: 0.5852682145716573, test loss: 1.277994336657155, acc: 0.5896076352067868
epoch: 44, train loss: 0.8609477213100811, acc: 0.6725092777619184, val loss: 0.9642709532132746, acc: 0.6322391246330398, test loss: 0.9446044195747578, acc: 0.6508483563096501
epoch: 45, train loss: 0.8385914942178936, acc: 0.6783899514701685, val loss: 1.0365766562046228, acc: 0.6338404056578596, test loss: 1.058744218164891, acc: 0.6232767762460233
epoch: 46, train loss: 0.8196754049717138, acc: 0.6851841278903796, val loss: 0.9318790125280563, acc: 0.6533226581265013, test loss: 0.9208677978050418, acc: 0.6436903499469777
epoch: 47, train loss: 0.8050867173728213, acc: 0.6896945475306879, val loss: 1.0354581202447717, acc: 0.6191619962636776, test loss: 1.020249884555853, acc: 0.6251325556733828
epoch: 48, train loss: 0.8208759926708297, acc: 0.6859834427633457, val loss: 1.0542246655269976, acc: 0.6188951160928743, test loss: 1.062684085690204, acc: 0.626458112407211
epoch: 49, train loss: 0.8170298457724211, acc: 0.6879246360262632, val loss: 1.0764825135381946, acc: 0.6095543101147585, test loss: 1.0916876451587374, acc: 0.6079003181336161
epoch: 50, train loss: 0.734352002646152, acc: 0.712589209249215, val loss: 0.8582289082099827, acc: 0.6768081131571925, test loss: 0.8550586354062514, acc: 0.6778897136797455
epoch: 51, train loss: 0.699148795463819, acc: 0.7265201256066229, val loss: 0.8548204699038568, acc: 0.685081398452095, test loss: 0.8522074826828117, acc: 0.6784199363732768
epoch: 52, train loss: 0.6786549768752791, acc: 0.7310305452469312, val loss: 1.1381560801059876, acc: 0.5967440619161997, test loss: 1.1423733191424437, acc: 0.5943796394485684
epoch: 53, train loss: 0.6867939307354125, acc: 0.7305737938909506, val loss: 0.863437270176452, acc: 0.6789431545236189, test loss: 0.8514921821946178, acc: 0.6876988335100742
epoch: 54, train loss: 0.6634640649731011, acc: 0.735883528404225, val loss: 0.8818158803700065, acc: 0.6826794769148652, test loss: 0.8778240991675336, acc: 0.6847826086956522
epoch: 55, train loss: 0.6589550542960737, acc: 0.7374250642306595, val loss: 0.9057130385374177, acc: 0.6706698692287163, test loss: 0.8976434124110233, acc: 0.6808059384941676
epoch: 56, train loss: 0.6534697360016434, acc: 0.7378247216671424, val loss: 0.9142268368935057, acc: 0.6658660261542567, test loss: 0.9087202154566096, acc: 0.6720572640509014
epoch: 57, train loss: 0.6492111110700868, acc: 0.7397088210105623, val loss: 0.9002879343574958, acc: 0.6733386709367494, test loss: 0.8904545661121259, acc: 0.6898197242841994
epoch: 58, train loss: 0.6451356747866426, acc: 0.7391949757350842, val loss: 0.9951439634271708, acc: 0.6493194555644516, test loss: 0.9854961617248309, acc: 0.6654294803817603
epoch: 59, train loss: 0.6444695228632606, acc: 0.7415929203539823, val loss: 0.8939736326567358, acc: 0.678142514011209, test loss: 0.9047942482654009, acc: 0.6770943796394485
epoch: 60, train loss: 0.6349549841921636, acc: 0.7455894947188124, val loss: 0.9134322262650209, acc: 0.6693354683746997, test loss: 0.8993512092440656, acc: 0.6741781548250265
epoch: 61, train loss: 0.6228304933793811, acc: 0.7490151298886668, val loss: 0.9325551953607156, acc: 0.669602348545503, test loss: 0.9401734696971776, acc: 0.6646341463414634
epoch: 62, train loss: 0.601409322715915, acc: 0.7548958035969169, val loss: 0.9486270871262948, acc: 0.671203629570323, test loss: 0.9673536326953488, acc: 0.6635737009544008
epoch: 63, train loss: 0.6179912019687961, acc: 0.7479303454182129, val loss: 1.0296476939089112, acc: 0.6541232986389112, test loss: 1.0106696559161674, acc: 0.66118769883351
epoch: 64, train loss: 0.6145646293300102, acc: 0.7494718812446475, val loss: 0.8769563191853939, acc: 0.6890846010141447, test loss: 0.8688178272287768, acc: 0.6948568398727466
epoch: 65, train loss: 0.5872249774343451, acc: 0.7587210962032543, val loss: 0.9232608589898245, acc: 0.6797437950360288, test loss: 0.9143419599482515, acc: 0.6842523860021209
epoch: 66, train loss: 0.5883109478143294, acc: 0.7603197259491864, val loss: 0.8611381253083928, acc: 0.6925540432345877, test loss: 0.8525651904457068, acc: 0.6932661717921527
epoch: 67, train loss: 0.5988766675740013, acc: 0.7566086211818441, val loss: 0.8578136631543394, acc: 0.692020282892981, test loss: 0.8627784072657532, acc: 0.6980381760339343
epoch: 68, train loss: 0.5811817717661083, acc: 0.7608335712246646, val loss: 0.8741473986811723, acc: 0.6848145182812917, test loss: 0.8681517294583366, acc: 0.6924708377518558
epoch: 69, train loss: 0.5800762692554113, acc: 0.7636311732800457, val loss: 0.8839256414133992, acc: 0.692020282892981, test loss: 0.9100483143949054, acc: 0.6813361611876988
epoch 70, gamma increased to 2.
epoch: 70, train loss: 0.4669945134287591, acc: 0.7621467313731087, val loss: 0.8039857299289036, acc: 0.6752068321323725, test loss: 0.8056039999803076, acc: 0.6707317073170732
epoch: 71, train loss: 0.4487680502016817, acc: 0.7687696260348272, val loss: 0.861204090008966, acc: 0.6650653856418468, test loss: 0.8508871955558073, acc: 0.6617179215270413
epoch: 72, train loss: 0.43917188932478446, acc: 0.7709962888952326, val loss: 0.7318364607991935, acc: 0.6954897251134241, test loss: 0.7510705927143926, acc: 0.7006892895015907
epoch: 73, train loss: 0.4408717467279186, acc: 0.7711675706537254, val loss: 0.8336293418025029, acc: 0.6786762743528156, test loss: 0.8188300122638269, acc: 0.6765641569459173
epoch: 74, train loss: 0.44457864342775816, acc: 0.7699685983442763, val loss: 0.8552204776771043, acc: 0.6623965839338137, test loss: 0.85285072020736, acc: 0.6662248144220573
epoch: 75, train loss: 0.42806543613752907, acc: 0.7703111618612618, val loss: 0.7742978285750549, acc: 0.6829463570856685, test loss: 0.7774218581261336, acc: 0.6861081654294804
epoch: 76, train loss: 0.4281803312827069, acc: 0.7767627747644876, val loss: 0.8129822714728994, acc: 0.6728049105951428, test loss: 0.8049249310255809, acc: 0.6728525980911984
epoch: 77, train loss: 0.42336079851749997, acc: 0.775906365972024, val loss: 0.8159973947786159, acc: 0.6810781958900454, test loss: 0.7932988280076869, acc: 0.6855779427359491
epoch: 78, train loss: 0.4408879790992149, acc: 0.7712817584927205, val loss: 0.857198972271893, acc: 0.6514544969308781, test loss: 0.8810899523131288, acc: 0.6484623541887593
epoch: 79, train loss: 0.44583295402341727, acc: 0.7660291178989438, val loss: 0.7903460934857607, acc: 0.685882038964505, test loss: 0.8243245917944235, acc: 0.6831919406150583
epoch: 80, train loss: 0.41512431649864995, acc: 0.7769911504424779, val loss: 0.7853890154436171, acc: 0.6800106752068321, test loss: 0.7994660812875499, acc: 0.6792152704135737
epoch: 81, train loss: 0.41378576869015826, acc: 0.7764202112475022, val loss: 0.7547859142136504, acc: 0.6973578863090473, test loss: 0.7658303838266675, acc: 0.6924708377518558
epoch: 82, train loss: 0.4064902965014504, acc: 0.7815015700827862, val loss: 0.7808788113508792, acc: 0.6856151587937016, test loss: 0.7560766477726424, acc: 0.690084835630965
epoch: 83, train loss: 0.4011990416434503, acc: 0.7803596916928347, val loss: 0.9072665386846424, acc: 0.6639978649586336, test loss: 0.9099374065217133, acc: 0.6625132555673383
epoch: 84, train loss: 0.39736167583641174, acc: 0.7853839566086211, val loss: 0.874638321369783, acc: 0.6530557779556979, test loss: 0.8663889038221454, acc: 0.6667550371155886
epoch: 85, train loss: 0.4361822179443932, acc: 0.7676277476448758, val loss: 0.7769274321243799, acc: 0.6853482786228983, test loss: 0.7810406263913734, acc: 0.6855779427359491
epoch: 86, train loss: 0.4075121735815249, acc: 0.7797316585783614, val loss: 0.8013564499340154, acc: 0.6794769148652255, test loss: 0.778116563850597, acc: 0.6837221633085896
epoch: 87, train loss: 0.4040872265637142, acc: 0.7807022552098202, val loss: 0.7917553634047986, acc: 0.682412596744062, test loss: 0.8003875713712464, acc: 0.6741781548250265
epoch: 88, train loss: 0.4097367256667115, acc: 0.7747073936625749, val loss: 0.7492609694699526, acc: 0.6864157993061115, test loss: 0.7555700175203423, acc: 0.6874337221633086
epoch: 89, train loss: 0.4051853526006111, acc: 0.7782472166714245, val loss: 0.8115121921857962, acc: 0.6741393114491593, test loss: 0.8230808661067474, acc: 0.6731177094379639
epoch: 90, train loss: 0.39166676049364524, acc: 0.7846988295746503, val loss: 0.7697969466042194, acc: 0.7037630104083267, test loss: 0.7807592904453945, acc: 0.7028101802757158
epoch: 91, train loss: 0.4153900925241264, acc: 0.7770482443619755, val loss: 0.9251240321658979, acc: 0.6599946623965839, test loss: 0.9070180885865099, acc: 0.6588016967126193
epoch: 92, train loss: 0.3930026825873674, acc: 0.7853268626891237, val loss: 0.7822232861693204, acc: 0.6882839605017347, test loss: 0.7961305726244493, acc: 0.6866383881230117
epoch: 93, train loss: 0.3700344319807064, acc: 0.7903511276049101, val loss: 0.8200316821094001, acc: 0.6786762743528156, test loss: 0.8422042189574823, acc: 0.6823966065747614
epoch: 94, train loss: 0.4186824121119259, acc: 0.7700256922637739, val loss: 0.8538080043146252, acc: 0.661862823592207, test loss: 0.8875583022169192, acc: 0.6545599151643691
epoch: 95, train loss: 0.4030156677037418, acc: 0.7786468741079076, val loss: 0.7684938614929012, acc: 0.6896183613557513, test loss: 0.7863762540503751, acc: 0.686373276776246
epoch: 96, train loss: 0.3830808825741963, acc: 0.7878389951470168, val loss: 0.8777113431671444, acc: 0.6688017080330931, test loss: 0.8656443758203015, acc: 0.6717921527041357
epoch: 97, train loss: 0.3683035603411498, acc: 0.7950328290037111, val loss: 0.8514458305332417, acc: 0.6682679476914866, test loss: 0.841043476588147, acc: 0.6781548250265111
epoch: 98, train loss: 0.3516321694054605, acc: 0.7989723094490436, val loss: 0.7949143364043245, acc: 0.6912196423805711, test loss: 0.8053284793744537, acc: 0.6882290562036055
epoch: 99, train loss: 0.3540080216469984, acc: 0.7982300884955752, val loss: 0.822883686766104, acc: 0.6941553242594075, test loss: 0.8258789853604702, acc: 0.6903499469777307
epoch: 100, train loss: 0.369695888292507, acc: 0.7866971167570653, val loss: 0.8147172532719168, acc: 0.6829463570856685, test loss: 0.8049967396423647, acc: 0.6948568398727466
best val loss 0.7318364607991935 at epoch 72.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.8841    0.9562    0.9187      5337
           1     0.6871    0.7566    0.7202      2502
           2     0.9559    0.8296    0.8883       810
           3     0.7547    0.8076    0.7803      1840
           4     0.9362    0.7571    0.8372       737
           5     0.8261    0.9823    0.8974       677
           6     0.7190    0.9070    0.8021      1323
           7     0.5606    0.5303    0.5450       907
           8     0.9228    0.7387    0.8206       421
           9     0.7785    0.8678    0.8208       401
          10     0.8865    0.9470    0.9158       396
          11     0.9968    0.9518    0.9738       332
          12     0.7251    0.4203    0.5322       295
          13     0.8311    0.6426    0.7248       291
          14     0.0000    0.0000    0.0000       261
          15     0.7888    0.2571    0.3878       494
          16     0.7027    0.1016    0.1775       256
          17     0.6333    0.7277    0.6772       235

    accuracy                         0.8018     17515
   macro avg     0.7550    0.6767    0.6900     17515
weighted avg     0.7917    0.8018    0.7859     17515

train confusion matrix:
[[5103   74    4   20    0    2    4   22   24   59    4    0    0   20
     0    1    0    0]
 [ 122 1893    4  147    4    5  274   28    0   12    0    0    4    2
     0    5    0    2]
 [   5    9  672    0    9   89   20    1    0    3    0    0    0    2
     0    0    0    0]
 [ 122  165    0 1486    0    0    3   25    0    5    4    0    0    2
     0   25    0    3]
 [   3   78    1   19  558    0   14   24    0    0    3    1   27    2
     0    2    2    3]
 [   0    1    2    0    0  665    8    0    0    1    0    0    0    0
     0    0    0    0]
 [   8   72    5    3    1   27 1200    1    0    3    0    0    0    3
     0    0    0    0]
 [  98  165    2   33   11    1   24  481    0    2   14    0   16    2
     0    0    6   52]
 [  92    8    0    2    0    0    0    3  311    4    0    0    0    1
     0    0    0    0]
 [  31   17    2    0    0    2    1    0    0  348    0    0    0    0
     0    0    0    0]
 [  11    3    0    0    0    0    0    1    0    0  375    0    0    3
     0    0    0    3]
 [   2    1    1    8    0    0    0    0    1    2    0  316    0    0
     0    1    0    0]
 [  14   36    4    0    9    1   12   88    0    1    0    0  124    0
     0    0    0    6]
 [  72    6    3    0    0    0   14    1    1    5    2    0    0  187
     0    0    0    0]
 [  45  140    0   63    0    0    1    6    0    1    5    0    0    0
     0    0    0    0]
 [  11   66    3  180    0   12   93    1    0    1    0    0    0    0
     0  127    0    0]
 [  21   19    0    8    2    0    1  136    0    0   12    0    0    1
     0    0   26   30]
 [  12    2    0    0    2    1    0   40    0    0    4    0    0    0
     0    0    3  171]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.7905    0.8915    0.8380      1143
           1     0.5682    0.6138    0.5901       536
           2     0.8369    0.6821    0.7516       173
           3     0.6457    0.7030    0.6731       394
           4     0.8293    0.6456    0.7260       158
           5     0.7574    0.8828    0.8153       145
           6     0.6471    0.8163    0.7219       283
           7     0.4138    0.3711    0.3913       194
           8     0.8966    0.5778    0.7027        90
           9     0.5529    0.5529    0.5529        85
          10     0.7200    0.8571    0.7826        84
          11     0.9655    0.7887    0.8682        71
          12     0.6562    0.3333    0.4421        63
          13     0.5556    0.5645    0.5600        62
          14     0.0000    0.0000    0.0000        56
          15     0.6923    0.1714    0.2748       105
          16     0.6364    0.1273    0.2121        55
          17     0.4151    0.4400    0.4272        50

    accuracy                         0.6955      3747
   macro avg     0.6433    0.5566    0.5739      3747
weighted avg     0.6866    0.6955    0.6791      3747

validation confusion matrix:
[[1019   36    4   19    0    0    3    8    4   19    6    1    1   15
     0    2    1    5]
 [  53  329    9   60    4    2   54   14    0    2    1    0    1    2
     0    1    0    4]
 [   5    1  118    1    4   26   10    1    0    5    0    0    0    2
     0    0    0    0]
 [  42   43    0  277    0    0    3   15    0    0    4    0    1    0
     0    4    1    4]
 [   3   28    0    7  102    0    3    6    0    0    3    0    3    3
     0    0    0    0]
 [   0    1    4    0    0  128   11    0    0    1    0    0    0    0
     0    0    0    0]
 [   6   25    2    1    3    9  231    0    0    4    0    0    1    1
     0    0    0    0]
 [  34   33    0   13    5    0    6   72    1    1   11    0    3    1
     0    0    2   12]
 [  27    2    0    2    0    0    0    2   52    1    0    1    1    2
     0    0    0    0]
 [  14    8    3    2    0    1    8    1    0   47    0    0    0    1
     0    0    0    0]
 [   8    1    0    0    0    0    0    1    0    0   72    0    0    0
     0    0    0    2]
 [   3    4    0    1    1    1    0    0    0    4    0   56    0    0
     0    1    0    0]
 [   6   14    0    1    2    0    1   18    0    0    0    0   21    0
     0    0    0    0]
 [  21    2    0    0    0    0    3    0    0    1    0    0    0   35
     0    0    0    0]
 [  17   23    0    9    0    0    0    4    1    0    1    0    0    0
     0    0    0    1]
 [  14   14    0   33    0    2   23    0    0    0    0    0    0    1
     0   18    0    0]
 [   6   14    1    3    1    0    1   18    0    0    1    0    0    0
     0    0    7    3]
 [  11    1    0    0    1    0    0   14    0    0    1    0    0    0
     0    0    0   22]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.7986    0.8865    0.8402      1145
           1     0.5667    0.6331    0.5981       537
           2     0.9306    0.7657    0.8401       175
           3     0.6544    0.6759    0.6650       395
           4     0.8376    0.6164    0.7101       159
           5     0.7572    0.8973    0.8213       146
           6     0.6575    0.8380    0.7368       284
           7     0.4264    0.4308    0.4286       195
           8     0.8143    0.6264    0.7081        91
           9     0.5446    0.6322    0.5851        87
          10     0.7333    0.7674    0.7500        86
          11     0.9194    0.7917    0.8507        72
          12     0.4516    0.2188    0.2947        64
          13     0.6939    0.5312    0.6018        64
          14     0.0000    0.0000    0.0000        57
          15     0.6250    0.1869    0.2878       107
          16     0.4286    0.0536    0.0952        56
          17     0.5172    0.5769    0.5455        52

    accuracy                         0.7007      3772
   macro avg     0.6309    0.5627    0.5755      3772
weighted avg     0.6881    0.7007    0.6844      3772

test confusion matrix:
[[1015   37    2   18    2    2    9   13   12   19    5    0    0    6
     0    0    0    5]
 [  51  340    1   48    4    3   59    9    1    9    2    1    0    4
     0    4    0    1]
 [   3    2  134    0    2   24    5    0    0    4    0    0    1    0
     0    0    0    0]
 [  45   49    1  267    0    0    2   15    0    1    1    2    0    0
     0    6    2    4]
 [   8   21    0    5   98    0    2   14    0    0    1    1    7    1
     0    1    0    0]
 [   2    1    1    0    0  131    9    0    0    1    0    0    0    0
     0    1    0    0]
 [  10   21    1    0    1    9  238    1    0    3    0    0    0    0
     0    0    0    0]
 [  23   44    1    7    3    1    8   84    0    0    8    0    5    0
     0    0    1   10]
 [  22    4    1    0    0    0    1    1   57    3    0    0    1    1
     0    0    0    0]
 [  17    9    0    0    2    0    2    1    0   55    0    0    0    1
     0    0    0    0]
 [   8    3    0    0    0    0    0    3    0    0   66    0    0    0
     0    0    1    5]
 [   6    3    1    2    0    0    0    0    0    3    0   57    0    0
     0    0    0    0]
 [   2   16    1    0    3    0    3   24    0    0    0    0   14    0
     0    0    0    1]
 [  17    5    0    4    0    0    2    0    0    1    0    0    1   34
     0    0    0    0]
 [  15   26    0   11    0    0    0    4    0    0    1    0    0    0
     0    0    0    0]
 [   8   13    0   38    0    3   20    2    0    1    1    1    0    0
     0   20    0    0]
 [  11    4    0    7    2    0    1   17    0    1    4    0    2    2
     0    0    3    2]
 [   8    2    0    1    0    0    1    9    0    0    1    0    0    0
     0    0    0   30]]
---------------------------------------
program finished.
