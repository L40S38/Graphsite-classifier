seed:  666
save trained model at:  ../trained_models/trained_classifier_model_17.pt
save loss at:  ./results/train_classifier_results_17.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  15
number of pockets in training set:  16504
number of pockets in validation set:  3531
number of pockets in test set:  3552
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=15, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2abf58996370>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0847194510296716, acc: 0.3595492001938924, val loss: 1.9003794235806397, acc: 0.3741149815916171, test loss: 1.900872769656482, acc: 0.3803490990990991
epoch: 2, train loss: 1.8006325256344882, acc: 0.4332283082888997, val loss: 1.7384207098114697, acc: 0.4511469838572642, test loss: 1.7249069621971063, acc: 0.44650900900900903
epoch: 3, train loss: 1.7169625480025934, acc: 0.4615850702859913, val loss: 1.6064149772300493, acc: 0.491362220334183, test loss: 1.5997400584521595, acc: 0.48817567567567566
epoch: 4, train loss: 1.6341935833622632, acc: 0.4833373727581192, val loss: 1.5812644254921584, acc: 0.49107901444350044, test loss: 1.577941454208649, acc: 0.49859234234234234
epoch: 5, train loss: 1.603543349336318, acc: 0.495576829859428, val loss: 1.6321348647023355, acc: 0.4684225431888983, test loss: 1.6086934300156328, acc: 0.48057432432432434
epoch: 6, train loss: 1.5375058661371657, acc: 0.5158143480368396, val loss: 1.5081985843515842, acc: 0.5157179269328802, test loss: 1.4879856861389436, acc: 0.527027027027027
epoch: 7, train loss: 1.5030047410446292, acc: 0.5285385361124576, val loss: 1.5161138439070432, acc: 0.5114698385726423, test loss: 1.5133173487207912, acc: 0.5233671171171171
epoch: 8, train loss: 1.4563393626349446, acc: 0.5378090159961222, val loss: 1.4575371853659087, acc: 0.5338431039365619, test loss: 1.4617206749615368, acc: 0.5416666666666666
epoch: 9, train loss: 1.4205528460713885, acc: 0.549503150751333, val loss: 1.4998208069321928, acc: 0.5261965448881337, test loss: 1.5053724598240208, acc: 0.5267454954954955
epoch: 10, train loss: 1.3742570870141062, acc: 0.5654386815317499, val loss: 1.4008970247115808, acc: 0.5420560747663551, test loss: 1.3767462013004061, acc: 0.5461711711711712
epoch: 11, train loss: 1.3416702397171294, acc: 0.5718613669413476, val loss: 1.4349407872205684, acc: 0.5528178986122911, test loss: 1.4470897451177374, acc: 0.5518018018018018
epoch: 12, train loss: 1.3300674300256194, acc: 0.5805259331071255, val loss: 1.3123504181010837, acc: 0.5771736052109884, test loss: 1.3164663637006604, acc: 0.5760135135135135
epoch: 13, train loss: 1.2858012049046756, acc: 0.5909476490547746, val loss: 1.474871832464883, acc: 0.5412064570943076, test loss: 1.4838704246658463, acc: 0.527027027027027
epoch: 14, train loss: 1.264922728194118, acc: 0.5933107125545323, val loss: 1.3692701189688805, acc: 0.5539507221750213, test loss: 1.3846124163619034, acc: 0.5453265765765766
epoch: 15, train loss: 1.2498891209723935, acc: 0.603429471643238, val loss: 1.240188561225876, acc: 0.5981308411214953, test loss: 1.2291588181847926, acc: 0.6019144144144144
epoch: 16, train loss: 1.2067409898260526, acc: 0.6181531749878817, val loss: 1.2440694201536133, acc: 0.600113282356273, test loss: 1.2453736378265932, acc: 0.5971283783783784
epoch: 17, train loss: 1.1815803624794314, acc: 0.6257876878332526, val loss: 1.3170677903892973, acc: 0.5689606343811952, test loss: 1.3189171413043599, acc: 0.5872747747747747
epoch: 18, train loss: 1.1835639057261125, acc: 0.6249394086282113, val loss: 1.2723063465827114, acc: 0.5876522231662419, test loss: 1.251993078369278, acc: 0.5940315315315315
epoch: 19, train loss: 1.1508148487944425, acc: 0.6331192438196801, val loss: 1.2324709362904027, acc: 0.615689606343812, test loss: 1.2428672657356605, acc: 0.606418918918919
epoch: 20, train loss: 1.1365201830921736, acc: 0.6374818225884634, val loss: 1.2049979222323868, acc: 0.6094590767487964, test loss: 1.207673693562413, acc: 0.615990990990991
epoch: 21, train loss: 1.1234643350107487, acc: 0.6387542413960252, val loss: 1.2517473096720593, acc: 0.6009629000283205, test loss: 1.2864539773614556, acc: 0.6007882882882883
epoch: 22, train loss: 1.1084406558483824, acc: 0.6489335918565197, val loss: 1.324248541248854, acc: 0.5828377230246389, test loss: 1.3210835564243901, acc: 0.5819256756756757
epoch: 23, train loss: 1.0854531248622419, acc: 0.656992244304411, val loss: 1.2962936107243332, acc: 0.5972812234494478, test loss: 1.318505385974506, acc: 0.5954391891891891
epoch: 24, train loss: 1.081295117093011, acc: 0.6555380513814832, val loss: 1.3321796866659739, acc: 0.5723591050693855, test loss: 1.3447124206268035, acc: 0.5819256756756757
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.8386844262820219, acc: 0.6600824042656326, val loss: 0.871143937684834, acc: 0.6488246955536675, test loss: 0.8736088232951121, acc: 0.6455518018018018
epoch: 26, train loss: 0.8186931234508594, acc: 0.6708676684440136, val loss: 0.9934610136842903, acc: 0.6154064004531294, test loss: 1.0008252569147058, acc: 0.6151463963963963
epoch: 27, train loss: 0.8081814308393019, acc: 0.6734125060591372, val loss: 0.8471330037302014, acc: 0.6547720192580005, test loss: 0.8385074439349475, acc: 0.6663851351351351
epoch: 28, train loss: 0.7934001507368471, acc: 0.6763814832767814, val loss: 0.903383286064794, acc: 0.6400453129425092, test loss: 0.9017298071234076, acc: 0.6368243243243243
epoch: 29, train loss: 0.8031275043543021, acc: 0.6685651963160446, val loss: 1.182061954113148, acc: 0.5647125460209572, test loss: 1.178874816980448, acc: 0.5717905405405406
epoch: 30, train loss: 0.7929078627719037, acc: 0.6755937954435288, val loss: 0.9520450697779756, acc: 0.62107051826678, test loss: 0.9670417287328221, acc: 0.6252815315315315
epoch: 31, train loss: 0.7722125240627331, acc: 0.6819558894813379, val loss: 0.8219212985383118, acc: 0.6658170489946191, test loss: 0.8618036798528723, acc: 0.6649774774774775
epoch: 32, train loss: 0.7610563038335052, acc: 0.6847430925836161, val loss: 1.0280939740656592, acc: 0.6202209005947323, test loss: 1.046912721685461, acc: 0.6241554054054054
epoch: 33, train loss: 0.757815529002302, acc: 0.692195831313621, val loss: 0.8899834018630031, acc: 0.64910790144435, test loss: 0.9025620587237246, acc: 0.6537162162162162
epoch: 34, train loss: 0.7538014938111335, acc: 0.6884997576345129, val loss: 0.9489120501899341, acc: 0.6256018125177004, test loss: 0.9436721651403753, acc: 0.6385135135135135
epoch: 35, train loss: 0.7315298753690789, acc: 0.6935288414929714, val loss: 0.8941542880793734, acc: 0.6479750778816199, test loss: 0.9084508376078563, acc: 0.6551238738738738
epoch: 36, train loss: 0.7247620159656286, acc: 0.7017086766844401, val loss: 1.018765951816707, acc: 0.631832342112716, test loss: 1.0336784156593117, acc: 0.6168355855855856
epoch: 37, train loss: 0.7257422920979173, acc: 0.6966795928259816, val loss: 0.8457478558635685, acc: 0.6576040781648258, test loss: 0.8423396445609428, acc: 0.6587837837837838
epoch: 38, train loss: 0.7163534875466184, acc: 0.6957707222491517, val loss: 0.986875305024533, acc: 0.6026621353724158, test loss: 1.0027674812454361, acc: 0.5875563063063063
epoch: 39, train loss: 0.7007882276381097, acc: 0.7058288899660689, val loss: 0.9596675266054121, acc: 0.6207873123760974, test loss: 0.9781422228426546, acc: 0.6264076576576577
epoch: 40, train loss: 0.7153527357309002, acc: 0.7033446437227339, val loss: 0.8925188095500063, acc: 0.6357972245822713, test loss: 0.924658135250882, acc: 0.6365427927927928
epoch: 41, train loss: 0.7201660278926356, acc: 0.7024963645176927, val loss: 0.8099801208843965, acc: 0.675446049277825, test loss: 0.8218559660353102, acc: 0.6776463963963963
epoch: 42, train loss: 0.6970679100658844, acc: 0.7093431895298109, val loss: 1.0173481637646273, acc: 0.6066270178419712, test loss: 1.0212229135874156, acc: 0.6131756756756757
epoch: 43, train loss: 0.6744229789214752, acc: 0.7123727581192438, val loss: 0.8246783632220644, acc: 0.6709147550269046, test loss: 0.8355975859874004, acc: 0.6739864864864865
epoch: 44, train loss: 0.6628130316098685, acc: 0.717462433349491, val loss: 0.8217556005967842, acc: 0.6813933729821581, test loss: 0.8307285115525529, acc: 0.6770833333333334
epoch: 45, train loss: 0.6739303461279155, acc: 0.7160082404265633, val loss: 0.7991905306257293, acc: 0.6836590201076183, test loss: 0.8051839068129256, acc: 0.6849662162162162
epoch: 46, train loss: 0.6523109278919277, acc: 0.7244910324769753, val loss: 0.8939801646642865, acc: 0.6417445482866043, test loss: 0.915920942753285, acc: 0.6385135135135135
epoch: 47, train loss: 0.6612431005661891, acc: 0.7230368395540475, val loss: 0.886975111585945, acc: 0.6476918719909374, test loss: 0.9081673568433469, acc: 0.6463963963963963
epoch: 48, train loss: 0.6536346028513315, acc: 0.7234609791565682, val loss: 0.9807693596029377, acc: 0.6312659303313509, test loss: 0.977051339707933, acc: 0.6421734234234234
epoch: 49, train loss: 0.6591719835635476, acc: 0.7219461948618516, val loss: 0.8237152325131338, acc: 0.6765788728405551, test loss: 0.8298654899940835, acc: 0.6725788288288288
epoch: 50, train loss: 0.6303042158072745, acc: 0.7339432864760058, val loss: 0.8547582808650948, acc: 0.6595865193996036, test loss: 0.8595183099712338, acc: 0.665259009009009
epoch: 51, train loss: 0.6366814869847393, acc: 0.7253999030538051, val loss: 0.8012844785959313, acc: 0.6830926083262532, test loss: 0.8047679308298472, acc: 0.6869369369369369
epoch: 52, train loss: 0.6363183307780609, acc: 0.7281265147842947, val loss: 0.768265541806433, acc: 0.6915887850467289, test loss: 0.7854918660344304, acc: 0.6894707207207207
epoch: 53, train loss: 0.6277279941075298, acc: 0.7336403296170625, val loss: 0.9698975110519915, acc: 0.6018125177003681, test loss: 0.9665971532598272, acc: 0.6216216216216216
epoch: 54, train loss: 0.609167563788127, acc: 0.7412142510906446, val loss: 0.7709130837741097, acc: 0.6944208439535542, test loss: 0.7873816511652492, acc: 0.6900337837837838
epoch: 55, train loss: 0.6024403322164837, acc: 0.7418201648085313, val loss: 0.8095062442083799, acc: 0.6847918436703483, test loss: 0.8269707791440122, acc: 0.6706081081081081
epoch: 56, train loss: 0.5943281529773744, acc: 0.7447285506543868, val loss: 0.779428106500825, acc: 0.702350608892665, test loss: 0.7920072132402712, acc: 0.6948198198198198
epoch: 57, train loss: 0.6005884289914869, acc: 0.7442438196800776, val loss: 0.8246876828159364, acc: 0.675446049277825, test loss: 0.8502836850312379, acc: 0.6722972972972973
epoch: 58, train loss: 0.5871882840261492, acc: 0.7460009694619486, val loss: 0.8014569019872925, acc: 0.6847918436703483, test loss: 0.7957605424228015, acc: 0.6905968468468469
epoch: 59, train loss: 0.588434993469536, acc: 0.7466674745516239, val loss: 0.8621090678242306, acc: 0.6618521665250637, test loss: 0.8709060265137268, acc: 0.652027027027027
epoch: 60, train loss: 0.5949438084287084, acc: 0.745758603974794, val loss: 0.7626788113144902, acc: 0.6978193146417445, test loss: 0.781127725635563, acc: 0.6987612612612613
epoch: 61, train loss: 0.5780746787569793, acc: 0.7506059137178865, val loss: 0.7785864064727887, acc: 0.6958368734069669, test loss: 0.7912989167479781, acc: 0.6908783783783784
epoch: 62, train loss: 0.5598263250527541, acc: 0.76308773630635, val loss: 0.828359074647958, acc: 0.6847918436703483, test loss: 0.8430119677706882, acc: 0.6801801801801802
epoch: 63, train loss: 0.5824421740710822, acc: 0.7537566650508968, val loss: 0.7446974027396531, acc: 0.7043330501274426, test loss: 0.7656956567420615, acc: 0.6967905405405406
epoch: 64, train loss: 0.5608289644548284, acc: 0.756362094037809, val loss: 0.8118084673839667, acc: 0.686207873123761, test loss: 0.8467189084302198, acc: 0.6770833333333334
epoch: 65, train loss: 0.562393805874359, acc: 0.7595734367426078, val loss: 0.7871561881605271, acc: 0.6836590201076183, test loss: 0.8096009074030696, acc: 0.6880630630630631
epoch: 66, train loss: 0.5658021121711565, acc: 0.7555138148327678, val loss: 0.8117457455394964, acc: 0.6768620787312376, test loss: 0.8278610631152317, acc: 0.6773648648648649
epoch: 67, train loss: 0.5424854230718941, acc: 0.7635118759088706, val loss: 0.8081204824358503, acc: 0.6901727555933164, test loss: 0.812446336488466, acc: 0.692286036036036
epoch: 68, train loss: 0.537015058089389, acc: 0.7652690256907416, val loss: 0.7776948998177635, acc: 0.7017841971112999, test loss: 0.849981181256406, acc: 0.6866554054054054
epoch: 69, train loss: 0.5616333811276177, acc: 0.7599975763451284, val loss: 0.835689083640618, acc: 0.6825261965448881, test loss: 0.839145927815824, acc: 0.6841216216216216
epoch: 70, train loss: 0.5658964194462129, acc: 0.7586039747939893, val loss: 0.8075661599146546, acc: 0.6947040498442367, test loss: 0.8290926722792892, acc: 0.682713963963964
epoch: 71, train loss: 0.5352322006768759, acc: 0.7666020358700921, val loss: 0.8130168807435664, acc: 0.6876239025771737, test loss: 0.8410899016234252, acc: 0.6796171171171171
epoch: 72, train loss: 0.548577816117694, acc: 0.7637542413960252, val loss: 0.7387389832534185, acc: 0.7108467856131407, test loss: 0.7465863260062965, acc: 0.7117117117117117
epoch: 73, train loss: 0.5336058893310058, acc: 0.7674503150751333, val loss: 0.7890066106945353, acc: 0.6998017558765223, test loss: 0.7880662711890968, acc: 0.6990427927927928
epoch: 74, train loss: 0.5232411667495114, acc: 0.7719946679592826, val loss: 0.7374595938441909, acc: 0.6983857264231096, test loss: 0.7716979132042275, acc: 0.7004504504504504
epoch: 75, train loss: 0.5241809062627236, acc: 0.7727823557925352, val loss: 0.791149707868193, acc: 0.7015009912206174, test loss: 0.7905548252500929, acc: 0.6998873873873874
epoch: 76, train loss: 0.5227124413644233, acc: 0.7695104217159476, val loss: 0.7570611257723255, acc: 0.6983857264231096, test loss: 0.7672464568335731, acc: 0.6973536036036037
epoch: 77, train loss: 0.5000125922974139, acc: 0.7799927290353854, val loss: 0.8362440864751508, acc: 0.6893231379212688, test loss: 0.8676461980149552, acc: 0.6829954954954955
epoch: 78, train loss: 0.49602472120503227, acc: 0.7850218128938439, val loss: 0.7788537344018315, acc: 0.7009345794392523, test loss: 0.7890188071104858, acc: 0.7088963963963963
epoch: 79, train loss: 0.511448406329504, acc: 0.775630150266602, val loss: 0.8404617549609409, acc: 0.6788445199660152, test loss: 0.8226561299315444, acc: 0.6872184684684685
epoch: 80, train loss: 0.5085371692407149, acc: 0.7780538051381484, val loss: 0.8684222123603313, acc: 0.686207873123761, test loss: 0.9008168519080222, acc: 0.6872184684684685
epoch: 81, train loss: 0.5064521037255221, acc: 0.7774478914202617, val loss: 0.8018540400548549, acc: 0.7037666383460776, test loss: 0.7822110609965282, acc: 0.6970720720720721
epoch: 82, train loss: 0.508496665110937, acc: 0.7796291808046534, val loss: 0.9030432592401151, acc: 0.6479750778816199, test loss: 0.9394693245758882, acc: 0.6407657657657657
epoch: 83, train loss: 0.5150332270001418, acc: 0.7780538051381484, val loss: 0.8015674516048016, acc: 0.6952704616256018, test loss: 0.8352408860180829, acc: 0.6962274774774775
Epoch    83: reducing learning rate of group 0 to 1.5000e-03.
epoch: 84, train loss: 0.4186669136792551, acc: 0.8111972855065439, val loss: 0.7124094678132806, acc: 0.7354856981025205, test loss: 0.7402746194117779, acc: 0.732545045045045
epoch: 85, train loss: 0.37643113463640326, acc: 0.8315559864275327, val loss: 0.757636108603703, acc: 0.7289719626168224, test loss: 0.7711877328855498, acc: 0.7266328828828829
epoch: 86, train loss: 0.3675566857813402, acc: 0.8328284052350945, val loss: 0.7968625793103218, acc: 0.7233078448031719, test loss: 0.8150023426021542, acc: 0.7221283783783784
epoch: 87, train loss: 0.3705592177125706, acc: 0.829738245273873, val loss: 0.7225479253187155, acc: 0.7419994335882186, test loss: 0.7346188205856461, acc: 0.7401463963963963
epoch: 88, train loss: 0.3591670299321774, acc: 0.8344037809015996, val loss: 0.8081903912061607, acc: 0.719626168224299, test loss: 0.8079903308335725, acc: 0.7179054054054054
epoch: 89, train loss: 0.35246531014527965, acc: 0.8367668444013573, val loss: 0.8162194077876093, acc: 0.7097139620504106, test loss: 0.8352365977055317, acc: 0.713963963963964
epoch: 90, train loss: 0.3516914374581021, acc: 0.8378574890935531, val loss: 0.8261027342197544, acc: 0.7182101387708865, test loss: 0.8364340850898812, acc: 0.7159346846846847
epoch: 91, train loss: 0.34840068218203, acc: 0.8397964129907901, val loss: 0.7542519622063779, acc: 0.7264231096006797, test loss: 0.7584529674804963, acc: 0.7347972972972973
epoch: 92, train loss: 0.3442361103321127, acc: 0.8417353368880272, val loss: 0.7306595058287213, acc: 0.7397337864627584, test loss: 0.7594433539622539, acc: 0.7317004504504504
epoch: 93, train loss: 0.34979134104636705, acc: 0.8379786718371304, val loss: 0.7503823291408006, acc: 0.7354856981025205, test loss: 0.7776223152607411, acc: 0.7356418918918919
epoch: 94, train loss: 0.33613108138233727, acc: 0.8435530780416869, val loss: 0.807917788561462, acc: 0.7269895213820448, test loss: 0.8254674447549356, acc: 0.7164977477477478
epoch: 95, train loss: 0.3453740390885298, acc: 0.8425836160930683, val loss: 0.7460881951374496, acc: 0.7386009629000283, test loss: 0.7835218229809323, acc: 0.7317004504504504
epoch: 96, train loss: 0.3268107206741699, acc: 0.8446437227338827, val loss: 0.7655912802800752, acc: 0.7377513452279807, test loss: 0.7934121441196751, acc: 0.7286036036036037
epoch: 97, train loss: 0.3142672816180397, acc: 0.8483397964129908, val loss: 0.783551004074073, acc: 0.7315208156329651, test loss: 0.8155818930617323, acc: 0.7260698198198198
epoch: 98, train loss: 0.3257459814665217, acc: 0.8465220552593311, val loss: 0.7883810802884832, acc: 0.72982158028887, test loss: 0.804547034942352, acc: 0.7193130630630631
epoch: 99, train loss: 0.3331786518066876, acc: 0.8420988851187591, val loss: 0.8924570673081261, acc: 0.7063154913622204, test loss: 0.8678017779513523, acc: 0.7066441441441441
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.24498385767363484, acc: 0.847249151720795, val loss: 0.6590059902809119, acc: 0.7258566978193146, test loss: 0.6556766946036536, acc: 0.7302927927927928
epoch: 101, train loss: 0.21725786915405595, acc: 0.8574285021812894, val loss: 0.6355953195958662, acc: 0.7394505805720759, test loss: 0.6595981711739892, acc: 0.7412725225225225
epoch: 102, train loss: 0.21569949738822167, acc: 0.8564590402326708, val loss: 0.6757799768137277, acc: 0.7360521098838856, test loss: 0.6738979966790827, acc: 0.7364864864864865
epoch: 103, train loss: 0.22043097166545866, acc: 0.8553683955404751, val loss: 0.6906006658289933, acc: 0.7241574624752195, test loss: 0.7174886916134808, acc: 0.7257882882882883
epoch: 104, train loss: 0.23058825595484042, acc: 0.8515511391177897, val loss: 0.7263370406286671, acc: 0.7006513735485698, test loss: 0.727450053971093, acc: 0.7159346846846847
epoch: 105, train loss: 0.2404213770971215, acc: 0.843189529810955, val loss: 0.660468419055296, acc: 0.7176437269895214, test loss: 0.6688662004900409, acc: 0.7224099099099099
epoch: 106, train loss: 0.220375010798813, acc: 0.8533082888996607, val loss: 0.6811447451790545, acc: 0.7320872274143302, test loss: 0.700564262029287, acc: 0.7252252252252253
epoch: 107, train loss: 0.23681307721646502, acc: 0.8433713039263209, val loss: 0.661257739412788, acc: 0.7318040215236477, test loss: 0.6556632862434731, acc: 0.7317004504504504
epoch: 108, train loss: 0.22907098249446056, acc: 0.8463402811439651, val loss: 0.6443582152010394, acc: 0.7425658453695837, test loss: 0.683509652679031, acc: 0.7294481981981982
epoch: 109, train loss: 0.2239552998912606, acc: 0.8510664081434803, val loss: 0.6305606379383382, acc: 0.7394505805720759, test loss: 0.6334946703266453, acc: 0.7443693693693694
epoch: 110, train loss: 0.22511922082332403, acc: 0.849188075618032, val loss: 0.6785183905947482, acc: 0.7193429623336165, test loss: 0.686993855613846, acc: 0.7170608108108109
epoch: 111, train loss: 0.237573483464964, acc: 0.8442801745031507, val loss: 0.6728524623596442, acc: 0.7241574624752195, test loss: 0.6865948717873376, acc: 0.7204391891891891
epoch: 112, train loss: 0.2113984456151305, acc: 0.8562772661173049, val loss: 0.7159997713326125, acc: 0.7182101387708865, test loss: 0.748163618482985, acc: 0.7136824324324325
epoch: 113, train loss: 0.2082038833441459, acc: 0.859791565681047, val loss: 0.6605140602443482, acc: 0.7250070801472671, test loss: 0.6587010428712174, acc: 0.7283220720720721
epoch: 114, train loss: 0.22179636012508172, acc: 0.8497333979641299, val loss: 0.6918483467150667, acc: 0.719059756442934, test loss: 0.6985022280667279, acc: 0.7170608108108109
epoch: 115, train loss: 0.2414136026388687, acc: 0.8399781871061561, val loss: 0.7156197329277831, acc: 0.7176437269895214, test loss: 0.6901949096370388, acc: 0.7255067567567568
epoch: 116, train loss: 0.2426717034602015, acc: 0.8379180804653418, val loss: 0.6424034869356893, acc: 0.7258566978193146, test loss: 0.6252196769456606, acc: 0.7305743243243243
epoch: 117, train loss: 0.22308455012518402, acc: 0.8480368395540475, val loss: 0.7406784009744412, acc: 0.7167941093174738, test loss: 0.7817653320931099, acc: 0.7066441441441441
epoch: 118, train loss: 0.22224783612089255, acc: 0.8496728065923412, val loss: 0.661633117615552, acc: 0.7366185216652507, test loss: 0.640965690483918, acc: 0.7418355855855856
epoch: 119, train loss: 0.20639658960190088, acc: 0.8554895782840524, val loss: 0.6947896795210707, acc: 0.7269895213820448, test loss: 0.686500385000899, acc: 0.7331081081081081
epoch: 120, train loss: 0.19509728097592058, acc: 0.8608822103732429, val loss: 0.8298573407529941, acc: 0.6947040498442367, test loss: 0.827325556729291, acc: 0.6903153153153153
epoch: 121, train loss: 0.2041792691194543, acc: 0.853974793989336, val loss: 0.7169436848693649, acc: 0.7170773152081563, test loss: 0.7210892986606907, acc: 0.7212837837837838
epoch: 122, train loss: 0.21904336483774403, acc: 0.8477944740668929, val loss: 0.6625305235942948, acc: 0.7436986689323138, test loss: 0.6513124199600907, acc: 0.7483108108108109
epoch: 123, train loss: 0.20976796649465038, acc: 0.8554895782840524, val loss: 0.705601804352995, acc: 0.7289719626168224, test loss: 0.7294267512656547, acc: 0.723536036036036
epoch: 124, train loss: 0.22403710069323535, acc: 0.8482186136694134, val loss: 0.6920635490909037, acc: 0.7363353157745681, test loss: 0.6886176010509869, acc: 0.7328265765765766
epoch: 125, train loss: 0.21496967248297177, acc: 0.8539142026175472, val loss: 0.7149262942122657, acc: 0.7162276975361087, test loss: 0.7115491682344729, acc: 0.714527027027027
epoch: 126, train loss: 0.1947035542316825, acc: 0.8597309743092584, val loss: 0.6598092814539755, acc: 0.7465307278391391, test loss: 0.6806153488588763, acc: 0.7333896396396397
epoch: 127, train loss: 0.2026785154236906, acc: 0.8603974793989336, val loss: 0.6957559372540811, acc: 0.7261399037099971, test loss: 0.7117425353677423, acc: 0.727759009009009
epoch: 128, train loss: 0.20065242863227253, acc: 0.8576102762966553, val loss: 0.7078105595793139, acc: 0.7182101387708865, test loss: 0.7169302433460683, acc: 0.7249436936936937
epoch: 129, train loss: 0.20268875552622137, acc: 0.8540959767329133, val loss: 0.7191620491544237, acc: 0.7278391390540924, test loss: 0.7405003513301815, acc: 0.7280405405405406
epoch: 130, train loss: 0.19398122156238418, acc: 0.8623364032961707, val loss: 0.6768073783113949, acc: 0.7315208156329651, test loss: 0.705638312004708, acc: 0.7311373873873874
epoch: 131, train loss: 0.21115269856838942, acc: 0.8550048473097431, val loss: 0.7118053588877991, acc: 0.7131124327386009, test loss: 0.7558189802341633, acc: 0.7086148648648649
epoch: 132, train loss: 0.20040648446115428, acc: 0.857670867668444, val loss: 0.7109255890979945, acc: 0.7349192863211554, test loss: 0.7482519654540328, acc: 0.7210022522522522
epoch: 133, train loss: 0.1961931601145056, acc: 0.8625181774115366, val loss: 0.6621882216648824, acc: 0.7363353157745681, test loss: 0.6883490912549131, acc: 0.732545045045045
epoch: 134, train loss: 0.19064847868562496, acc: 0.861730489578284, val loss: 0.6837489130948697, acc: 0.7292551685075049, test loss: 0.688736393645003, acc: 0.7311373873873874
epoch: 135, train loss: 0.18880714092564363, acc: 0.8628211342704799, val loss: 0.7293541736961998, acc: 0.7301047861795525, test loss: 0.7229183117548624, acc: 0.7331081081081081
epoch: 136, train loss: 0.18725465189699866, acc: 0.862457586039748, val loss: 0.7080802510663153, acc: 0.7360521098838856, test loss: 0.7219210465749105, acc: 0.7271959459459459
epoch: 137, train loss: 0.1812889155865409, acc: 0.86597188560349, val loss: 0.7326085457076638, acc: 0.7354856981025205, test loss: 0.7246035348187696, acc: 0.7260698198198198
Epoch   137: reducing learning rate of group 0 to 7.5000e-04.
epoch: 138, train loss: 0.1529427610781952, acc: 0.8801502666020359, val loss: 0.682940751693972, acc: 0.7553101104502974, test loss: 0.6831023467553629, acc: 0.7522522522522522
epoch: 139, train loss: 0.12441979702228027, acc: 0.9002060106640815, val loss: 0.7106365476239339, acc: 0.7524780515434721, test loss: 0.7046257633346695, acc: 0.7485923423423423
epoch: 140, train loss: 0.11470396101648116, acc: 0.90523509452254, val loss: 0.7561422346739551, acc: 0.7479467572925517, test loss: 0.7618305275032112, acc: 0.7471846846846847
epoch: 141, train loss: 0.1111846563238804, acc: 0.9094158991759573, val loss: 0.7469282802864298, acc: 0.7578589634664401, test loss: 0.7383245831137305, acc: 0.75
epoch: 142, train loss: 0.10974043510200558, acc: 0.9062651478429472, val loss: 0.7568439513959753, acc: 0.7490795808552818, test loss: 0.7594541740847064, acc: 0.7460585585585585
epoch: 143, train loss: 0.10907740885201324, acc: 0.9078405235094522, val loss: 0.795924928633291, acc: 0.7519116397621071, test loss: 0.7926448905790174, acc: 0.7505630630630631
epoch: 144, train loss: 0.11122569842313715, acc: 0.9066892874454677, val loss: 0.7490980065061363, acc: 0.7530444633248372, test loss: 0.7538740377168398, acc: 0.7483108108108109
epoch: 145, train loss: 0.10686861744969434, acc: 0.9103247697527872, val loss: 0.7494904188419732, acc: 0.7604078164825828, test loss: 0.7813820076418353, acc: 0.7423986486486487
epoch: 146, train loss: 0.10043611809416297, acc: 0.9163233155598642, val loss: 0.8138356032907642, acc: 0.7442650807136788, test loss: 0.8343708880312808, acc: 0.7393018018018018
epoch: 147, train loss: 0.10287841996317977, acc: 0.91335433834222, val loss: 0.778290768143679, acc: 0.7519116397621071, test loss: 0.7915540987306887, acc: 0.7421171171171171
epoch: 148, train loss: 0.12869027536497843, acc: 0.8980853126514784, val loss: 0.8101285806427391, acc: 0.7388841687907108, test loss: 0.8051140952754665, acc: 0.7347972972972973
epoch: 149, train loss: 0.11977136605322158, acc: 0.9040838584585555, val loss: 0.7390451186846417, acc: 0.7524780515434721, test loss: 0.7597899533606864, acc: 0.7446509009009009
epoch: 150, train loss: 0.10863771243720625, acc: 0.9095976732913233, val loss: 0.7966429359748795, acc: 0.7485131690739167, test loss: 0.7981219678311735, acc: 0.7390202702702703
epoch: 151, train loss: 0.11174485302641735, acc: 0.9088099854580708, val loss: 0.7871697995251483, acc: 0.7439818748229963, test loss: 0.7956796175724751, acc: 0.7347972972972973
epoch: 152, train loss: 0.10108203720075669, acc: 0.9152932622394571, val loss: 0.7943091158217325, acc: 0.7499291985273294, test loss: 0.7950839040515659, acc: 0.745213963963964
epoch: 153, train loss: 0.10613531969267363, acc: 0.909112942317014, val loss: 0.7922229960632541, acc: 0.7470971396205041, test loss: 0.7917880646817319, acc: 0.7440878378378378
epoch: 154, train loss: 0.10814136176420194, acc: 0.9083858458555502, val loss: 0.7871243200501988, acc: 0.7408666100254885, test loss: 0.8065163788494764, acc: 0.7376126126126126
epoch: 155, train loss: 0.10923858232149729, acc: 0.9062045564711585, val loss: 0.7547840149062985, acc: 0.7499291985273294, test loss: 0.7677176739718463, acc: 0.75
epoch: 156, train loss: 0.10810819742781297, acc: 0.9107489093553078, val loss: 0.7837366061519881, acc: 0.7507788161993769, test loss: 0.7810702495746784, acc: 0.7446509009009009
epoch: 157, train loss: 0.10619291145999167, acc: 0.9119607367910809, val loss: 0.7517453137351582, acc: 0.7527612574341547, test loss: 0.7862231280352618, acc: 0.7443693693693694
epoch: 158, train loss: 0.09288444980740605, acc: 0.9173533688802714, val loss: 0.800550239446041, acc: 0.7516284338714245, test loss: 0.8369355932012335, acc: 0.740990990990991
epoch: 159, train loss: 0.09920741485858854, acc: 0.916383906931653, val loss: 0.8343217247641819, acc: 0.7417162276975361, test loss: 0.8421597137107505, acc: 0.737331081081081
epoch: 160, train loss: 0.09981300691580576, acc: 0.914081434803684, val loss: 0.8118224917103636, acc: 0.7442650807136788, test loss: 0.819897207053932, acc: 0.7440878378378378
epoch: 161, train loss: 0.11035457897752567, acc: 0.9091735336888027, val loss: 0.80244320403142, acc: 0.7451146983857264, test loss: 0.825745103595493, acc: 0.7435247747747747
epoch: 162, train loss: 0.11392990104994956, acc: 0.9082646631119728, val loss: 0.8176306559390638, acc: 0.7414330218068536, test loss: 0.824400485098899, acc: 0.7359234234234234
epoch: 163, train loss: 0.10682343106881195, acc: 0.91335433834222, val loss: 0.8325055323454652, acc: 0.7417162276975361, test loss: 0.8344676537556691, acc: 0.7415540540540541
epoch: 164, train loss: 0.0949958424565576, acc: 0.915838584585555, val loss: 0.7864645910276565, acc: 0.7516284338714245, test loss: 0.8125022026869628, acc: 0.7435247747747747
epoch: 165, train loss: 0.10621673371453223, acc: 0.9106883179835191, val loss: 0.8037770745842695, acc: 0.7504956103086944, test loss: 0.8051586365914559, acc: 0.7387387387387387
epoch: 166, train loss: 0.10957108091308373, acc: 0.9115365971885604, val loss: 0.8107920102511341, acc: 0.7405834041348061, test loss: 0.8115529434101002, acc: 0.737331081081081
epoch: 167, train loss: 0.10819661816996506, acc: 0.9084464372273389, val loss: 0.833835928982292, acc: 0.7374681393372983, test loss: 0.8436591539297018, acc: 0.7322635135135135
epoch: 168, train loss: 0.10816533036504737, acc: 0.9097188560349007, val loss: 0.7754519072175869, acc: 0.741149815916171, test loss: 0.8080378598995037, acc: 0.7387387387387387
epoch: 169, train loss: 0.10178900152170363, acc: 0.914081434803684, val loss: 0.8303637955621564, acc: 0.7425658453695837, test loss: 0.8385648394490147, acc: 0.7443693693693694
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.0685597358635341, acc: 0.9180198739699467, val loss: 0.7125370933934603, acc: 0.7533276692155196, test loss: 0.7174434565209054, acc: 0.7483108108108109
epoch: 171, train loss: 0.0717519797597819, acc: 0.9145055744062045, val loss: 0.6904957146591926, acc: 0.745397904276409, test loss: 0.6852050053106772, acc: 0.7502815315315315
epoch: 172, train loss: 0.06708245161763685, acc: 0.9158991759573437, val loss: 0.6801109461161814, acc: 0.7485131690739167, test loss: 0.7040896791595597, acc: 0.7480292792792793
epoch: 173, train loss: 0.06951440870039566, acc: 0.9140208434318953, val loss: 0.6935254575442539, acc: 0.7439818748229963, test loss: 0.6731735532348221, acc: 0.745213963963964
epoch: 174, train loss: 0.07403293168926146, acc: 0.911778962675715, val loss: 0.7241999940945115, acc: 0.7470971396205041, test loss: 0.7300823228853243, acc: 0.740990990990991
epoch: 175, train loss: 0.062301891340801455, acc: 0.9192922927775085, val loss: 0.7073342227895278, acc: 0.7451146983857264, test loss: 0.6894986855017172, acc: 0.7516891891891891
epoch: 176, train loss: 0.057028552725860576, acc: 0.9249272903538536, val loss: 0.6990771511059515, acc: 0.7530444633248372, test loss: 0.7104072839290172, acc: 0.7491554054054054
epoch: 177, train loss: 0.058954351730157145, acc: 0.9204435288414929, val loss: 0.7089702493912841, acc: 0.7516284338714245, test loss: 0.6927426333899971, acc: 0.7466216216216216
epoch: 178, train loss: 0.06460744518853483, acc: 0.9156568104701891, val loss: 0.7088356028194638, acc: 0.7490795808552818, test loss: 0.7065206035837397, acc: 0.7404279279279279
epoch: 179, train loss: 0.07337198684364918, acc: 0.9114154144449831, val loss: 0.7144320100604965, acc: 0.7414330218068536, test loss: 0.7143170930243827, acc: 0.7384572072072072
epoch: 180, train loss: 0.07851572641746199, acc: 0.9059621909840039, val loss: 0.6996732079520235, acc: 0.7354856981025205, test loss: 0.6993700384019731, acc: 0.7454954954954955
epoch: 181, train loss: 0.06783082198949329, acc: 0.9145661657779932, val loss: 0.6736446072446898, acc: 0.7507788161993769, test loss: 0.6966741525375091, acc: 0.745213963963964
epoch: 182, train loss: 0.06225066531812515, acc: 0.9166868637905963, val loss: 0.7009369738883183, acc: 0.7479467572925517, test loss: 0.712742923616289, acc: 0.7469031531531531
epoch: 183, train loss: 0.06400327003816927, acc: 0.9189287445467765, val loss: 0.7180970723215091, acc: 0.7445482866043613, test loss: 0.7101236302573402, acc: 0.7418355855855856
epoch: 184, train loss: 0.07488047794208212, acc: 0.9095370819195346, val loss: 0.7286878330651507, acc: 0.7394505805720759, test loss: 0.7355350202268308, acc: 0.7300112612612613
epoch: 185, train loss: 0.07907563638279753, acc: 0.9039626757149782, val loss: 0.7132821421351794, acc: 0.7267063154913622, test loss: 0.7021502578580702, acc: 0.7401463963963963
epoch: 186, train loss: 0.07431008365913554, acc: 0.9067498788172564, val loss: 0.7331733451082699, acc: 0.72982158028887, test loss: 0.7207331528534761, acc: 0.7322635135135135
epoch: 187, train loss: 0.07249262343172996, acc: 0.9103853611245759, val loss: 0.6991561147519125, acc: 0.7456811101670915, test loss: 0.709901870907964, acc: 0.7432432432432432
epoch: 188, train loss: 0.0742402692414347, acc: 0.9086282113427048, val loss: 0.6639086948984644, acc: 0.7468139337298216, test loss: 0.6753740171054462, acc: 0.7446509009009009
Epoch   188: reducing learning rate of group 0 to 3.7500e-04.
epoch: 189, train loss: 0.05495548118998342, acc: 0.9251090644692196, val loss: 0.6598403565237455, acc: 0.7598414047012177, test loss: 0.6990149891054308, acc: 0.7494369369369369
epoch: 190, train loss: 0.04331238573246423, acc: 0.9357731459040233, val loss: 0.6839792729436895, acc: 0.7657887284055508, test loss: 0.7033506137830717, acc: 0.7514076576576577
epoch: 191, train loss: 0.041161520235959155, acc: 0.9355307804168687, val loss: 0.696782937943851, acc: 0.7618238459359955, test loss: 0.7028953084000596, acc: 0.75
epoch: 192, train loss: 0.03914565568084689, acc: 0.9375302956858943, val loss: 0.6986822894260317, acc: 0.7606910223732654, test loss: 0.7197711768451037, acc: 0.7516891891891891
epoch: 193, train loss: 0.03760394957088118, acc: 0.9442559379544353, val loss: 0.7445934189510427, acc: 0.7510620220900595, test loss: 0.7532928645073831, acc: 0.7494369369369369
epoch: 194, train loss: 0.038873833902559406, acc: 0.9406810470189045, val loss: 0.718234836734479, acc: 0.756159728122345, test loss: 0.72117999437693, acc: 0.7511261261261262
epoch: 195, train loss: 0.03730288419874328, acc: 0.9419534658264663, val loss: 0.7379096316670062, acc: 0.7536108751062022, test loss: 0.7398699532758009, acc: 0.7539414414414415
epoch: 196, train loss: 0.03977086896395516, acc: 0.9401963160445953, val loss: 0.752229629242194, acc: 0.7530444633248372, test loss: 0.7716035134083515, acc: 0.7514076576576577
epoch: 197, train loss: 0.03738292793856624, acc: 0.9454677653902084, val loss: 0.7567105635608165, acc: 0.7465307278391391, test loss: 0.7774394686157639, acc: 0.7449324324324325
epoch: 198, train loss: 0.041085591974026704, acc: 0.9386815317498788, val loss: 0.7415451356284986, acc: 0.7555933163409799, test loss: 0.7681598888861166, acc: 0.7528153153153153
epoch: 199, train loss: 0.04118032505716795, acc: 0.9375302956858943, val loss: 0.7460241730643278, acc: 0.7519116397621071, test loss: 0.7504712010289097, acc: 0.7466216216216216
epoch: 200, train loss: 0.041172322223502, acc: 0.9371667474551624, val loss: 0.7278986240844489, acc: 0.7550269045596149, test loss: 0.7359249699223149, acc: 0.7497184684684685
best val acc 0.7657887284055508 at epoch 190.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9933    0.9948    0.9940      5337
           1     0.9641    0.8909    0.9260      2502
           2     0.9923    0.9494    0.9703       810
           3     0.9463    0.9777    0.9618      1840
           4     0.9824    0.9864    0.9844       737
           5     0.9413    0.9956    0.9677       677
           6     0.8834    0.9849    0.9314      1323
           7     0.9624    0.9305    0.9462       907
           8     0.9628    0.9834    0.9730       421
           9     0.9950    1.0000    0.9975       401
          10     0.9949    0.9924    0.9937       396
          11     0.9940    0.9970    0.9955       332
          12     0.9825    0.9525    0.9673       295
          13     0.9931    0.9828    0.9879       291
          14     0.9298    0.9021    0.9158       235

    accuracy                         0.9678     16504
   macro avg     0.9678    0.9680    0.9675     16504
weighted avg     0.9687    0.9678    0.9677     16504

train confusion matrix:
[[9.94753607e-01 1.87371182e-04 0.00000000e+00 9.36855912e-04
  0.00000000e+00 1.87371182e-04 0.00000000e+00 5.62113547e-04
  2.99793892e-03 3.74742365e-04 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [1.19904077e-03 8.90887290e-01 0.00000000e+00 3.87689848e-02
  3.99680256e-04 0.00000000e+00 6.67466027e-02 1.99840128e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.23456790e-03 9.49382716e-01 0.00000000e+00
  1.23456790e-03 4.69135802e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.23456790e-03 0.00000000e+00 0.00000000e+00]
 [5.43478261e-03 1.57608696e-02 0.00000000e+00 9.77717391e-01
  0.00000000e+00 5.43478261e-04 0.00000000e+00 5.43478261e-04
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 8.14111262e-03 0.00000000e+00 0.00000000e+00
  9.86431479e-01 0.00000000e+00 0.00000000e+00 1.35685210e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.07055631e-03 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 4.43131462e-03 0.00000000e+00
  0.00000000e+00 9.95568685e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.43613001e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.84882842e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 7.55857899e-04 0.00000000e+00]
 [1.43329658e-02 2.97684675e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 4.41014333e-03 9.30540243e-01
  0.00000000e+00 0.00000000e+00 2.20507166e-03 0.00000000e+00
  1.10253583e-03 0.00000000e+00 1.76405733e-02]
 [1.66270784e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.83372922e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [5.05050505e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.92424242e-01 0.00000000e+00
  0.00000000e+00 2.52525253e-03 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 3.01204819e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.96987952e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 1.01694915e-02 0.00000000e+00
  2.71186441e-02 0.00000000e+00 0.00000000e+00 1.01694915e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.52542373e-01 0.00000000e+00 0.00000000e+00]
 [3.43642612e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.43642612e-03 0.00000000e+00 3.43642612e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.87285223e-03
  0.00000000e+00 9.82817869e-01 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  8.51063830e-03 4.25531915e-03 0.00000000e+00 8.51063830e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.02127660e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8804    0.8696    0.8750      1143
           1     0.6229    0.6287    0.6258       536
           2     0.8457    0.7919    0.8179       173
           3     0.7325    0.7716    0.7515       394
           4     0.7785    0.7342    0.7557       158
           5     0.8714    0.8414    0.8561       145
           6     0.7248    0.8375    0.7770       283
           7     0.5327    0.5464    0.5394       194
           8     0.7727    0.7556    0.7640        90
           9     0.6250    0.6471    0.6358        85
          10     0.8987    0.8452    0.8712        84
          11     0.8806    0.8310    0.8551        71
          12     0.6364    0.4444    0.5234        63
          13     0.6964    0.6290    0.6610        62
          14     0.6596    0.6200    0.6392        50

    accuracy                         0.7658      3531
   macro avg     0.7439    0.7196    0.7299      3531
weighted avg     0.7675    0.7658    0.7658      3531

validation confusion matrix:
[[0.86964129 0.04286964 0.00524934 0.02624672 0.00174978 0.00087489
  0.00087489 0.01749781 0.0096238  0.01749781 0.00174978 0.00087489
  0.00087489 0.00262467 0.00174978]
 [0.06343284 0.62873134 0.01119403 0.09514925 0.02425373 0.00559701
  0.0858209  0.05223881 0.         0.00746269 0.         0.00373134
  0.01119403 0.00373134 0.00746269]
 [0.02890173 0.04624277 0.79190751 0.00578035 0.         0.05202312
  0.04046243 0.         0.01734104 0.01156069 0.         0.00578035
  0.         0.         0.        ]
 [0.04568528 0.11928934 0.00507614 0.7715736  0.01015228 0.
  0.00507614 0.02791878 0.00253807 0.         0.         0.00253807
  0.00507614 0.00253807 0.00253807]
 [0.01898734 0.13291139 0.         0.03797468 0.73417722 0.
  0.02531646 0.01265823 0.01265823 0.         0.01265823 0.
  0.         0.01265823 0.        ]
 [0.00689655 0.0137931  0.04827586 0.0137931  0.         0.84137931
  0.07586207 0.         0.         0.         0.         0.
  0.         0.         0.        ]
 [0.01413428 0.09187279 0.         0.00353357 0.00706714 0.01413428
  0.83745583 0.01060071 0.00353357 0.00353357 0.         0.
  0.         0.01413428 0.        ]
 [0.13402062 0.12371134 0.00515464 0.05154639 0.02061856 0.
  0.03092784 0.54639175 0.00515464 0.         0.02061856 0.00515464
  0.02061856 0.00515464 0.03092784]
 [0.13333333 0.         0.         0.04444444 0.02222222 0.
  0.01111111 0.         0.75555556 0.         0.         0.01111111
  0.         0.02222222 0.        ]
 [0.11764706 0.05882353 0.02352941 0.         0.01176471 0.
  0.09411765 0.01176471 0.01176471 0.64705882 0.         0.01176471
  0.         0.01176471 0.        ]
 [0.08333333 0.         0.         0.         0.         0.
  0.         0.03571429 0.         0.         0.8452381  0.
  0.01190476 0.         0.02380952]
 [0.         0.07042254 0.01408451 0.01408451 0.         0.01408451
  0.01408451 0.         0.         0.02816901 0.         0.83098592
  0.         0.01408451 0.        ]
 [0.03174603 0.19047619 0.         0.03174603 0.07936508 0.
  0.03174603 0.17460317 0.         0.         0.         0.
  0.44444444 0.         0.01587302]
 [0.19354839 0.06451613 0.         0.03225806 0.         0.
  0.         0.01612903 0.         0.06451613 0.         0.
  0.         0.62903226 0.        ]
 [0.02       0.02       0.         0.02       0.         0.
  0.02       0.26       0.         0.         0.         0.
  0.04       0.         0.62      ]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8464    0.8611    0.8537      1145
           1     0.6300    0.6406    0.6353       537
           2     0.8250    0.7543    0.7881       175
           3     0.7222    0.7570    0.7392       395
           4     0.8322    0.7484    0.7881       159
           5     0.8217    0.8836    0.8515       146
           6     0.7441    0.7782    0.7608       284
           7     0.4853    0.5077    0.4962       195
           8     0.7374    0.8022    0.7684        91
           9     0.5579    0.6092    0.5824        87
          10     0.9420    0.7558    0.8387        86
          11     0.9032    0.7778    0.8358        72
          12     0.6735    0.5156    0.5841        64
          13     0.6531    0.5000    0.5664        64
          14     0.6512    0.5385    0.5895        52

    accuracy                         0.7514      3552
   macro avg     0.7350    0.6953    0.7119      3552
weighted avg     0.7533    0.7514    0.7512      3552

test confusion matrix:
[[8.61135371e-01 3.75545852e-02 7.86026201e-03 1.39737991e-02
  8.73362445e-04 8.73362445e-04 3.49344978e-03 2.27074236e-02
  1.65938865e-02 2.09606987e-02 0.00000000e+00 1.74672489e-03
  8.73362445e-04 7.86026201e-03 3.49344978e-03]
 [7.82122905e-02 6.40595903e-01 3.72439479e-03 1.04283054e-01
  1.11731844e-02 5.58659218e-03 8.75232775e-02 4.09683426e-02
  7.44878957e-03 5.58659218e-03 1.86219739e-03 1.86219739e-03
  1.86219739e-03 5.58659218e-03 3.72439479e-03]
 [2.28571429e-02 4.57142857e-02 7.54285714e-01 1.71428571e-02
  5.71428571e-03 9.14285714e-02 1.14285714e-02 5.71428571e-03
  0.00000000e+00 3.42857143e-02 0.00000000e+00 5.71428571e-03
  5.71428571e-03 0.00000000e+00 0.00000000e+00]
 [6.83544304e-02 1.03797468e-01 2.53164557e-03 7.56962025e-01
  1.01265823e-02 0.00000000e+00 5.06329114e-03 3.79746835e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.53164557e-03
  5.06329114e-03 2.53164557e-03 5.06329114e-03]
 [2.51572327e-02 1.06918239e-01 0.00000000e+00 1.88679245e-02
  7.48427673e-01 0.00000000e+00 1.25786164e-02 5.03144654e-02
  6.28930818e-03 0.00000000e+00 6.28930818e-03 0.00000000e+00
  1.88679245e-02 6.28930818e-03 0.00000000e+00]
 [0.00000000e+00 2.73972603e-02 2.73972603e-02 0.00000000e+00
  6.84931507e-03 8.83561644e-01 4.79452055e-02 0.00000000e+00
  0.00000000e+00 6.84931507e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [2.11267606e-02 1.23239437e-01 1.76056338e-02 7.04225352e-03
  0.00000000e+00 2.81690141e-02 7.78169014e-01 7.04225352e-03
  0.00000000e+00 1.05633803e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 3.52112676e-03 3.52112676e-03]
 [1.53846154e-01 1.43589744e-01 5.12820513e-03 8.20512821e-02
  2.05128205e-02 0.00000000e+00 2.05128205e-02 5.07692308e-01
  0.00000000e+00 1.02564103e-02 5.12820513e-03 0.00000000e+00
  2.56410256e-02 0.00000000e+00 2.56410256e-02]
 [1.53846154e-01 1.09890110e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  8.02197802e-01 2.19780220e-02 0.00000000e+00 0.00000000e+00
  1.09890110e-02 0.00000000e+00 0.00000000e+00]
 [1.60919540e-01 8.04597701e-02 2.29885057e-02 3.44827586e-02
  0.00000000e+00 0.00000000e+00 4.59770115e-02 0.00000000e+00
  0.00000000e+00 6.09195402e-01 0.00000000e+00 0.00000000e+00
  2.29885057e-02 2.29885057e-02 0.00000000e+00]
 [1.39534884e-01 0.00000000e+00 0.00000000e+00 2.32558140e-02
  2.32558140e-02 0.00000000e+00 0.00000000e+00 4.65116279e-02
  0.00000000e+00 0.00000000e+00 7.55813953e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.16279070e-02]
 [6.94444444e-02 4.16666667e-02 2.77777778e-02 6.94444444e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.38888889e-02 0.00000000e+00 7.77777778e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [6.25000000e-02 7.81250000e-02 3.12500000e-02 3.12500000e-02
  7.81250000e-02 0.00000000e+00 1.56250000e-02 1.56250000e-01
  3.12500000e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  5.15625000e-01 0.00000000e+00 0.00000000e+00]
 [2.18750000e-01 1.40625000e-01 0.00000000e+00 3.12500000e-02
  0.00000000e+00 0.00000000e+00 4.68750000e-02 4.68750000e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.56250000e-02
  0.00000000e+00 5.00000000e-01 0.00000000e+00]
 [5.76923077e-02 1.92307692e-02 0.00000000e+00 9.61538462e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.69230769e-01
  0.00000000e+00 0.00000000e+00 1.92307692e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 5.38461538e-01]]
---------------------------------------
program finished.
