seed:  666
number of classes (from original clusters): 10
how to merge clusters:  [[0, 9], [1, 5], 2, [3, 8], 4, 6, 7]
whether to further subcluster data according to chemical reaction: False
positive training pair sampling threshold:  13500
negative training pair sampling threshold:  4500
positive validation pair sampling threshold:  3900
negative validation pair sampling threshold:  1300
number of epochs to train: 60
batch size: 256
similar margin of contrastive loss: 0.0
dissimilar margin of contrastive loss: 1.8
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['x', 'y', 'z', 'charge', 'hydrophobicity', 'binding_probability', 'sasa', 'sequence_entropy']
number of classes after merging:  7
number of pockets in training set:  10527
number of pockets in validation set:  2254
number of pockets in test set:  2263
number of train positive pairs: 94500
number of train negative pairs: 94500
number of validation positive pairs: 27300
number of validation negative pairs: 27300
model architecture:
SiameseNet(
  (embedding_net): EmbeddingNet(
    (set2set): Set2Set(32, 64)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=1.8, normalize=True, mean=True)
epoch: 1, train loss: 0.6770235263885014, validation loss: 0.6119166738209707.
epoch: 2, train loss: 0.5373608436786308, validation loss: 0.5938646303952395.
epoch: 3, train loss: 0.48661618203460855, validation loss: 0.60755797738994.
epoch: 4, train loss: 0.4462610869634719, validation loss: 0.5490793637565641.
epoch: 5, train loss: 0.4166909931727818, validation loss: 0.5430624687278663.
epoch: 6, train loss: 0.3897960914329246, validation loss: 0.5660698452624646.
epoch: 7, train loss: 0.37037054382808626, validation loss: 0.5571242183643383.
epoch: 8, train loss: 0.35630519590428267, validation loss: 0.5462864883478744.
epoch: 9, train loss: 0.34663739477894295, validation loss: 0.540884888897012.
epoch: 10, train loss: 0.3324106976140744, validation loss: 0.5408302367213882.
epoch: 11, train loss: 0.32120452834437135, validation loss: 0.5175348362905202.
epoch: 12, train loss: 0.31614456207033187, validation loss: 0.5652260535103935.
epoch: 13, train loss: 0.315907775212848, validation loss: 0.5795654405866351.
epoch: 14, train loss: 0.31379607857598196, validation loss: 0.5341217296726101.
epoch: 15, train loss: 0.3107566228190427, validation loss: 0.5498816634796478.
epoch: 16, train loss: 0.2972771750556098, validation loss: 0.55231078235221.
epoch: 17, train loss: 0.29161112675338824, validation loss: 0.5434179674924074.
epoch: 18, train loss: 0.2883664783497967, validation loss: 0.5462641651027805.
epoch: 19, train loss: 0.2881148664484579, validation loss: 0.5620703615461077.
epoch: 20, train loss: 0.2898546537772688, validation loss: 0.5916876660860502.
epoch: 21, train loss: 0.28441040848424193, validation loss: 0.6382420363094344.
epoch: 22, train loss: 0.28838705640116696, validation loss: 0.56936887887808.
epoch: 23, train loss: 0.2824549439041703, validation loss: 0.5630149201333741.
epoch: 24, train loss: 0.2800986934439846, validation loss: 0.5484125560718578.
epoch: 25, train loss: 0.27861676649063355, validation loss: 0.5788943004957485.
epoch: 26, train loss: 0.2753502730515899, validation loss: 0.5466841159024082.
epoch: 27, train loss: 0.27597550179214075, validation loss: 0.5458773742228638.
epoch: 28, train loss: 0.2731404336999964, validation loss: 0.5471068584176647.
epoch: 29, train loss: 0.26924729521817, validation loss: 0.591202015719571.
epoch: 30, train loss: 0.2698630790508613, validation loss: 0.5510441547435718.
epoch: 31, train loss: 0.26941450845627557, validation loss: 0.5341420329446758.
epoch: 32, train loss: 0.2675099913581969, validation loss: 0.5397749596899682.
epoch: 33, train loss: 0.26613827405657087, validation loss: 0.533352290562221.
epoch: 34, train loss: 0.2664846808620231, validation loss: 0.5128152207315186.
epoch: 35, train loss: 0.27028257323572874, validation loss: 0.5587779954064896.
epoch: 36, train loss: 0.2641473540250586, validation loss: 0.5267180000556694.
epoch: 37, train loss: 0.2618996831500341, validation loss: 0.5747179764967698.
epoch: 38, train loss: 0.2627316967393986, validation loss: 0.5578606463701297.
epoch: 39, train loss: 0.2648041109357561, validation loss: 0.5301538104102725.
epoch: 40, train loss: 0.2593932630952704, validation loss: 0.5659328776488811.
epoch: 41, train loss: 0.2657809200589619, validation loss: 0.5482844711136032.
epoch: 42, train loss: 0.26020528518838226, validation loss: 0.5637355752742335.
epoch: 43, train loss: 0.2640226267052706, validation loss: 0.5737183838449555.
epoch: 44, train loss: 0.2579138852477704, validation loss: 0.5561587305034037.
epoch: 45, train loss: 0.25991071868573545, validation loss: 0.5785279105609153.
epoch: 46, train loss: 0.26087173494207794, validation loss: 0.5612843888670533.
epoch: 47, train loss: 0.2566151119010158, validation loss: 0.554719845418965.
epoch: 48, train loss: 0.25485318941792484, validation loss: 0.5282685164566878.
epoch: 49, train loss: 0.259511607074233, validation loss: 0.5588486906897018.
epoch: 50, train loss: 0.264713736034575, validation loss: 0.5501837388761751.
epoch: 51, train loss: 0.25145599197710633, validation loss: 0.5455348689040859.
epoch: 52, train loss: 0.25470831145432893, validation loss: 0.5894386059547956.
epoch: 53, train loss: 0.25170717235343165, validation loss: 0.5643098234518981.
epoch: 54, train loss: 0.2551208150954474, validation loss: 0.5494914344815545.
epoch: 55, train loss: 0.2579431359023644, validation loss: 0.5499750267280327.
epoch: 56, train loss: 0.25653367404836824, validation loss: 0.5723102628267729.
epoch: 57, train loss: 0.251349734715053, validation loss: 0.5663838883284684.
epoch: 58, train loss: 0.2530384430254578, validation loss: 0.5762949515262367.
epoch: 59, train loss: 0.2544842728710679, validation loss: 0.5646088591076079.
epoch: 60, train loss: 0.26505360888930224, validation loss: 0.5547300794273069.
best validation loss 0.5128152207315186 at epoch 34.
