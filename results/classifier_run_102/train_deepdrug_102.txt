seed:  2
save trained model at:  ../trained_models/trained_classifier_model_102.pt
save loss at:  ./results/train_classifier_results_102.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4eakC00', '1jlqA00', '2ce7C00', '4z3wC00', '5v1fD00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['4nstA00', '6cpgA00', '1lvwA00', '1okkB01', '6abmA01']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b40e5a71d60>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.052941398605364, acc: 0.38043092222090685; test loss: 1.7496033341460295, acc: 0.4485937130701962
epoch: 2, train loss: 1.761234792503532, acc: 0.45465845862436366; test loss: 1.6406374164180986, acc: 0.4790829591113212
epoch: 3, train loss: 1.6505014424890234, acc: 0.4940215461110453; test loss: 1.669823363440957, acc: 0.47884660836681636
epoch: 4, train loss: 1.6070131386075766, acc: 0.5100035515567657; test loss: 1.6726519315977013, acc: 0.4927913022926022
epoch: 5, train loss: 1.5519159191442522, acc: 0.5269918314194388; test loss: 1.4821015001611206, acc: 0.5329709288584259
epoch: 6, train loss: 1.513322629278844, acc: 0.5413164437078253; test loss: 1.4960015006055225, acc: 0.5365161900259986
epoch: 7, train loss: 1.481784010619345, acc: 0.5523854622943056; test loss: 1.4096597813737222, acc: 0.56487827936658
epoch: 8, train loss: 1.4428596790689003, acc: 0.5616787024979283; test loss: 1.4533629429253019, acc: 0.5561333018199007
epoch: 9, train loss: 1.4043839882009197, acc: 0.5697288978335504; test loss: 1.3791099045468798, acc: 0.5755140628692981
epoch: 10, train loss: 1.3910697151446916, acc: 0.5789629454244111; test loss: 1.4013729932262553, acc: 0.5613330181990073
epoch: 11, train loss: 1.3561964909875408, acc: 0.5863620220196519; test loss: 1.3455250051900078, acc: 0.5764594658473174
epoch: 12, train loss: 1.3381174491032406, acc: 0.5934059429383213; test loss: 1.313891696017055, acc: 0.5868588986055306
epoch: 13, train loss: 1.3135547884769152, acc: 0.5998579377293713; test loss: 1.3045879566086238, acc: 0.597021980619239
epoch: 14, train loss: 1.3141457184653487, acc: 0.601752101337753; test loss: 1.370156522339271, acc: 0.5885133538170645
epoch: 15, train loss: 1.288056448915237, acc: 0.6074345921628981; test loss: 1.3517432269743503, acc: 0.5757504136138029
epoch: 16, train loss: 1.297206330415101, acc: 0.6022848348526104; test loss: 1.2707932671033246, acc: 0.6041125029543843
epoch: 17, train loss: 1.259001116393035, acc: 0.6170237954303303; test loss: 1.4081861796432058, acc: 0.5644055778775703
epoch: 18, train loss: 1.2540547323605147, acc: 0.6182076476855688; test loss: 1.3867296943200222, acc: 0.574095958402269
epoch: 19, train loss: 1.2473241383329121, acc: 0.6214040487747129; test loss: 1.3174033516604577, acc: 0.5849680926494919
epoch: 20, train loss: 1.2368822032154643, acc: 0.6231206345448088; test loss: 1.3198829608954201, acc: 0.5892224060505791
epoch: 21, train loss: 1.2251142899468197, acc: 0.6276192731147153; test loss: 1.2430975294034268, acc: 0.6171117939021508
epoch: 22, train loss: 1.2590225814178284, acc: 0.6148928613709009; test loss: 1.27400533996894, acc: 0.5941857716851808
epoch: 23, train loss: 1.214829760231717, acc: 0.6337161122291938; test loss: 1.1971351388103149, acc: 0.6338926967619948
epoch: 24, train loss: 1.1868710400466314, acc: 0.6385699064756718; test loss: 1.2213470124939152, acc: 0.6159300401796266
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9497336329504401, acc: 0.6439564342370072; test loss: 0.9582788022351699, acc: 0.6362562042070432
epoch: 26, train loss: 0.9354770203179772, acc: 0.64496270865396; test loss: 1.0659588312488504, acc: 0.5885133538170645
epoch: 27, train loss: 0.9149524833064566, acc: 0.6521250147981532; test loss: 1.0393228902796485, acc: 0.604585204443394
epoch: 28, train loss: 0.9045057853213482, acc: 0.6556173789511069; test loss: 1.1591869304434286, acc: 0.5577877570314347
epoch: 29, train loss: 0.9155118613535517, acc: 0.6520066295726293; test loss: 1.0512560283960222, acc: 0.6090758685889861
epoch: 30, train loss: 0.9056772016428964, acc: 0.6588729726530129; test loss: 1.0145978200692032, acc: 0.6258567714488301
epoch: 31, train loss: 0.8941202447007108, acc: 0.6586362022019652; test loss: 0.9954781193621641, acc: 0.624438666981801
epoch: 32, train loss: 0.8801756535068099, acc: 0.6642003078015863; test loss: 1.2233817229116535, acc: 0.535807137792484
epoch: 33, train loss: 0.8881242513811854, acc: 0.6583994317509175; test loss: 0.9233454825099944, acc: 0.6379106594185772
epoch: 34, train loss: 0.8641182542327058, acc: 0.6666272049248254; test loss: 0.9759683609008789, acc: 0.6327109430394706
epoch: 35, train loss: 0.8630694418711621, acc: 0.671836154847875; test loss: 0.9379511736042814, acc: 0.6435830772866935
epoch: 36, train loss: 0.8436333652825219, acc: 0.6788208831537824; test loss: 0.901698373502886, acc: 0.6497281966438194
epoch: 37, train loss: 0.8407563993433206, acc: 0.6768083343198769; test loss: 0.947342992307897, acc: 0.642637674308674
epoch: 38, train loss: 0.831496702216286, acc: 0.6827275955960697; test loss: 0.9437953173705687, acc: 0.6381470101630821
epoch: 39, train loss: 0.8140862492664306, acc: 0.6846809518172132; test loss: 0.9258384663454046, acc: 0.6452375324982274
epoch: 40, train loss: 0.8300204474897488, acc: 0.6763939860305433; test loss: 0.8858582020594814, acc: 0.6667454502481683
epoch: 41, train loss: 0.8148563253578949, acc: 0.684917722268261; test loss: 0.9352824379157524, acc: 0.6421649728196643
epoch: 42, train loss: 0.8136243403233112, acc: 0.6885876642595005; test loss: 0.8442887714979244, acc: 0.6738359725833136
epoch: 43, train loss: 0.8041550061738574, acc: 0.6887652420977862; test loss: 0.9014682688067298, acc: 0.6476010399432758
epoch: 44, train loss: 0.7964618168484604, acc: 0.6901858648040724; test loss: 0.935476541913618, acc: 0.6301110848499173
epoch: 45, train loss: 0.7786278521942643, acc: 0.6965786669823606; test loss: 0.901065165707474, acc: 0.6499645473883243
epoch: 46, train loss: 0.7734470895596393, acc: 0.6967562448206464; test loss: 0.8744635179064792, acc: 0.658000472701489
epoch: 47, train loss: 0.7728889421611139, acc: 0.6995974902332189; test loss: 0.8686011944276958, acc: 0.6693453084377216
epoch: 48, train loss: 0.7727866774834838, acc: 0.6978217118503611; test loss: 0.8673906321222409, acc: 0.6546915622784212
epoch: 49, train loss: 0.762621877940573, acc: 0.6973481709482656; test loss: 0.8199642424717549, acc: 0.6816355471519735
epoch: 50, train loss: 0.7527141712100545, acc: 0.7054575588966497; test loss: 0.9468551098497391, acc: 0.6225478610257622
epoch: 51, train loss: 0.7583205032393793, acc: 0.7022019651947436; test loss: 0.836623563971595, acc: 0.6757267785393524
epoch: 52, train loss: 0.7498108308259109, acc: 0.7066414111518883; test loss: 0.851366836729976, acc: 0.6601276294020326
epoch: 53, train loss: 0.7416129183512377, acc: 0.7085947673730318; test loss: 0.8897238439263758, acc: 0.6615457338690617
epoch: 54, train loss: 0.7438079792747303, acc: 0.7104297383686516; test loss: 0.8326873240586968, acc: 0.676199480028362
epoch: 55, train loss: 0.7200358731303255, acc: 0.7154611104534154; test loss: 0.8528111244707461, acc: 0.6735996218388088
epoch: 56, train loss: 0.7267065129577019, acc: 0.714158872972653; test loss: 0.8487830369417844, acc: 0.6683999054597022
epoch: 57, train loss: 0.7299813857385643, acc: 0.7129158281046526; test loss: 0.8268892605010005, acc: 0.6847081068305365
epoch: 58, train loss: 0.7098891538911505, acc: 0.7194862081212264; test loss: 0.9965576298888821, acc: 0.6322382415504609
epoch: 59, train loss: 0.7178704347203466, acc: 0.7153427252278916; test loss: 0.8153025179263182, acc: 0.677853935239896
epoch: 60, train loss: 0.7069500094760137, acc: 0.7197821711850361; test loss: 0.9780368702937401, acc: 0.6265658236823446
epoch: 61, train loss: 0.7024939354177028, acc: 0.7241624245294187; test loss: 0.9359926352459216, acc: 0.6468919877097613
epoch: 62, train loss: 0.6897007028308862, acc: 0.7251686989463715; test loss: 0.884175666199596, acc: 0.6591822264240133
epoch: 63, train loss: 0.7090260042048523, acc: 0.7178288149638925; test loss: 0.7668504708203772, acc: 0.7019617111793902
epoch: 64, train loss: 0.6781388715666342, acc: 0.7330413164437078; test loss: 0.8060494742194013, acc: 0.6906168754431576
epoch: 65, train loss: 0.6727976139457998, acc: 0.7295489522907541; test loss: 0.8009286386638182, acc: 0.691562278421177
epoch: 66, train loss: 0.680376885402156, acc: 0.7291937966141826; test loss: 0.8390309045399877, acc: 0.676199480028362
epoch: 67, train loss: 0.6720036602491625, acc: 0.7341659760861844; test loss: 0.7943497672201584, acc: 0.6896714724651383
epoch: 68, train loss: 0.6515821726897687, acc: 0.7396116964602818; test loss: 0.8802945479183607, acc: 0.6646182935476247
epoch: 69, train loss: 0.6668756595444818, acc: 0.7356457914052327; test loss: 0.8689945210127987, acc: 0.6686362562042071
epoch: 70, train loss: 0.6677469822857187, acc: 0.7360009470818042; test loss: 0.7762062878677451, acc: 0.6948711888442448
epoch: 71, train loss: 0.6509512459877554, acc: 0.7415650526814254; test loss: 0.8040493119554694, acc: 0.6887260694871189
epoch: 72, train loss: 0.6628151965434839, acc: 0.735290635728661; test loss: 0.8446084303485111, acc: 0.6771448830063814
epoch: 73, train loss: 0.646680597195163, acc: 0.7414466674559015; test loss: 0.7643843611147183, acc: 0.7085795320255259
epoch: 74, train loss: 0.6422758573425639, acc: 0.7425121344856161; test loss: 0.7693952068147635, acc: 0.708343181281021
epoch: 75, train loss: 0.6411292577503954, acc: 0.7420385935835208; test loss: 0.9522012779544189, acc: 0.6450011817537226
epoch: 76, train loss: 0.644599642685296, acc: 0.74263051971114; test loss: 0.8058446708003896, acc: 0.6851808083195462
epoch: 77, train loss: 0.6178675308317295, acc: 0.7514502190126672; test loss: 0.787736615906988, acc: 0.6875443157645946
epoch: 78, train loss: 0.6306030467951722, acc: 0.7506215224340003; test loss: 0.8048658030615211, acc: 0.6899078232096431
epoch: 79, train loss: 0.6366663656178855, acc: 0.7460044986385699; test loss: 0.7987620061916314, acc: 0.691562278421177
epoch: 80, train loss: 0.6139766630003604, acc: 0.7529892269444773; test loss: 0.8256315854784164, acc: 0.6785629874734106
epoch: 81, train loss: 0.6121359638904162, acc: 0.7577246359654315; test loss: 0.7819807020605273, acc: 0.7014890096903805
epoch: 82, train loss: 0.6164008234610455, acc: 0.7566591689357168; test loss: 0.8960926391305836, acc: 0.6558733160009454
epoch: 83, train loss: 0.6257765799340969, acc: 0.7480170474724754; test loss: 0.765651260079348, acc: 0.7031434649019145
epoch: 84, train loss: 0.6004584299765834, acc: 0.7564223984846691; test loss: 0.8782691655218728, acc: 0.6752540770503427
Epoch    84: reducing learning rate of group 0 to 1.5000e-03.
epoch: 85, train loss: 0.5405739404942929, acc: 0.7823487628743933; test loss: 0.6952699480595472, acc: 0.7355235168990782
epoch: 86, train loss: 0.515098111261429, acc: 0.7909316917248728; test loss: 0.7344260059105141, acc: 0.7241786811628457
epoch: 87, train loss: 0.5017759546494645, acc: 0.798804309222209; test loss: 0.7151375740477729, acc: 0.7315055542424959
epoch: 88, train loss: 0.4976451511536602, acc: 0.7957262933585888; test loss: 0.721079160856654, acc: 0.7291420467974474
epoch: 89, train loss: 0.5078820885392044, acc: 0.794779211554398; test loss: 0.7360776487858519, acc: 0.7196880170172536
epoch: 90, train loss: 0.5081271709063018, acc: 0.7950751746182076; test loss: 0.7287112757038828, acc: 0.7199243677617585
epoch: 91, train loss: 0.4963797838663623, acc: 0.7971469160648751; test loss: 0.7916395483330276, acc: 0.6941621366107303
epoch: 92, train loss: 0.48767404676513926, acc: 0.8005800876050669; test loss: 0.7346097920922009, acc: 0.7151973528716615
epoch: 93, train loss: 0.47876776419857825, acc: 0.8051379187877353; test loss: 0.7328632630177125, acc: 0.7239423304183408
epoch: 94, train loss: 0.4816797072328279, acc: 0.8027110216644963; test loss: 0.7592204984156862, acc: 0.7187426140392342
epoch: 95, train loss: 0.47889040018364254, acc: 0.8040724517580206; test loss: 0.7271872415409616, acc: 0.7255967856298747
epoch: 96, train loss: 0.4654053131108935, acc: 0.8093997869065941; test loss: 0.737472581964834, acc: 0.7239423304183408
epoch: 97, train loss: 0.4626035395036354, acc: 0.8111755652894519; test loss: 0.739475018925713, acc: 0.7272512408414087
epoch: 98, train loss: 0.4693956594820261, acc: 0.8079791642003078; test loss: 0.746880120516556, acc: 0.7279602930749232
epoch: 99, train loss: 0.46154531835804075, acc: 0.8095773647448798; test loss: 0.7881403524411877, acc: 0.7133065469156228
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.36100801391560355, acc: 0.8124778027702143; test loss: 0.5952595297367796, acc: 0.7279602930749232
epoch: 101, train loss: 0.3499644044039164, acc: 0.8130105362850716; test loss: 0.6353537697680479, acc: 0.7168518080831955
epoch: 102, train loss: 0.3460196788471606, acc: 0.8133064993488812; test loss: 0.6406327815969851, acc: 0.7073977783030017
epoch: 103, train loss: 0.34723983372868034, acc: 0.815970166923168; test loss: 0.6246350977443911, acc: 0.7305601512644765
epoch: 104, train loss: 0.35841847585900743, acc: 0.8049603409494495; test loss: 0.6176919615936459, acc: 0.7270148900969038
epoch: 105, train loss: 0.3449770180278944, acc: 0.8131881141233575; test loss: 0.6475046000506574, acc: 0.7196880170172536
epoch: 106, train loss: 0.3501679078952877, acc: 0.8082159346513555; test loss: 0.6570515108852346, acc: 0.7291420467974474
epoch: 107, train loss: 0.3487207899576844, acc: 0.8096957499704037; test loss: 0.6523196897842563, acc: 0.7128338454266131
epoch: 108, train loss: 0.34707015339292635, acc: 0.8139576180892625; test loss: 0.6113872959156799, acc: 0.7234696289293311
epoch: 109, train loss: 0.3248291733145813, acc: 0.8218302355865988; test loss: 0.6672005555049885, acc: 0.7111793902150791
epoch: 110, train loss: 0.3458120337785229, acc: 0.8093405942938321; test loss: 0.6090971111715953, acc: 0.7232332781848263
epoch: 111, train loss: 0.33338467865155635, acc: 0.8162661299869777; test loss: 0.6594992452016031, acc: 0.7166154573386906
epoch: 112, train loss: 0.3353885028141271, acc: 0.8199360719782172; test loss: 0.6831832466269745, acc: 0.725124084140865
epoch: 113, train loss: 0.3438914163452472, acc: 0.811826684029833; test loss: 0.6235620291963928, acc: 0.7279602930749232
epoch: 114, train loss: 0.3377926923282558, acc: 0.8147271220551675; test loss: 0.6397602970620978, acc: 0.7196880170172536
epoch: 115, train loss: 0.34339295242458484, acc: 0.8107020243873565; test loss: 0.6319078819544933, acc: 0.7234696289293311
epoch: 116, train loss: 0.326299646041288, acc: 0.8181011009825974; test loss: 0.7011614533863685, acc: 0.7000709052233515
epoch: 117, train loss: 0.34697576583215045, acc: 0.8088670533917367; test loss: 0.6134845589498232, acc: 0.7229969274403214
epoch: 118, train loss: 0.32604673691821606, acc: 0.8168580561145969; test loss: 0.6398767538877617, acc: 0.7194516662727488
epoch: 119, train loss: 0.3319985998640741, acc: 0.8134840771871671; test loss: 0.7404183479832915, acc: 0.6927440321437013
epoch: 120, train loss: 0.33459971484748996, acc: 0.8124186101574523; test loss: 0.6438129391689387, acc: 0.7253604348853699
epoch: 121, train loss: 0.33220970805642225, acc: 0.814312773765834; test loss: 0.6387025641651194, acc: 0.7185062632947293
epoch: 122, train loss: 0.312360769625287, acc: 0.8228957026163135; test loss: 0.6479830165135891, acc: 0.7227605766958166
epoch: 123, train loss: 0.3150210982316258, acc: 0.8214750799100272; test loss: 0.6283879834296602, acc: 0.7300874497754668
epoch: 124, train loss: 0.3165584338110721, acc: 0.820883153782408; test loss: 0.6605251167322608, acc: 0.7232332781848263
epoch: 125, train loss: 0.31435114054605356, acc: 0.8221261986504085; test loss: 0.6598191551496109, acc: 0.7279602930749232
epoch: 126, train loss: 0.326758519007325, acc: 0.817627560080502; test loss: 0.6872888665141802, acc: 0.7140155991491374
epoch: 127, train loss: 0.31474903186030595, acc: 0.8234284361311708; test loss: 0.6435471214433282, acc: 0.7225242259513117
epoch: 128, train loss: 0.33325672411447277, acc: 0.8185746418846928; test loss: 0.6488737670616235, acc: 0.7147246513826518
epoch: 129, train loss: 0.32502436291060816, acc: 0.814312773765834; test loss: 0.6412565580244973, acc: 0.7222878752068069
epoch: 130, train loss: 0.3063530777070005, acc: 0.8227181247780277; test loss: 0.6420663793834775, acc: 0.7187426140392342
epoch: 131, train loss: 0.30415309438680604, acc: 0.8239611696460282; test loss: 0.6273469185327644, acc: 0.7281966438194281
epoch: 132, train loss: 0.3063085828114504, acc: 0.8253225997395525; test loss: 0.6277993223228175, acc: 0.7222878752068069
epoch: 133, train loss: 0.3163470330535554, acc: 0.820883153782408; test loss: 0.6703677541734592, acc: 0.7154337036161664
epoch: 134, train loss: 0.3068203902165537, acc: 0.8246714809991713; test loss: 0.6954276132065195, acc: 0.7029071141574096
epoch: 135, train loss: 0.30210655217793364, acc: 0.8262696815437434; test loss: 0.6802133766085058, acc: 0.709997636492555
Epoch   135: reducing learning rate of group 0 to 7.5000e-04.
epoch: 136, train loss: 0.2578612423280407, acc: 0.8464543624955606; test loss: 0.6107788744673778, acc: 0.7426140392342235
epoch: 137, train loss: 0.23192813063537185, acc: 0.8610749378477566; test loss: 0.645904659182432, acc: 0.7428503899787284
epoch: 138, train loss: 0.22268108563323238, acc: 0.8652184207410916; test loss: 0.6496041295264129, acc: 0.7456865989127865
epoch: 139, train loss: 0.21826278770500573, acc: 0.8649816502900438; test loss: 0.6517138408166357, acc: 0.7442684944457575
epoch: 140, train loss: 0.2144544422344243, acc: 0.8679412809281402; test loss: 0.6730262072648148, acc: 0.735759867643583
epoch: 141, train loss: 0.21631672461526527, acc: 0.8663430803835681; test loss: 0.6742176191374586, acc: 0.737414322855117
epoch: 142, train loss: 0.214765339542934, acc: 0.8692435184089026; test loss: 0.7153209740882843, acc: 0.734341763176554
epoch: 143, train loss: 0.22929339015414704, acc: 0.8592991594648988; test loss: 0.6488664157281838, acc: 0.7452138974237769
epoch: 144, train loss: 0.21381820188099865, acc: 0.864804072451758; test loss: 0.6878890488565292, acc: 0.7400141810446703
epoch: 145, train loss: 0.2180876687170839, acc: 0.8662246951580442; test loss: 0.6748939383201806, acc: 0.7426140392342235
epoch: 146, train loss: 0.21078214470953777, acc: 0.8682372439919498; test loss: 0.716186486077179, acc: 0.7362325691325927
epoch: 147, train loss: 0.21719837787095733, acc: 0.8622587901029951; test loss: 0.698776698681627, acc: 0.7348144646655637
epoch: 148, train loss: 0.21271302116803478, acc: 0.8684148218302356; test loss: 0.6969741711067663, acc: 0.7312692034979911
epoch: 149, train loss: 0.2150173996984994, acc: 0.8666982360601397; test loss: 0.7342971323568245, acc: 0.7338690616875443
epoch: 150, train loss: 0.2110160853670972, acc: 0.8668166212856635; test loss: 0.7092551033819745, acc: 0.7352871661545733
epoch: 151, train loss: 0.21121590873384188, acc: 0.8670533917367113; test loss: 0.6575594201377593, acc: 0.7463956511463011
epoch: 152, train loss: 0.20621670067938344, acc: 0.8719071859831893; test loss: 0.7085171401204863, acc: 0.7428503899787284
epoch: 153, train loss: 0.21466686495016266, acc: 0.8637977980348053; test loss: 0.685057307692798, acc: 0.7376506735996219
epoch: 154, train loss: 0.2072228285262073, acc: 0.8683556292174737; test loss: 0.7108362877349205, acc: 0.74048688253368
epoch: 155, train loss: 0.20350886256868098, acc: 0.8691843257961407; test loss: 0.6952409407850756, acc: 0.7501772630583786
epoch: 156, train loss: 0.19743314446098384, acc: 0.8743932757191902; test loss: 0.7202884184781667, acc: 0.7371779721106122
epoch: 157, train loss: 0.20272135809884206, acc: 0.8699538297620457; test loss: 0.7161101841301085, acc: 0.7442684944457575
epoch: 158, train loss: 0.1966439409518194, acc: 0.8724991121108086; test loss: 0.716481953021117, acc: 0.7416686362562042
epoch: 159, train loss: 0.20057874848655768, acc: 0.8701906002130934; test loss: 0.7036326508577257, acc: 0.74048688253368
epoch: 160, train loss: 0.1947244572575253, acc: 0.8762874393275719; test loss: 0.7405203529577451, acc: 0.734341763176554
epoch: 161, train loss: 0.19714858920190043, acc: 0.8739197348170948; test loss: 0.7098502836394214, acc: 0.7367052706216024
epoch: 162, train loss: 0.1933662016511429, acc: 0.8761098614892862; test loss: 0.7896848687943036, acc: 0.7376506735996219
epoch: 163, train loss: 0.19666654321023158, acc: 0.8717296081449035; test loss: 0.6985733003600555, acc: 0.7376506735996219
epoch: 164, train loss: 0.19250171219045148, acc: 0.8758138984254765; test loss: 0.70158181779916, acc: 0.7341054124320492
epoch: 165, train loss: 0.1984396617243498, acc: 0.8723807268852847; test loss: 0.6679664065837296, acc: 0.7411959347671945
epoch: 166, train loss: 0.18394465420434386, acc: 0.878714336450811; test loss: 0.7445249943495417, acc: 0.725124084140865
epoch: 167, train loss: 0.18868850972633525, acc: 0.8779448324849058; test loss: 0.7001259314537724, acc: 0.7376506735996219
epoch: 168, train loss: 0.19017298551893974, acc: 0.8796614182550018; test loss: 0.7244647648057823, acc: 0.7289056960529425
epoch: 169, train loss: 0.18729718517266436, acc: 0.8775896768083343; test loss: 0.7357252967805621, acc: 0.7336327109430395
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.15020046260450892, acc: 0.8733870013022375; test loss: 0.6209684634879249, acc: 0.7395414795556606
epoch: 171, train loss: 0.14223885375839218, acc: 0.8722031490469989; test loss: 0.6199647342023261, acc: 0.7364689198770976
epoch: 172, train loss: 0.13837994682470608, acc: 0.8790102995146206; test loss: 0.6959856034057751, acc: 0.7260694871188844
epoch: 173, train loss: 0.1406065007984617, acc: 0.8751035870723334; test loss: 0.6142593754912854, acc: 0.734341763176554
epoch: 174, train loss: 0.1392556483048835, acc: 0.8785367586125251; test loss: 0.647615529831012, acc: 0.7263058378633893
epoch: 175, train loss: 0.1386230853329442, acc: 0.8775304841955724; test loss: 0.6248810268468368, acc: 0.7449775466792721
epoch: 176, train loss: 0.13311011887265617, acc: 0.8788327216763347; test loss: 0.6200004403006802, acc: 0.7423776884897187
epoch: 177, train loss: 0.13562369412242328, acc: 0.8797798034805256; test loss: 0.6641334807498042, acc: 0.7265421886078941
epoch: 178, train loss: 0.13326390836808163, acc: 0.8798981887060495; test loss: 0.6565715564505988, acc: 0.7331600094540298
epoch: 179, train loss: 0.15020215357670885, acc: 0.8692435184089026; test loss: 0.6344783438263535, acc: 0.7293783975419522
epoch: 180, train loss: 0.12879681347833946, acc: 0.8811412335740499; test loss: 0.6431983048063281, acc: 0.7324509572205152
epoch: 181, train loss: 0.12601792904167636, acc: 0.8820883153782408; test loss: 0.6568696773579092, acc: 0.7322146064760104
epoch: 182, train loss: 0.1414812096332084, acc: 0.8767609802296673; test loss: 0.6577981856099766, acc: 0.734341763176554
epoch: 183, train loss: 0.14393626845340213, acc: 0.8748668166212856; test loss: 0.6709520325145663, acc: 0.7232332781848263
epoch: 184, train loss: 0.15511463653255217, acc: 0.865691961643187; test loss: 0.6196794029782213, acc: 0.7359962183880879
epoch: 185, train loss: 0.1476784541918933, acc: 0.8709009115662365; test loss: 0.5996353303926883, acc: 0.7400141810446703
epoch: 186, train loss: 0.13400058141894436, acc: 0.8764650171658577; test loss: 0.6340640345467261, acc: 0.7333963601985346
Epoch   186: reducing learning rate of group 0 to 3.7500e-04.
epoch: 187, train loss: 0.10954410837160788, acc: 0.8909672072925299; test loss: 0.6274601729647034, acc: 0.7492318600803592
epoch: 188, train loss: 0.09747446181101109, acc: 0.9034568485852965; test loss: 0.6556552283675415, acc: 0.7461593004017962
epoch: 189, train loss: 0.09929328851513824, acc: 0.9010891440748194; test loss: 0.670605317494233, acc: 0.7463956511463011
epoch: 190, train loss: 0.09362959574916398, acc: 0.9033384633597727; test loss: 0.6635128992495878, acc: 0.7485228078468447
epoch: 191, train loss: 0.09800387442868758, acc: 0.9024505741683438; test loss: 0.6671737863560485, acc: 0.7473410541243205
epoch: 192, train loss: 0.0921315744411625, acc: 0.9051734343553924; test loss: 0.6720538716934951, acc: 0.7452138974237769
epoch: 193, train loss: 0.09523765839450216, acc: 0.9021546111045341; test loss: 0.6977092284622048, acc: 0.7508863152918932
epoch: 194, train loss: 0.09250501782139532, acc: 0.9057653604830117; test loss: 0.6860033487546701, acc: 0.7471047033798156
epoch: 195, train loss: 0.08991723202369165, acc: 0.9086657985083462; test loss: 0.7032186901335512, acc: 0.7421413377452138
epoch: 196, train loss: 0.09435741377043359, acc: 0.901503492364153; test loss: 0.6916766644820229, acc: 0.7430867407232333
epoch: 197, train loss: 0.09215619295263934, acc: 0.9048774712915828; test loss: 0.6938651862986346, acc: 0.7352871661545733
epoch: 198, train loss: 0.0900968818634138, acc: 0.9069492127382502; test loss: 0.6926913113493751, acc: 0.7468683526353108
epoch: 199, train loss: 0.09354760910337939, acc: 0.9054693974192021; test loss: 0.7013192533798687, acc: 0.7485228078468447
epoch: 200, train loss: 0.09417705817629463, acc: 0.9045815082277732; test loss: 0.6951087755786136, acc: 0.7419049870007091
best test acc 0.7508863152918932 at epoch 193.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9604    0.9697    0.9650      6100
           1     0.9772    0.9244    0.9501       926
           2     0.8619    0.9750    0.9150      2400
           3     0.9709    0.9514    0.9611       843
           4     0.9115    0.9716    0.9406       774
           5     0.9452    0.9815    0.9630      1512
           6     0.8856    0.8617    0.8735      1330
           7     0.9628    0.9148    0.9382       481
           8     0.8875    0.7751    0.8275       458
           9     0.9629    0.9757    0.9692       452
          10     0.9570    0.9317    0.9442       717
          11     0.9749    0.9339    0.9540       333
          12     0.7826    0.1204    0.2087       299
          13     0.8685    0.8104    0.8385       269

    accuracy                         0.9331     16894
   macro avg     0.9221    0.8641    0.8749     16894
weighted avg     0.9321    0.9331    0.9275     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8238    0.8525    0.8379      1525
           1     0.8612    0.7759    0.8163       232
           2     0.6722    0.8020    0.7314       601
           3     0.7970    0.7630    0.7797       211
           4     0.8250    0.8505    0.8376       194
           5     0.7566    0.8386    0.7955       378
           6     0.5271    0.5556    0.5409       333
           7     0.6126    0.5620    0.5862       121
           8     0.6180    0.4783    0.5392       115
           9     0.8300    0.7281    0.7757       114
          10     0.8382    0.6333    0.7215       180
          11     0.6271    0.4405    0.5175        84
          12     0.0667    0.0133    0.0222        75
          13     0.6444    0.4265    0.5133        68

    accuracy                         0.7509      4231
   macro avg     0.6786    0.6229    0.6439      4231
weighted avg     0.7426    0.7509    0.7435      4231

---------------------------------------
program finished.
