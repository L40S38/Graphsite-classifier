seed:  666
save trained model at:  ../trained_models/trained_classifier_model_26.pt
save loss at:  ./results/train_classifier_results_26.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  14780
number of pockets in validation set:  3162
number of pockets in test set:  3183
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv6): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn6): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0006
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b3e59e95a90>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.063663218437577, acc: 0.3817997293640054, val loss: 1.8248398608606424, acc: 0.4443390259329538, test loss: 1.8276809276816908, acc: 0.4555450832547911
epoch: 2, train loss: 1.7163183471991987, acc: 0.47320703653585927, val loss: 1.613703826196129, acc: 0.49683744465528146, test loss: 1.6196051746503835, acc: 0.5083254791077599
epoch: 3, train loss: 1.5965021517021891, acc: 0.5085250338294993, val loss: 1.5246383011303395, acc: 0.5224541429475016, test loss: 1.521462658160968, acc: 0.5268614514608859
epoch: 4, train loss: 1.566495312631372, acc: 0.5154939106901217, val loss: 1.5273149467434481, acc: 0.5325743200506009, test loss: 1.5293655069676577, acc: 0.5322023248507697
epoch: 5, train loss: 1.5067679994967698, acc: 0.5361299052774019, val loss: 1.4856893648308036, acc: 0.5199240986717267, test loss: 1.4728028699357743, acc: 0.525918944392083
epoch: 6, train loss: 1.4612733848040094, acc: 0.5490527740189445, val loss: 1.392755139560205, acc: 0.5664136622390892, test loss: 1.397692882673867, acc: 0.5645617342130066
epoch: 7, train loss: 1.4340114496718885, acc: 0.5625845737483085, val loss: 1.35787325882595, acc: 0.5768500948766604, test loss: 1.3696286201177437, acc: 0.5758718190386428
epoch: 8, train loss: 1.3926715229780329, acc: 0.5776725304465494, val loss: 1.3906270151723419, acc: 0.5670461733080329, test loss: 1.4037139722846268, acc: 0.5626767200754006
epoch: 9, train loss: 1.3851944185238891, acc: 0.577063599458728, val loss: 1.389519478081903, acc: 0.5654648956356736, test loss: 1.3911755658005005, acc: 0.571473452717562
epoch: 10, train loss: 1.3499657615757117, acc: 0.5897834912043302, val loss: 1.3480484677145257, acc: 0.584123972169513, test loss: 1.354473423141374, acc: 0.5808985234055922
epoch: 11, train loss: 1.306901039225484, acc: 0.5991880920162381, val loss: 1.3231055704873247, acc: 0.5834914611005693, test loss: 1.3109065848927883, acc: 0.5922086082312285
epoch: 12, train loss: 1.3058421737608954, acc: 0.6024357239512855, val loss: 1.359808279697387, acc: 0.579696394686907, test loss: 1.373838773584201, acc: 0.5893810870248194
epoch: 13, train loss: 1.2724247045542778, acc: 0.6131935047361299, val loss: 1.2888949237089984, acc: 0.6024667931688804, test loss: 1.2701983626368958, acc: 0.6167137920201068
epoch: 14, train loss: 1.2479047329725852, acc: 0.6186738836265223, val loss: 1.388043715334029, acc: 0.5616698292220114, test loss: 1.3928571655208721, acc: 0.5783851712221175
epoch: 15, train loss: 1.2315347355015385, acc: 0.6289580514208389, val loss: 1.3097059949902625, acc: 0.5879190385831752, test loss: 1.3210350249946923, acc: 0.5978636506440465
epoch: 16, train loss: 1.2057596950311622, acc: 0.6354533152909337, val loss: 1.2754263012262932, acc: 0.6040480708412397, test loss: 1.2971150614426565, acc: 0.590951932139491
epoch: 17, train loss: 1.1794931233655133, acc: 0.6429634641407307, val loss: 1.3245922947593463, acc: 0.5869702719797596, test loss: 1.3245745404052016, acc: 0.5896952560477537
epoch: 18, train loss: 1.1621069918627345, acc: 0.6476995940460081, val loss: 1.2182646811121556, acc: 0.6239721695129665, test loss: 1.1997076032898015, acc: 0.6220546654099905
epoch: 19, train loss: 1.1367372333438859, acc: 0.6558186738836265, val loss: 1.5183401242002212, acc: 0.5528146742567995, test loss: 1.4845749197087121, acc: 0.5526233113415018
epoch: 20, train loss: 1.122698029323263, acc: 0.6622462787550745, val loss: 1.261298319571083, acc: 0.6230234029095509, test loss: 1.229579895425809, acc: 0.6289663839145461
epoch: 21, train loss: 1.0889528479401895, acc: 0.66765899864682, val loss: 1.1117118336896517, acc: 0.6511701454775458, test loss: 1.102942412730249, acc: 0.6562990889098335
epoch: 22, train loss: 1.0706015850113597, acc: 0.6763870094722598, val loss: 1.407347952844522, acc: 0.5898165717900063, test loss: 1.4096665783120221, acc: 0.6044612001256676
epoch: 23, train loss: 1.0585848468888919, acc: 0.6786197564276049, val loss: 1.1066832964666127, acc: 0.6527514231499051, test loss: 1.1174995368132978, acc: 0.6519007225887528
epoch: 24, train loss: 1.049386099533396, acc: 0.6817320703653585, val loss: 1.1486756634214572, acc: 0.6502213788741303, test loss: 1.1194034476644243, acc: 0.6616399622997172
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.8253286684478894, acc: 0.681190798376184, val loss: 0.9658570781227609, acc: 0.6265022137887413, test loss: 0.9806339869587593, acc: 0.6289663839145461
epoch: 26, train loss: 0.8045029818608409, acc: 0.6938430311231394, val loss: 1.081001691920783, acc: 0.5983554712207464, test loss: 1.0437791472592894, acc: 0.6101162425384857
epoch: 27, train loss: 0.8338039169130854, acc: 0.6790257104194858, val loss: 0.9349794820620243, acc: 0.650853889943074, test loss: 0.9216625095724572, acc: 0.65315739868049
epoch: 28, train loss: 0.7729087074812758, acc: 0.7006089309878214, val loss: 0.8397119438851807, acc: 0.6739405439595193, test loss: 0.8358742302407118, acc: 0.6691800188501413
epoch: 29, train loss: 0.7715122842337023, acc: 0.7, val loss: 0.8283555074857052, acc: 0.6701454775458571, test loss: 0.8312256928700679, acc: 0.6779767514923029
epoch: 30, train loss: 0.7527265163171275, acc: 0.7081867388362653, val loss: 0.9004541734288871, acc: 0.6568627450980392, test loss: 0.9015793893983669, acc: 0.6622683003455859
epoch: 31, train loss: 0.7412493514433927, acc: 0.7128552097428958, val loss: 0.8529763841538547, acc: 0.6717267552182163, test loss: 0.8694187806830854, acc: 0.6685516808042727
epoch: 32, train loss: 0.7369624648752329, acc: 0.7150202976995941, val loss: 0.9154905729999578, acc: 0.653067678684377, test loss: 0.8975704756690914, acc: 0.6575557650015709
epoch: 33, train loss: 0.7447840049883026, acc: 0.7089309878213802, val loss: 0.8664325322628322, acc: 0.6540164452877926, test loss: 0.8585200658804809, acc: 0.6584982720703738
epoch: 34, train loss: 0.7252972228278649, acc: 0.71765899864682, val loss: 1.0191529356626525, acc: 0.6201771030993042, test loss: 1.0082735572638768, acc: 0.6142004398366321
epoch: 35, train loss: 0.718108233176323, acc: 0.7206359945872801, val loss: 0.8674139910148413, acc: 0.6672991777356104, test loss: 0.8609943535205818, acc: 0.6672950047125353
epoch: 36, train loss: 0.7197672822962594, acc: 0.7217185385656292, val loss: 0.8310891171937348, acc: 0.676786843769766, test loss: 0.8295754697238205, acc: 0.6852026390197926
epoch: 37, train loss: 0.7024262338917052, acc: 0.7206359945872801, val loss: 0.8309838773328092, acc: 0.6701454775458571, test loss: 0.8226888101130134, acc: 0.6742067232170907
epoch: 38, train loss: 0.6996078604774321, acc: 0.724424898511502, val loss: 0.7749338412420573, acc: 0.706831119544592, test loss: 0.7911388387179696, acc: 0.700282752120641
epoch: 39, train loss: 0.6906047689414637, acc: 0.7294993234100136, val loss: 0.8154176521120307, acc: 0.6884882985452245, test loss: 0.8057079112196434, acc: 0.6996544140747722
epoch: 40, train loss: 0.6781348413962957, acc: 0.732679296346414, val loss: 0.9164739386481925, acc: 0.6593927893738141, test loss: 0.9236138746343692, acc: 0.666980835689601
epoch: 41, train loss: 0.6611663245541801, acc: 0.7381596752368065, val loss: 0.8830168366356789, acc: 0.67235926628716, test loss: 0.8938826792576461, acc: 0.6751492302858938
epoch: 42, train loss: 0.6729669923872683, acc: 0.7343031123139377, val loss: 0.812188205652641, acc: 0.6815306767868438, test loss: 0.806363582611084, acc: 0.6949418787307572
epoch: 43, train loss: 0.6691084036001171, acc: 0.7384303112313938, val loss: 0.9931752435924281, acc: 0.6144845034788109, test loss: 0.9920571341740797, acc: 0.6226830034558593
epoch: 44, train loss: 0.646538037682741, acc: 0.7467523680649526, val loss: 0.7638974601448827, acc: 0.7017710309930424, test loss: 0.7790403344366785, acc: 0.700282752120641
epoch: 45, train loss: 0.6446985532208289, acc: 0.7446549391069012, val loss: 0.8286056720629891, acc: 0.6951296647691335, test loss: 0.8223630379027553, acc: 0.6808042726987119
epoch: 46, train loss: 0.6313833404619735, acc: 0.7514884979702301, val loss: 0.8205931061510041, acc: 0.6998734977862112, test loss: 0.790272489794013, acc: 0.7097078228086711
epoch: 47, train loss: 0.6487714750679646, acc: 0.7426928281461435, val loss: 0.7519227069514526, acc: 0.7039848197343453, test loss: 0.7509360173345548, acc: 0.7175620483820295
epoch: 48, train loss: 0.6146171368668625, acc: 0.7566305818673884, val loss: 0.901367789804747, acc: 0.676786843769766, test loss: 0.8960760644199037, acc: 0.6713792020106818
epoch: 49, train loss: 0.6206436645194223, acc: 0.7548037889039242, val loss: 0.784573269523751, acc: 0.6998734977862112, test loss: 0.8054689745314272, acc: 0.6980835689601005
Epoch    49: reducing learning rate of group 0 to 1.5000e-03.
epoch: 50, train loss: 0.5604213566031604, acc: 0.7757780784844385, val loss: 0.7032912073069023, acc: 0.7305502846299811, test loss: 0.7008101451033659, acc: 0.7408105560791706
epoch: 51, train loss: 0.5129883155125887, acc: 0.7940460081190799, val loss: 0.6858396421573646, acc: 0.7371916508538899, test loss: 0.6828188456494292, acc: 0.7354696826892868
epoch: 52, train loss: 0.4972878312548378, acc: 0.79851150202977, val loss: 0.6913868357002697, acc: 0.7362428842504743, test loss: 0.6911895110327756, acc: 0.7436380772855796
epoch: 53, train loss: 0.4838895505432831, acc: 0.802638700947226, val loss: 0.7085522798553892, acc: 0.7308665401644528, test loss: 0.7300509350351518, acc: 0.7279296261388627
epoch: 54, train loss: 0.48292242446674866, acc: 0.8066982408660351, val loss: 0.7683304264001044, acc: 0.7128399746995573, test loss: 0.796882479886682, acc: 0.7106503298774741
epoch: 55, train loss: 0.48156465751715055, acc: 0.8041948579161028, val loss: 0.7856799707165404, acc: 0.7087286527514232, test loss: 0.8056533539628518, acc: 0.7125353440150801
epoch: 56, train loss: 0.46818153139541535, acc: 0.810148849797023, val loss: 0.7786249514247404, acc: 0.7137887413029728, test loss: 0.7948991972958933, acc: 0.708765315739868
epoch: 57, train loss: 0.4677303032720202, acc: 0.810893098782138, val loss: 0.7477172846133614, acc: 0.7296015180265655, test loss: 0.7476783593495687, acc: 0.7304429783223374
epoch: 58, train loss: 0.45957919367272737, acc: 0.8123815967523681, val loss: 0.763120841256129, acc: 0.7308665401644528, test loss: 0.7526257593902147, acc: 0.727301288092994
epoch: 59, train loss: 0.4583518881275141, acc: 0.8143437077131258, val loss: 0.7182149154185344, acc: 0.7416192283364959, test loss: 0.7292533078135239, acc: 0.7445805843543827
epoch: 60, train loss: 0.4530071260158038, acc: 0.8145466847090663, val loss: 0.8278215097370667, acc: 0.7128399746995573, test loss: 0.814238770667989, acc: 0.7119070059692114
epoch: 61, train loss: 0.4406944236348866, acc: 0.8196887686062246, val loss: 0.716383227360995, acc: 0.7450980392156863, test loss: 0.7161823173682345, acc: 0.7445805843543827
epoch: 62, train loss: 0.4323506790504404, acc: 0.8230040595399188, val loss: 0.7716775750299558, acc: 0.7324478178368121, test loss: 0.7886589924719136, acc: 0.7216462456801759
epoch: 63, train loss: 0.41860173839032244, acc: 0.8292963464140731, val loss: 0.7518701005932049, acc: 0.7283364958886781, test loss: 0.7579672125349844, acc: 0.7222745837260446
epoch: 64, train loss: 0.42609879010746377, acc: 0.824424898511502, val loss: 0.8164883652191838, acc: 0.7024035420619861, test loss: 0.8330141910771098, acc: 0.708765315739868
epoch: 65, train loss: 0.4357470766052664, acc: 0.8207036535859269, val loss: 0.7183258851670827, acc: 0.7501581277672359, test loss: 0.7476142038536491, acc: 0.7386113729186302
epoch: 66, train loss: 0.4016401802735335, acc: 0.83382949932341, val loss: 0.726114384665058, acc: 0.7492093611638204, test loss: 0.7521521362762499, acc: 0.7360980207351555
epoch: 67, train loss: 0.404282894363584, acc: 0.833085250338295, val loss: 0.8019389813644535, acc: 0.7229601518026565, test loss: 0.8135364056382582, acc: 0.7037386113729186
epoch: 68, train loss: 0.4110004237604722, acc: 0.8307848443843031, val loss: 0.7896052436840677, acc: 0.722011385199241, test loss: 0.7945819145248179, acc: 0.7257304429783223
epoch: 69, train loss: 0.4251988917387548, acc: 0.8245602165087956, val loss: 0.71422561266089, acc: 0.7495256166982922, test loss: 0.7319915452694091, acc: 0.7461514294690543
epoch: 70, train loss: 0.3967955948777386, acc: 0.8359269282814614, val loss: 0.852337565473331, acc: 0.7058823529411765, test loss: 0.8743500371717859, acc: 0.7034244423499842
epoch: 71, train loss: 0.4007606864782083, acc: 0.8337618403247632, val loss: 0.754032684533676, acc: 0.7419354838709677, test loss: 0.7486183089053448, acc: 0.7323279924599434
epoch: 72, train loss: 0.3689993229544695, acc: 0.8434370771312585, val loss: 0.8121353115634327, acc: 0.7273877292852625, test loss: 0.822936306724824, acc: 0.7191328934967012
epoch: 73, train loss: 0.3752043634451452, acc: 0.842489851150203, val loss: 0.8293285700130282, acc: 0.7087286527514232, test loss: 0.8345774987041295, acc: 0.7100219918316054
epoch: 74, train loss: 0.3979372223110741, acc: 0.8346414073071718, val loss: 0.7723381422804098, acc: 0.7352941176470589, test loss: 0.7912778715798213, acc: 0.7320138234370092
epoch: 75, train loss: 0.39645015260202154, acc: 0.8318673883626523, val loss: 0.7253447851751667, acc: 0.74573055028463, test loss: 0.7321677753994984, acc: 0.7408105560791706
epoch: 76, train loss: 0.352708023012249, acc: 0.8525710419485791, val loss: 0.7461772360129119, acc: 0.7469955724225174, test loss: 0.7606662832039016, acc: 0.7474081055607917
epoch: 77, train loss: 0.361448306280802, acc: 0.8504059539918809, val loss: 0.7251361729901173, acc: 0.7577482605945604, test loss: 0.7600243926010827, acc: 0.7505497957901351
epoch: 78, train loss: 0.3507080549396907, acc: 0.8543301759133964, val loss: 0.7434996928239759, acc: 0.7488931056293485, test loss: 0.7749467414390955, acc: 0.7360980207351555
epoch: 79, train loss: 0.3280924137454878, acc: 0.8606901217861975, val loss: 0.7430543404904293, acc: 0.7466793168880456, test loss: 0.7719974793468, acc: 0.7442664153314483
epoch: 80, train loss: 0.33122783217765644, acc: 0.8599458728010826, val loss: 0.7251684269069645, acc: 0.765022137887413, test loss: 0.7548634479377692, acc: 0.746779767514923
epoch: 81, train loss: 0.3395049999144145, acc: 0.8581867388362652, val loss: 0.8182206775175477, acc: 0.7264389626818469, test loss: 0.8302268033652346, acc: 0.7257304429783223
epoch: 82, train loss: 0.33964438066592234, acc: 0.8561569688768607, val loss: 0.8448479943151793, acc: 0.7150537634408602, test loss: 0.8624643726690437, acc: 0.7194470625196355
epoch: 83, train loss: 0.3538548808136554, acc: 0.8537889039242219, val loss: 0.8049630385874798, acc: 0.7232764073371284, test loss: 0.8106305349034182, acc: 0.727301288092994
epoch: 84, train loss: 0.35758609853190887, acc: 0.8495263870094723, val loss: 0.866329672817638, acc: 0.732764073371284, test loss: 0.8989492919585108, acc: 0.728243795161797
epoch: 85, train loss: 0.3504937663613863, acc: 0.8552097428958051, val loss: 0.748179574618982, acc: 0.7466793168880456, test loss: 0.7783196422614057, acc: 0.7338988375746152
epoch: 86, train loss: 0.36025765940688137, acc: 0.849594046008119, val loss: 0.8088949417329, acc: 0.7254901960784313, test loss: 0.8023176475624374, acc: 0.728243795161797
epoch: 87, train loss: 0.33038891236498164, acc: 0.8575778078484438, val loss: 0.7479747656702166, acc: 0.7473118279569892, test loss: 0.7788876144158702, acc: 0.7404963870562362
epoch: 88, train loss: 0.32498073598044813, acc: 0.8655615696887686, val loss: 0.7961263872263629, acc: 0.7507906388361796, test loss: 0.8224484084576059, acc: 0.7470939365378574
epoch: 89, train loss: 0.3284618142977135, acc: 0.8600811907983762, val loss: 0.7966762311393742, acc: 0.7378241619228336, test loss: 0.819918604043906, acc: 0.7335846685516808
epoch: 90, train loss: 0.3198345088749035, acc: 0.8670500676589986, val loss: 0.9949382951484309, acc: 0.6910183428209994, test loss: 0.9713047534985771, acc: 0.7015394282123782
epoch: 91, train loss: 0.3130432746165176, acc: 0.8660351826792964, val loss: 0.7633280743533188, acc: 0.7517394054395952, test loss: 0.8236292493519377, acc: 0.7426955702167767
epoch: 92, train loss: 0.29345398242805903, acc: 0.8744925575101489, val loss: 0.8070302584139627, acc: 0.7466793168880456, test loss: 0.8236940858351371, acc: 0.7436380772855796
epoch: 93, train loss: 0.29616688196326785, acc: 0.8750338294993234, val loss: 0.8305536700833228, acc: 0.7409867172675522, test loss: 0.8341268086410939, acc: 0.7357838517122212
epoch: 94, train loss: 0.3256183384316539, acc: 0.8592016238159675, val loss: 1.1766892440103414, acc: 0.6524351676154333, test loss: 1.221545187212782, acc: 0.6556707508639649
epoch: 95, train loss: 0.2825289666088089, acc: 0.8790933694181327, val loss: 0.8511514739097484, acc: 0.7280202403542062, test loss: 0.8907008379313918, acc: 0.7225887527489789
epoch: 96, train loss: 0.3135504077026099, acc: 0.8656968876860622, val loss: 0.7892452396221511, acc: 0.7362428842504743, test loss: 0.8169048334344018, acc: 0.7291863022306001
epoch: 97, train loss: 0.32104109424054217, acc: 0.8655615696887686, val loss: 0.7189441380208237, acc: 0.7583807716635041, test loss: 0.7699477562468419, acc: 0.7574615142946906
epoch: 98, train loss: 0.3067164240493181, acc: 0.8699594046008119, val loss: 0.8034653390682928, acc: 0.7359266287160026, test loss: 0.8387623084928493, acc: 0.7276154571159283
epoch: 99, train loss: 0.2836243919574521, acc: 0.8769959404600812, val loss: 0.7955292227296872, acc: 0.7577482605945604, test loss: 0.8419481617378957, acc: 0.7518064718818724
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.1976617956758352, acc: 0.8860622462787551, val loss: 0.6389035780470557, acc: 0.7659709044908286, test loss: 0.6699603871283499, acc: 0.7599748664781653
epoch: 101, train loss: 0.1960116916523896, acc: 0.8881596752368065, val loss: 0.6586740811665853, acc: 0.7428842504743833, test loss: 0.6619151416830098, acc: 0.7533773169965441
epoch: 102, train loss: 0.20724180773082057, acc: 0.8803112313937753, val loss: 0.6786349670258147, acc: 0.75426944971537, test loss: 0.7103161067285517, acc: 0.7477222745837261
epoch: 103, train loss: 0.19515602070036697, acc: 0.8865358592692828, val loss: 0.639385194057607, acc: 0.7599620493358634, test loss: 0.654549194101339, acc: 0.7612315425699026
epoch: 104, train loss: 0.20566548921258587, acc: 0.8827469553450609, val loss: 0.6996005612939766, acc: 0.743516761543327, test loss: 0.735776751737268, acc: 0.7279296261388627
epoch: 105, train loss: 0.2246135913228795, acc: 0.8713125845737483, val loss: 0.6638054271652154, acc: 0.7501581277672359, test loss: 0.6717832961847975, acc: 0.7445805843543827
epoch: 106, train loss: 0.20858041713460374, acc: 0.8798376184032476, val loss: 0.7065045620956276, acc: 0.7384566729917773, test loss: 0.7329482039003076, acc: 0.7414388941250393
epoch: 107, train loss: 0.23076924036820298, acc: 0.8687415426251691, val loss: 0.8383590775453011, acc: 0.7052498418722327, test loss: 0.8954438399938979, acc: 0.6833176248821866
epoch: 108, train loss: 0.20499723239906426, acc: 0.879702300405954, val loss: 0.6604513294858166, acc: 0.7533206831119544, test loss: 0.6695366806104578, acc: 0.7433239082626453
epoch: 109, train loss: 0.20309868058106573, acc: 0.8788227334235453, val loss: 0.6595593760995304, acc: 0.7586970271979759, test loss: 0.6649653884625233, acc: 0.7502356267672008
epoch: 110, train loss: 0.20115184860155289, acc: 0.8798376184032476, val loss: 0.6634079030915803, acc: 0.7514231499051234, test loss: 0.704351376371117, acc: 0.7436380772855796
epoch: 111, train loss: 0.19227333435831598, acc: 0.8889039242219215, val loss: 0.6507823642185412, acc: 0.7571157495256167, test loss: 0.7020071358415566, acc: 0.7417530631479736
Epoch   111: reducing learning rate of group 0 to 7.5000e-04.
epoch: 112, train loss: 0.1613800683710346, acc: 0.9014208389715832, val loss: 0.6819663126961782, acc: 0.7656546489563567, test loss: 0.7028082517419564, acc: 0.7483506126295947
epoch: 113, train loss: 0.15207658057203152, acc: 0.9046684709066306, val loss: 0.6473240099852029, acc: 0.7669196710942442, test loss: 0.6662952640765949, acc: 0.7662582469368521
epoch: 114, train loss: 0.11977580027586714, acc: 0.9191474966170501, val loss: 0.7123582015529153, acc: 0.7637571157495257, test loss: 0.7275595310308857, acc: 0.7596606974552309
epoch: 115, train loss: 0.1077740437727981, acc: 0.928958051420839, val loss: 0.7150909473140507, acc: 0.7713472485768501, test loss: 0.7401194737985979, acc: 0.7618598806157713
epoch: 116, train loss: 0.12524734412462366, acc: 0.9164411366711772, val loss: 0.7217371475839826, acc: 0.7643896268184693, test loss: 0.721569109891819, acc: 0.764373232799246
epoch: 117, train loss: 0.1100281571321623, acc: 0.927807848443843, val loss: 0.7501360136523118, acc: 0.767235926628716, test loss: 0.7408070486370139, acc: 0.7650015708451147
epoch: 118, train loss: 0.10892992091638633, acc: 0.9273342354533153, val loss: 0.7308339961038067, acc: 0.7688172043010753, test loss: 0.7202767094762828, acc: 0.7703424442349984
epoch: 119, train loss: 0.10273005509892723, acc: 0.9307848443843031, val loss: 0.7537443278938212, acc: 0.7647058823529411, test loss: 0.7394464165768158, acc: 0.763430725730443
epoch: 120, train loss: 0.10279593253974502, acc: 0.929702300405954, val loss: 0.7553581325861565, acc: 0.7618595825426945, test loss: 0.7698407184017779, acc: 0.7590323594093622
epoch: 121, train loss: 0.10097076009268367, acc: 0.9339648173207037, val loss: 0.7705383375428433, acc: 0.7681846932321316, test loss: 0.7719685067330522, acc: 0.7640590637763116
epoch: 122, train loss: 0.10190330994225968, acc: 0.93382949932341, val loss: 0.779888041482403, acc: 0.7681846932321316, test loss: 0.7902513093406062, acc: 0.7656299088909834
epoch: 123, train loss: 0.11071718039951402, acc: 0.925575101488498, val loss: 0.7721107227752512, acc: 0.7602783048703352, test loss: 0.77411694418811, acc: 0.7521206409048068
epoch: 124, train loss: 0.11036308463032739, acc: 0.9246955345060893, val loss: 0.7663069516326113, acc: 0.7685009487666035, test loss: 0.7736618583995593, acc: 0.7602890355010996
epoch: 125, train loss: 0.11075430329137952, acc: 0.9259133964817321, val loss: 0.8006151153798499, acc: 0.7637571157495257, test loss: 0.776455983140354, acc: 0.7662582469368521
epoch: 126, train loss: 0.11409214744709825, acc: 0.9223951285520974, val loss: 0.7687812637181919, acc: 0.765022137887413, test loss: 0.79932965754414, acc: 0.7599748664781653
epoch: 127, train loss: 0.10500407706900766, acc: 0.9284167794316645, val loss: 0.760621102379516, acc: 0.7659709044908286, test loss: 0.7774150280168961, acc: 0.7552623311341502
epoch: 128, train loss: 0.10849866997727844, acc: 0.926319350473613, val loss: 0.7564257429341287, acc: 0.7640733712839974, test loss: 0.7690035672896395, acc: 0.7555765001570846
epoch: 129, train loss: 0.0951512173160971, acc: 0.9370771312584574, val loss: 0.7447561317422132, acc: 0.7726122707147375, test loss: 0.774670661338425, acc: 0.760603204524034
epoch: 130, train loss: 0.08973626241350045, acc: 0.9352503382949933, val loss: 0.7778905998069526, acc: 0.7656546489563567, test loss: 0.8081335279127355, acc: 0.7577756833176249
epoch: 131, train loss: 0.0978041257179961, acc: 0.9330175913396481, val loss: 0.8148411288433328, acc: 0.7495256166982922, test loss: 0.8393360484219407, acc: 0.7483506126295947
epoch: 132, train loss: 0.09219183429611874, acc: 0.9344384303112314, val loss: 0.7755290906237772, acc: 0.7710309930423782, test loss: 0.8216773232985247, acc: 0.7628023876845743
epoch: 133, train loss: 0.09749020007571284, acc: 0.9328822733423545, val loss: 0.8041197639865864, acc: 0.7647058823529411, test loss: 0.8527204941699537, acc: 0.7533773169965441
epoch: 134, train loss: 0.10710859317256892, acc: 0.9273342354533153, val loss: 0.7678121332123942, acc: 0.7624920936116382, test loss: 0.805055879325639, acc: 0.7555765001570846
epoch: 135, train loss: 0.11109575038833773, acc: 0.9261163734776725, val loss: 0.7501395864625433, acc: 0.7713472485768501, test loss: 0.7681483893584725, acc: 0.7640590637763116
epoch: 136, train loss: 0.09929703462752987, acc: 0.9318673883626523, val loss: 0.7886756647243898, acc: 0.7653383934218849, test loss: 0.8129508490087549, acc: 0.7668865849827207
epoch: 137, train loss: 0.09418720545320292, acc: 0.9369418132611638, val loss: 0.8138557893426113, acc: 0.7615433270082227, test loss: 0.8350301352904497, acc: 0.7599748664781653
epoch: 138, train loss: 0.1059451111820457, acc: 0.9271989174560217, val loss: 0.7766245369063685, acc: 0.7590132827324478, test loss: 0.8127982125056078, acc: 0.7524348099277411
epoch: 139, train loss: 0.09957564586171117, acc: 0.9278755074424898, val loss: 0.801347326734713, acc: 0.7552182163187856, test loss: 0.7916181970608898, acc: 0.7580898523405593
epoch: 140, train loss: 0.09498462512867377, acc: 0.9335588633288228, val loss: 0.7987971500382252, acc: 0.7514231499051234, test loss: 0.8150224031759364, acc: 0.7590323594093622
epoch: 141, train loss: 0.09330082458213315, acc: 0.9337618403247632, val loss: 0.8076333495199793, acc: 0.7526881720430108, test loss: 0.7927585131261395, acc: 0.7558906691800189
epoch: 142, train loss: 0.0903807141179161, acc: 0.93382949932341, val loss: 0.766247096655868, acc: 0.7583807716635041, test loss: 0.791417897590256, acc: 0.7568331762488218
epoch: 143, train loss: 0.09950895253392453, acc: 0.9316644113667117, val loss: 0.7910132071852759, acc: 0.7643896268184693, test loss: 0.7992869887080838, acc: 0.7621740496387056
epoch: 144, train loss: 0.10239998726060813, acc: 0.9286197564276049, val loss: 0.7974300024103772, acc: 0.7615433270082227, test loss: 0.7917479276657104, acc: 0.7621740496387056
epoch: 145, train loss: 0.08736854999934869, acc: 0.9348443843031123, val loss: 0.8140865943614325, acc: 0.7590132827324478, test loss: 0.8106930425172327, acc: 0.7640590637763116
epoch: 146, train loss: 0.09259804522716951, acc: 0.9340324763193505, val loss: 0.7836415502257471, acc: 0.765022137887413, test loss: 0.8061725825885913, acc: 0.7612315425699026
epoch: 147, train loss: 0.09685749109930211, acc: 0.9326792963464141, val loss: 0.7814415083287413, acc: 0.767235926628716, test loss: 0.8062568326585142, acc: 0.7612315425699026
epoch: 148, train loss: 0.09562958774934441, acc: 0.9339648173207037, val loss: 0.7887140431787152, acc: 0.7653383934218849, test loss: 0.7908842282740155, acc: 0.760603204524034
epoch: 149, train loss: 0.08801248898283555, acc: 0.9380243572395128, val loss: 0.9250196051853959, acc: 0.7318153067678684, test loss: 0.9196540101908529, acc: 0.7379830348727615
epoch: 150, train loss: 0.10075973309425282, acc: 0.9322056833558864, val loss: 0.7884601875621258, acc: 0.7580645161290323, test loss: 0.807033410882785, acc: 0.7521206409048068
epoch: 151, train loss: 0.08661255327347006, acc: 0.9374830852503383, val loss: 0.7982915860802822, acc: 0.7656546489563567, test loss: 0.8285712147148533, acc: 0.7590323594093622
epoch: 152, train loss: 0.08441324883489712, acc: 0.9376860622462787, val loss: 0.9056526748987785, acc: 0.7387729285262492, test loss: 0.9404590602494844, acc: 0.7329563305058121
epoch: 153, train loss: 0.09313003625493894, acc: 0.9318673883626523, val loss: 0.8116068426828306, acc: 0.7612270714737508, test loss: 0.8490373789421463, acc: 0.7477222745837261
epoch: 154, train loss: 0.08443622191276537, acc: 0.9378213802435724, val loss: 0.8063370575262429, acc: 0.7605945604048071, test loss: 0.844642045772791, acc: 0.7521206409048068
epoch: 155, train loss: 0.09965778616268355, acc: 0.9291610284167794, val loss: 0.8504175595434914, acc: 0.7523719165085389, test loss: 0.8477880607938152, acc: 0.7511781338360037
epoch: 156, train loss: 0.10433000617324097, acc: 0.9296346414073072, val loss: 0.8064336774471362, acc: 0.7577482605945604, test loss: 0.8281519440708766, acc: 0.7518064718818724
epoch: 157, train loss: 0.08313757130354442, acc: 0.9414749661705006, val loss: 0.7799347343511177, acc: 0.7653383934218849, test loss: 0.8234230451077513, acc: 0.7618598806157713
epoch: 158, train loss: 0.07151705559761502, acc: 0.9476995940460081, val loss: 0.863603546979833, acc: 0.7628083491461101, test loss: 0.8820664198788839, acc: 0.7587181903864278
epoch: 159, train loss: 0.07334550492899021, acc: 0.9446549391069012, val loss: 0.8448862721843104, acc: 0.7602783048703352, test loss: 0.8375631877546823, acc: 0.7568331762488218
epoch: 160, train loss: 0.07537081032507474, acc: 0.9426251691474966, val loss: 0.8658705302087059, acc: 0.75426944971537, test loss: 0.8965305957710298, acc: 0.7505497957901351
epoch: 161, train loss: 0.08027901465579847, acc: 0.9442489851150203, val loss: 0.826652932212294, acc: 0.7634408602150538, test loss: 0.8432047639745532, acc: 0.7602890355010996
epoch: 162, train loss: 0.09203260780308341, acc: 0.9362652232746955, val loss: 0.8368515883873415, acc: 0.7460468058191019, test loss: 0.862796144593335, acc: 0.7417530631479736
Epoch   162: reducing learning rate of group 0 to 3.7500e-04.
epoch: 163, train loss: 0.06479840061963814, acc: 0.95148849797023, val loss: 0.8052377200895741, acc: 0.7669196710942442, test loss: 0.8221710334961196, acc: 0.7678290920515237
epoch: 164, train loss: 0.049209056379265974, acc: 0.961299052774019, val loss: 0.8126766954623469, acc: 0.7688172043010753, test loss: 0.8278501543877375, acc: 0.7725416273955388
epoch: 165, train loss: 0.04297720782146241, acc: 0.9658998646820027, val loss: 0.8241461081869763, acc: 0.7726122707147375, test loss: 0.8391003858882078, acc: 0.7662582469368521
epoch: 166, train loss: 0.04066501295393149, acc: 0.96914749661705, val loss: 0.8467404150193737, acc: 0.7726122707147375, test loss: 0.8516463929289285, acc: 0.7775683317624882
epoch: 167, train loss: 0.03840989352926027, acc: 0.9694181326116373, val loss: 0.8591091079398245, acc: 0.7741935483870968, test loss: 0.867620013998321, acc: 0.7706566132579328
epoch: 168, train loss: 0.03572455061869305, acc: 0.971447902571042, val loss: 0.863023129239405, acc: 0.7770398481973435, test loss: 0.8996076798086978, acc: 0.7703424442349984
epoch: 169, train loss: 0.03552966832318148, acc: 0.9723274695534506, val loss: 0.8973812688987969, acc: 0.7675521821631879, test loss: 0.9100418818884588, acc: 0.7656299088909834
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.023647565407746537, acc: 0.9718538565629229, val loss: 0.8324999942725252, acc: 0.7697659709044908, test loss: 0.8643663113798093, acc: 0.7665724159597863
epoch: 171, train loss: 0.02309703434312134, acc: 0.9716508795669824, val loss: 0.7886338065049077, acc: 0.7722960151802657, test loss: 0.8261295311712372, acc: 0.7715991203267358
epoch: 172, train loss: 0.023363507729583243, acc: 0.9696887686062247, val loss: 0.788642298639914, acc: 0.7685009487666035, test loss: 0.8180509873467199, acc: 0.7693999371661954
epoch: 173, train loss: 0.02142090209846729, acc: 0.9727334235453315, val loss: 0.7943723584035769, acc: 0.7722960151802657, test loss: 0.8094282318801952, acc: 0.7756833176248822
epoch: 174, train loss: 0.02118714198732247, acc: 0.9743572395128552, val loss: 0.7751527562464136, acc: 0.7735610373181531, test loss: 0.8137331743716745, acc: 0.7681432610744581
epoch: 175, train loss: 0.020919753278886673, acc: 0.9711772665764546, val loss: 0.7874598454102016, acc: 0.7710309930423782, test loss: 0.8174807226886324, acc: 0.7737983034872762
epoch: 176, train loss: 0.023258320513371684, acc: 0.9694181326116373, val loss: 0.7861144685956363, acc: 0.7675521821631879, test loss: 0.798688097641298, acc: 0.7731699654414075
epoch: 177, train loss: 0.02212227643826656, acc: 0.9725981055480379, val loss: 0.7838153896416237, acc: 0.7703984819734345, test loss: 0.8006281020541705, acc: 0.7731699654414075
epoch: 178, train loss: 0.027278589302515144, acc: 0.9670500676589987, val loss: 0.7929935369364903, acc: 0.7707147375079064, test loss: 0.8078432801456673, acc: 0.7687715991203268
epoch: 179, train loss: 0.03683434247577384, acc: 0.9598782138024358, val loss: 0.7842754702414235, acc: 0.7615433270082227, test loss: 0.8015815118854989, acc: 0.7609173735469683
epoch: 180, train loss: 0.03373901588345735, acc: 0.9618403247631935, val loss: 0.7498047103013215, acc: 0.7703984819734345, test loss: 0.7616379734408433, acc: 0.7665724159597863
epoch: 181, train loss: 0.025156340524251794, acc: 0.9707036535859269, val loss: 0.7540667467219855, acc: 0.7751423149905123, test loss: 0.7765726383999013, acc: 0.7706566132579328
epoch: 182, train loss: 0.02297606472687163, acc: 0.9706359945872801, val loss: 0.7673337779265277, acc: 0.7732447817836812, test loss: 0.7738757990382581, acc: 0.7744266415331448
epoch: 183, train loss: 0.02224178914590682, acc: 0.9705006765899865, val loss: 0.7603340822258454, acc: 0.7722960151802657, test loss: 0.8155733176533096, acc: 0.7693999371661954
epoch: 184, train loss: 0.02186176529575104, acc: 0.972192151556157, val loss: 0.7692496328697711, acc: 0.7783048703352309, test loss: 0.8055497796663578, acc: 0.7684574300973924
epoch: 185, train loss: 0.02990022173733608, acc: 0.9672530446549391, val loss: 0.7949060030484184, acc: 0.7681846932321316, test loss: 0.797548539452009, acc: 0.7637448947533774
epoch: 186, train loss: 0.046541954211497504, acc: 0.953044654939107, val loss: 0.7530685758982785, acc: 0.765022137887413, test loss: 0.7607884252367251, acc: 0.7596606974552309
epoch: 187, train loss: 0.03448609510217371, acc: 0.9616373477672531, val loss: 0.7589933083526399, acc: 0.7669196710942442, test loss: 0.745153661198186, acc: 0.7615457115928369
epoch: 188, train loss: 0.030959459350864037, acc: 0.9625845737483085, val loss: 0.7777799442551846, acc: 0.7719797596457938, test loss: 0.7924483260605045, acc: 0.7709707822808671
epoch: 189, train loss: 0.02810382106630018, acc: 0.9673883626522327, val loss: 0.760511912702383, acc: 0.7666034155597723, test loss: 0.7631898813939791, acc: 0.7684574300973924
epoch: 190, train loss: 0.02543424698492671, acc: 0.9670500676589987, val loss: 0.7841247090201525, acc: 0.7666034155597723, test loss: 0.7982019039583101, acc: 0.7640590637763116
epoch: 191, train loss: 0.02295102570584001, acc: 0.9700947225981056, val loss: 0.7808360779760458, acc: 0.7643896268184693, test loss: 0.7884942005911302, acc: 0.7693999371661954
epoch: 192, train loss: 0.028479966721894453, acc: 0.9677266576454668, val loss: 0.7954605313360804, acc: 0.7643896268184693, test loss: 0.8002595313494961, acc: 0.765315739868049
epoch: 193, train loss: 0.03419055154040476, acc: 0.9631935047361299, val loss: 0.7709859125528209, acc: 0.7653383934218849, test loss: 0.8055524650775072, acc: 0.760603204524034
epoch: 194, train loss: 0.03310402193899406, acc: 0.9629905277401895, val loss: 0.778316097422388, acc: 0.7628083491461101, test loss: 0.7781958734318778, acc: 0.7646874018221803
epoch: 195, train loss: 0.029491133884059238, acc: 0.9663058186738837, val loss: 0.7592778546684984, acc: 0.7624920936116382, test loss: 0.7646189858768857, acc: 0.7684574300973924
epoch: 196, train loss: 0.028590891309616853, acc: 0.9664411366711773, val loss: 0.7839963935282621, acc: 0.7612270714737508, test loss: 0.7800923502748581, acc: 0.7715991203267358
epoch: 197, train loss: 0.024851968431283237, acc: 0.9686738836265223, val loss: 0.7671714985694861, acc: 0.7707147375079064, test loss: 0.7710384088157297, acc: 0.7684574300973924
epoch: 198, train loss: 0.02239486391785906, acc: 0.972936400541272, val loss: 0.7859295226591135, acc: 0.7688172043010753, test loss: 0.7866770558637378, acc: 0.7681432610744581
epoch: 199, train loss: 0.02742153903285447, acc: 0.9655615696887686, val loss: 0.7829053688471563, acc: 0.7643896268184693, test loss: 0.7797251897452951, acc: 0.767200754005655
epoch: 200, train loss: 0.029976646322126883, acc: 0.9651556156968877, val loss: 0.7880501327900703, acc: 0.7621758380771664, test loss: 0.7853814575831695, acc: 0.7590323594093622
best val acc 0.7783048703352309 at epoch 184.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9976    0.9985    0.9980      5337
           1     0.9853    0.9963    0.9908       810
           2     0.9686    0.9976    0.9829      2100
           3     0.9959    0.9973    0.9966       737
           4     0.9940    0.9852    0.9896       677
           5     0.9910    0.9970    0.9940      1323
           6     0.9791    0.9656    0.9723      1164
           7     0.9835    0.9929    0.9882       421
           8     0.9925    0.9875    0.9900       401
           9     0.9899    0.9924    0.9912       396
          10     0.9984    0.9825    0.9904       627
          11     1.0000    1.0000    1.0000       291
          12     0.9845    0.7318    0.8396       261
          13     0.9102    0.9489    0.9292       235

    accuracy                         0.9881     14780
   macro avg     0.9836    0.9695    0.9752     14780
weighted avg     0.9882    0.9881    0.9878     14780

train confusion matrix:
[[9.98501031e-01 0.00000000e+00 1.87371182e-04 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.12422709e-03
  1.87371182e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 9.96296296e-01 0.00000000e+00 0.00000000e+00
  1.23456790e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.23456790e-03 0.00000000e+00 1.23456790e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.76190476e-04 0.00000000e+00 9.97619048e-01 4.76190476e-04
  4.76190476e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.52380952e-04 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 1.35685210e-03 9.97286296e-01
  0.00000000e+00 0.00000000e+00 1.35685210e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.47710487e-02 0.00000000e+00 0.00000000e+00
  9.85228951e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 7.55857899e-04 0.00000000e+00 0.00000000e+00
  1.51171580e-03 9.96976568e-01 0.00000000e+00 0.00000000e+00
  7.55857899e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [8.59106529e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.03092784e-02 9.65635739e-01 0.00000000e+00
  0.00000000e+00 3.43642612e-03 0.00000000e+00 0.00000000e+00
  8.59106529e-04 1.89003436e-02]
 [4.75059382e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.37529691e-03 9.92874109e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.24688279e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.87531172e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 2.52525253e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 5.05050505e-03 0.00000000e+00
  0.00000000e+00 9.92424242e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.59489633e-03 0.00000000e+00 1.59489633e-03
  0.00000000e+00 0.00000000e+00 1.27591707e-02 1.59489633e-03
  0.00000000e+00 0.00000000e+00 9.82456140e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.53256705e-02 0.00000000e+00 2.49042146e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.83141762e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  7.31800766e-01 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.25531915e-03
  0.00000000e+00 0.00000000e+00 4.68085106e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.48936170e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8291    0.8828    0.8551      1143
           1     0.8363    0.8266    0.8314       173
           2     0.8138    0.7867    0.8000       450
           3     0.8649    0.8101    0.8366       158
           4     0.8158    0.8552    0.8350       145
           5     0.8301    0.7597    0.7934       283
           6     0.5668    0.6305    0.5970       249
           7     0.7733    0.6444    0.7030        90
           8     0.5949    0.5529    0.5732        85
           9     0.8333    0.7738    0.8025        84
          10     0.8362    0.7239    0.7760       134
          11     0.6042    0.4677    0.5273        62
          12     0.1250    0.1429    0.1333        56
          13     0.6279    0.5400    0.5806        50

    accuracy                         0.7783      3162
   macro avg     0.7108    0.6712    0.6889      3162
weighted avg     0.7804    0.7783    0.7780      3162

validation confusion matrix:
[[0.88276465 0.00262467 0.02449694 0.00174978 0.         0.00174978
  0.03499563 0.01224847 0.00874891 0.00349956 0.00174978 0.00437445
  0.01662292 0.00437445]
 [0.04046243 0.8265896  0.         0.         0.05202312 0.01734104
  0.         0.         0.04046243 0.         0.01734104 0.
  0.00578035 0.        ]
 [0.08444444 0.00666667 0.78666667 0.00888889 0.00444444 0.01333333
  0.05111111 0.         0.00222222 0.00222222 0.00444444 0.00444444
  0.02444444 0.00666667]
 [0.03797468 0.00632911 0.02531646 0.81012658 0.00632911 0.01898734
  0.03797468 0.         0.         0.         0.01265823 0.03164557
  0.01265823 0.        ]
 [0.0137931  0.06896552 0.00689655 0.         0.85517241 0.05517241
  0.         0.         0.         0.         0.         0.
  0.         0.        ]
 [0.0565371  0.02120141 0.01413428 0.01766784 0.04946996 0.75971731
  0.         0.00706714 0.03180212 0.         0.00353357 0.01413428
  0.02473498 0.        ]
 [0.14056225 0.00401606 0.08032129 0.01204819 0.         0.02008032
  0.63052209 0.         0.         0.03212851 0.02008032 0.00401606
  0.02811245 0.02811245]
 [0.3        0.         0.         0.01111111 0.         0.
  0.01111111 0.64444444 0.         0.         0.01111111 0.
  0.01111111 0.01111111]
 [0.23529412 0.02352941 0.02352941 0.         0.01176471 0.09411765
  0.02352941 0.         0.55294118 0.         0.         0.01176471
  0.02352941 0.        ]
 [0.07142857 0.         0.         0.         0.         0.
  0.13095238 0.         0.         0.77380952 0.01190476 0.
  0.01190476 0.        ]
 [0.05970149 0.00746269 0.03731343 0.01492537 0.         0.01492537
  0.10447761 0.         0.02238806 0.         0.7238806  0.
  0.01492537 0.        ]
 [0.33870968 0.01612903 0.01612903 0.03225806 0.01612903 0.0483871
  0.         0.01612903 0.01612903 0.         0.         0.46774194
  0.03225806 0.        ]
 [0.26785714 0.         0.28571429 0.01785714 0.         0.07142857
  0.14285714 0.         0.01785714 0.         0.03571429 0.01785714
  0.14285714 0.        ]
 [0.14       0.         0.         0.         0.         0.
  0.3        0.         0.         0.         0.         0.
  0.02       0.54      ]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8473    0.8769    0.8618      1145
           1     0.7910    0.8000    0.7955       175
           2     0.7860    0.7982    0.7921       451
           3     0.8301    0.7987    0.8141       159
           4     0.8406    0.7945    0.8169       146
           5     0.8687    0.7923    0.8287       284
           6     0.4667    0.5880    0.5204       250
           7     0.7111    0.7033    0.7072        91
           8     0.6486    0.5517    0.5963        87
           9     0.8286    0.6744    0.7436        86
          10     0.8286    0.6397    0.7220       136
          11     0.6731    0.5469    0.6034        64
          12     0.1077    0.1228    0.1148        57
          13     0.6667    0.5385    0.5957        52

    accuracy                         0.7685      3183
   macro avg     0.7068    0.6590    0.6795      3183
weighted avg     0.7760    0.7685    0.7703      3183

test confusion matrix:
[[8.76855895e-01 3.49344978e-03 2.79475983e-02 1.74672489e-03
  8.73362445e-04 3.49344978e-03 4.01746725e-02 1.13537118e-02
  1.31004367e-02 2.62008734e-03 8.73362445e-04 6.11353712e-03
  9.60698690e-03 1.74672489e-03]
 [2.85714286e-02 8.00000000e-01 1.71428571e-02 5.71428571e-03
  6.85714286e-02 1.71428571e-02 1.71428571e-02 0.00000000e+00
  2.85714286e-02 0.00000000e+00 1.71428571e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [7.53880266e-02 4.43458980e-03 7.98226164e-01 8.86917960e-03
  0.00000000e+00 4.43458980e-03 5.54323725e-02 2.21729490e-03
  0.00000000e+00 4.43458980e-03 2.21729490e-03 6.65188470e-03
  3.32594235e-02 4.43458980e-03]
 [1.25786164e-02 0.00000000e+00 2.51572327e-02 7.98742138e-01
  6.28930818e-03 3.14465409e-02 6.91823899e-02 6.28930818e-03
  0.00000000e+00 0.00000000e+00 1.88679245e-02 1.25786164e-02
  1.88679245e-02 0.00000000e+00]
 [6.84931507e-03 1.09589041e-01 6.84931507e-03 6.84931507e-03
  7.94520548e-01 5.47945205e-02 6.84931507e-03 0.00000000e+00
  6.84931507e-03 0.00000000e+00 0.00000000e+00 6.84931507e-03
  0.00000000e+00 0.00000000e+00]
 [5.28169014e-02 1.76056338e-02 2.46478873e-02 1.05633803e-02
  2.46478873e-02 7.92253521e-01 3.16901408e-02 1.05633803e-02
  7.04225352e-03 0.00000000e+00 3.52112676e-03 0.00000000e+00
  2.11267606e-02 3.52112676e-03]
 [9.20000000e-02 1.20000000e-02 9.20000000e-02 2.40000000e-02
  4.00000000e-03 2.00000000e-02 5.88000000e-01 8.00000000e-03
  4.00000000e-03 2.40000000e-02 2.40000000e-02 0.00000000e+00
  7.60000000e-02 3.20000000e-02]
 [2.19780220e-01 0.00000000e+00 3.29670330e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 4.39560440e-02 7.03296703e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.75862069e-01 0.00000000e+00 1.14942529e-02 0.00000000e+00
  0.00000000e+00 2.29885057e-02 5.74712644e-02 4.59770115e-02
  5.51724138e-01 0.00000000e+00 1.14942529e-02 2.29885057e-02
  0.00000000e+00 0.00000000e+00]
 [8.13953488e-02 0.00000000e+00 2.32558140e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.97674419e-01 0.00000000e+00
  0.00000000e+00 6.74418605e-01 0.00000000e+00 2.32558140e-02
  0.00000000e+00 0.00000000e+00]
 [1.17647059e-01 2.20588235e-02 2.94117647e-02 2.94117647e-02
  0.00000000e+00 2.94117647e-02 9.55882353e-02 7.35294118e-03
  7.35294118e-03 0.00000000e+00 6.39705882e-01 0.00000000e+00
  1.47058824e-02 7.35294118e-03]
 [2.50000000e-01 1.56250000e-02 1.56250000e-02 3.12500000e-02
  0.00000000e+00 1.56250000e-02 4.68750000e-02 1.56250000e-02
  1.56250000e-02 0.00000000e+00 3.12500000e-02 5.46875000e-01
  1.56250000e-02 0.00000000e+00]
 [2.10526316e-01 5.26315789e-02 2.98245614e-01 5.26315789e-02
  0.00000000e+00 0.00000000e+00 2.45614035e-01 0.00000000e+00
  0.00000000e+00 1.75438596e-02 0.00000000e+00 0.00000000e+00
  1.22807018e-01 0.00000000e+00]
 [1.15384615e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.26923077e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.92307692e-02 5.38461538e-01]]
---------------------------------------
program finished.
