seed:  666
number of classes (from original clusters): 10
how to merge clusters:  [[0, 9], [1, 5], 2, [3, 8], 4, 6, 7]
whether to further subcluster data according to chemical reaction: False
positive training pair sampling threshold:  13500
negative training pair sampling threshold:  4500
positive validation pair sampling threshold:  3900
negative validation pair sampling threshold:  1300
number of epochs to train: 60
learning rate decay to half at epoch 45.
batch size: 256
similar margin of contrastive loss: 0.1
dissimilar margin of contrastive loss: 2.0
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['x', 'y', 'z', 'charge', 'hydrophobicity', 'binding_probability', 'sasa', 'sequence_entropy']
number of classes after merging:  7
number of pockets in training set:  10527
number of pockets in validation set:  2254
number of pockets in test set:  2263
number of train positive pairs: 94500
number of train negative pairs: 94500
number of validation positive pairs: 27300
number of validation negative pairs: 27300
model architecture:
SiameseNet(
  (embedding_net): EmbeddingNet(
    (set2set): Set2Set(32, 64)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.1, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.7743619616473163, validation loss: 0.7140685342083047.
epoch: 2, train loss: 0.6218083300716662, validation loss: 0.7026268952757447.
epoch: 3, train loss: 0.5582072311885773, validation loss: 0.6404913928133228.
epoch: 4, train loss: 0.520756615007996, validation loss: 0.6099601052532266.
epoch: 5, train loss: 0.49018102292661314, validation loss: 0.6352015208150005.
epoch: 6, train loss: 0.4662556216931217, validation loss: 0.5939176966069819.
epoch: 7, train loss: 0.4443914671802016, validation loss: 0.6140113190591554.
epoch: 8, train loss: 0.4291699202603133, validation loss: 0.5829741524776696.
epoch: 9, train loss: 0.41078193535375845, validation loss: 0.6271033810346555.
epoch: 10, train loss: 0.4014835618841585, validation loss: 0.5652109862945892.
epoch: 11, train loss: 0.38985982113792783, validation loss: 0.6258057968083756.
epoch: 12, train loss: 0.37913808142445077, validation loss: 0.63203858651521.
epoch: 13, train loss: 0.37736654990059987, validation loss: 0.6269364546332167.
epoch: 14, train loss: 0.3625852370691047, validation loss: 0.5930860262301378.
epoch: 15, train loss: 0.36179736671245916, validation loss: 0.5880067809946807.
epoch: 16, train loss: 0.3542168215393389, validation loss: 0.5838742821382515.
epoch: 17, train loss: 0.35296988338894314, validation loss: 0.5986721372779036.
epoch: 18, train loss: 0.3494018311273484, validation loss: 0.5991798428825407.
epoch: 19, train loss: 0.34385795252159157, validation loss: 0.6088644942957839.
epoch: 20, train loss: 0.34632969473783304, validation loss: 0.593447397672213.
epoch: 21, train loss: 0.339146063002329, validation loss: 0.603389535561586.
epoch: 22, train loss: 0.3350840039732595, validation loss: 0.5909815974811932.
epoch: 23, train loss: 0.33262579735246284, validation loss: 0.5719997983101087.
epoch: 24, train loss: 0.33348167090441183, validation loss: 0.612510747106084.
epoch: 25, train loss: 0.32963665916806173, validation loss: 0.587559583894499.
epoch: 26, train loss: 0.3270312119741288, validation loss: 0.6126609561906192.
epoch: 27, train loss: 0.32512983699576564, validation loss: 0.6114108376974587.
epoch: 28, train loss: 0.3229270279899476, validation loss: 0.6270000516451322.
epoch: 29, train loss: 0.32628703604804143, validation loss: 0.5836018677596208.
epoch: 30, train loss: 0.3204317024271324, validation loss: 0.596622541378706.
epoch: 31, train loss: 0.32060450671968005, validation loss: 0.5880857956715119.
epoch: 32, train loss: 0.31870270695762026, validation loss: 0.5729201862488911.
epoch: 33, train loss: 0.31974818313437164, validation loss: 0.5776856459453429.
epoch: 34, train loss: 0.32092748216094164, validation loss: 0.606195541493622.
epoch: 35, train loss: 0.3176687348986429, validation loss: 0.5801395351982815.
epoch: 36, train loss: 0.3166027546837216, validation loss: 0.5847485032972398.
epoch: 37, train loss: 0.3144420152492624, validation loss: 0.6021584360590785.
epoch: 38, train loss: 0.31478124905137156, validation loss: 0.5857090805389069.
epoch: 39, train loss: 0.30764775686920004, validation loss: 0.6077397111047319.
epoch: 40, train loss: 0.3144522305443173, validation loss: 0.6168728260417561.
epoch: 41, train loss: 0.30505922563744603, validation loss: 0.5890893528138325.
epoch: 42, train loss: 0.30921255319585245, validation loss: 0.6048097817277733.
epoch: 43, train loss: 0.30845603103234026, validation loss: 0.5927259393489405.
epoch: 44, train loss: 0.30997285776289685, validation loss: 0.5862224697630047.
epoch: 45, train loss: 0.26079631365417805, validation loss: 0.5990659832517743.
epoch: 46, train loss: 0.2575121727393418, validation loss: 0.5860962653247428.
epoch: 47, train loss: 0.25788451668068213, validation loss: 0.5806125681391566.
epoch: 48, train loss: 0.2582543753366622, validation loss: 0.5937970901321579.
epoch: 49, train loss: 0.25463528444401173, validation loss: 0.6012218764588073.
epoch: 50, train loss: 0.2542626976336121, validation loss: 0.5860585937220535.
epoch: 51, train loss: 0.25824676598442925, validation loss: 0.6131430239555163.
epoch: 52, train loss: 0.25230276452927364, validation loss: 0.613762690226237.
epoch: 53, train loss: 0.2524329353009582, validation loss: 0.5945842576638246.
epoch: 54, train loss: 0.25602840219729794, validation loss: 0.6033941348568423.
epoch: 55, train loss: 0.2507329040809914, validation loss: 0.591430408072559.
epoch: 56, train loss: 0.24861602429990415, validation loss: 0.5996932650835086.
epoch: 57, train loss: 0.25094290427556115, validation loss: 0.5954422909523541.
epoch: 58, train loss: 0.24758313967185044, validation loss: 0.5950521339164986.
epoch: 59, train loss: 0.251615969945514, validation loss: 0.594949230431637.
epoch: 60, train loss: 0.25099880174101974, validation loss: 0.5984620458302481.
best validation loss 0.5652109862945892 at epoch 10.
