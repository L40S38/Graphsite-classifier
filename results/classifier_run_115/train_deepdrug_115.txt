seed:  15
save trained model at:  ../trained_models/trained_classifier_model_115.pt
save loss at:  ./results/train_classifier_results_115.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['3iewB00', '2cnvA01', '6bzrA00', '4y5hA00', '5ec0A00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['4hyuA01', '5jcaA01', '3ddjA00', '4z1fA00', '4xj5A00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2ad04ae74730>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.039040195641326, acc: 0.38877708062033856; test loss: 1.8257225298537794, acc: 0.45521153391633185
epoch: 2, train loss: 1.72247579537497, acc: 0.46845033739789277; test loss: 1.6855733565364541, acc: 0.49539116048215553
epoch: 3, train loss: 1.638398258999648, acc: 0.4979874511660945; test loss: 1.5821856388496862, acc: 0.5176081304656109
epoch: 4, train loss: 1.5738222671427275, acc: 0.5136143009352433; test loss: 1.5377034152605042, acc: 0.5254077050342708
epoch: 5, train loss: 1.5234143001674492, acc: 0.5348052563040132; test loss: 1.5162145275507264, acc: 0.5303710706688726
epoch: 6, train loss: 1.504875037703159, acc: 0.5387119687463005; test loss: 1.4480792753692497, acc: 0.5582604585204444
epoch: 7, train loss: 1.4600429281872858, acc: 0.554575588966497; test loss: 1.4314263405683785, acc: 0.5748050106357835
epoch: 8, train loss: 1.4291346494130048, acc: 0.5648159109743104; test loss: 1.4211728280386837, acc: 0.5658236823445993
epoch: 9, train loss: 1.414463729820012, acc: 0.5711495205398366; test loss: 1.3878658874974243, acc: 0.5776412195698416
epoch: 10, train loss: 1.3705440092273078, acc: 0.5834615839943175; test loss: 1.3312611419859859, acc: 0.5951311746632002
epoch: 11, train loss: 1.348144759422818, acc: 0.586243636794128; test loss: 1.3632319792876202, acc: 0.5818955329709289
epoch: 12, train loss: 1.355482344727862, acc: 0.5892624600449864; test loss: 1.4361117415832758, acc: 0.5554242495863861
epoch: 13, train loss: 1.3284420351933004, acc: 0.5908014679767964; test loss: 1.3618080264370764, acc: 0.5840226896714724
epoch: 14, train loss: 1.3652046450188955, acc: 0.5800284124541257; test loss: 1.4358636631917177, acc: 0.5523516899078232
epoch: 15, train loss: 1.3136715702267105, acc: 0.5937610986148929; test loss: 1.3126278199700987, acc: 0.598440085086268
epoch: 16, train loss: 1.2920115096930063, acc: 0.6021072570143247; test loss: 1.3033222348040312, acc: 0.5996218388087923
epoch: 17, train loss: 1.265385181914847, acc: 0.6124659642476619; test loss: 1.312254370090049, acc: 0.5967856298747342
epoch: 18, train loss: 1.2560551587513837, acc: 0.6137682017284243; test loss: 1.2701793559982095, acc: 0.6147482864571023
epoch: 19, train loss: 1.249484619417458, acc: 0.6192139221025216; test loss: 1.3014162816327393, acc: 0.6017489955093358
epoch: 20, train loss: 1.2217049217653144, acc: 0.6219367822895703; test loss: 1.3944341315357602, acc: 0.590876861262113
epoch: 21, train loss: 1.2211456788253738, acc: 0.6269681543743341; test loss: 1.3758313973669576, acc: 0.5840226896714724
epoch: 22, train loss: 1.2342854367638108, acc: 0.6183852255238547; test loss: 1.2957655139297382, acc: 0.6048215551878988
epoch: 23, train loss: 1.1934588264682329, acc: 0.6339528826802415; test loss: 1.6975806844175299, acc: 0.5532970928858426
epoch: 24, train loss: 1.1915075048350916, acc: 0.6386882917011957; test loss: 1.2268594387039116, acc: 0.6239659654927913
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9527601475572818, acc: 0.6369125133183379; test loss: 0.9786344198322949, acc: 0.6246750177263058
epoch: 26, train loss: 0.9445832512908481, acc: 0.6391026399905292; test loss: 1.1064282914082566, acc: 0.6041125029543843
epoch: 27, train loss: 0.9395451572176492, acc: 0.6418255001775779; test loss: 0.9943302639041829, acc: 0.6201843535807138
epoch: 28, train loss: 0.9240390611454145, acc: 0.6498756955131999; test loss: 1.0147365177531413, acc: 0.6223115102812574
epoch: 29, train loss: 0.9245045276256628, acc: 0.6452586717177696; test loss: 0.9908137929493077, acc: 0.6216024580477428
epoch: 30, train loss: 0.9115034778668244, acc: 0.6516514738960578; test loss: 1.016524838826471, acc: 0.6258567714488301
epoch: 31, train loss: 0.910778284665606, acc: 0.6508819699301527; test loss: 0.9579417060887213, acc: 0.6388560623965965
epoch: 32, train loss: 0.9180303851683436, acc: 0.6487510358707234; test loss: 1.0962824233737405, acc: 0.5932403687071615
epoch: 33, train loss: 0.9053226424220385, acc: 0.6540783710192968; test loss: 0.9561802965618144, acc: 0.6421649728196643
epoch: 34, train loss: 0.8796103140496919, acc: 0.6594648987806322; test loss: 1.1053360863741732, acc: 0.6100212715670055
epoch: 35, train loss: 0.8736541575971716, acc: 0.6611814845507281; test loss: 0.9729984545363966, acc: 0.635074450484519
epoch: 36, train loss: 0.8903667644465394, acc: 0.6557949567893927; test loss: 0.9945971760110803, acc: 0.6355471519735287
epoch: 37, train loss: 0.8787966658127185, acc: 0.6606487510358707; test loss: 1.1150274409720362, acc: 0.5986764358307729
epoch: 38, train loss: 0.8783341466964202, acc: 0.6605303658103469; test loss: 0.9814795650395624, acc: 0.6322382415504609
epoch: 39, train loss: 0.8606859639408716, acc: 0.6726056588137801; test loss: 1.038483707314827, acc: 0.6005672417868116
epoch: 40, train loss: 0.8565430309457216, acc: 0.671007458269208; test loss: 1.060152693353796, acc: 0.6147482864571023
epoch: 41, train loss: 0.8305187248415928, acc: 0.679116846217592; test loss: 0.9273123181590344, acc: 0.6497281966438194
epoch: 42, train loss: 0.8216231912568545, acc: 0.6769267195454007; test loss: 0.9150340770666437, acc: 0.6556369652564406
epoch: 43, train loss: 0.8238587965270365, acc: 0.6826092103705458; test loss: 0.9172337564050489, acc: 0.6591822264240133
epoch: 44, train loss: 0.815731387731379, acc: 0.6845033739789275; test loss: 0.9468956166084594, acc: 0.645710233987237
epoch: 45, train loss: 0.8126090960106342, acc: 0.6818397064046408; test loss: 0.8897378523562203, acc: 0.6679272039706925
epoch: 46, train loss: 0.8016116877723843, acc: 0.6875813898425477; test loss: 0.9454096836616626, acc: 0.6428740250531789
epoch: 47, train loss: 0.8036162691157542, acc: 0.6852728779448325; test loss: 1.1546784494819722, acc: 0.5759867643583078
epoch: 48, train loss: 0.7931808957802948, acc: 0.6917840653486445; test loss: 0.8478842296658946, acc: 0.6750177263058379
epoch: 49, train loss: 0.7905010347664392, acc: 0.6923759914762637; test loss: 0.8878337878252254, acc: 0.6669818009926731
epoch: 50, train loss: 0.8332366629000795, acc: 0.675920445128448; test loss: 0.9064700489093778, acc: 0.6688726069487119
epoch: 51, train loss: 0.7856524452678237, acc: 0.6916656801231207; test loss: 0.876459866487453, acc: 0.6698180099267312
epoch: 52, train loss: 0.7905133793279802, acc: 0.6926719545400734; test loss: 0.9043176885869479, acc: 0.6558733160009454
epoch: 53, train loss: 0.769950258424187, acc: 0.6976441340120753; test loss: 0.8537377379121467, acc: 0.6769085322618766
epoch: 54, train loss: 0.7552881314653722, acc: 0.7049840179945542; test loss: 0.8807389293824829, acc: 0.6665090995036634
epoch: 55, train loss: 0.7600735909306284, acc: 0.7007221498756955; test loss: 0.8443661190831783, acc: 0.6797447411959348
epoch: 56, train loss: 0.7497556670270644, acc: 0.7100153900793181; test loss: 0.9220592670760744, acc: 0.6554006145119358
epoch: 57, train loss: 0.7446852682172242, acc: 0.7064046407008405; test loss: 1.0734820902925157, acc: 0.5982037343417632
epoch: 58, train loss: 0.7436002516754988, acc: 0.7048064401562685; test loss: 0.9174697910237611, acc: 0.6537461593004018
epoch: 59, train loss: 0.7414970707368382, acc: 0.7081804190836983; test loss: 0.8657855227381815, acc: 0.6700543606712361
epoch: 60, train loss: 0.7307978472766671, acc: 0.7130934059429384; test loss: 0.9023284495679855, acc: 0.6622547861025763
epoch: 61, train loss: 0.7386102218648817, acc: 0.7088315378240796; test loss: 0.8205071943247467, acc: 0.6870716142755849
epoch: 62, train loss: 0.7092091418715416, acc: 0.7161122291937966; test loss: 0.8477239310670986, acc: 0.6802174426849444
epoch: 63, train loss: 0.7144948599803401, acc: 0.7140996803598911; test loss: 1.017016171225699, acc: 0.63365634601749
epoch: 64, train loss: 0.7099854062757112, acc: 0.717414466674559; test loss: 0.9224363781548763, acc: 0.6428740250531789
epoch: 65, train loss: 0.709360038456556, acc: 0.7173552740617971; test loss: 0.8663993075056804, acc: 0.6702907114157409
epoch: 66, train loss: 0.71359935567635, acc: 0.7169409257724636; test loss: 0.9016301700674989, acc: 0.6719451666272749
epoch: 67, train loss: 0.6924778089391221, acc: 0.7197821711850361; test loss: 0.836400503470976, acc: 0.6908532261876625
epoch: 68, train loss: 0.6905718684916216, acc: 0.7283059074227537; test loss: 1.0340275790950024, acc: 0.6454738832427322
epoch: 69, train loss: 0.7034406923282439, acc: 0.7216171421806559; test loss: 0.8116279149331598, acc: 0.6958165918222642
epoch: 70, train loss: 0.6886891440497329, acc: 0.7240440393038948; test loss: 0.9141943598435974, acc: 0.6556369652564406
epoch: 71, train loss: 0.6851502410318635, acc: 0.7273588256185628; test loss: 0.8947665312700986, acc: 0.6825809501299929
epoch: 72, train loss: 0.6727233158564079, acc: 0.7352314431158992; test loss: 0.8689396800625214, acc: 0.6759631292838573
epoch: 73, train loss: 0.6833815230368665, acc: 0.7264709364271339; test loss: 0.8783674006258109, acc: 0.6785629874734106
epoch: 74, train loss: 0.6712145153779284, acc: 0.7302000710311353; test loss: 0.8044640695589481, acc: 0.6974710470337981
epoch: 75, train loss: 0.6744253260645906, acc: 0.7316798863501836; test loss: 0.8812941749223833, acc: 0.6757267785393524
epoch: 76, train loss: 0.6565966018843512, acc: 0.7366520658221853; test loss: 0.9126782310231587, acc: 0.6424013235641692
epoch: 77, train loss: 0.6635052075721363, acc: 0.7336332425713271; test loss: 0.9428072876187539, acc: 0.6530371070668872
epoch: 78, train loss: 0.6579562448052467, acc: 0.7347579022138038; test loss: 0.9097426102195781, acc: 0.6650909950366344
epoch: 79, train loss: 0.6477930894811943, acc: 0.740262815200663; test loss: 0.8079331858280392, acc: 0.6969983455447885
epoch: 80, train loss: 0.6445057626037015, acc: 0.7416242452941872; test loss: 0.8078393623975962, acc: 0.7007799574568659
epoch: 81, train loss: 0.6455337459820607, acc: 0.7397892742985676; test loss: 0.8274765974295897, acc: 0.6870716142755849
epoch: 82, train loss: 0.6492146075131753, acc: 0.7397892742985676; test loss: 0.7898047600928505, acc: 0.7031434649019145
epoch: 83, train loss: 0.6343269651565832, acc: 0.7469515804427608; test loss: 0.7891061622689053, acc: 0.708343181281021
epoch: 84, train loss: 0.6197305134489535, acc: 0.7496152480170475; test loss: 0.8224936958408559, acc: 0.6936894351217207
epoch: 85, train loss: 0.6312212950663693, acc: 0.7452349946726649; test loss: 0.8315456649480039, acc: 0.6873079650200898
epoch: 86, train loss: 0.6293473114794608, acc: 0.7466556173789511; test loss: 0.8866715266106682, acc: 0.6797447411959348
epoch: 87, train loss: 0.6568536744451359, acc: 0.7372439919498046; test loss: 0.7968485805616963, acc: 0.706925076813992
epoch: 88, train loss: 0.631603713421987, acc: 0.7436959867408547; test loss: 0.8053208803403634, acc: 0.6979437485228078
epoch: 89, train loss: 0.6269871969963653, acc: 0.746714809991713; test loss: 0.86590161686444, acc: 0.6811628456629638
epoch: 90, train loss: 0.6192006524588295, acc: 0.7513910263999053; test loss: 0.7489430667716711, acc: 0.7201607185062633
epoch: 91, train loss: 0.6198536990323095, acc: 0.7503255593701906; test loss: 0.8250450757738266, acc: 0.6804537934294493
epoch: 92, train loss: 0.5977395630678765, acc: 0.7593228365100035; test loss: 0.8719372928522913, acc: 0.6780902859844008
epoch: 93, train loss: 0.6161858887168321, acc: 0.7525748786551438; test loss: 0.8218944514510592, acc: 0.6851808083195462
epoch: 94, train loss: 0.6117712087330345, acc: 0.7545282348762874; test loss: 0.788131063808873, acc: 0.703852517135429
epoch: 95, train loss: 0.6028402397247888, acc: 0.7585533325440985; test loss: 0.7931486480034883, acc: 0.7057433230914677
epoch: 96, train loss: 0.5894660021520104, acc: 0.7615129631821949; test loss: 0.7744235188874813, acc: 0.7128338454266131
epoch: 97, train loss: 0.5982768498638953, acc: 0.75245649342962; test loss: 0.8148484828482844, acc: 0.706925076813992
epoch: 98, train loss: 0.5873362970194111, acc: 0.7590268734461939; test loss: 0.8216748187458, acc: 0.6991255022453321
epoch: 99, train loss: 0.8306373652232615, acc: 0.6744406298093998; test loss: 0.8848073835266986, acc: 0.6572914204679745
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.6004586264030657, acc: 0.7027938913223629; test loss: 0.7007616815542222, acc: 0.6773812337508863
epoch: 101, train loss: 0.5317530082967615, acc: 0.7306144193204688; test loss: 0.6394961209929432, acc: 0.7026707634129048
Epoch   101: reducing learning rate of group 0 to 1.5000e-03.
epoch: 102, train loss: 0.46411184989673593, acc: 0.7619273114715284; test loss: 0.6157842722662626, acc: 0.7246513826518554
epoch: 103, train loss: 0.436644962176326, acc: 0.774416952764295; test loss: 0.6207774862051856, acc: 0.7218151737177972
epoch: 104, train loss: 0.42852846164103303, acc: 0.7787972061086776; test loss: 0.6050717840801032, acc: 0.7345781139210589
epoch: 105, train loss: 0.4164945086713682, acc: 0.7796850953001065; test loss: 0.610371148493295, acc: 0.7293783975419522
epoch: 106, train loss: 0.4068501788553464, acc: 0.7838877708062034; test loss: 0.6371155453472097, acc: 0.7168518080831955
epoch: 107, train loss: 0.4083590956958946, acc: 0.7848940452231562; test loss: 0.6238805662230436, acc: 0.7286693453084377
epoch: 108, train loss: 0.40448526810355, acc: 0.786610630993252; test loss: 0.6324800138804851, acc: 0.7281966438194281
epoch: 109, train loss: 0.3925596522243401, acc: 0.7919379661418255; test loss: 0.6111493378884684, acc: 0.7263058378633893
epoch: 110, train loss: 0.38714773900920685, acc: 0.7904581508227774; test loss: 0.6549898554771061, acc: 0.7123611439376034
epoch: 111, train loss: 0.3889916339286957, acc: 0.7938913223629691; test loss: 0.6182695851205399, acc: 0.734341763176554
epoch: 112, train loss: 0.3892890252036231, acc: 0.7913460400142063; test loss: 0.6341965427863009, acc: 0.7286693453084377
epoch: 113, train loss: 0.3884343595826246, acc: 0.7890375281164911; test loss: 0.6349611006795134, acc: 0.7260694871188844
epoch: 114, train loss: 0.3821600198364687, acc: 0.7930626257843021; test loss: 0.636918624167013, acc: 0.7227605766958166
epoch: 115, train loss: 0.386296143796552, acc: 0.7890375281164911; test loss: 0.6624883835097628, acc: 0.7163791065941858
epoch: 116, train loss: 0.3803587194672841, acc: 0.7888599502782053; test loss: 0.635707939975624, acc: 0.7274875915859135
epoch: 117, train loss: 0.3675961119791058, acc: 0.8008760506688766; test loss: 0.644740688071074, acc: 0.7355235168990782
epoch: 118, train loss: 0.3718617238299274, acc: 0.7959038711968747; test loss: 0.6372121788598324, acc: 0.7315055542424959
epoch: 119, train loss: 0.3742639630860064, acc: 0.7959630638096366; test loss: 0.63098023468032, acc: 0.7208697707397779
epoch: 120, train loss: 0.3706403188808749, acc: 0.7956671007458269; test loss: 0.6204496189032681, acc: 0.7289056960529425
epoch: 121, train loss: 0.3581109081194417, acc: 0.8033621404048775; test loss: 0.6420264418765856, acc: 0.722051524462302
epoch: 122, train loss: 0.3569366080818535, acc: 0.8014679767964958; test loss: 0.643973090693168, acc: 0.7232332781848263
epoch: 123, train loss: 0.35186373525587666, acc: 0.8024150586006866; test loss: 0.6514133405133191, acc: 0.7312692034979911
epoch: 124, train loss: 0.3573579893664551, acc: 0.7998105836391618; test loss: 0.6436007898543017, acc: 0.7315055542424959
epoch: 125, train loss: 0.3566365814800303, acc: 0.8024742512134485; test loss: 0.6557809872650926, acc: 0.734341763176554
epoch: 126, train loss: 0.34468463500378416, acc: 0.8061441932046881; test loss: 0.6812096784876132, acc: 0.7062160245804774
epoch: 127, train loss: 0.35409212059857254, acc: 0.8022374807624009; test loss: 0.6803775166710114, acc: 0.6953438903332545
epoch: 128, train loss: 0.34700043066388775, acc: 0.8032437551793536; test loss: 0.641116458286041, acc: 0.7215788229732923
epoch: 129, train loss: 0.3480629879291966, acc: 0.8044276074345922; test loss: 0.6585777812295984, acc: 0.7170881588277003
epoch: 130, train loss: 0.3503509287765298, acc: 0.8018823250858292; test loss: 0.6687219599352129, acc: 0.7258331363743796
epoch: 131, train loss: 0.33868755899943925, acc: 0.8101692908724991; test loss: 0.6670382336165939, acc: 0.725124084140865
epoch: 132, train loss: 0.33582959165089904, acc: 0.8112939505149758; test loss: 0.636111808764233, acc: 0.7274875915859135
epoch: 133, train loss: 0.3341725780524431, acc: 0.812714573221262; test loss: 0.6670368029924184, acc: 0.7244150319073505
epoch: 134, train loss: 0.3391485523934814, acc: 0.8056706523025926; test loss: 0.6517799372257168, acc: 0.7281966438194281
epoch: 135, train loss: 0.32831565772370785, acc: 0.8124778027702143; test loss: 0.6445518545832822, acc: 0.7348144646655637
epoch: 136, train loss: 0.32163767234184226, acc: 0.8144903516041198; test loss: 0.6608393373739576, acc: 0.7298510990309619
epoch: 137, train loss: 0.32414011254499153, acc: 0.8104652539363087; test loss: 0.7019034204570939, acc: 0.7066887260694871
epoch: 138, train loss: 0.33130700101411537, acc: 0.8109387948384041; test loss: 0.6761759811190391, acc: 0.725124084140865
epoch: 139, train loss: 0.33905696113518574, acc: 0.80774239374926; test loss: 0.6854490366592894, acc: 0.7232332781848263
epoch: 140, train loss: 0.32787411342686673, acc: 0.8131289215105955; test loss: 0.6592210073387671, acc: 0.7286693453084377
epoch: 141, train loss: 0.3178566039751673, acc: 0.8126553806085001; test loss: 0.7169388697463496, acc: 0.7248877333963601
epoch: 142, train loss: 0.32023164652516406, acc: 0.8124186101574523; test loss: 0.6821982280155976, acc: 0.7300874497754668
epoch: 143, train loss: 0.31818891225361295, acc: 0.8122410323191666; test loss: 0.718880168486081, acc: 0.7244150319073505
epoch: 144, train loss: 0.3304507381339969, acc: 0.8080383568130697; test loss: 0.657368787719521, acc: 0.7296147482864571
epoch: 145, train loss: 0.33370861156009085, acc: 0.8098141351959276; test loss: 0.6479762710033932, acc: 0.7272512408414087
epoch: 146, train loss: 0.31686602037334294, acc: 0.8148455072806914; test loss: 0.6611681773402792, acc: 0.725124084140865
epoch: 147, train loss: 0.3053288616886559, acc: 0.8180419083698355; test loss: 0.6642997309942388, acc: 0.7312692034979911
epoch: 148, train loss: 0.3146494435984608, acc: 0.8157925890848822; test loss: 0.6688832446098102, acc: 0.7303238005199716
epoch: 149, train loss: 0.30796823501445786, acc: 0.8186338344974547; test loss: 0.7089680443698141, acc: 0.7208697707397779
epoch: 150, train loss: 0.29585360799426225, acc: 0.821652657748313; test loss: 0.6999943343157803, acc: 0.7208697707397779
epoch: 151, train loss: 0.30193527219449895, acc: 0.8225405469397419; test loss: 0.6740419097161975, acc: 0.726778539352399
epoch: 152, train loss: 0.29514091876366866, acc: 0.8231324730673611; test loss: 0.7101596549058237, acc: 0.7147246513826518
Epoch   152: reducing learning rate of group 0 to 7.5000e-04.
epoch: 153, train loss: 0.2543465974091834, acc: 0.8392920563513674; test loss: 0.6484645602551512, acc: 0.7485228078468447
epoch: 154, train loss: 0.229923886213131, acc: 0.8546821356694685; test loss: 0.6776861760723932, acc: 0.7456865989127865
epoch: 155, train loss: 0.2268805928599754, acc: 0.8539718243163253; test loss: 0.7077266068865407, acc: 0.7411959347671945
epoch: 156, train loss: 0.22324703944968488, acc: 0.8559251805374689; test loss: 0.698693031510741, acc: 0.7428503899787284
epoch: 157, train loss: 0.22704838656794887, acc: 0.8522552385462294; test loss: 0.6797474141775702, acc: 0.74048688253368
epoch: 158, train loss: 0.22012897603706846, acc: 0.8572274180182313; test loss: 0.7102283397991025, acc: 0.7385960765776413
epoch: 159, train loss: 0.2203249760096463, acc: 0.8572274180182313; test loss: 0.7135325085043316, acc: 0.737414322855117
epoch: 160, train loss: 0.2207169534716364, acc: 0.8577009589203267; test loss: 0.6904015663920551, acc: 0.7426140392342235
epoch: 161, train loss: 0.21925614464278756, acc: 0.8591807742393749; test loss: 0.7256071649243493, acc: 0.7440321437012527
epoch: 162, train loss: 0.20986054994000955, acc: 0.8608381673967089; test loss: 0.7075244996091824, acc: 0.7452138974237769
epoch: 163, train loss: 0.2085743598667308, acc: 0.8630282940689002; test loss: 0.7310086062432913, acc: 0.7317419049870008
epoch: 164, train loss: 0.208695108206142, acc: 0.8597727003669942; test loss: 0.709659831965119, acc: 0.7395414795556606
epoch: 165, train loss: 0.20496408321511128, acc: 0.8645081093879484; test loss: 0.7477263481277181, acc: 0.7348144646655637
epoch: 166, train loss: 0.20843152770124498, acc: 0.863146679294424; test loss: 0.7101536555032137, acc: 0.7393051288111557
epoch: 167, train loss: 0.21492538365920338, acc: 0.8601870486563277; test loss: 0.7298972850092701, acc: 0.7284329945639328
epoch: 168, train loss: 0.20686839744969066, acc: 0.862377175328519; test loss: 0.7380079552513633, acc: 0.738832427322146
epoch: 169, train loss: 0.20401798575019306, acc: 0.862436367941281; test loss: 0.7382376508435524, acc: 0.737414322855117
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.14968597157207258, acc: 0.8676453178643305; test loss: 0.6466350346359906, acc: 0.7381233750886316
epoch: 171, train loss: 0.14888809482324783, acc: 0.8664022729963301; test loss: 0.6529262012354563, acc: 0.7400141810446703
epoch: 172, train loss: 0.1456273535260565, acc: 0.8698946371492837; test loss: 0.6488916855899755, acc: 0.7442684944457575
epoch: 173, train loss: 0.14866969938622263, acc: 0.8700130223748076; test loss: 0.6442658428434671, acc: 0.7286693453084377
epoch: 174, train loss: 0.1713415268710256, acc: 0.8529063572866107; test loss: 0.6210103693145917, acc: 0.7331600094540298
epoch: 175, train loss: 0.15549205151855472, acc: 0.8616668639753758; test loss: 0.6510449424192319, acc: 0.7352871661545733
epoch: 176, train loss: 0.14702424756116214, acc: 0.8664022729963301; test loss: 0.6595917345924711, acc: 0.7393051288111557
epoch: 177, train loss: 0.15026654575271015, acc: 0.8671717769622351; test loss: 0.6379051530820431, acc: 0.7333963601985346
epoch: 178, train loss: 0.14619736022196733, acc: 0.865632769030425; test loss: 0.6856230991271561, acc: 0.7310328527534862
epoch: 179, train loss: 0.17174173010387717, acc: 0.8528471646738487; test loss: 0.6614621622684689, acc: 0.7258331363743796
epoch: 180, train loss: 0.16047185414907453, acc: 0.8549189061205161; test loss: 0.6270583145332742, acc: 0.743559442212243
epoch: 181, train loss: 0.14297596675073576, acc: 0.8661655025452824; test loss: 0.6584950751712829, acc: 0.738832427322146
epoch: 182, train loss: 0.1420001402529045, acc: 0.8664022729963301; test loss: 0.6438455297658979, acc: 0.7454502481682818
epoch: 183, train loss: 0.1402682611434096, acc: 0.8671125843494731; test loss: 0.6499773787708323, acc: 0.7385960765776413
epoch: 184, train loss: 0.1496699892219967, acc: 0.8617260565881378; test loss: 0.6686071794496814, acc: 0.7274875915859135
epoch: 185, train loss: 0.14454016247147905, acc: 0.8646264946134722; test loss: 0.6553300053649015, acc: 0.7454502481682818
epoch: 186, train loss: 0.1451200854906009, acc: 0.8645673020007103; test loss: 0.658878166099103, acc: 0.7390687780666509
epoch: 187, train loss: 0.1547969287847702, acc: 0.8616668639753758; test loss: 0.6488221765719697, acc: 0.7317419049870008
epoch: 188, train loss: 0.14458694914692152, acc: 0.8642713389369007; test loss: 0.6533309896063844, acc: 0.7315055542424959
epoch: 189, train loss: 0.1413445678002964, acc: 0.8694210962471883; test loss: 0.6425318935469121, acc: 0.7393051288111557
epoch: 190, train loss: 0.13583700972942964, acc: 0.8723807268852847; test loss: 0.652254878587121, acc: 0.735759867643583
epoch: 191, train loss: 0.13726083411963802, acc: 0.8725583047235705; test loss: 0.6558447242543872, acc: 0.7395414795556606
epoch: 192, train loss: 0.1473929357095429, acc: 0.863975375873091; test loss: 0.6589028374913792, acc: 0.7324509572205152
epoch: 193, train loss: 0.14691799354790314, acc: 0.864804072451758; test loss: 0.6549620314485157, acc: 0.7286693453084377
epoch: 194, train loss: 0.13424817737748923, acc: 0.8701314076003315; test loss: 0.6494132029308661, acc: 0.7409595840226897
epoch: 195, train loss: 0.13940985043034465, acc: 0.8662838877708062; test loss: 0.6652412330588673, acc: 0.7345781139210589
epoch: 196, train loss: 0.13890028415582753, acc: 0.8663430803835681; test loss: 0.6928859491828372, acc: 0.7333963601985346
epoch: 197, train loss: 0.1488522689141676, acc: 0.8585296554989937; test loss: 0.6846856644913762, acc: 0.7289056960529425
epoch: 198, train loss: 0.1352740039102309, acc: 0.8681188587664259; test loss: 0.6603044968805424, acc: 0.7265421886078941
epoch: 199, train loss: 0.13064949620901883, acc: 0.8722623416597609; test loss: 0.6924100309870987, acc: 0.7378870243441267
epoch: 200, train loss: 0.13218283743227222, acc: 0.8710192967917604; test loss: 0.6909981717795126, acc: 0.7300874497754668
best test acc 0.7485228078468447 at epoch 153.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9230    0.9426    0.9327      6100
           1     0.9593    0.8661    0.9103       926
           2     0.8296    0.9513    0.8863      2400
           3     0.8970    0.9087    0.9028       843
           4     0.8622    0.9380    0.8985       774
           5     0.8979    0.9425    0.9197      1512
           6     0.7729    0.7729    0.7729      1330
           7     0.8689    0.7713    0.8172       481
           8     0.8301    0.8537    0.8418       458
           9     0.9182    0.9690    0.9429       452
          10     0.9523    0.7796    0.8574       717
          11     0.9024    0.6667    0.7668       333
          12     0.4667    0.0234    0.0446       299
          13     0.9042    0.8067    0.8527       269

    accuracy                         0.8870     16894
   macro avg     0.8561    0.7995    0.8105     16894
weighted avg     0.8818    0.8870    0.8791     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8131    0.8643    0.8379      1525
           1     0.8621    0.7543    0.8046       232
           2     0.6739    0.7770    0.7218       601
           3     0.7653    0.7725    0.7689       211
           4     0.8116    0.8660    0.8379       194
           5     0.7927    0.7989    0.7958       378
           6     0.4852    0.5405    0.5114       333
           7     0.7529    0.5289    0.6214       121
           8     0.7097    0.5739    0.6346       115
           9     0.7830    0.7281    0.7545       114
          10     0.7740    0.6278    0.6933       180
          11     0.7091    0.4643    0.5612        84
          12     0.1429    0.0133    0.0244        75
          13     0.5600    0.4118    0.4746        68

    accuracy                         0.7485      4231
   macro avg     0.6882    0.6230    0.6459      4231
weighted avg     0.7409    0.7485    0.7408      4231

---------------------------------------
program finished.
