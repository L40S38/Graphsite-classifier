seed:  4
save trained model at:  ../trained_models/trained_classifier_model_104.pt
save loss at:  ./results/train_classifier_results_104.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4eqmC00', '6hxjB01', '4zirA01', '3k8tA01', '4mo5B01']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['2hcrB00', '6i0oA00', '1hwkA02', '3b1fA00', '5vxaA00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b888aa0a610>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 1.9802551581702375, acc: 0.406357286610631; test loss: 1.8007441094456427, acc: 0.4372488773339636
epoch: 2, train loss: 1.7070435481028006, acc: 0.47584941399313363; test loss: 1.6098748368592557, acc: 0.5116993618529898
epoch: 3, train loss: 1.6342346703524955, acc: 0.4997632295489523; test loss: 1.5328503132880762, acc: 0.5121720633419995
epoch: 4, train loss: 1.5558105068713712, acc: 0.5258671717769622; test loss: 1.4899725908366424, acc: 0.5457338690616875
epoch: 5, train loss: 1.5215266855761922, acc: 0.5355155676571564; test loss: 1.4476388202889032, acc: 0.5589695107539588
epoch: 6, train loss: 1.4719309535014695, acc: 0.5544572037409732; test loss: 1.4373632992618446, acc: 0.5606239659654928
epoch: 7, train loss: 1.4597331940019938, acc: 0.5581271457322127; test loss: 1.3842975743919703, acc: 0.5745686598912787
epoch: 8, train loss: 1.4318596801913617, acc: 0.567597963774121; test loss: 1.4054725099697487, acc: 0.5594422122429685
epoch: 9, train loss: 1.3937547353367643, acc: 0.5752930034331716; test loss: 1.3556980209918599, acc: 0.5807137792484046
epoch: 10, train loss: 1.3836486397148575, acc: 0.5795548715520303; test loss: 1.3570162192835815, acc: 0.5826045852044434
epoch: 11, train loss: 1.390143586923995, acc: 0.578134248845744; test loss: 1.4310680545833032, acc: 0.5632238241550461
epoch: 12, train loss: 1.352271143100495, acc: 0.587190718598319; test loss: 1.4078266124188885, acc: 0.581659182226424
epoch: 13, train loss: 1.3228474535207657, acc: 0.5966023440274654; test loss: 1.3019513516977193, acc: 0.5944221224296856
epoch: 14, train loss: 1.323268093201371, acc: 0.5957144548360365; test loss: 1.299388181174289, acc: 0.5977310328527535
epoch: 15, train loss: 1.2916430556253384, acc: 0.6069018586480407; test loss: 1.2759257094906848, acc: 0.6041125029543843
epoch: 16, train loss: 1.2918225495197426, acc: 0.603409494495087; test loss: 1.2724112836048411, acc: 0.6041125029543843
epoch: 17, train loss: 1.2645443139646948, acc: 0.6131170829880431; test loss: 1.212737101410839, acc: 0.6220751595367525
epoch: 18, train loss: 1.2575294720825958, acc: 0.6176157215579495; test loss: 1.2119843540561308, acc: 0.627511226660364
epoch: 19, train loss: 1.245023073325344, acc: 0.6204569669705221; test loss: 1.2604411989485673, acc: 0.612148428267549
epoch: 20, train loss: 1.2330984428703142, acc: 0.6252515686042381; test loss: 1.1946433982903217, acc: 0.6239659654927913
epoch: 21, train loss: 1.2040761186764115, acc: 0.6295726293358589; test loss: 1.1877711685350165, acc: 0.6343653982510045
epoch: 22, train loss: 1.2301243703238594, acc: 0.6273825026636676; test loss: 1.2542417305802318, acc: 0.6180571968801701
epoch: 23, train loss: 1.1965533219753277, acc: 0.6325322599739552; test loss: 1.2327239481840988, acc: 0.628929331127393
epoch: 24, train loss: 1.1941708640486612, acc: 0.6388066769267196; test loss: 1.1861204451856475, acc: 0.6317655400614512
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9745826723449426, acc: 0.6333017639398603; test loss: 1.0249045247362174, acc: 0.5977310328527535
epoch: 26, train loss: 0.9608656825067532, acc: 0.6369125133183379; test loss: 0.9252478804776528, acc: 0.6424013235641692
epoch: 27, train loss: 0.9423860229443302, acc: 0.6434828933349118; test loss: 0.9628789723553406, acc: 0.6249113684708106
epoch: 28, train loss: 0.9281522883132066, acc: 0.6466201018112939; test loss: 0.9802405953097361, acc: 0.6220751595367525
epoch: 29, train loss: 0.921123882786288, acc: 0.6500532733514858; test loss: 1.0139281557570448, acc: 0.6128574805010636
epoch: 30, train loss: 0.9102860189294256, acc: 0.6516514738960578; test loss: 0.9087115144425322, acc: 0.645710233987237
epoch: 31, train loss: 0.8887198085515079, acc: 0.6640819225760625; test loss: 0.9073087890163377, acc: 0.6419286220751595
epoch: 32, train loss: 0.9086963408047227, acc: 0.6544927193086303; test loss: 1.1342541217465854, acc: 0.5790593240368708
epoch: 33, train loss: 0.8983045408990209, acc: 0.6565644607552977; test loss: 0.962017928305824, acc: 0.6331836445284803
epoch: 34, train loss: 0.8808025460038874, acc: 0.6637859595122528; test loss: 0.9198172658195847, acc: 0.6312928385724415
epoch: 35, train loss: 0.8835277284576128, acc: 0.6591097431040606; test loss: 0.9821465900112794, acc: 0.6185298983691798
epoch: 36, train loss: 0.866810007964392, acc: 0.6640227299633006; test loss: 0.9104224784637394, acc: 0.6511463011108485
epoch: 37, train loss: 0.8733278933863563, acc: 0.6624245294187285; test loss: 1.0844995386632614, acc: 0.5963129283857244
epoch: 38, train loss: 0.848682391081842, acc: 0.6714809991713034; test loss: 0.9054295852713427, acc: 0.6450011817537226
epoch: 39, train loss: 0.8463911195824213, acc: 0.6695276429501599; test loss: 1.08340889199973, acc: 0.5932403687071615
epoch: 40, train loss: 0.8407250191172693, acc: 0.67597963774121; test loss: 0.8647919226357684, acc: 0.654927913022926
epoch: 41, train loss: 0.8425497479787081, acc: 0.6774002604474961; test loss: 0.8614283596981899, acc: 0.6620184353580714
epoch: 42, train loss: 0.8236960075487819, acc: 0.6797087723452113; test loss: 0.8689997982285711, acc: 0.6584731741904987
epoch: 43, train loss: 0.8084432686152819, acc: 0.6860423819107375; test loss: 1.0377871551234175, acc: 0.5960765776412196
epoch: 44, train loss: 0.8095322319201581, acc: 0.6819580916301645; test loss: 1.0033014910465097, acc: 0.6317655400614512
epoch: 45, train loss: 0.810897124481156, acc: 0.6862199597490233; test loss: 0.9069376257795217, acc: 0.658000472701489
epoch: 46, train loss: 0.7988228181402853, acc: 0.6921392210252161; test loss: 0.8677791076697122, acc: 0.6598912786575277
epoch: 47, train loss: 0.805894101182568, acc: 0.6898307091275009; test loss: 0.8287643079525526, acc: 0.6632001890805956
epoch: 48, train loss: 0.7910493443784762, acc: 0.6911921392210252; test loss: 1.0138955017658757, acc: 0.6135665327345781
epoch: 49, train loss: 0.7748885513819702, acc: 0.697880904463123; test loss: 0.9701371092628401, acc: 0.6246750177263058
epoch: 50, train loss: 0.778836368321949, acc: 0.6953356221143602; test loss: 0.8512960687874902, acc: 0.6665090995036634
epoch: 51, train loss: 0.7734957301815657, acc: 0.6937374215697881; test loss: 0.9005087211999282, acc: 0.6532734578113921
epoch: 52, train loss: 0.7605832702128212, acc: 0.701906002130934; test loss: 0.8202072184465085, acc: 0.6804537934294493
epoch: 53, train loss: 0.7587975146249157, acc: 0.7030898543861726; test loss: 0.8080255512480081, acc: 0.6847081068305365
epoch: 54, train loss: 0.734138393177709, acc: 0.7111992423345567; test loss: 0.7498428446269999, acc: 0.7066887260694871
epoch: 55, train loss: 0.7462712119525179, acc: 0.7061678702497928; test loss: 0.7952109128131114, acc: 0.6903805246986529
epoch: 56, train loss: 0.737278021057922, acc: 0.7120871315259856; test loss: 0.8619385121759583, acc: 0.6650909950366344
epoch: 57, train loss: 0.7484280792046993, acc: 0.7077068781816029; test loss: 0.8573278626829872, acc: 0.6587095249350036
epoch: 58, train loss: 0.7424414749468404, acc: 0.7053983662838877; test loss: 0.7967166986850891, acc: 0.6873079650200898
epoch: 59, train loss: 0.7173222052538707, acc: 0.7154019178406534; test loss: 0.7852490046322416, acc: 0.6896714724651383
epoch: 60, train loss: 0.7266281444362822, acc: 0.7151059547768438; test loss: 0.9906405099766442, acc: 0.6260931221933349
epoch: 61, train loss: 0.7233967543339834, acc: 0.7137445246833195; test loss: 0.7534278238166227, acc: 0.6972346962892934
epoch: 62, train loss: 0.7112763753385534, acc: 0.7193678228957027; test loss: 1.0534882086778865, acc: 0.6057669581659182
epoch: 63, train loss: 0.710349859712363, acc: 0.7164081922576062; test loss: 0.8042483945682916, acc: 0.690144173954148
epoch: 64, train loss: 0.7136335647979741, acc: 0.7152243400023677; test loss: 0.7945081083748082, acc: 0.688489718742614
epoch: 65, train loss: 0.7288471306797116, acc: 0.7120279389132236; test loss: 0.8102858746144767, acc: 0.6856535098085559
Epoch    65: reducing learning rate of group 0 to 1.5000e-03.
epoch: 66, train loss: 0.6558885029029237, acc: 0.7390197703326625; test loss: 0.7036520943971876, acc: 0.7177972110612149
epoch: 67, train loss: 0.6141786649196941, acc: 0.7527524564934296; test loss: 0.6880006520806179, acc: 0.7298510990309619
epoch: 68, train loss: 0.6114651977317325, acc: 0.7503847519829525; test loss: 0.6897543316275142, acc: 0.7258331363743796
epoch: 69, train loss: 0.5912922024374052, acc: 0.7596779921865752; test loss: 0.6943037054606911, acc: 0.7286693453084377
epoch: 70, train loss: 0.5922275211481677, acc: 0.75890848822067; test loss: 0.752757963405042, acc: 0.7222878752068069
epoch: 71, train loss: 0.6029142325794089, acc: 0.7567775541612407; test loss: 0.8318015783849098, acc: 0.6832900023635075
epoch: 72, train loss: 0.593743911181256, acc: 0.7580205990292411; test loss: 0.7687442453159449, acc: 0.7081068305365162
epoch: 73, train loss: 0.5914715743493903, acc: 0.7612170001183852; test loss: 0.7106668542277247, acc: 0.7239423304183408
epoch: 74, train loss: 0.5767443337016548, acc: 0.7641766307564816; test loss: 0.6846503288406087, acc: 0.7376506735996219
epoch: 75, train loss: 0.5803190962625027, acc: 0.7629335858884811; test loss: 0.6996571154381417, acc: 0.7274875915859135
epoch: 76, train loss: 0.5661134241995366, acc: 0.7683201136498164; test loss: 0.676003806389863, acc: 0.7376506735996219
epoch: 77, train loss: 0.5672481518899871, acc: 0.7683201136498164; test loss: 0.7017481910847119, acc: 0.7303238005199716
epoch: 78, train loss: 0.5629163564302886, acc: 0.7729963300580087; test loss: 0.7352642348065666, acc: 0.725124084140865
epoch: 79, train loss: 0.551910809720072, acc: 0.7733514857345803; test loss: 0.7352657893452907, acc: 0.7144883006381471
epoch: 80, train loss: 0.5549259143805976, acc: 0.7739434118621996; test loss: 0.6782904069662038, acc: 0.735759867643583
epoch: 81, train loss: 0.5649645092347055, acc: 0.7686752693263881; test loss: 0.6979651618471554, acc: 0.7395414795556606
epoch: 82, train loss: 0.5473406896783682, acc: 0.7760151533088671; test loss: 0.7276599902516138, acc: 0.7227605766958166
epoch: 83, train loss: 0.5334025029813212, acc: 0.7799810583639162; test loss: 0.7115487281494574, acc: 0.7286693453084377
epoch: 84, train loss: 0.5327976046616726, acc: 0.7787972061086776; test loss: 0.7305757451694194, acc: 0.7234696289293311
epoch: 85, train loss: 0.5381978646007193, acc: 0.7776133538534391; test loss: 0.8335577450979689, acc: 0.6811628456629638
epoch: 86, train loss: 0.5404893799097393, acc: 0.7753640345684859; test loss: 0.6837827337743317, acc: 0.7428503899787284
epoch: 87, train loss: 0.516392826427154, acc: 0.7883272167633479; test loss: 0.7418744739306168, acc: 0.7187426140392342
epoch: 88, train loss: 0.5296454443739084, acc: 0.7802178288149639; test loss: 0.7594682492423187, acc: 0.7185062632947293
epoch: 89, train loss: 0.5219458152136606, acc: 0.7837693855806795; test loss: 0.7361343832225615, acc: 0.7333963601985346
epoch: 90, train loss: 0.5295324581070484, acc: 0.7800994435894401; test loss: 0.70489457002404, acc: 0.7315055542424959
epoch: 91, train loss: 0.5079055278205555, acc: 0.7878536758612525; test loss: 0.7030676391157972, acc: 0.737414322855117
epoch: 92, train loss: 0.5031182627374778, acc: 0.7911684621759204; test loss: 0.7154491126917688, acc: 0.7305601512644765
epoch: 93, train loss: 0.5061515351788058, acc: 0.7896886468568722; test loss: 0.7005032375613388, acc: 0.7364689198770976
epoch: 94, train loss: 0.5083497742146194, acc: 0.7884456019888718; test loss: 0.7048554779986799, acc: 0.7362325691325927
epoch: 95, train loss: 0.49664273798091746, acc: 0.7920563513673493; test loss: 0.6715919241164768, acc: 0.7430867407232333
epoch: 96, train loss: 0.4823747050822861, acc: 0.7978572274180182; test loss: 0.7394338037578301, acc: 0.726778539352399
epoch: 97, train loss: 0.49665655945622317, acc: 0.7929442405587782; test loss: 0.7519384300531267, acc: 0.7156700543606712
epoch: 98, train loss: 0.4883067637833672, acc: 0.7929442405587782; test loss: 0.6929830754797838, acc: 0.7407232332781848
epoch: 99, train loss: 0.4861565149012467, acc: 0.7985083461583994; test loss: 0.7011022173859667, acc: 0.7338690616875443
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.386865240424374, acc: 0.7937729371374452; test loss: 0.5602577681858132, acc: 0.7475774048688253
epoch: 101, train loss: 0.3755075721235773, acc: 0.7979164200307801; test loss: 0.6054623157976368, acc: 0.7312692034979911
epoch: 102, train loss: 0.3704876567084817, acc: 0.8038948739197348; test loss: 0.6885221339207511, acc: 0.7095249350035453
epoch: 103, train loss: 0.382727941295953, acc: 0.7925890848822067; test loss: 0.5760183558625213, acc: 0.7364689198770976
epoch: 104, train loss: 0.37688514118901906, acc: 0.7938913223629691; test loss: 0.6669532904188786, acc: 0.6951075395887497
epoch: 105, train loss: 0.36741041867242163, acc: 0.8002249319284953; test loss: 0.6777638280625548, acc: 0.7128338454266131
epoch: 106, train loss: 0.3675551315560289, acc: 0.7952527524564934; test loss: 0.5815816225382319, acc: 0.7359962183880879
epoch: 107, train loss: 0.35759384361571694, acc: 0.7991002722860187; test loss: 0.5935416304453753, acc: 0.7291420467974474
epoch: 108, train loss: 0.3655442466810072, acc: 0.8040132591452587; test loss: 0.615455179004854, acc: 0.7303238005199716
epoch: 109, train loss: 0.34750407563026975, acc: 0.8080975494258317; test loss: 0.658930252472353, acc: 0.7215788229732923
epoch: 110, train loss: 0.3533840475773069, acc: 0.8034805256304013; test loss: 0.6097888083244942, acc: 0.7289056960529425
epoch: 111, train loss: 0.36216720175189887, acc: 0.7995146205753522; test loss: 0.5851303570886448, acc: 0.7400141810446703
epoch: 112, train loss: 0.3623935268999797, acc: 0.799692198413638; test loss: 0.6348032113987908, acc: 0.7123611439376034
epoch: 113, train loss: 0.3535601404576354, acc: 0.803717296081449; test loss: 0.6063905371415984, acc: 0.7248877333963601
epoch: 114, train loss: 0.3409579394377942, acc: 0.8075648159109743; test loss: 0.6323634746423485, acc: 0.7300874497754668
epoch: 115, train loss: 0.35337932183288945, acc: 0.8036581034686872; test loss: 0.5934728209895181, acc: 0.7284329945639328
epoch: 116, train loss: 0.35342581860692, acc: 0.7998105836391618; test loss: 0.6407999052445789, acc: 0.7168518080831955
Epoch   116: reducing learning rate of group 0 to 7.5000e-04.
epoch: 117, train loss: 0.30121959902886825, acc: 0.828163845152125; test loss: 0.5828419747687321, acc: 0.7499409123138738
epoch: 118, train loss: 0.27344924740725274, acc: 0.8399431750917485; test loss: 0.5879234247696763, acc: 0.7478137556133302
epoch: 119, train loss: 0.26448304719038374, acc: 0.842074109151178; test loss: 0.6145482256293606, acc: 0.7333963601985346
epoch: 120, train loss: 0.2713800558673287, acc: 0.8419557239256541; test loss: 0.6341425325819235, acc: 0.723705979673836
epoch: 121, train loss: 0.2661299152114313, acc: 0.8435539244702261; test loss: 0.619172719854239, acc: 0.7303238005199716
epoch: 122, train loss: 0.2633558145088177, acc: 0.8442642358233693; test loss: 0.6238395505074321, acc: 0.7447411959347672
epoch: 123, train loss: 0.26628055511261, acc: 0.8397064046407008; test loss: 0.6062451371400527, acc: 0.7546679272039707
epoch: 124, train loss: 0.2572337880887632, acc: 0.8460400142062271; test loss: 0.6247923655364565, acc: 0.7364689198770976
epoch: 125, train loss: 0.2523348437013803, acc: 0.8439090801467977; test loss: 0.6344614472971885, acc: 0.7419049870007091
epoch: 126, train loss: 0.2575426842773456, acc: 0.8453888954658458; test loss: 0.6257347356227801, acc: 0.7452138974237769
epoch: 127, train loss: 0.25678860893881195, acc: 0.8467503255593702; test loss: 0.6327755852417303, acc: 0.735759867643583
epoch: 128, train loss: 0.25793943589179913, acc: 0.8442642358233693; test loss: 0.6780469045377572, acc: 0.7315055542424959
epoch: 129, train loss: 0.25407078145622036, acc: 0.8453888954658458; test loss: 0.6530103125320882, acc: 0.7376506735996219
epoch: 130, train loss: 0.24816774471322361, acc: 0.8465727477210844; test loss: 0.6472797955612403, acc: 0.738832427322146
epoch: 131, train loss: 0.2564711197813799, acc: 0.8404759086066059; test loss: 0.6344420238466473, acc: 0.7430867407232333
epoch: 132, train loss: 0.2435012017871214, acc: 0.8491180300698473; test loss: 0.6259481006129587, acc: 0.7508863152918932
epoch: 133, train loss: 0.2392313094538204, acc: 0.8531431277376583; test loss: 0.6672322250826932, acc: 0.7419049870007091
epoch: 134, train loss: 0.24345134006557598, acc: 0.8484077187167042; test loss: 0.6557199305829864, acc: 0.7409595840226897
epoch: 135, train loss: 0.24646095984099503, acc: 0.8482893334911803; test loss: 0.6676268653930487, acc: 0.7440321437012527
epoch: 136, train loss: 0.25517456583670783, acc: 0.8462767846572747; test loss: 0.642721994242694, acc: 0.7376506735996219
epoch: 137, train loss: 0.2399186415052476, acc: 0.8532023203504203; test loss: 0.6536774707725217, acc: 0.7411959347671945
epoch: 138, train loss: 0.24192724341017913, acc: 0.8513673493548005; test loss: 0.6503506156181619, acc: 0.7506499645473883
epoch: 139, train loss: 0.23015683009467267, acc: 0.8548597135077542; test loss: 0.6648038552743384, acc: 0.7355235168990782
epoch: 140, train loss: 0.24589195405080697, acc: 0.8491180300698473; test loss: 0.6602318489193719, acc: 0.7414322855116994
epoch: 141, train loss: 0.24091481354880984, acc: 0.852610394222801; test loss: 0.6463251996339001, acc: 0.737414322855117
epoch: 142, train loss: 0.22751221693350998, acc: 0.859121581626613; test loss: 0.668723959163187, acc: 0.7284329945639328
epoch: 143, train loss: 0.236206263926476, acc: 0.8514857345803244; test loss: 0.6519386393892599, acc: 0.7315055542424959
epoch: 144, train loss: 0.23723969843785014, acc: 0.8530839351248964; test loss: 0.665456915403252, acc: 0.7440321437012527
epoch: 145, train loss: 0.22136939377760456, acc: 0.861607671362614; test loss: 0.664987141599725, acc: 0.7442684944457575
epoch: 146, train loss: 0.22024786674836633, acc: 0.8580561145968983; test loss: 0.7019320409987109, acc: 0.7201607185062633
epoch: 147, train loss: 0.21985979410506035, acc: 0.8588848111755653; test loss: 0.6852986531121261, acc: 0.7447411959347672
epoch: 148, train loss: 0.21857024787995635, acc: 0.8596543151414704; test loss: 0.7060665365258336, acc: 0.7272512408414087
epoch: 149, train loss: 0.2218509713661689, acc: 0.857523381082041; test loss: 0.6696890881370917, acc: 0.7303238005199716
epoch: 150, train loss: 0.21788144863835873, acc: 0.8602462412690897; test loss: 0.6840655213128137, acc: 0.737414322855117
epoch: 151, train loss: 0.22375226837227977, acc: 0.8578785367586125; test loss: 0.6722480836305683, acc: 0.7369416213661073
epoch: 152, train loss: 0.21875790761775118, acc: 0.8610157452349947; test loss: 0.6794507954876295, acc: 0.7341054124320492
epoch: 153, train loss: 0.2207441494174013, acc: 0.8581744998224221; test loss: 0.6756368383564473, acc: 0.7336327109430395
epoch: 154, train loss: 0.2062847041598266, acc: 0.8681188587664259; test loss: 0.7312530745459178, acc: 0.7326873079650201
epoch: 155, train loss: 0.20630988984774368, acc: 0.8646856872262342; test loss: 0.7044732434280204, acc: 0.7279602930749232
epoch: 156, train loss: 0.21180721310005818, acc: 0.861548478749852; test loss: 0.7104655221911137, acc: 0.74048688253368
epoch: 157, train loss: 0.20264627817567035, acc: 0.8683556292174737; test loss: 0.6934934923536696, acc: 0.737414322855117
epoch: 158, train loss: 0.20793247774174556, acc: 0.8664022729963301; test loss: 0.7257577699567939, acc: 0.7452138974237769
epoch: 159, train loss: 0.20831062478808604, acc: 0.8651000355155677; test loss: 0.7006932523227476, acc: 0.7440321437012527
epoch: 160, train loss: 0.20801127757830803, acc: 0.8620812122647094; test loss: 0.6836578126946569, acc: 0.74048688253368
epoch: 161, train loss: 0.2047640746537646, acc: 0.8669350065111874; test loss: 0.7310457866373876, acc: 0.734341763176554
epoch: 162, train loss: 0.1971194310976201, acc: 0.8693027110216645; test loss: 0.6935279072387595, acc: 0.7499409123138738
epoch: 163, train loss: 0.19379045412683538, acc: 0.869717059310998; test loss: 0.7823313458031781, acc: 0.7279602930749232
epoch: 164, train loss: 0.20716484996028323, acc: 0.8655735764176631; test loss: 0.6947440941884314, acc: 0.734341763176554
epoch: 165, train loss: 0.2031429154930474, acc: 0.865691961643187; test loss: 0.6913626973326397, acc: 0.734341763176554
epoch: 166, train loss: 0.2103650511524812, acc: 0.8602462412690897; test loss: 0.6872612890186278, acc: 0.7487591585913496
epoch: 167, train loss: 0.19701371957342118, acc: 0.8678820883153783; test loss: 0.6985910892542923, acc: 0.7445048451902624
Epoch   167: reducing learning rate of group 0 to 3.7500e-04.
epoch: 168, train loss: 0.16642185983021923, acc: 0.8829762045696697; test loss: 0.7154977115382261, acc: 0.7452138974237769
epoch: 169, train loss: 0.14997588167377404, acc: 0.8940452231561501; test loss: 0.7293265848626371, acc: 0.7416686362562042
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.10765167501701806, acc: 0.8922694447732923; test loss: 0.6653411822858868, acc: 0.7445048451902624
epoch: 171, train loss: 0.10666600957810984, acc: 0.8934532970285308; test loss: 0.6467390070567878, acc: 0.7437957929567478
epoch: 172, train loss: 0.09863352810678869, acc: 0.8994317509174855; test loss: 0.6423572949612572, acc: 0.7463956511463011
epoch: 173, train loss: 0.1057333712046762, acc: 0.8956434237007221; test loss: 0.6523740322588184, acc: 0.7437957929567478
epoch: 174, train loss: 0.10598690418443397, acc: 0.8921510595477684; test loss: 0.6504202971811188, acc: 0.752777121247932
epoch: 175, train loss: 0.10012210167726568, acc: 0.8964129276666272; test loss: 0.6563898614044027, acc: 0.7468683526353108
epoch: 176, train loss: 0.09480293859310146, acc: 0.9018586480407245; test loss: 0.6581157500849242, acc: 0.7449775466792721
epoch: 177, train loss: 0.09935708387467203, acc: 0.9013851071386291; test loss: 0.672276924333232, acc: 0.7447411959347672
epoch: 178, train loss: 0.0970875446939237, acc: 0.898247898662247; test loss: 0.6818732178766264, acc: 0.7440321437012527
epoch: 179, train loss: 0.11181817344004295, acc: 0.8912039777435776; test loss: 0.6624877393118909, acc: 0.7426140392342235
epoch: 180, train loss: 0.10377936766946952, acc: 0.8926837930626258; test loss: 0.6602816362522984, acc: 0.7407232332781848
epoch: 181, train loss: 0.10148096990668945, acc: 0.8939268379306262; test loss: 0.6468947800979128, acc: 0.743559442212243
epoch: 182, train loss: 0.09862663545974884, acc: 0.8944003788327217; test loss: 0.6633877957354649, acc: 0.7468683526353108
epoch: 183, train loss: 0.10413412925204794, acc: 0.8948739197348171; test loss: 0.6655125988484895, acc: 0.7423776884897187
epoch: 184, train loss: 0.10473235967516462, acc: 0.8934532970285308; test loss: 0.6535734836442677, acc: 0.7395414795556606
epoch: 185, train loss: 0.0939597017781698, acc: 0.8999644844323429; test loss: 0.680826833811079, acc: 0.7411959347671945
epoch: 186, train loss: 0.09826412332316214, acc: 0.8968864685687227; test loss: 0.6702040459804856, acc: 0.7333963601985346
epoch: 187, train loss: 0.10258962904805755, acc: 0.8975967799218657; test loss: 0.6631197904869897, acc: 0.7341054124320492
epoch: 188, train loss: 0.09975434755137987, acc: 0.8955842310879603; test loss: 0.677720964081714, acc: 0.7430867407232333
epoch: 189, train loss: 0.09983178919393579, acc: 0.8929797561264354; test loss: 0.6877865740605433, acc: 0.7338690616875443
epoch: 190, train loss: 0.10421483699116352, acc: 0.8906120516159584; test loss: 0.6478106009427663, acc: 0.7461593004017962
epoch: 191, train loss: 0.09408338675682286, acc: 0.8999052918195809; test loss: 0.6652236101447592, acc: 0.7456865989127865
epoch: 192, train loss: 0.09740827070524653, acc: 0.898247898662247; test loss: 0.658166078128197, acc: 0.7426140392342235
epoch: 193, train loss: 0.09451153229979829, acc: 0.8983070912750089; test loss: 0.6704144588009288, acc: 0.7390687780666509
epoch: 194, train loss: 0.09280438959153994, acc: 0.9002012548833905; test loss: 0.6748176584906456, acc: 0.7440321437012527
epoch: 195, train loss: 0.09548983962222275, acc: 0.8981295134367231; test loss: 0.681413892058722, acc: 0.7421413377452138
epoch: 196, train loss: 0.12511175191725615, acc: 0.8827986267313839; test loss: 0.6561628409069894, acc: 0.735759867643583
epoch: 197, train loss: 0.12506373486138384, acc: 0.8768793654551912; test loss: 0.6554283500422092, acc: 0.735759867643583
epoch: 198, train loss: 0.09740953473589245, acc: 0.8954066532496744; test loss: 0.6659599873676448, acc: 0.737414322855117
epoch: 199, train loss: 0.08963430539519741, acc: 0.9019770332662483; test loss: 0.6848225673584475, acc: 0.7447411959347672
epoch: 200, train loss: 0.08701510030595003, acc: 0.901503492364153; test loss: 0.6856996964067186, acc: 0.7369416213661073
best test acc 0.7546679272039707 at epoch 123.
****************************************************************
/opt/python/anaconda-2020.7/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
train report:
              precision    recall  f1-score   support

           0     0.9101    0.9498    0.9296      6100
           1     0.9533    0.8812    0.9158       926
           2     0.8438    0.9092    0.8753      2400
           3     0.8280    0.8968    0.8610       843
           4     0.9042    0.9509    0.9270       774
           5     0.8930    0.9438    0.9177      1512
           6     0.7486    0.6805    0.7129      1330
           7     0.8817    0.7900    0.8333       481
           8     0.8491    0.8231    0.8359       458
           9     0.8219    0.9801    0.8940       452
          10     0.8929    0.8020    0.8450       717
          11     0.8893    0.7477    0.8124       333
          12     0.0000    0.0000    0.0000       299
          13     0.8037    0.6394    0.7122       269

    accuracy                         0.8768     16894
   macro avg     0.8014    0.7853    0.7909     16894
weighted avg     0.8607    0.8768    0.8674     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8193    0.8564    0.8374      1525
           1     0.8545    0.7845    0.8180       232
           2     0.7302    0.7970    0.7621       601
           3     0.6967    0.8057    0.7473       211
           4     0.7980    0.8351    0.8161       194
           5     0.7542    0.8280    0.7894       378
           6     0.5483    0.5285    0.5382       333
           7     0.7976    0.5537    0.6537       121
           8     0.5619    0.5130    0.5364       115
           9     0.6319    0.7982    0.7054       114
          10     0.7808    0.6333    0.6994       180
          11     0.6182    0.4048    0.4892        84
          12     0.0000    0.0000    0.0000        75
          13     0.7843    0.5882    0.6723        68

    accuracy                         0.7547      4231
   macro avg     0.6697    0.6376    0.6475      4231
weighted avg     0.7410    0.7547    0.7451      4231

---------------------------------------
program finished.
