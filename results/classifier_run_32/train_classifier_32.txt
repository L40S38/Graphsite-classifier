seed:  666
save trained model at:  ../trained_models/trained_classifier_model_32.pt
save loss at:  ./results/train_classifier_results_32.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  14780
number of pockets in validation set:  3162
number of pockets in test set:  3183
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKMCNMMEmbeddingNet(
    (conv0): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=33, out_features=128, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=384, out_features=128, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=384, out_features=128, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=384, out_features=128, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=384, out_features=128, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=384, out_features=128, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(128, 256)
  )
  (fc1): Linear(in_features=256, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0008
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b5377463bb0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0114176269151844, acc: 0.39952638700947224, val loss: 1.7199040416823694, acc: 0.45540796963946867, test loss: 1.7254582726277534, acc: 0.4671693371033616
epoch: 2, train loss: 1.6982621755264447, acc: 0.4754397834912043, val loss: 1.6253290367910638, acc: 0.49082858950031627, test loss: 1.6192545808414, acc: 0.49355953502984606
epoch: 3, train loss: 1.5936932796070475, acc: 0.5084573748308525, val loss: 1.520764585690435, acc: 0.5177103099304238, test loss: 1.532681967378449, acc: 0.5186930568645931
epoch: 4, train loss: 1.4889744324355068, acc: 0.5495940460081191, val loss: 1.7710334790197828, acc: 0.48418722327640734, test loss: 1.7606725941429413, acc: 0.4941878730757147
epoch: 5, train loss: 1.4772821833219516, acc: 0.5558863328822733, val loss: 1.5245076245254237, acc: 0.5284629981024668, test loss: 1.518839494110165, acc: 0.532516493873704
epoch: 6, train loss: 1.4104441331429798, acc: 0.5689445196211096, val loss: 1.3706917602905815, acc: 0.5698924731182796, test loss: 1.3657012579765404, acc: 0.5746151429469054
epoch: 7, train loss: 1.3268059297892012, acc: 0.5993910690121786, val loss: 1.318800503509396, acc: 0.5784313725490197, test loss: 1.3172164191774254, acc: 0.5896952560477537
epoch: 8, train loss: 1.2689522110883535, acc: 0.6175913396481733, val loss: 1.2697337539342293, acc: 0.6075268817204301, test loss: 1.2778003083959226, acc: 0.6189129751806471
epoch: 9, train loss: 1.234157459274519, acc: 0.6301082543978349, val loss: 1.2663923110636182, acc: 0.6065781151170145, test loss: 1.2724531108839423, acc: 0.6025761859880616
epoch: 10, train loss: 1.2100157395751292, acc: 0.633085250338295, val loss: 1.4247981312152782, acc: 0.5433270082226439, test loss: 1.4285836267725986, acc: 0.5589066918001885
epoch: 11, train loss: 1.196534557045716, acc: 0.6378213802435724, val loss: 1.4267881197087906, acc: 0.5752688172043011, test loss: 1.4339702003525743, acc: 0.5852968897266729
epoch: 12, train loss: 1.1749329636319565, acc: 0.6456698240866036, val loss: 1.270032337690591, acc: 0.6299810246679317, test loss: 1.25350243431796, acc: 0.6289663839145461
epoch: 13, train loss: 1.1043925048533894, acc: 0.6681326116373477, val loss: 1.465423948775357, acc: 0.5585072738772928, test loss: 1.4698331120101829, acc: 0.565818410304744
epoch: 14, train loss: 1.1085971152185587, acc: 0.6646143437077131, val loss: 1.092971648542584, acc: 0.6628716002530044, test loss: 1.0946669126133703, acc: 0.6619541313226516
epoch: 15, train loss: 1.1041442900613776, acc: 0.6660351826792964, val loss: 1.2973152173613536, acc: 0.5917141049968374, test loss: 1.3161548859586965, acc: 0.5846685516808042
epoch: 16, train loss: 1.0591498291702168, acc: 0.6767929634641408, val loss: 1.2976973175927102, acc: 0.5822264389626819, test loss: 1.3082862181218584, acc: 0.5884385799560163
epoch: 17, train loss: 1.0423151471779375, acc: 0.6844384303112314, val loss: 1.2906593382170048, acc: 0.5765338393421885, test loss: 1.305131179373331, acc: 0.5830977065661326
epoch: 18, train loss: 1.007728423994836, acc: 0.695872801082544, val loss: 1.2550486863828172, acc: 0.6087919038583175, test loss: 1.2690089151913964, acc: 0.6098020735155514
epoch: 19, train loss: 1.0191261077927316, acc: 0.6914073071718538, val loss: 1.0863459687833166, acc: 0.6669829222011385, test loss: 1.07964231164209, acc: 0.666038328620798
epoch: 20, train loss: 0.9731930180396053, acc: 0.706021650879567, val loss: 1.1551040384605065, acc: 0.640101201771031, test loss: 1.1291727771033966, acc: 0.6443606660383286
epoch: 21, train loss: 0.9543703119712851, acc: 0.710148849797023, val loss: 1.1211478595564743, acc: 0.6546489563567363, test loss: 1.1150241180519394, acc: 0.6657241595978637
epoch: 22, train loss: 0.9382602880384989, acc: 0.7145466847090663, val loss: 1.2501078608064693, acc: 0.620809614168248, test loss: 1.2318755912211494, acc: 0.6302230600062834
epoch: 23, train loss: 0.9123884139912699, acc: 0.7234776725304466, val loss: 1.1161424463719676, acc: 0.6426312460468058, test loss: 1.1112286924679176, acc: 0.6500157084511468
epoch: 24, train loss: 0.927351798353079, acc: 0.7213802435723952, val loss: 1.4197177784417265, acc: 0.5290955091714105, test loss: 1.4330441703820356, acc: 0.5340873389883758
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.7057794496880817, acc: 0.7261840324763194, val loss: 0.8478551431217953, acc: 0.6682479443390259, test loss: 0.8344295398641449, acc: 0.6830034558592523
epoch: 26, train loss: 0.6756516760999037, acc: 0.7374154262516914, val loss: 0.795487673202698, acc: 0.6894370651486401, test loss: 0.801000929537937, acc: 0.6918001885014138
epoch: 27, train loss: 0.675663979756816, acc: 0.7391069012178619, val loss: 0.838800519723669, acc: 0.6707779886148008, test loss: 0.842348910786242, acc: 0.6751492302858938
epoch: 28, train loss: 0.6747924551750875, acc: 0.7393775372124493, val loss: 0.8459029466121126, acc: 0.6669829222011385, test loss: 0.8393288649667782, acc: 0.6663524976437323
epoch: 29, train loss: 0.6595125113998279, acc: 0.743978349120433, val loss: 0.904564729904088, acc: 0.6777356103731815, test loss: 0.888144076413566, acc: 0.6729500471253534
epoch: 30, train loss: 0.6568318713179138, acc: 0.745872801082544, val loss: 0.7961296097226417, acc: 0.6925996204933587, test loss: 0.7821284893957504, acc: 0.7093936537857367
epoch: 31, train loss: 0.6403627345461969, acc: 0.7513531799729364, val loss: 0.7887799860479559, acc: 0.7039848197343453, test loss: 0.7798168058482274, acc: 0.704052780395853
epoch: 32, train loss: 0.6246929920090712, acc: 0.7552097428958051, val loss: 0.7593178171511318, acc: 0.693864642631246, test loss: 0.758972027898771, acc: 0.6999685830977066
epoch: 33, train loss: 0.6686172055617399, acc: 0.7351826792963464, val loss: 0.7840304397315183, acc: 0.6891208096141682, test loss: 0.8032078447860252, acc: 0.6918001885014138
epoch: 34, train loss: 0.6091433517503803, acc: 0.7633964817320704, val loss: 0.867433364145519, acc: 0.6574952561669829, test loss: 0.8806627079858519, acc: 0.6584982720703738
epoch: 35, train loss: 0.6071506917395998, acc: 0.7631258457374831, val loss: 0.9478432955732834, acc: 0.6774193548387096, test loss: 0.9109143028834538, acc: 0.6848884699968583
epoch: 36, train loss: 0.6193734868453545, acc: 0.7589309878213802, val loss: 0.8068024858804084, acc: 0.6932321315623023, test loss: 0.8066307341119908, acc: 0.6974552309142319
epoch: 37, train loss: 0.5804401566759659, acc: 0.7761163734776725, val loss: 0.841812930628893, acc: 0.6837444655281467, test loss: 0.816899144758116, acc: 0.6943135406848885
epoch: 38, train loss: 0.594478464836358, acc: 0.7660351826792964, val loss: 0.8060960439396085, acc: 0.700506008855155, test loss: 0.7808682435419513, acc: 0.702167766258247
epoch: 39, train loss: 0.581167906482423, acc: 0.7701623815967523, val loss: 1.0274473260581531, acc: 0.6242884250474383, test loss: 1.0168322845708264, acc: 0.6286522148916117
epoch: 40, train loss: 0.6380684438997096, acc: 0.7549391069012179, val loss: 0.9071336507344834, acc: 0.66382036685642, test loss: 0.905325959847103, acc: 0.6600691171850456
epoch: 41, train loss: 0.5811524963507956, acc: 0.7716508795669824, val loss: 1.0280286685188385, acc: 0.6729917773561037, test loss: 1.0022739268982144, acc: 0.671693371033616
epoch: 42, train loss: 0.5835474910529606, acc: 0.7700270635994587, val loss: 0.7381892709472676, acc: 0.7160025300442757, test loss: 0.7136825581897253, acc: 0.7229029217719133
epoch: 43, train loss: 0.5790320381905294, acc: 0.7724627875507443, val loss: 0.7269503014665251, acc: 0.7128399746995573, test loss: 0.7264867002695116, acc: 0.7134778510838832
epoch: 44, train loss: 0.5374909574833872, acc: 0.7866035182679296, val loss: 0.7226728165473914, acc: 0.734977862112587, test loss: 0.7389902785406374, acc: 0.7251021049324536
epoch: 45, train loss: 0.5475970273572796, acc: 0.7846414073071718, val loss: 0.8029028957663722, acc: 0.6790006325110689, test loss: 0.8095788970669223, acc: 0.6789192585611059
epoch: 46, train loss: 0.5444529431114661, acc: 0.7852503382949932, val loss: 0.679148086183513, acc: 0.7330803289057558, test loss: 0.6939415104764458, acc: 0.7260446120012567
epoch: 47, train loss: 0.5054702233074483, acc: 0.7979025710419486, val loss: 0.7453071953449575, acc: 0.7147375079063883, test loss: 0.7575303500004995, acc: 0.7056236255105247
epoch: 48, train loss: 0.5143552146680623, acc: 0.7942489851150203, val loss: 0.796322295667249, acc: 0.6992409867172675, test loss: 0.7931285455771449, acc: 0.7037386113729186
epoch: 49, train loss: 0.5108368590171669, acc: 0.7991204330175914, val loss: 0.6936700565765207, acc: 0.7305502846299811, test loss: 0.7024103330060292, acc: 0.7291863022306001
epoch: 50, train loss: 0.5327839277755262, acc: 0.7866711772665764, val loss: 0.7584683421591578, acc: 0.7106261859582542, test loss: 0.7859308827496384, acc: 0.6990260760289035
epoch: 51, train loss: 0.5131782562548801, acc: 0.7994587280108254, val loss: 0.6705618028321046, acc: 0.7390891840607211, test loss: 0.6998239372497904, acc: 0.743952246308514
epoch: 52, train loss: 0.4758889316947921, acc: 0.8080514208389716, val loss: 0.7694820345843012, acc: 0.7087286527514232, test loss: 0.7589422594328703, acc: 0.7175620483820295
epoch: 53, train loss: 0.4946298839436171, acc: 0.8013531799729364, val loss: 0.7524488518164174, acc: 0.7248576850094877, test loss: 0.7343759345364129, acc: 0.7266729500471254
epoch: 54, train loss: 0.48916716491740514, acc: 0.8065629228687415, val loss: 0.7890287741010209, acc: 0.7099936748893105, test loss: 0.8178896512245181, acc: 0.708765315739868
epoch: 55, train loss: 0.4901503638824365, acc: 0.8075778078484438, val loss: 0.7654259738704964, acc: 0.7109424414927261, test loss: 0.7944172473587482, acc: 0.6899151743638078
epoch: 56, train loss: 0.4749969965792154, acc: 0.8087280108254398, val loss: 0.7059650593057303, acc: 0.7419354838709677, test loss: 0.7317918792596374, acc: 0.7251021049324536
epoch: 57, train loss: 0.46672727748731474, acc: 0.8149526387009473, val loss: 0.7093313343535489, acc: 0.7292852624920936, test loss: 0.7447378423428932, acc: 0.7122211749921458
epoch: 58, train loss: 0.4576954996392272, acc: 0.815426251691475, val loss: 0.7450333523041534, acc: 0.7226438962681847, test loss: 0.7667326005120118, acc: 0.7100219918316054
epoch: 59, train loss: 0.46816485929069723, acc: 0.8078484438430311, val loss: 0.8098462777074419, acc: 0.6929158760278304, test loss: 0.8313066565387472, acc: 0.6933710336160854
epoch: 60, train loss: 0.43954088265260277, acc: 0.8235453315290934, val loss: 0.6907289224210832, acc: 0.7422517394054396, test loss: 0.7104076017121492, acc: 0.7423814011938423
epoch: 61, train loss: 0.4352108903682926, acc: 0.8242219215155616, val loss: 0.7099525755074566, acc: 0.7337128399746996, test loss: 0.7209035288115324, acc: 0.7291863022306001
epoch: 62, train loss: 0.42394944184203914, acc: 0.828213802435724, val loss: 0.7544646596395841, acc: 0.7182163187855788, test loss: 0.7554226965039846, acc: 0.7128495130380145
epoch: 63, train loss: 0.4347324655568971, acc: 0.8252368064952639, val loss: 0.7736124200658057, acc: 0.7163187855787476, test loss: 0.7962864848228883, acc: 0.7241595978636507
epoch: 64, train loss: 0.44833440716115, acc: 0.8201623815967524, val loss: 0.715863450277764, acc: 0.7333965844402277, test loss: 0.7517282677415853, acc: 0.7332704995287465
epoch: 65, train loss: 0.4211436672123585, acc: 0.8320027063599459, val loss: 0.7546973462198295, acc: 0.7311827956989247, test loss: 0.7771687210261579, acc: 0.7304429783223374
epoch: 66, train loss: 0.426299663341739, acc: 0.8261840324763193, val loss: 0.7919438162760822, acc: 0.7188488298545225, test loss: 0.7913528123143106, acc: 0.7181903864278982
epoch: 67, train loss: 0.4059023989553542, acc: 0.8366711772665765, val loss: 0.7917090887362862, acc: 0.6916508538899431, test loss: 0.7919024597201526, acc: 0.6955702167766258
epoch: 68, train loss: 0.4068542766796881, acc: 0.8337618403247632, val loss: 0.8094112090412987, acc: 0.724225173940544, test loss: 0.8131847607052031, acc: 0.7197612315425699
epoch: 69, train loss: 0.4119960979050648, acc: 0.8336941813261164, val loss: 0.7857563676598854, acc: 0.7122074636306135, test loss: 0.7736507770665367, acc: 0.7219604147031102
epoch: 70, train loss: 0.4195787390046255, acc: 0.8311231393775372, val loss: 1.1166526039215223, acc: 0.6578115117014548, test loss: 1.1546963389999194, acc: 0.6500157084511468
epoch: 71, train loss: 0.40017922061705946, acc: 0.832679296346414, val loss: 0.7439954095366331, acc: 0.7311827956989247, test loss: 0.7471165678765539, acc: 0.7241595978636507
Epoch    71: reducing learning rate of group 0 to 1.5000e-03.
epoch: 72, train loss: 0.31774119149202906, acc: 0.8698240866035183, val loss: 0.6461171303121144, acc: 0.7666034155597723, test loss: 0.671501866272325, acc: 0.7656299088909834
epoch: 73, train loss: 0.24948910859505444, acc: 0.8945196211096076, val loss: 0.6428336512054694, acc: 0.782416192283365, test loss: 0.70284969020931, acc: 0.7734841344643418
epoch: 74, train loss: 0.2743391627629813, acc: 0.8846414073071719, val loss: 0.7176005600525713, acc: 0.7685009487666035, test loss: 0.7600738751300281, acc: 0.746779767514923
epoch: 75, train loss: 0.26834259726036547, acc: 0.8868064952638701, val loss: 0.6406255310506175, acc: 0.7697659709044908, test loss: 0.6580225350232833, acc: 0.7628023876845743
epoch: 76, train loss: 0.2482089074925899, acc: 0.8956698240866036, val loss: 0.7104581768643924, acc: 0.7596457938013915, test loss: 0.7188372918355775, acc: 0.7543198240653471
epoch: 77, train loss: 0.24478529507397637, acc: 0.8951285520974289, val loss: 0.7839913319214321, acc: 0.7485768500948766, test loss: 0.8007231573619596, acc: 0.742067232170908
epoch: 78, train loss: 0.2449978556942714, acc: 0.8965493910690122, val loss: 0.7847254196350368, acc: 0.7466793168880456, test loss: 0.8242606921949516, acc: 0.7417530631479736
epoch: 79, train loss: 0.22950320140192731, acc: 0.9012178619756428, val loss: 0.7572532182701927, acc: 0.7549019607843137, test loss: 0.774384119658211, acc: 0.7505497957901351
epoch: 80, train loss: 0.21841288656197963, acc: 0.9050067658998647, val loss: 0.8125317123100624, acc: 0.7580645161290323, test loss: 0.8605064153296746, acc: 0.7483506126295947
epoch: 81, train loss: 0.22376113952577356, acc: 0.9016914749661705, val loss: 0.7466257852063308, acc: 0.7593295382669196, test loss: 0.7449741584116881, acc: 0.7646874018221803
epoch: 82, train loss: 0.20857551230951962, acc: 0.9104194857916103, val loss: 0.6801210003966851, acc: 0.7802024035420619, test loss: 0.7476641793240552, acc: 0.7665724159597863
epoch: 83, train loss: 0.20903730532797166, acc: 0.9104194857916103, val loss: 0.7298667078000092, acc: 0.7741935483870968, test loss: 0.7921207772086261, acc: 0.7543198240653471
epoch: 84, train loss: 0.2074489629599335, acc: 0.909404600811908, val loss: 0.8542595793369976, acc: 0.7466793168880456, test loss: 0.8568577627846553, acc: 0.74583726044612
epoch: 85, train loss: 0.20779746190136922, acc: 0.9100135317997293, val loss: 0.771333327103385, acc: 0.7691334598355472, test loss: 0.7967385898548741, acc: 0.7631165567075087
epoch: 86, train loss: 0.2131343310030936, acc: 0.9055480378890393, val loss: 0.8237485207009361, acc: 0.7482605945604048, test loss: 0.854088415411932, acc: 0.743952246308514
epoch: 87, train loss: 0.23571690340326024, acc: 0.8991204330175914, val loss: 0.7658529237129506, acc: 0.7552182163187856, test loss: 0.7750105642500892, acc: 0.7524348099277411
epoch: 88, train loss: 0.19588011270930866, acc: 0.912043301759134, val loss: 0.7156421159506599, acc: 0.775774826059456, test loss: 0.7728892013856611, acc: 0.7628023876845743
epoch: 89, train loss: 0.18748200559164416, acc: 0.9146143437077131, val loss: 0.7921428608034774, acc: 0.7476280834914611, test loss: 0.8248854280454424, acc: 0.7445805843543827
epoch: 90, train loss: 0.22014439385056658, acc: 0.9014208389715832, val loss: 0.7780266459571795, acc: 0.7428842504743833, test loss: 0.767133576293955, acc: 0.7568331762488218
epoch: 91, train loss: 0.1939729481975829, acc: 0.911637347767253, val loss: 0.7521147545495416, acc: 0.7634408602150538, test loss: 0.7848743772941156, acc: 0.7565190072258875
epoch: 92, train loss: 0.189156633861494, acc: 0.9166441136671177, val loss: 0.8009130872555129, acc: 0.7640733712839974, test loss: 0.8138162184915114, acc: 0.7712849513038015
epoch: 93, train loss: 0.1843311973450471, acc: 0.9186738836265224, val loss: 1.1028437278151588, acc: 0.6951296647691335, test loss: 1.0869802311835257, acc: 0.7027961043041157
epoch: 94, train loss: 0.22977856623465057, acc: 0.8981732070365359, val loss: 0.8006567297518669, acc: 0.7501581277672359, test loss: 0.8453496700931636, acc: 0.7477222745837261
epoch: 95, train loss: 0.19314548120366382, acc: 0.9140054127198918, val loss: 0.7874479293823242, acc: 0.7836812144212524, test loss: 0.8028941696033963, acc: 0.7734841344643418
epoch: 96, train loss: 0.17503798779437926, acc: 0.9213802435723951, val loss: 0.7478327914780262, acc: 0.7624920936116382, test loss: 0.7842003969587398, acc: 0.7511781338360037
epoch: 97, train loss: 0.1703150716661923, acc: 0.9242895805142084, val loss: 0.8487471471022836, acc: 0.775774826059456, test loss: 0.8587021538569348, acc: 0.7640590637763116
epoch: 98, train loss: 0.18731153578332377, acc: 0.915426251691475, val loss: 0.7413279151403775, acc: 0.7811511701454775, test loss: 0.7507176327697893, acc: 0.7800816839459629
epoch: 99, train loss: 0.16784949647073655, acc: 0.9228687415426252, val loss: 0.7301626819964077, acc: 0.7827324478178368, test loss: 0.7692834732856835, acc: 0.7719132893496701
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.12799319781171453, acc: 0.921786197564276, val loss: 0.5723504363397192, acc: 0.7817836812144212, test loss: 0.5899250445933676, acc: 0.7788250078542256
epoch: 101, train loss: 0.09987063614074368, acc: 0.9361975642760487, val loss: 0.6541039080351384, acc: 0.7735610373181531, test loss: 0.7026740761998687, acc: 0.76248821866164
epoch: 102, train loss: 0.08586311025169448, acc: 0.943978349120433, val loss: 0.6307301791237548, acc: 0.7871600253004427, test loss: 0.6657556720099058, acc: 0.7844800502670437
epoch: 103, train loss: 0.07803988366996484, acc: 0.947361299052774, val loss: 0.64148528940839, acc: 0.7896900695762176, test loss: 0.6775706144312512, acc: 0.7832233741753063
epoch: 104, train loss: 0.09864710842804915, acc: 0.935723951285521, val loss: 0.7219074531720455, acc: 0.7628083491461101, test loss: 0.7599035007115471, acc: 0.7496072887213321
epoch: 105, train loss: 0.14371378740373902, acc: 0.91765899864682, val loss: 0.5877905017133758, acc: 0.7662871600253004, test loss: 0.6080242114137487, acc: 0.7584040213634936
epoch: 106, train loss: 0.11309707530941789, acc: 0.9259810554803789, val loss: 0.6890411006733257, acc: 0.7590132827324478, test loss: 0.6823007917763713, acc: 0.7540056550424128
epoch: 107, train loss: 0.11893573363433833, acc: 0.9253721244925575, val loss: 0.641192726437462, acc: 0.7681846932321316, test loss: 0.6979098443598483, acc: 0.7737983034872762
epoch: 108, train loss: 0.08888457445717632, acc: 0.9415426251691476, val loss: 0.6875453345161838, acc: 0.7666034155597723, test loss: 0.6764782286274557, acc: 0.7690857681432611
epoch: 109, train loss: 0.09348947935723807, acc: 0.9390392422192152, val loss: 0.8067287621356354, acc: 0.7359266287160026, test loss: 0.8210255301227144, acc: 0.7295004712535345
epoch: 110, train loss: 0.13125415975895882, acc: 0.9182679296346414, val loss: 0.6323146397067655, acc: 0.7675521821631879, test loss: 0.6712303263490169, acc: 0.763430725730443
epoch: 111, train loss: 0.12759525022987422, acc: 0.9196211096075778, val loss: 0.6223787813530475, acc: 0.7643896268184693, test loss: 0.6520674227919175, acc: 0.7637448947533774
epoch: 112, train loss: 0.11369702154026948, acc: 0.926725304465494, val loss: 0.6250529776337326, acc: 0.7688172043010753, test loss: 0.6778457338166544, acc: 0.7621740496387056
epoch: 113, train loss: 0.125353424309555, acc: 0.9217185385656292, val loss: 0.668120730156992, acc: 0.7479443390259329, test loss: 0.68274365664652, acc: 0.7524348099277411
epoch: 114, train loss: 0.10718567499611148, acc: 0.9294316644113667, val loss: 0.7531629293346466, acc: 0.7450980392156863, test loss: 0.7721094184801184, acc: 0.74583726044612
epoch: 115, train loss: 0.10594814426198863, acc: 0.9302435723951286, val loss: 0.6853097024256485, acc: 0.7713472485768501, test loss: 0.7241306938617214, acc: 0.7640590637763116
epoch: 116, train loss: 0.11250763946440932, acc: 0.9254397834912044, val loss: 0.7759950623943563, acc: 0.7533206831119544, test loss: 0.7231568666213045, acc: 0.7681432610744581
epoch: 117, train loss: 0.09986002390979914, acc: 0.9317320703653585, val loss: 0.6462209591904748, acc: 0.7839974699557243, test loss: 0.6778821294651517, acc: 0.7703424442349984
epoch: 118, train loss: 0.10730045762855726, acc: 0.9259810554803789, val loss: 0.7259122449940024, acc: 0.7359266287160026, test loss: 0.7254390328176884, acc: 0.7392397109644989
epoch: 119, train loss: 0.1092955391324744, acc: 0.9258457374830853, val loss: 0.6871433534025015, acc: 0.7511068943706515, test loss: 0.7103761642787428, acc: 0.7499214577442664
epoch: 120, train loss: 0.12416738294296883, acc: 0.9192828146143437, val loss: 0.6773312070715058, acc: 0.7466793168880456, test loss: 0.7073674559256133, acc: 0.7464655984919887
epoch: 121, train loss: 0.10541090209122762, acc: 0.9307171853856563, val loss: 0.6597412630849666, acc: 0.7741935483870968, test loss: 0.6850309374944391, acc: 0.7668865849827207
epoch: 122, train loss: 0.0984319366847388, acc: 0.9332205683355886, val loss: 0.6992507272698621, acc: 0.7555344718532574, test loss: 0.7325458404892764, acc: 0.7558906691800189
Epoch   122: reducing learning rate of group 0 to 7.5000e-04.
epoch: 123, train loss: 0.06053364070608748, acc: 0.956359945872801, val loss: 0.6383603435766086, acc: 0.7912713472485768, test loss: 0.6691466634692241, acc: 0.7835375431982406
epoch: 124, train loss: 0.033967556423319854, acc: 0.9743572395128552, val loss: 0.6567826050131627, acc: 0.7969639468690702, test loss: 0.711519330730612, acc: 0.7913917687715991
epoch: 125, train loss: 0.030798024323760092, acc: 0.9757104194857916, val loss: 0.6761696295674883, acc: 0.7925363693864642, test loss: 0.7217051030478686, acc: 0.7882500785422557
epoch: 126, train loss: 0.025206201939423932, acc: 0.9822733423545331, val loss: 0.7171448159263075, acc: 0.7890575585072739, test loss: 0.7521643410903495, acc: 0.7913917687715991
epoch: 127, train loss: 0.02883705390574403, acc: 0.9775372124492557, val loss: 0.7164673340011443, acc: 0.790955091714105, test loss: 0.7495465489104975, acc: 0.7854225573358466
epoch: 128, train loss: 0.027835561761554425, acc: 0.978213802435724, val loss: 0.7011486900373021, acc: 0.7915876027830487, test loss: 0.726084911407557, acc: 0.7895067546339931
epoch: 129, train loss: 0.027018571993353724, acc: 0.9781461434370772, val loss: 0.7177940763151397, acc: 0.7950664136622391, test loss: 0.7490855984218908, acc: 0.7913917687715991
epoch: 130, train loss: 0.027654039020951288, acc: 0.9786197564276049, val loss: 0.739229104248954, acc: 0.7884250474383302, test loss: 0.7531938616150099, acc: 0.7898209236569275
epoch: 131, train loss: 0.02650065638064451, acc: 0.980446549391069, val loss: 0.748683984938639, acc: 0.7874762808349146, test loss: 0.7813982095802275, acc: 0.7797675149230285
epoch: 132, train loss: 0.028105491090248336, acc: 0.9796346414073072, val loss: 0.7275084349869324, acc: 0.793168880455408, test loss: 0.7648849216153177, acc: 0.7851083883129123
epoch: 133, train loss: 0.038686713313177246, acc: 0.972936400541272, val loss: 0.7800886149198326, acc: 0.7814674256799494, test loss: 0.7931604205981538, acc: 0.7759974866478165
epoch: 134, train loss: 0.045391168143897646, acc: 0.9663734776725305, val loss: 0.7075691239732794, acc: 0.7833649588867805, test loss: 0.7586227809712455, acc: 0.7722274583726044
epoch: 135, train loss: 0.0366696875678509, acc: 0.9719891745602165, val loss: 0.7231682570654708, acc: 0.7814674256799494, test loss: 0.7409136552987442, acc: 0.7781966698083569
epoch: 136, train loss: 0.028871430178375786, acc: 0.9779431664411367, val loss: 0.7615027012966766, acc: 0.7912713472485768, test loss: 0.818794086748723, acc: 0.7807100219918316
epoch: 137, train loss: 0.032802047923267776, acc: 0.9748985115020298, val loss: 0.7265116087173376, acc: 0.7839974699557243, test loss: 0.792605634983553, acc: 0.7697141061891297
epoch: 138, train loss: 0.03840142299392146, acc: 0.9726657645466847, val loss: 0.7260402589565135, acc: 0.786527514231499, test loss: 0.7509431214292133, acc: 0.787621740496387
epoch: 139, train loss: 0.0589111570455386, acc: 0.9611637347767253, val loss: 0.6724161478931734, acc: 0.7786211258697027, test loss: 0.7094628372321217, acc: 0.7785108388312912
epoch: 140, train loss: 0.046520784942884405, acc: 0.9675913396481732, val loss: 0.7095595945066323, acc: 0.7827324478178368, test loss: 0.7513949687549089, acc: 0.7769399937166196
epoch: 141, train loss: 0.03429617027176168, acc: 0.9734100135317997, val loss: 0.7395961935349765, acc: 0.7783048703352309, test loss: 0.7525140542187033, acc: 0.7750549795790135
epoch: 142, train loss: 0.035428650326351355, acc: 0.9740189445196211, val loss: 0.7936617512253551, acc: 0.7612270714737508, test loss: 0.8011182928999153, acc: 0.7602890355010996
epoch: 143, train loss: 0.05194978831242644, acc: 0.9638700947225981, val loss: 0.7495407780690105, acc: 0.7710309930423782, test loss: 0.7587239514571708, acc: 0.7640590637763116
epoch: 144, train loss: 0.0418652111606443, acc: 0.9690121786197564, val loss: 0.7020512044919887, acc: 0.7783048703352309, test loss: 0.7661689385055485, acc: 0.7741124725102105
epoch: 145, train loss: 0.04361749818808171, acc: 0.9681326116373478, val loss: 0.7328006153541, acc: 0.7710309930423782, test loss: 0.7808583212692783, acc: 0.7640590637763116
epoch: 146, train loss: 0.038287752981679524, acc: 0.971447902571042, val loss: 0.7608590949418649, acc: 0.7770398481973435, test loss: 0.78032327878037, acc: 0.767200754005655
epoch: 147, train loss: 0.07516341335033047, acc: 0.9527063599458728, val loss: 0.7000049462731619, acc: 0.7713472485768501, test loss: 0.7225358072332417, acc: 0.7665724159597863
epoch: 148, train loss: 0.06176366549746432, acc: 0.9571041948579161, val loss: 0.6864039364832252, acc: 0.773877292852625, test loss: 0.7016180052234435, acc: 0.7813383600377003
epoch: 149, train loss: 0.04366552448442082, acc: 0.9698240866035183, val loss: 0.7397262252334701, acc: 0.7852624920936117, test loss: 0.7374157134070173, acc: 0.7885642475651901
epoch: 150, train loss: 0.037544721120991144, acc: 0.9731393775372125, val loss: 0.704101293619338, acc: 0.788741302972802, test loss: 0.748622840993104, acc: 0.782909205152372
epoch: 151, train loss: 0.035010229528998814, acc: 0.9738836265223275, val loss: 0.7862306003100054, acc: 0.7767235926628716, test loss: 0.7973408760082186, acc: 0.7700282752120641
epoch: 152, train loss: 0.03024455463319654, acc: 0.9765223274695535, val loss: 0.7509684795672799, acc: 0.7836812144212524, test loss: 0.7613389490911356, acc: 0.7907634307257304
epoch: 153, train loss: 0.04138349051384384, acc: 0.9699594046008119, val loss: 0.7028283367874801, acc: 0.786527514231499, test loss: 0.7840473579978403, acc: 0.7637448947533774
epoch: 154, train loss: 0.030881544953169294, acc: 0.9752368064952639, val loss: 0.7326652218012173, acc: 0.7786211258697027, test loss: 0.761242571695024, acc: 0.7775683317624882
epoch: 155, train loss: 0.031204451416418744, acc: 0.9784844384303112, val loss: 0.7381573622170918, acc: 0.788741302972802, test loss: 0.7827662410430477, acc: 0.7813383600377003
epoch: 156, train loss: 0.030696687135588333, acc: 0.9771312584573748, val loss: 0.7272217842991182, acc: 0.7839974699557243, test loss: 0.745092551699982, acc: 0.783851712221175
epoch: 157, train loss: 0.0352311710553579, acc: 0.975575101488498, val loss: 0.7645668849245345, acc: 0.7722960151802657, test loss: 0.7749300626999845, acc: 0.7700282752120641
epoch: 158, train loss: 0.038068983422564553, acc: 0.9728010825439783, val loss: 0.7225449483353906, acc: 0.7783048703352309, test loss: 0.7641941193248953, acc: 0.7731699654414075
epoch: 159, train loss: 0.04201324301820969, acc: 0.970297699594046, val loss: 0.7262754643890844, acc: 0.7773561037318153, test loss: 0.7644014484358778, acc: 0.7741124725102105
epoch: 160, train loss: 0.042059316428148856, acc: 0.9715832205683356, val loss: 0.6970497577873535, acc: 0.7858950031625553, test loss: 0.7249474167037302, acc: 0.7832233741753063
epoch: 161, train loss: 0.04442520606481497, acc: 0.9683355886332883, val loss: 0.6990862587145202, acc: 0.7817836812144212, test loss: 0.7697689445221757, acc: 0.7731699654414075
epoch: 162, train loss: 0.058709182141598246, acc: 0.9611637347767253, val loss: 0.6905320590994616, acc: 0.7773561037318153, test loss: 0.7383873706863169, acc: 0.7788250078542256
epoch: 163, train loss: 0.04381812574576299, acc: 0.96914749661705, val loss: 0.7075564138200146, acc: 0.7811511701454775, test loss: 0.7501467148013509, acc: 0.7844800502670437
epoch: 164, train loss: 0.03422275978935749, acc: 0.9751014884979702, val loss: 0.7109248424013682, acc: 0.7884250474383302, test loss: 0.7217165968757435, acc: 0.783851712221175
epoch: 165, train loss: 0.026609713462192732, acc: 0.9801759133964817, val loss: 0.7661947897911675, acc: 0.7764073371283997, test loss: 0.8338991534286724, acc: 0.7637448947533774
epoch: 166, train loss: 0.04246043568131727, acc: 0.970297699594046, val loss: 0.7517444559624495, acc: 0.7697659709044908, test loss: 0.8007715668200398, acc: 0.7665724159597863
epoch: 167, train loss: 0.043650376591050095, acc: 0.971041948579161, val loss: 0.738445806322333, acc: 0.7802024035420619, test loss: 0.7684834956972996, acc: 0.7800816839459629
epoch: 168, train loss: 0.03368757557784269, acc: 0.9771312584573748, val loss: 0.8319542694514044, acc: 0.7669196710942442, test loss: 0.8422170712293936, acc: 0.7628023876845743
epoch: 169, train loss: 0.04688495990791244, acc: 0.9652232746955345, val loss: 0.7413674889298498, acc: 0.7751423149905123, test loss: 0.6939484021441201, acc: 0.7854225573358466
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.02499456714047308, acc: 0.9724627875507442, val loss: 0.6270612557427481, acc: 0.7786211258697027, test loss: 0.6172386977869574, acc: 0.7813383600377003
epoch: 171, train loss: 0.0189049196813926, acc: 0.9803112313937754, val loss: 0.637818959903898, acc: 0.7849462365591398, test loss: 0.6448985400381158, acc: 0.7785108388312912
epoch: 172, train loss: 0.013652954396258672, acc: 0.98531799729364, val loss: 0.6204583750428616, acc: 0.790955091714105, test loss: 0.6403719532089042, acc: 0.7907634307257304
epoch: 173, train loss: 0.01367181772630818, acc: 0.9874154262516914, val loss: 0.6184522035678378, acc: 0.7934851359898798, test loss: 0.6785020100182795, acc: 0.7832233741753063
Epoch   173: reducing learning rate of group 0 to 3.7500e-04.
epoch: 174, train loss: 0.00982748487364395, acc: 0.9903247631935047, val loss: 0.5996810988788738, acc: 0.7985452245414295, test loss: 0.6313004610188683, acc: 0.7863650644046497
epoch: 175, train loss: 0.008220796310122305, acc: 0.9935723951285521, val loss: 0.6194617197831773, acc: 0.7988614800759013, test loss: 0.6378214558152856, acc: 0.7885642475651901
epoch: 176, train loss: 0.007144666141395599, acc: 0.9928281461434371, val loss: 0.6073023037847124, acc: 0.801707779886148, test loss: 0.6328386726373402, acc: 0.7910775997486648
epoch: 177, train loss: 0.007238383716191587, acc: 0.9937077131258457, val loss: 0.6118739731774762, acc: 0.8020240354206198, test loss: 0.6332216589023136, acc: 0.7964184731385485
epoch: 178, train loss: 0.006412538972334163, acc: 0.9945196211096076, val loss: 0.6211059753235799, acc: 0.7979127134724858, test loss: 0.6386556708134835, acc: 0.7920201068174678
epoch: 179, train loss: 0.005746421116846582, acc: 0.9946549391069012, val loss: 0.6261380554979772, acc: 0.7979127134724858, test loss: 0.6415589807395324, acc: 0.7926484448633365
epoch: 180, train loss: 0.006041130894374138, acc: 0.9953991880920162, val loss: 0.63416438092166, acc: 0.7985452245414295, test loss: 0.6381023842847239, acc: 0.7935909519321395
epoch: 181, train loss: 0.0055615055610467356, acc: 0.9952638700947226, val loss: 0.6337812553848517, acc: 0.795382669196711, test loss: 0.6425039725149349, acc: 0.7948476280238769
epoch: 182, train loss: 0.0050941289176960275, acc: 0.995466847090663, val loss: 0.6408096227670003, acc: 0.7988614800759013, test loss: 0.6377854259591638, acc: 0.7942192899780082
epoch: 183, train loss: 0.005240887808027464, acc: 0.995872801082544, val loss: 0.6555514444210045, acc: 0.7950664136622391, test loss: 0.6512794840684148, acc: 0.7929626138862708
epoch: 184, train loss: 0.005491326733697735, acc: 0.9958051420838971, val loss: 0.6631410219185266, acc: 0.7947501581277673, test loss: 0.6537884576044254, acc: 0.7967326421614829
epoch: 185, train loss: 0.006925504725219755, acc: 0.9953315290933694, val loss: 0.6448020009885929, acc: 0.7925363693864642, test loss: 0.6466339747934944, acc: 0.7904492617027961
epoch: 186, train loss: 0.005988913576757593, acc: 0.9958051420838971, val loss: 0.6448214192091853, acc: 0.7975964579380139, test loss: 0.6479761073172111, acc: 0.7961043041156142
epoch: 187, train loss: 0.009531734647993024, acc: 0.9922192151556157, val loss: 0.6563360675236306, acc: 0.7814674256799494, test loss: 0.6644125683218512, acc: 0.7813383600377003
epoch: 188, train loss: 0.009904249123371987, acc: 0.9907983761840324, val loss: 0.6519192590659212, acc: 0.7890575585072739, test loss: 0.6681570740866654, acc: 0.7866792334275841
epoch: 189, train loss: 0.008768169517365457, acc: 0.991745602165088, val loss: 0.6576828703255988, acc: 0.7896900695762176, test loss: 0.6805046024915747, acc: 0.7835375431982406
epoch: 190, train loss: 0.005834321913098934, acc: 0.9944519621109608, val loss: 0.659424228354544, acc: 0.793168880455408, test loss: 0.6909559601476047, acc: 0.7869934024505184
epoch: 191, train loss: 0.009174686665766455, acc: 0.9916779431664411, val loss: 0.689854582452382, acc: 0.7808349146110057, test loss: 0.719604211822831, acc: 0.7687715991203268
epoch: 192, train loss: 0.01015757303206056, acc: 0.9905277401894452, val loss: 0.6820553052583727, acc: 0.7836812144212524, test loss: 0.6842157794662964, acc: 0.7832233741753063
epoch: 193, train loss: 0.01924626978066358, acc: 0.984979702300406, val loss: 0.6876434908872311, acc: 0.7659709044908286, test loss: 0.7116100922568654, acc: 0.7628023876845743
epoch: 194, train loss: 0.017897744209118722, acc: 0.9828146143437078, val loss: 0.7096640376031286, acc: 0.7748260594560404, test loss: 0.716733334613528, acc: 0.7741124725102105
epoch: 195, train loss: 0.022287263919842582, acc: 0.9796346414073072, val loss: 0.6655860432788286, acc: 0.7707147375079064, test loss: 0.6577589324612008, acc: 0.7697141061891297
epoch: 196, train loss: 0.017208549038523428, acc: 0.9838971583220568, val loss: 0.6399748078785992, acc: 0.7820999367488931, test loss: 0.6482515931316715, acc: 0.7835375431982406
epoch: 197, train loss: 0.014887768851595658, acc: 0.9855209742895805, val loss: 0.6495007127240667, acc: 0.7814674256799494, test loss: 0.6606730409586837, acc: 0.7822808671065034
epoch: 198, train loss: 0.015282011119817687, acc: 0.9861299052774019, val loss: 0.661052208245366, acc: 0.7871600253004427, test loss: 0.6680823705348435, acc: 0.782909205152372
epoch: 199, train loss: 0.01314788198448889, acc: 0.9870094722598105, val loss: 0.6788866603623305, acc: 0.7760910815939279, test loss: 0.6702346349339271, acc: 0.7825950361294376
epoch: 200, train loss: 0.014032590616096179, acc: 0.9875507442489851, val loss: 0.6518070385623476, acc: 0.7912713472485768, test loss: 0.6665360699499923, acc: 0.7860508953817154
best val acc 0.8020240354206198 at epoch 177.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9998    1.0000    0.9999      5337
           1     1.0000    0.9926    0.9963       810
           2     0.9990    0.9995    0.9993      2100
           3     1.0000    1.0000    1.0000       737
           4     0.9912    1.0000    0.9956       677
           5     0.9992    1.0000    0.9996      1323
           6     0.9983    0.9940    0.9961      1164
           7     1.0000    1.0000    1.0000       421
           8     1.0000    1.0000    1.0000       401
           9     0.9875    1.0000    0.9937       396
          10     1.0000    1.0000    1.0000       627
          11     1.0000    1.0000    1.0000       291
          12     1.0000    0.9885    0.9942       261
          13     0.9957    0.9957    0.9957       235

    accuracy                         0.9988     14780
   macro avg     0.9979    0.9979    0.9979     14780
weighted avg     0.9988    0.9988    0.9988     14780

train confusion matrix:
[[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 9.92592593e-01 0.00000000e+00 0.00000000e+00
  7.40740741e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 9.99523810e-01 0.00000000e+00
  0.00000000e+00 4.76190476e-04 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [8.59106529e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.93986254e-01 0.00000000e+00
  0.00000000e+00 4.29553265e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 8.59106529e-04]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 7.66283525e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.83141762e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.88505747e-01 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 4.25531915e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.95744681e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8629    0.8863    0.8744      1143
           1     0.8944    0.8324    0.8623       173
           2     0.8089    0.8089    0.8089       450
           3     0.8205    0.8101    0.8153       158
           4     0.8322    0.8552    0.8435       145
           5     0.8100    0.8587    0.8336       283
           6     0.6000    0.6145    0.6071       249
           7     0.7857    0.7333    0.7586        90
           8     0.6667    0.6353    0.6506        85
           9     0.8701    0.7976    0.8323        84
          10     0.8347    0.7537    0.7922       134
          11     0.6852    0.5968    0.6379        62
          12     0.1587    0.1786    0.1681        56
          13     0.8649    0.6400    0.7356        50

    accuracy                         0.8020      3162
   macro avg     0.7496    0.7144    0.7300      3162
weighted avg     0.8036    0.8020    0.8021      3162

validation confusion matrix:
[[8.86264217e-01 3.49956255e-03 2.71216098e-02 8.74890639e-04
  8.74890639e-04 4.37445319e-03 3.14960630e-02 1.31233596e-02
  1.13735783e-02 0.00000000e+00 1.74978128e-03 6.99912511e-03
  1.13735783e-02 8.74890639e-04]
 [2.31213873e-02 8.32369942e-01 0.00000000e+00 0.00000000e+00
  9.24855491e-02 1.73410405e-02 5.78034682e-03 0.00000000e+00
  2.31213873e-02 0.00000000e+00 5.78034682e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [7.77777778e-02 0.00000000e+00 8.08888889e-01 6.66666667e-03
  0.00000000e+00 1.55555556e-02 3.33333333e-02 2.22222222e-03
  0.00000000e+00 6.66666667e-03 2.22222222e-03 2.22222222e-03
  4.22222222e-02 2.22222222e-03]
 [4.43037975e-02 6.32911392e-03 5.06329114e-02 8.10126582e-01
  0.00000000e+00 2.53164557e-02 1.89873418e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.53164557e-02 1.26582278e-02
  6.32911392e-03 0.00000000e+00]
 [1.37931034e-02 3.44827586e-02 0.00000000e+00 6.89655172e-03
  8.55172414e-01 8.96551724e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.24028269e-02 3.53356890e-03 1.76678445e-02 1.41342756e-02
  2.82685512e-02 8.58657244e-01 1.06007067e-02 0.00000000e+00
  1.06007067e-02 0.00000000e+00 3.53356890e-03 7.06713781e-03
  3.53356890e-03 0.00000000e+00]
 [1.04417671e-01 4.01606426e-03 6.02409639e-02 3.21285141e-02
  0.00000000e+00 5.22088353e-02 6.14457831e-01 4.01606426e-03
  8.03212851e-03 2.00803213e-02 2.00803213e-02 4.01606426e-03
  6.42570281e-02 1.20481928e-02]
 [2.11111111e-01 0.00000000e+00 1.11111111e-02 0.00000000e+00
  0.00000000e+00 1.11111111e-02 1.11111111e-02 7.33333333e-01
  0.00000000e+00 0.00000000e+00 1.11111111e-02 1.11111111e-02
  0.00000000e+00 0.00000000e+00]
 [1.64705882e-01 2.35294118e-02 1.17647059e-02 1.17647059e-02
  0.00000000e+00 7.05882353e-02 2.35294118e-02 0.00000000e+00
  6.35294118e-01 0.00000000e+00 2.35294118e-02 1.17647059e-02
  2.35294118e-02 0.00000000e+00]
 [1.07142857e-01 0.00000000e+00 1.19047619e-02 1.19047619e-02
  0.00000000e+00 0.00000000e+00 4.76190476e-02 0.00000000e+00
  0.00000000e+00 7.97619048e-01 1.19047619e-02 0.00000000e+00
  1.19047619e-02 0.00000000e+00]
 [4.47761194e-02 1.49253731e-02 4.47761194e-02 2.23880597e-02
  0.00000000e+00 0.00000000e+00 8.95522388e-02 0.00000000e+00
  7.46268657e-03 1.49253731e-02 7.53731343e-01 7.46268657e-03
  0.00000000e+00 0.00000000e+00]
 [2.09677419e-01 1.61290323e-02 1.61290323e-02 1.61290323e-02
  0.00000000e+00 4.83870968e-02 1.61290323e-02 0.00000000e+00
  6.45161290e-02 0.00000000e+00 1.61290323e-02 5.96774194e-01
  0.00000000e+00 0.00000000e+00]
 [1.96428571e-01 0.00000000e+00 2.85714286e-01 8.92857143e-02
  0.00000000e+00 1.78571429e-02 1.96428571e-01 1.78571429e-02
  0.00000000e+00 0.00000000e+00 1.78571429e-02 0.00000000e+00
  1.78571429e-01 0.00000000e+00]
 [6.00000000e-02 0.00000000e+00 2.00000000e-02 0.00000000e+00
  0.00000000e+00 2.00000000e-02 2.60000000e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 6.40000000e-01]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8642    0.8952    0.8795      1145
           1     0.9026    0.7943    0.8450       175
           2     0.8159    0.7960    0.8058       451
           3     0.8301    0.7987    0.8141       159
           4     0.7935    0.8425    0.8173       146
           5     0.8741    0.8556    0.8648       284
           6     0.5461    0.6160    0.5789       250
           7     0.7660    0.7912    0.7784        91
           8     0.7342    0.6667    0.6988        87
           9     0.8514    0.7326    0.7875        86
          10     0.7768    0.6397    0.7016       136
          11     0.6393    0.6094    0.6240        64
          12     0.1739    0.2105    0.1905        57
          13     0.7391    0.6538    0.6939        52

    accuracy                         0.7964      3183
   macro avg     0.7362    0.7073    0.7200      3183
weighted avg     0.8011    0.7964    0.7977      3183

test confusion matrix:
[[8.95196507e-01 2.62008734e-03 2.44541485e-02 3.49344978e-03
  8.73362445e-04 6.11353712e-03 2.79475983e-02 1.22270742e-02
  6.11353712e-03 2.62008734e-03 2.62008734e-03 6.11353712e-03
  7.86026201e-03 1.74672489e-03]
 [3.42857143e-02 7.94285714e-01 5.71428571e-03 5.71428571e-03
  1.08571429e-01 3.42857143e-02 5.71428571e-03 0.00000000e+00
  1.14285714e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.87361419e-02 0.00000000e+00 7.96008869e-01 6.65188470e-03
  4.43458980e-03 2.21729490e-03 5.32150776e-02 0.00000000e+00
  2.21729490e-03 0.00000000e+00 6.65188470e-03 4.43458980e-03
  4.65631929e-02 8.86917960e-03]
 [6.28930818e-03 6.28930818e-03 1.25786164e-02 7.98742138e-01
  0.00000000e+00 1.25786164e-02 7.54716981e-02 0.00000000e+00
  6.28930818e-03 0.00000000e+00 2.51572327e-02 1.88679245e-02
  3.77358491e-02 0.00000000e+00]
 [2.73972603e-02 4.79452055e-02 6.84931507e-03 0.00000000e+00
  8.42465753e-01 5.47945205e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 6.84931507e-03 1.36986301e-02
  0.00000000e+00 0.00000000e+00]
 [4.92957746e-02 7.04225352e-03 1.76056338e-02 3.52112676e-03
  1.76056338e-02 8.55633803e-01 1.76056338e-02 0.00000000e+00
  1.40845070e-02 0.00000000e+00 7.04225352e-03 7.04225352e-03
  3.52112676e-03 0.00000000e+00]
 [1.04000000e-01 4.00000000e-03 8.00000000e-02 3.60000000e-02
  4.00000000e-03 2.00000000e-02 6.16000000e-01 2.00000000e-02
  4.00000000e-03 1.60000000e-02 2.00000000e-02 0.00000000e+00
  6.40000000e-02 1.20000000e-02]
 [1.53846154e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 7.91208791e-01
  0.00000000e+00 0.00000000e+00 4.39560440e-02 1.09890110e-02
  0.00000000e+00 0.00000000e+00]
 [2.18390805e-01 0.00000000e+00 1.14942529e-02 0.00000000e+00
  1.14942529e-02 0.00000000e+00 4.59770115e-02 1.14942529e-02
  6.66666667e-01 0.00000000e+00 0.00000000e+00 3.44827586e-02
  0.00000000e+00 0.00000000e+00]
 [8.13953488e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.39534884e-01 0.00000000e+00
  1.16279070e-02 7.32558140e-01 0.00000000e+00 0.00000000e+00
  1.16279070e-02 2.32558140e-02]
 [6.61764706e-02 7.35294118e-03 5.88235294e-02 2.94117647e-02
  2.20588235e-02 2.20588235e-02 1.10294118e-01 7.35294118e-03
  7.35294118e-03 0.00000000e+00 6.39705882e-01 1.47058824e-02
  7.35294118e-03 7.35294118e-03]
 [2.18750000e-01 0.00000000e+00 1.56250000e-02 1.56250000e-02
  0.00000000e+00 1.56250000e-02 1.56250000e-02 0.00000000e+00
  4.68750000e-02 3.12500000e-02 0.00000000e+00 6.09375000e-01
  3.12500000e-02 0.00000000e+00]
 [2.45614035e-01 0.00000000e+00 2.45614035e-01 5.26315789e-02
  0.00000000e+00 3.50877193e-02 1.40350877e-01 1.75438596e-02
  0.00000000e+00 1.75438596e-02 3.50877193e-02 0.00000000e+00
  2.10526316e-01 0.00000000e+00]
 [3.84615385e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.69230769e-01 0.00000000e+00
  0.00000000e+00 1.92307692e-02 1.92307692e-02 0.00000000e+00
  0.00000000e+00 6.53846154e-01]]
---------------------------------------
program finished.
