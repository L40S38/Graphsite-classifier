seed:  666
save trained model at:  ../trained_models/trained_classifier_model_3.pt
save loss at:  ./results/train_classifier_results_3.json
how to merge clusters:  [[0, 9, 12], [1, 5, 11], 2, [3, 8, 13], 4, 6, 7, [10, 16], 15, 17, 18]
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  120
learning rate decay at epoch:  60
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  11
number of pockets in training set:  14097
number of pockets in validation set:  3016
number of pockets in test set:  3031
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=11, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [20, 60, 100]
loss function:
FocalLoss(gamma=0, alpha=1, reduction=mean)
begin training...
epoch: 1, train loss: 1.9094857202909292, acc: 0.3858977087323544, val loss: 1.6724377190718915, acc: 0.4360079575596817, test loss: 1.6967955698317247, acc: 0.42758165621906963
epoch: 2, train loss: 1.6748508335791927, acc: 0.44264737178123004, val loss: 1.567948054255478, acc: 0.473474801061008, test loss: 1.5843655844404925, acc: 0.4737710326624876
epoch: 3, train loss: 1.5749884458808279, acc: 0.46662410441938, val loss: 1.5202791728771017, acc: 0.4598806366047745, test loss: 1.5480594431280263, acc: 0.4556252062025734
epoch: 4, train loss: 1.5347124876228133, acc: 0.4758459246648223, val loss: 1.4523095755741513, acc: 0.4870689655172414, test loss: 1.4849282188881194, acc: 0.48465852853843616
epoch: 5, train loss: 1.4890601825078194, acc: 0.49265801234305173, val loss: 1.3409790259141188, acc: 0.5321618037135278, test loss: 1.369720827506659, acc: 0.5278785879247773
epoch: 6, train loss: 1.4132399523504289, acc: 0.5190466056607789, val loss: 1.4337417839060411, acc: 0.5122679045092838, test loss: 1.4796510848696969, acc: 0.5133619267568459
epoch: 7, train loss: 1.3655929156395614, acc: 0.5345108888415975, val loss: 1.4402688776466512, acc: 0.5285145888594165, test loss: 1.4723504764024513, acc: 0.5206202573408116
epoch: 8, train loss: 1.3312666238584678, acc: 0.5512520394410159, val loss: 1.2702465196503252, acc: 0.5583554376657824, test loss: 1.27986787089968, acc: 0.5618607720224348
epoch: 9, train loss: 1.311978661608948, acc: 0.5578491877704476, val loss: 1.2685839902184686, acc: 0.5600132625994695, test loss: 1.2874671826855022, acc: 0.5704387990762124
epoch: 10, train loss: 1.2587543926256264, acc: 0.5693409945378449, val loss: 1.2285395560277235, acc: 0.5653183023872679, test loss: 1.2446143079692875, acc: 0.5714285714285714
epoch: 11, train loss: 1.242836437792256, acc: 0.5779953181527985, val loss: 1.2342370429152836, acc: 0.5623342175066313, test loss: 1.2571557446778392, acc: 0.5770372814252722
epoch: 12, train loss: 1.2048890906597287, acc: 0.5902674327871178, val loss: 1.362172562500526, acc: 0.5573607427055703, test loss: 1.3453352437165655, acc: 0.5635103926096998
epoch: 13, train loss: 1.2119836489714704, acc: 0.5878555721075406, val loss: 1.1492538395231535, acc: 0.5925066312997348, test loss: 1.174342680276325, acc: 0.5988122731771692
epoch: 14, train loss: 1.153843519780165, acc: 0.6120451159821239, val loss: 1.4754426681710808, acc: 0.5129310344827587, test loss: 1.4948612149892093, acc: 0.510722533817222
epoch: 15, train loss: 1.135584288195802, acc: 0.6216925587004327, val loss: 1.2844783570469216, acc: 0.5689655172413793, test loss: 1.2960734099139872, acc: 0.5720884196634775
epoch: 16, train loss: 1.1524909863347168, acc: 0.6127544867702348, val loss: 1.0844638986359856, acc: 0.610079575596817, test loss: 1.128665644278506, acc: 0.6143187066974596
epoch: 17, train loss: 1.0920720278747438, acc: 0.6382918351422289, val loss: 1.0868933093326478, acc: 0.6190318302387268, test loss: 1.155290254859081, acc: 0.605080831408776
epoch: 18, train loss: 1.0686733746956518, acc: 0.6399943250336951, val loss: 1.5118920252873347, acc: 0.5321618037135278, test loss: 1.5354803084383228, acc: 0.5258990432200594
epoch: 19, train loss: 1.1011698677826158, acc: 0.6307015677094417, val loss: 1.0233295289527833, acc: 0.639920424403183, test loss: 1.0627928154179853, acc: 0.6370834708017156
epoch 20, gamma increased to 1.
epoch: 20, train loss: 0.8516926762778142, acc: 0.6329715542313967, val loss: 0.9167674542738209, acc: 0.6074270557029178, test loss: 0.933328842652666, acc: 0.6034312108215111
epoch: 21, train loss: 0.8453293198633339, acc: 0.6422643115556501, val loss: 1.5731567994985403, acc: 0.3895888594164456, test loss: 1.5808916166645983, acc: 0.38436159683272847
epoch: 22, train loss: 0.935098838356274, acc: 0.5843087181669859, val loss: 0.9547350375026228, acc: 0.5822281167108754, test loss: 1.002174777191726, acc: 0.5780270537776312
epoch: 23, train loss: 0.8861863213650274, acc: 0.6194225721784777, val loss: 0.8150606882667035, acc: 0.6256631299734748, test loss: 0.8461991315311191, acc: 0.6232266578686902
epoch: 24, train loss: 0.8231206652863935, acc: 0.6395687025608285, val loss: 0.8202847204410745, acc: 0.6256631299734748, test loss: 0.8554264039577961, acc: 0.6380732431540745
epoch: 25, train loss: 0.7771462218392916, acc: 0.6563807902390579, val loss: 0.8423427204238325, acc: 0.628315649867374, test loss: 0.8936682696469898, acc: 0.6219069613988782
epoch: 26, train loss: 0.7822556375163392, acc: 0.6581542172093353, val loss: 0.9089320034816347, acc: 0.6170424403183024, test loss: 0.9479379517062195, acc: 0.6080501484658528
epoch: 27, train loss: 0.8373380056076661, acc: 0.6329006171525856, val loss: 0.8617168184300633, acc: 0.6243368700265252, test loss: 0.8962821872743275, acc: 0.6238865061035962
epoch: 28, train loss: 0.8039421640559021, acc: 0.6455274171809605, val loss: 0.7307090221728824, acc: 0.6624668435013262, test loss: 0.7876260929161155, acc: 0.6535796766743649
epoch: 29, train loss: 0.7595194558805715, acc: 0.6570192239483578, val loss: 0.779697219003733, acc: 0.653183023872679, test loss: 0.8144890628054526, acc: 0.6450016496205873
epoch: 30, train loss: 0.7367546857946636, acc: 0.6780165992764418, val loss: 0.8324084661367401, acc: 0.618368700265252, test loss: 0.8983611115839804, acc: 0.603761134938964
epoch: 31, train loss: 0.7202074248426948, acc: 0.6824856352415407, val loss: 0.7069822976696712, acc: 0.6807029177718833, test loss: 0.7636576189297885, acc: 0.6605080831408776
epoch: 32, train loss: 0.7113296713648581, acc: 0.6825565723203518, val loss: 0.84689828729756, acc: 0.6226790450928382, test loss: 0.8958313945731249, acc: 0.6172880237545365
epoch: 33, train loss: 0.700969881924144, acc: 0.6890118464921614, val loss: 0.7739642052182468, acc: 0.6508620689655172, test loss: 0.8117644700701974, acc: 0.6469811943253052
epoch: 34, train loss: 0.7048263923406618, acc: 0.6867418599702064, val loss: 0.7347819068387585, acc: 0.6578249336870027, test loss: 0.7837722632248619, acc: 0.6542395249092708
epoch: 35, train loss: 0.6910285335648539, acc: 0.6942611903241824, val loss: 0.7079805875646656, acc: 0.6929708222811671, test loss: 0.7470941903448781, acc: 0.6799736060706038
epoch: 36, train loss: 0.69357439854039, acc: 0.6914946442505497, val loss: 0.6749945376216575, acc: 0.7002652519893899, test loss: 0.7346598859354042, acc: 0.6895414054767404
epoch: 37, train loss: 0.6494951762167572, acc: 0.7157551252039441, val loss: 0.828963668972491, acc: 0.6422413793103449, test loss: 0.8911709314681091, acc: 0.6469811943253052
epoch: 38, train loss: 0.6782711842486797, acc: 0.6994395970773923, val loss: 0.7122810139896383, acc: 0.6684350132625995, test loss: 0.7974273043865027, acc: 0.6483008907951171
epoch: 39, train loss: 0.6482534899080427, acc: 0.7103639072143009, val loss: 0.7272406642569788, acc: 0.6737400530503979, test loss: 0.773818282198489, acc: 0.6651270207852193
epoch: 40, train loss: 0.6409104790401398, acc: 0.7154004398098887, val loss: 0.7235199870734379, acc: 0.6876657824933687, test loss: 0.772436947839876, acc: 0.6717255031342791
epoch: 41, train loss: 0.6293294143277618, acc: 0.7202241611690431, val loss: 0.9317937940754372, acc: 0.6352785145888594, test loss: 0.9754377480821222, acc: 0.6182777961068954
epoch: 42, train loss: 0.6514834151493283, acc: 0.7119954600269561, val loss: 0.966595798651799, acc: 0.5951591511936339, test loss: 0.9991641961399457, acc: 0.5882547014186738
epoch: 43, train loss: 0.6689462355580695, acc: 0.7071717386678017, val loss: 0.7619200752015456, acc: 0.6644562334217506, test loss: 0.8317268319902386, acc: 0.6420323325635104
epoch: 44, train loss: 0.6222941092229815, acc: 0.7221394622969426, val loss: 0.6781988087003996, acc: 0.6992705570291777, test loss: 0.7298542333884194, acc: 0.6895414054767404
epoch: 45, train loss: 0.6180259681571635, acc: 0.725473505001064, val loss: 0.7805017659771664, acc: 0.6624668435013262, test loss: 0.8132647432851461, acc: 0.6558891454965358
epoch: 46, train loss: 0.6447077802396746, acc: 0.710434844293112, val loss: 0.7239329258390067, acc: 0.6830238726790451, test loss: 0.7904566295545749, acc: 0.6618277796106895
epoch: 47, train loss: 0.6222628921813691, acc: 0.7189472937504433, val loss: 0.6742622806475713, acc: 0.7015915119363395, test loss: 0.7343326319329851, acc: 0.6905311778290993
epoch: 48, train loss: 0.6749412831054593, acc: 0.7027027027027027, val loss: 0.7820778367373924, acc: 0.6647877984084881, test loss: 0.826762766855379, acc: 0.6417024084460574
epoch: 49, train loss: 0.5887757275995316, acc: 0.7383840533446833, val loss: 0.6929845645509917, acc: 0.7088859416445623, test loss: 0.7422539048445024, acc: 0.6892114813592873
epoch: 50, train loss: 0.5900693212983252, acc: 0.732212527488118, val loss: 0.6914890764246568, acc: 0.7098806366047745, test loss: 0.755909147926464, acc: 0.6902012537116463
epoch: 51, train loss: 0.5884441942147789, acc: 0.740441228630205, val loss: 0.6354880547966185, acc: 0.7211538461538461, test loss: 0.6826550623482504, acc: 0.697789508413065
epoch: 52, train loss: 0.5882361082394106, acc: 0.7377456196353834, val loss: 0.6365846474543807, acc: 0.7304376657824934, test loss: 0.6857212754375114, acc: 0.7060376113493897
epoch: 53, train loss: 0.5730674918546755, acc: 0.7474639994325034, val loss: 0.9936130400994095, acc: 0.6130636604774535, test loss: 1.0383240815555643, acc: 0.5928736390630155
epoch: 54, train loss: 0.6193288190149877, acc: 0.7243385117400866, val loss: 0.7112088396315233, acc: 0.7005968169761273, test loss: 0.7419240856202118, acc: 0.6819531507753217
epoch: 55, train loss: 0.5992877744246587, acc: 0.7302262892814074, val loss: 0.631197111676163, acc: 0.7248010610079576, test loss: 0.6965071653934329, acc: 0.7050478389970307
epoch: 56, train loss: 0.5570979445608252, acc: 0.7489536780875363, val loss: 0.8064272950751712, acc: 0.6634615384615384, test loss: 0.8906686273355839, acc: 0.6357637743319037
epoch: 57, train loss: 0.5741479709197961, acc: 0.7400865432361495, val loss: 1.0808506707613916, acc: 0.596816976127321, test loss: 1.1176000203051137, acc: 0.5836357637743319
epoch: 58, train loss: 0.8181360705428034, acc: 0.6442505497623607, val loss: 0.7361558160351822, acc: 0.6833554376657824, test loss: 0.7722160595157916, acc: 0.6634774001979544
epoch: 59, train loss: 0.6350993813733188, acc: 0.7166063701496772, val loss: 0.8497567998951879, acc: 0.6558355437665783, test loss: 0.9105094564665036, acc: 0.6519300560871
epoch 60, gamma increased to 2.
epoch: 60, train loss: 0.4708805633574722, acc: 0.7341278286160176, val loss: 0.4851485792774737, acc: 0.7317639257294429, test loss: 0.5385601435585488, acc: 0.7172550313427911
epoch: 61, train loss: 0.41751948350785073, acc: 0.7556927005745904, val loss: 0.5011830269816068, acc: 0.7387267904509284, test loss: 0.5574649706881104, acc: 0.720224348399868
epoch: 62, train loss: 0.39535325822055933, acc: 0.7639214017166773, val loss: 0.48990529522655496, acc: 0.7344164456233422, test loss: 0.5494819401278821, acc: 0.7099967007588255
epoch: 63, train loss: 0.38464319642540445, acc: 0.7712279208342201, val loss: 0.4897588847170458, acc: 0.741710875331565, test loss: 0.5290686471436292, acc: 0.7278126031012867
epoch: 64, train loss: 0.40161713372395125, acc: 0.7610129814854224, val loss: 0.5045324187696139, acc: 0.7274535809018567, test loss: 0.5532861629440147, acc: 0.7099967007588255
epoch: 65, train loss: 0.37857448936751814, acc: 0.7715826062282756, val loss: 0.4660665966787768, acc: 0.7506631299734748, test loss: 0.5239885093472807, acc: 0.7264929066314748
epoch: 66, train loss: 0.37240356300759503, acc: 0.7778960062424629, val loss: 0.4980527763341403, acc: 0.7490053050397878, test loss: 0.5451152891815397, acc: 0.7235235895743979
epoch: 67, train loss: 0.3685667758513285, acc: 0.7783216287153295, val loss: 0.49844708398419285, acc: 0.7390583554376657, test loss: 0.5302018604432738, acc: 0.7215440448696799
epoch: 68, train loss: 0.3602218547192602, acc: 0.7807334893949067, val loss: 0.5041106455522127, acc: 0.73342175066313, test loss: 0.5582692537398984, acc: 0.7159353348729792
epoch: 69, train loss: 0.3624636173108761, acc: 0.7769738242179187, val loss: 0.49854920429007127, acc: 0.7274535809018567, test loss: 0.5593678562289534, acc: 0.7205542725173211
epoch: 70, train loss: 0.3548276805694605, acc: 0.7860537703057389, val loss: 0.48474211812967966, acc: 0.7503315649867374, test loss: 0.5302325419882508, acc: 0.7324315407456286
epoch: 71, train loss: 0.33895721032685944, acc: 0.7923671703199262, val loss: 0.5107577805177602, acc: 0.7397214854111406, test loss: 0.5427354345696077, acc: 0.7251732101616628
epoch: 72, train loss: 0.3417250777720864, acc: 0.7875434489607718, val loss: 0.534729645644322, acc: 0.7347480106100795, test loss: 0.5631511008845662, acc: 0.7192345760475091
epoch: 73, train loss: 0.38917478318627763, acc: 0.7740654039866638, val loss: 0.47145137483308425, acc: 0.7486737400530504, test loss: 0.5277785220896596, acc: 0.7294622236885516
epoch: 74, train loss: 0.34613870579934064, acc: 0.7886075051429382, val loss: 0.533850203458447, acc: 0.7281167108753316, test loss: 0.5996136663377737, acc: 0.7106565489937314
epoch: 75, train loss: 0.40799478167649356, acc: 0.7521458466340356, val loss: 0.5389981775764445, acc: 0.7287798408488063, test loss: 0.5815526451540638, acc: 0.7047179148795777
epoch: 76, train loss: 0.38136008716503966, acc: 0.7705185500461091, val loss: 0.5111360262180197, acc: 0.7364058355437666, test loss: 0.5582878140874603, acc: 0.7070273837017486
epoch: 77, train loss: 0.3328984942412202, acc: 0.7939277860537703, val loss: 0.49991360994485706, acc: 0.7523209549071618, test loss: 0.5574999405817795, acc: 0.7330913889805345
epoch: 78, train loss: 0.3774821226999524, acc: 0.772008228701142, val loss: 0.5028395143680927, acc: 0.745026525198939, test loss: 0.5633700378103957, acc: 0.7139557901682613
epoch: 79, train loss: 0.32545040869295083, acc: 0.7979002624671916, val loss: 0.4992166205805872, acc: 0.7407161803713528, test loss: 0.5520536146790075, acc: 0.7142857142857143
epoch: 80, train loss: 0.3433862708262148, acc: 0.7864084556997942, val loss: 0.4850648679530905, acc: 0.75, test loss: 0.5435569834606909, acc: 0.7231936654569449
epoch: 81, train loss: 0.3455243594297037, acc: 0.7845640916507058, val loss: 0.4765515700575528, acc: 0.7543103448275862, test loss: 0.5324533454107789, acc: 0.7334213130979874
epoch: 82, train loss: 0.3264748892270733, acc: 0.7945662197630702, val loss: 0.49512107365959834, acc: 0.751657824933687, test loss: 0.5457474936041963, acc: 0.720884196634774
epoch: 83, train loss: 0.3087523495308717, acc: 0.8030786692204015, val loss: 0.536776405746804, acc: 0.7294429708222812, test loss: 0.591864964467234, acc: 0.7033982184097658
epoch: 84, train loss: 0.3126183657603644, acc: 0.8015889905653685, val loss: 0.5288004331310485, acc: 0.7370689655172413, test loss: 0.5942472959658133, acc: 0.7165951831078852
epoch: 85, train loss: 0.2924500622820869, acc: 0.8108817478896219, val loss: 0.5008407508662904, acc: 0.7463527851458885, test loss: 0.5417186477639737, acc: 0.7304519960409106
epoch: 86, train loss: 0.30063730371313807, acc: 0.8077605164219337, val loss: 0.48823374082934634, acc: 0.7536472148541115, test loss: 0.5255996102902882, acc: 0.7347410095677994
epoch: 87, train loss: 0.3131008082931411, acc: 0.7996027523586579, val loss: 0.5804907663747549, acc: 0.7175066312997348, test loss: 0.6225690887690928, acc: 0.7080171560541075
epoch: 88, train loss: 0.3340824489991969, acc: 0.7906646804284599, val loss: 0.5560115401877649, acc: 0.726790450928382, test loss: 0.5852080002274698, acc: 0.721873968987133
epoch: 89, train loss: 0.31307925039520074, acc: 0.8030786692204015, val loss: 0.5317044659698041, acc: 0.7407161803713528, test loss: 0.5778904490563705, acc: 0.7251732101616628
epoch: 90, train loss: 0.3007239987810106, acc: 0.8062708377669008, val loss: 0.534932593768092, acc: 0.743368700265252, test loss: 0.603977784666831, acc: 0.7241834378093038
epoch: 91, train loss: 0.2865696633725485, acc: 0.8133645456480102, val loss: 0.5394396018286283, acc: 0.7314323607427056, test loss: 0.5941945203693343, acc: 0.7248432860442098
epoch: 92, train loss: 0.3595515053018726, acc: 0.7777541320848408, val loss: 0.515671053046573, acc: 0.7317639257294429, test loss: 0.5676745034727236, acc: 0.7106565489937314
epoch: 93, train loss: 0.30760809674731143, acc: 0.8040717883237568, val loss: 0.5100849879515266, acc: 0.7572944297082228, test loss: 0.5759695956353817, acc: 0.719894424282415
epoch: 94, train loss: 0.29675705135270297, acc: 0.8082570759736114, val loss: 0.5669443325907861, acc: 0.7460212201591512, test loss: 0.6083305537287688, acc: 0.7205542725173211
epoch: 95, train loss: 0.2918240993331322, acc: 0.810243314180322, val loss: 0.5561919028942401, acc: 0.7400530503978779, test loss: 0.6065125049674759, acc: 0.720884196634774
epoch: 96, train loss: 0.27890566830061053, acc: 0.8155635950911542, val loss: 0.5229602802337323, acc: 0.7490053050397878, test loss: 0.5870154277203068, acc: 0.7251732101616628
epoch: 97, train loss: 0.29182635464401563, acc: 0.8084698872100446, val loss: 0.5672308889561054, acc: 0.7397214854111406, test loss: 0.6195543900609056, acc: 0.7225338172220389
epoch: 98, train loss: 0.28487712322540926, acc: 0.8065545860821451, val loss: 0.5437015979612537, acc: 0.7380636604774535, test loss: 0.6071552153036407, acc: 0.7165951831078852
epoch: 99, train loss: 0.27252655086335015, acc: 0.8163439029580762, val loss: 0.5691816259758542, acc: 0.7496684350132626, test loss: 0.6218107131714476, acc: 0.7317716925107225
epoch 100, gamma increased to 3.
epoch: 100, train loss: 0.21465501116718597, acc: 0.8128679860963326, val loss: 0.46217731850849225, acc: 0.741710875331565, test loss: 0.49584717968437625, acc: 0.7278126031012867
epoch: 101, train loss: 0.2844683631727135, acc: 0.7723629140951975, val loss: 0.43037559309435774, acc: 0.7377320954907162, test loss: 0.4771110965489004, acc: 0.7083470801715606
epoch: 102, train loss: 0.21954113844566212, acc: 0.8048520961906789, val loss: 0.494786968914204, acc: 0.7211538461538461, test loss: 0.5401751204159975, acc: 0.7040580666446717
epoch: 103, train loss: 0.22934208928189936, acc: 0.801943675959424, val loss: 0.4335994020064883, acc: 0.7546419098143236, test loss: 0.47508084754352325, acc: 0.7294622236885516
epoch: 104, train loss: 0.21172272012979076, acc: 0.8104561254167554, val loss: 0.4508139998590282, acc: 0.7311007957559682, test loss: 0.491357812272641, acc: 0.7109864731111845
epoch: 105, train loss: 0.2194934164933586, acc: 0.8087536355252891, val loss: 0.47208954642875123, acc: 0.7347480106100795, test loss: 0.5284039575406247, acc: 0.7047179148795777
epoch: 106, train loss: 0.1948076800255012, acc: 0.8219479321841526, val loss: 0.4502498981807213, acc: 0.7437002652519894, test loss: 0.5049565165241887, acc: 0.7278126031012867
epoch: 107, train loss: 0.18568532132608564, acc: 0.8240051074696744, val loss: 0.6206305438074572, acc: 0.6860079575596817, test loss: 0.6552670141998512, acc: 0.6644671725503134
epoch: 108, train loss: 0.33456099630480346, acc: 0.7477477477477478, val loss: 0.41533283687712974, acc: 0.7390583554376657, test loss: 0.45079523346597944, acc: 0.7169251072253382
epoch: 109, train loss: 0.22831494974195343, acc: 0.8020855501170462, val loss: 0.4114325252072564, acc: 0.7413793103448276, test loss: 0.45612366173535596, acc: 0.7119762454635434
epoch: 110, train loss: 0.21817986768421116, acc: 0.8037171029297013, val loss: 0.4458377345487357, acc: 0.7413793103448276, test loss: 0.48783690636010896, acc: 0.7172550313427911
epoch: 111, train loss: 0.2505671907470794, acc: 0.7888203163793714, val loss: 0.4637787857485703, acc: 0.735079575596817, test loss: 0.5225151032356884, acc: 0.7073573078192016
epoch: 112, train loss: 0.19949949892618188, acc: 0.8105979995743775, val loss: 0.40947507610371636, acc: 0.7612732095490716, test loss: 0.4702640335695927, acc: 0.7344110854503464
epoch: 113, train loss: 0.19045686189473973, acc: 0.8248563524154076, val loss: 0.5240754328608829, acc: 0.7175066312997348, test loss: 0.5848746313115859, acc: 0.6938304190036292
epoch: 114, train loss: 0.24147455156855763, acc: 0.7935731006597149, val loss: 0.4288639747179471, acc: 0.7506631299734748, test loss: 0.49627834045230185, acc: 0.720884196634774
epoch: 115, train loss: 0.2837622795890069, acc: 0.7712988579130311, val loss: 0.4733679171266227, acc: 0.6992705570291777, test loss: 0.5406122650890923, acc: 0.6888815572418344
epoch: 116, train loss: 0.2504930289542412, acc: 0.7926509186351706, val loss: 0.4294574077312763, acc: 0.7387267904509284, test loss: 0.49473893960998694, acc: 0.7192345760475091
epoch: 117, train loss: 0.2825467640426621, acc: 0.7654820174505214, val loss: 0.5380169633845119, acc: 0.6946286472148541, test loss: 0.5896033916643294, acc: 0.675024744308809
epoch: 118, train loss: 0.22621584607602316, acc: 0.7998155635950912, val loss: 0.4379919356313245, acc: 0.7496684350132626, test loss: 0.4873093749304016, acc: 0.7205542725173211
epoch: 119, train loss: 0.2742897091059344, acc: 0.7744200893807193, val loss: 0.4277626473644368, acc: 0.7407161803713528, test loss: 0.4580007156339348, acc: 0.7225338172220389
epoch: 120, train loss: 0.22198468431932938, acc: 0.8010214939348798, val loss: 0.43064224024350195, acc: 0.736737400530504, test loss: 0.49228010164633396, acc: 0.722203893104586
best val loss 0.40947507610371636 at epoch 112.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9086    0.9438    0.9259      5074
           1     0.7350    0.6909    0.7123      2200
           2     0.9396    0.8062    0.8678       810
           3     0.7656    0.8554    0.8080      1840
           4     0.9393    0.7775    0.8508       737
           5     0.7766    0.9705    0.8628       677
           6     0.7558    0.7713    0.7635       634
           7     0.7658    0.6020    0.6741       907
           8     0.8575    0.8860    0.8715       421
           9     0.8343    0.7282    0.7776       401
          10     0.9064    0.9293    0.9177       396

    accuracy                         0.8395     14097
   macro avg     0.8349    0.8146    0.8211     14097
weighted avg     0.8401    0.8395    0.8373     14097

validation report:
              precision    recall  f1-score   support

           0     0.8349    0.8887    0.8610      1087
           1     0.6397    0.6221    0.6308       471
           2     0.8298    0.6763    0.7452       173
           3     0.7213    0.7817    0.7503       394
           4     0.9154    0.7532    0.8264       158
           5     0.7389    0.9172    0.8185       145
           6     0.6934    0.7037    0.6985       135
           7     0.5488    0.4639    0.5028       194
           8     0.7528    0.7444    0.7486        90
           9     0.7206    0.5765    0.6405        85
          10     0.9077    0.7024    0.7919        84

    accuracy                         0.7613      3016
   macro avg     0.7548    0.7118    0.7286      3016
weighted avg     0.7605    0.7613    0.7583      3016

test report: 
              precision    recall  f1-score   support

           0     0.8339    0.8860    0.8592      1088
           1     0.6059    0.5636    0.5840       472
           2     0.8146    0.7029    0.7546       175
           3     0.6512    0.7468    0.6958       395
           4     0.8718    0.6415    0.7391       159
           5     0.7247    0.8836    0.7963       146
           6     0.6842    0.6642    0.6741       137
           7     0.5172    0.4615    0.4878       195
           8     0.7174    0.7253    0.7213        91
           9     0.6154    0.4598    0.5263        87
          10     0.8219    0.6977    0.7547        86

    accuracy                         0.7344      3031
   macro avg     0.7144    0.6757    0.6903      3031
weighted avg     0.7330    0.7344    0.7307      3031

