seed:  666
number of classes: 60
number of epochs to train: 60
batch size: 64
number of workers to load data:  36
device:  cuda
number of gpus:  2
number of classes after further clustering:  87
number of pockets in training set:  22516
number of pockets in validation set:  4793
number of pockets in test set:  4914
model architecture:
MoNet(
  (conv1): GINMolecularConv(nn=Sequential(
    (0): Linear(in_features=5, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
  ))(edge_transformer=Sequential(
    (0): Linear(in_features=1, out_features=5, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=5, out_features=5, bias=True)
    (3): ELU(alpha=1.0)
  ))
  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): GINMolecularConv(nn=Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
  ))(edge_transformer=Sequential(
    (0): Linear(in_features=1, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ELU(alpha=1.0)
  ))
  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): GINMolecularConv(nn=Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
  ))(edge_transformer=Sequential(
    (0): Linear(in_features=1, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ELU(alpha=1.0)
  ))
  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv4): GINMolecularConv(nn=Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
  ))(edge_transformer=Sequential(
    (0): Linear(in_features=1, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ELU(alpha=1.0)
  ))
  (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=32, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=87, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)
loss function:
NLLLoss()
begin training...
epoch: 1, train loss: 7.1573745197588226, acc: 0.018120447681648607, val loss: 4.448659272972885, acc: 0.021281034842478615
epoch: 2, train loss: 4.4470313705502615, acc: 0.02647006573103571, val loss: 4.422567233744828, acc: 0.019820571667014397
epoch: 3, train loss: 4.420080764019631, acc: 0.03464203233256351, val loss: 4.397405553356531, acc: 0.021489672438973505
epoch: 4, train loss: 4.405751793777187, acc: 0.01869781488719133, val loss: 4.370309232496504, acc: 0.03150427707072814
epoch: 5, train loss: 4.332001143692865, acc: 0.028557470243382482, val loss: 4.358576667948006, acc: 0.02587106196536616
epoch: 6, train loss: 4.34179144402045, acc: 0.028868360277136258, val loss: 4.330775470443143, acc: 0.027122887544335488
epoch: 7, train loss: 4.27621728689045, acc: 0.04867649671344822, val loss: 4.304229782589188, acc: 0.030461089088253703
epoch: 8, train loss: 4.32415012517859, acc: 0.04672233078699591, val loss: 4.2915179069371, acc: 0.043187982474441895
epoch: 9, train loss: 4.306514915087482, acc: 0.04374666903535264, val loss: 4.27083821259882, acc: 0.0682244940538285
epoch: 10, train loss: 4.288669744844991, acc: 0.05418369159708652, val loss: 4.260582083629065, acc: 0.0682244940538285
epoch: 11, train loss: 4.270612820106706, acc: 0.061955942440930896, val loss: 4.254827547938894, acc: 0.06843313165032339
epoch: 12, train loss: 4.256824533561128, acc: 0.06568662284597619, val loss: 4.260425360198313, acc: 0.06801585645733361
epoch: 13, train loss: 4.251529465138457, acc: 0.0692396518031622, val loss: 4.348459921076182, acc: 0.06446901731692051
epoch: 14, train loss: 4.249565329003618, acc: 0.075635103926097, val loss: 4.293753055001061, acc: 0.06572084289588984
epoch: 15, train loss: 4.244697615019825, acc: 0.06772961449635814, val loss: 4.295179897525627, acc: 0.04214479449196745
epoch: 16, train loss: 4.225670917014242, acc: 0.058980280689287616, val loss: 4.5068824490698995, acc: 0.022950135614437723
epoch: 17, train loss: 4.179886415725195, acc: 0.052851305738141764, val loss: 4.249244523103118, acc: 0.0682244940538285
epoch: 18, train loss: 4.159065761200211, acc: 0.0829188132883283, val loss: 4.244859854114993, acc: 0.0682244940538285
epoch: 19, train loss: 4.113217243781457, acc: 0.08482856635281577, val loss: 4.2410799013954, acc: 0.0682244940538285
epoch: 20, train loss: 4.182901105441935, acc: 0.07856635281577545, val loss: 4.23603795193044, acc: 0.0682244940538285
epoch: 21, train loss: 4.2422315542810995, acc: 0.0678184402202878, val loss: 4.234493640626668, acc: 0.0682244940538285
epoch: 22, train loss: 4.234633058357205, acc: 0.06786285308225262, val loss: 4.235486789956462, acc: 0.0682244940538285
epoch: 23, train loss: 4.231498506399979, acc: 0.06772961449635814, val loss: 4.23730525158652, acc: 0.0682244940538285
epoch: 24, train loss: 4.194872404292668, acc: 0.06821815597797122, val loss: 4.237608147741931, acc: 0.0682244940538285
epoch: 25, train loss: 4.230146709482122, acc: 0.0678184402202878, val loss: 4.231930537336235, acc: 0.0682244940538285
epoch: 26, train loss: 4.226634493939808, acc: 0.06786285308225262, val loss: 4.232681202286598, acc: 0.0682244940538285
epoch: 27, train loss: 4.052770134567854, acc: 0.09491028601883106, val loss: 4.233142204531394, acc: 0.0682244940538285
epoch: 28, train loss: 3.927550289511871, acc: 0.09952922366317285, val loss: 4.230436746397124, acc: 0.0682244940538285
epoch: 29, train loss: 3.924280434650984, acc: 0.09699769053117784, val loss: 4.234570983554037, acc: 0.0682244940538285
epoch: 30, train loss: 3.97537096009667, acc: 0.10383727127376088, val loss: 4.23192948626497, acc: 0.0682244940538285
epoch: 31, train loss: 4.053387732462596, acc: 0.10645763012968555, val loss: 4.225019054026835, acc: 0.0682244940538285
epoch: 32, train loss: 4.1393683727140385, acc: 0.0784775270918458, val loss: 4.217030310516389, acc: 0.0682244940538285
epoch: 33, train loss: 4.231192475208954, acc: 0.06786285308225262, val loss: 4.209588416553404, acc: 0.0682244940538285
epoch: 34, train loss: 4.21945983577016, acc: 0.06786285308225262, val loss: 4.208464731832847, acc: 0.0682244940538285
epoch: 35, train loss: 4.21352806674119, acc: 0.06790726594421745, val loss: 4.203095011190808, acc: 0.0682244940538285
epoch: 36, train loss: 4.2084337195699515, acc: 0.0678184402202878, val loss: 4.20508546747645, acc: 0.0682244940538285
epoch: 37, train loss: 4.202090780441493, acc: 0.06786285308225262, val loss: 4.292435527171169, acc: 0.030043813895263928
epoch: 38, train loss: 4.211987461453328, acc: 0.06795167880618228, val loss: 10.01674667957703, acc: 0.016482370123096182
epoch: 39, train loss: 4.13477767070798, acc: 0.08465091490495648, val loss: 4.31636647981118, acc: 0.06238264135197163
epoch: 40, train loss: 4.007471612427794, acc: 0.08034286729436844, val loss: 4.356350530942348, acc: 0.0682244940538285
epoch: 41, train loss: 3.8419059656678223, acc: 0.10650204299165038, val loss: 4.203951914524848, acc: 0.0771959107031087
epoch: 42, train loss: 4.068190202596112, acc: 0.09579854325812755, val loss: 4.206797608547262, acc: 0.07677863551011892
epoch: 43, train loss: 4.108708865182145, acc: 0.0822082074968911, val loss: 4.206735396668728, acc: 0.06926768203630294
epoch: 44, train loss: 4.05576273043428, acc: 0.05462782021673477, val loss: 4.214046266465453, acc: 0.0682244940538285
epoch: 45, train loss: 4.215750527987579, acc: 0.06750755018653402, val loss: 4.208315305114912, acc: 0.0682244940538285
epoch: 46, train loss: 4.208164669561395, acc: 0.0678184402202878, val loss: 4.205106696827835, acc: 0.0682244940538285
epoch: 47, train loss: 4.250424491721713, acc: 0.08211938177296145, val loss: 4.207107302492406, acc: 0.0682244940538285
epoch: 48, train loss: 4.208565042251421, acc: 0.06777402735832297, val loss: 4.202749011960777, acc: 0.0682244940538285
epoch: 49, train loss: 4.202396955217344, acc: 0.06786285308225262, val loss: 4.19998604933454, acc: 0.0682244940538285
epoch: 50, train loss: 4.288634206287644, acc: 0.08389589625155446, val loss: 4.231739176406558, acc: 0.05633215105361986
epoch: 51, train loss: 4.180524937020968, acc: 0.0818529046011725, val loss: 4.195288758523624, acc: 0.0682244940538285
epoch: 52, train loss: 4.046106524271786, acc: 0.07816663705809203, val loss: 4.196236152776268, acc: 0.0682244940538285
epoch: 53, train loss: 3.9488426864327195, acc: 0.07501332385858944, val loss: 4.2002630818743265, acc: 0.0682244940538285
epoch: 54, train loss: 4.16276927976122, acc: 0.06075679516788062, val loss: 4.193500180346916, acc: 0.0682244940538285
epoch: 55, train loss: 4.04544798318521, acc: 0.06795167880618228, val loss: 4.196851830728214, acc: 0.0682244940538285
epoch: 56, train loss: 4.1936022498760535, acc: 0.06777402735832297, val loss: 4.2912581602168585, acc: 0.0701022324222825
epoch: 57, train loss: 4.040204258832526, acc: 0.058669390655533844, val loss: 4.196538091377403, acc: 0.0682244940538285
epoch: 58, train loss: 4.030061658776914, acc: 0.07621247113163972, val loss: 4.200744941093816, acc: 0.0682244940538285
epoch: 59, train loss: 4.158705226818904, acc: 0.06777402735832297, val loss: 4.948189939856654, acc: 0.01022324222824953
epoch: 60, train loss: 4.036888074612275, acc: 0.0743915437910819, val loss: 4.198840903260876, acc: 0.0682244940538285
best val loss 4.193500180346916 at epoch 54.
