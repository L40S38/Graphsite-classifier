seed:  666
save trained model at:  ../trained_models/trained_classifier_model_2.pt
save loss at:  ./results/train_classifier_results_2.json
how to merge clusters:  [[0, 9, 12], [1, 5, 11], 2, [3, 8, 13], 4, 6, 7, [10, 16], 15, 17, 18]
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  120
learning rate decay at epoch:  60
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  11
number of pockets in training set:  14097
number of pockets in validation set:  3016
number of pockets in test set:  3031
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=11, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
FocalLoss(gamma=2, alpha=1, reduction=mean)
begin training...
epoch: 1, train loss: 1.4636691146313978, acc: 0.36958218060580267, val loss: 1.195656759985562, acc: 0.44927055702917773, test loss: 1.202213309850287, acc: 0.44077862091718906
epoch: 2, train loss: 1.1599937976357988, acc: 0.45697666170107115, val loss: 1.0498534488425015, acc: 0.47944297082228116, test loss: 1.0532068286701934, acc: 0.48267898383371827
epoch: 3, train loss: 1.0628660448334728, acc: 0.4901752145846634, val loss: 1.0707794702970064, acc: 0.45590185676392575, test loss: 1.0882074840782263, acc: 0.45463543385021443
epoch: 4, train loss: 0.9997150467551075, acc: 0.5069163651840817, val loss: 0.9839765437401891, acc: 0.504973474801061, test loss: 1.0110694844508006, acc: 0.5057736720554272
epoch: 5, train loss: 0.9693809879107738, acc: 0.5147903809321133, val loss: 0.9556694973052655, acc: 0.5212201591511937, test loss: 0.9703187331316377, acc: 0.5199604091059057
epoch: 6, train loss: 0.9192399522628548, acc: 0.5308930978222317, val loss: 0.8842866774895463, acc: 0.5321618037135278, test loss: 0.9018968578928259, acc: 0.5258990432200594
epoch: 7, train loss: 0.9035466101953152, acc: 0.53791586862453, val loss: 0.7899641402520299, acc: 0.5686339522546419, test loss: 0.8127031566600366, acc: 0.5671395579016826
epoch: 8, train loss: 0.8390046370539368, acc: 0.5609704192381357, val loss: 0.8241469430038088, acc: 0.5550397877984085, test loss: 0.8338370123384653, acc: 0.5483338832068624
epoch: 9, train loss: 0.8178988012970192, acc: 0.5687734979073562, val loss: 0.9004826514094831, acc: 0.5421087533156499, test loss: 0.94309938005078, acc: 0.5308479049818542
epoch: 10, train loss: 0.8184134178384193, acc: 0.5670710080158899, val loss: 0.7770904014850485, acc: 0.5852122015915119, test loss: 0.7943717711674266, acc: 0.5701088749587595
epoch: 11, train loss: 0.8086439279103825, acc: 0.5699794282471448, val loss: 0.8402823105414919, acc: 0.5557029177718833, test loss: 0.8686226070300851, acc: 0.559881227317717
epoch: 12, train loss: 0.782048512703393, acc: 0.5759381428672767, val loss: 0.7937871929188939, acc: 0.5666445623342176, test loss: 0.8104034385756665, acc: 0.5674694820191356
epoch: 13, train loss: 0.7957825781842905, acc: 0.5704050507200114, val loss: 0.7325068146227526, acc: 0.583554376657825, test loss: 0.7371102680038281, acc: 0.5846255361266909
epoch: 14, train loss: 0.7606507318277109, acc: 0.5874299496346741, val loss: 0.7634299003793327, acc: 0.5789124668435013, test loss: 0.786362067617154, acc: 0.5786869020125371
epoch: 15, train loss: 0.7444843045445954, acc: 0.5950911541462722, val loss: 0.755176227668236, acc: 0.5812334217506632, test loss: 0.7736726687002481, acc: 0.581326294952161
epoch: 16, train loss: 0.7441934931529383, acc: 0.5968645811165496, val loss: 0.7709578800580862, acc: 0.5789124668435013, test loss: 0.7831496316896576, acc: 0.5753876608380073
epoch: 17, train loss: 0.7157695831272072, acc: 0.6080726395687026, val loss: 0.6573438998558793, acc: 0.6220159151193634, test loss: 0.6830487210378517, acc: 0.6225668096337842
epoch: 18, train loss: 0.7034484815993292, acc: 0.6116904305880684, val loss: 0.7698868588996503, acc: 0.5732758620689655, test loss: 0.7792962920111187, acc: 0.5885846255361267
epoch: 19, train loss: 0.7230658576261357, acc: 0.6036036036036037, val loss: 0.6912589351441563, acc: 0.5958222811671088, test loss: 0.7043878179460183, acc: 0.6087099967007589
epoch: 20, train loss: 0.6806824034627063, acc: 0.6182875789175002, val loss: 0.6927685579507358, acc: 0.6130636604774535, test loss: 0.7100725138553178, acc: 0.5998020455295282
epoch: 21, train loss: 0.6802401226987701, acc: 0.6231822373554657, val loss: 0.8103960031539755, acc: 0.5573607427055703, test loss: 0.8257226589606249, acc: 0.5631804684922468
epoch: 22, train loss: 0.6861862414076183, acc: 0.6184294530751223, val loss: 0.7066889865328841, acc: 0.6070954907161804, test loss: 0.7333982572111899, acc: 0.6034312108215111
epoch: 23, train loss: 0.6615357799585672, acc: 0.6268709654536426, val loss: 0.6997861498546853, acc: 0.6057692307692307, test loss: 0.7320683091357768, acc: 0.6083800725833058
epoch: 24, train loss: 0.6524606635488162, acc: 0.6329006171525856, val loss: 0.9481369269621467, acc: 0.5553713527851459, test loss: 0.9626297482717395, acc: 0.5539425932035632
epoch: 25, train loss: 0.6452074943683397, acc: 0.6388593317727176, val loss: 0.715402337537204, acc: 0.6170424403183024, test loss: 0.7307611069086164, acc: 0.6077202243483999
epoch: 26, train loss: 0.6228573967643162, acc: 0.6449599205504717, val loss: 0.8996081048677075, acc: 0.5228779840848806, test loss: 0.9205968214788283, acc: 0.5278785879247773
epoch: 27, train loss: 0.6498821806660728, acc: 0.6284315811874867, val loss: 0.7824182750691786, acc: 0.5586870026525199, test loss: 0.8000387840245825, acc: 0.5681293302540416
epoch: 28, train loss: 0.6768925554779242, acc: 0.6206994395970774, val loss: 0.6951840606861469, acc: 0.6160477453580901, test loss: 0.7210659286992379, acc: 0.6064005278785879
epoch: 29, train loss: 0.6357698669571095, acc: 0.6411293182946727, val loss: 0.6449379737560565, acc: 0.6303050397877984, test loss: 0.6759032761055579, acc: 0.6265258990432201
epoch: 30, train loss: 0.6203721323199007, acc: 0.6454564801021494, val loss: 0.5832081672683634, acc: 0.6548408488063661, test loss: 0.6239114159288598, acc: 0.6552292972616298
epoch: 31, train loss: 0.6001512243738071, acc: 0.6570192239483578, val loss: 0.7448033635116699, acc: 0.5865384615384616, test loss: 0.7695179444577033, acc: 0.5846255361266909
epoch: 32, train loss: 0.5913853709762161, acc: 0.657870468894091, val loss: 0.5931081768688535, acc: 0.6541777188328912, test loss: 0.6243810536876365, acc: 0.6430221049158693
epoch: 33, train loss: 0.5832885345127631, acc: 0.6596438958643683, val loss: 0.8691330171074095, acc: 0.5464190981432361, test loss: 0.8995567673073079, acc: 0.535796766743649
epoch: 34, train loss: 0.6109543134491858, acc: 0.6380080868269845, val loss: 0.6489065904516124, acc: 0.6326259946949602, test loss: 0.6775389641120229, acc: 0.6298251402177499
epoch: 35, train loss: 0.6333470653538806, acc: 0.6397105767184508, val loss: 0.6387462625452948, acc: 0.6326259946949602, test loss: 0.6712025135352406, acc: 0.6176179478719894
epoch: 36, train loss: 0.574761676616666, acc: 0.6645385543023339, val loss: 0.5704181333435625, acc: 0.6591511936339522, test loss: 0.5978416129332177, acc: 0.6621577037281425
epoch: 37, train loss: 0.5643125374900488, acc: 0.6724835071291764, val loss: 0.8437419418314723, acc: 0.5825596816976127, test loss: 0.8669577112531395, acc: 0.5866050808314087
epoch: 38, train loss: 0.5711138240029526, acc: 0.6643966801447116, val loss: 0.5600852814530188, acc: 0.6737400530503979, test loss: 0.5942121265509545, acc: 0.6664467172550314
epoch: 39, train loss: 0.5655362804773015, acc: 0.6702844576860325, val loss: 0.6095306440120667, acc: 0.6571618037135278, test loss: 0.6159493701365945, acc: 0.651270207852194
epoch: 40, train loss: 0.569056747198071, acc: 0.6688657161098106, val loss: 0.5690761469403376, acc: 0.6611405835543767, test loss: 0.5972624625439142, acc: 0.6552292972616298
epoch: 41, train loss: 0.5458190326082831, acc: 0.6757466127544868, val loss: 0.718935253765602, acc: 0.6027851458885941, test loss: 0.7336269877052118, acc: 0.6106895414054767
epoch: 42, train loss: 0.5546639391342072, acc: 0.6776619138823863, val loss: 0.738020658809247, acc: 0.6054376657824934, test loss: 0.7487759383273023, acc: 0.5981524249422633
epoch: 43, train loss: 0.5308964820274134, acc: 0.6850393700787402, val loss: 0.707621851081241, acc: 0.6296419098143236, test loss: 0.7208581640947695, acc: 0.6199274166941603
epoch: 44, train loss: 0.5448102854605033, acc: 0.6787969071433638, val loss: 0.6033295652278222, acc: 0.6425729442970822, test loss: 0.6313350459837905, acc: 0.6238865061035962
epoch: 45, train loss: 0.5232289883569005, acc: 0.6855359296304179, val loss: 0.6539523857658042, acc: 0.6525198938992043, test loss: 0.7080358009612115, acc: 0.6344440778620917
epoch: 46, train loss: 0.54766172226451, acc: 0.6746116194935092, val loss: 0.743341589479927, acc: 0.5815649867374005, test loss: 0.7576682303298704, acc: 0.5776971296601782
epoch: 47, train loss: 0.5073907807265359, acc: 0.6931261970632049, val loss: 0.5368870885682043, acc: 0.6810344827586207, test loss: 0.5688629384404242, acc: 0.6756845925437149
epoch: 48, train loss: 0.5067799220636837, acc: 0.6938355678513158, val loss: 0.6147006497142802, acc: 0.6687665782493368, test loss: 0.6497953558707544, acc: 0.6483008907951171
epoch: 49, train loss: 0.5493294534221849, acc: 0.6765269206214088, val loss: 0.6652640249748129, acc: 0.6372679045092838, test loss: 0.7080008079808209, acc: 0.6473111184427581
epoch: 50, train loss: 0.5335346014776864, acc: 0.6846137476058736, val loss: 0.5796917540957504, acc: 0.669761273209549, test loss: 0.615644195259935, acc: 0.6588584625536127
epoch: 51, train loss: 0.5301088667756456, acc: 0.6773781655671419, val loss: 0.5662885951742885, acc: 0.6790450928381963, test loss: 0.610030466438087, acc: 0.6591883866710656
epoch: 52, train loss: 0.5703681056390564, acc: 0.6656735475633113, val loss: 0.5327478912844266, acc: 0.6933023872679045, test loss: 0.560429470958351, acc: 0.6707357307819202
epoch: 53, train loss: 0.4904762773388809, acc: 0.7036248847272469, val loss: 0.5668605344681272, acc: 0.6949602122015915, test loss: 0.58871190707623, acc: 0.6872319366545695
epoch: 54, train loss: 0.48237874393404956, acc: 0.7061076824856353, val loss: 0.6116130940161586, acc: 0.6568302387267905, test loss: 0.6478205995565985, acc: 0.649950511382382
epoch: 55, train loss: 0.5384610139085628, acc: 0.679648152089097, val loss: 0.6965737039277661, acc: 0.6210212201591512, test loss: 0.7287013422573647, acc: 0.606070603761135
epoch: 56, train loss: 0.49471183748137676, acc: 0.6990139746045257, val loss: 0.6047701557371914, acc: 0.6710875331564987, test loss: 0.6453537430475349, acc: 0.6539096007918179
epoch: 57, train loss: 0.5341368425993444, acc: 0.6875221678371285, val loss: 0.6035841127921795, acc: 0.6767241379310345, test loss: 0.6236729967598079, acc: 0.6624876278455956
epoch: 58, train loss: 0.48671105699377837, acc: 0.7026317656238916, val loss: 0.5299160014096875, acc: 0.6797082228116711, test loss: 0.5756567499890103, acc: 0.6786539096007919
epoch: 59, train loss: 0.49614519792016776, acc: 0.6963893026885153, val loss: 0.6371098614497905, acc: 0.6558355437665783, test loss: 0.6909593493582116, acc: 0.652259980204553
epoch: 60, train loss: 0.43527828517928907, acc: 0.7266794353408527, val loss: 0.4732338022490079, acc: 0.7277851458885941, test loss: 0.5190818852144906, acc: 0.7149455625206202
epoch: 61, train loss: 0.3987974772628693, acc: 0.7435624600978932, val loss: 0.4674166255983813, acc: 0.7320954907161804, test loss: 0.5086834920943275, acc: 0.7165951831078852
epoch: 62, train loss: 0.3918611639280171, acc: 0.7473930623536923, val loss: 0.4853008299354533, acc: 0.7317639257294429, test loss: 0.5367175951840971, acc: 0.7152754866380733
epoch: 63, train loss: 0.37624555330221693, acc: 0.7564020713627013, val loss: 0.49275143740980315, acc: 0.7188328912466844, test loss: 0.5296241336433615, acc: 0.6984493566479709
epoch: 64, train loss: 0.375817884212945, acc: 0.7527133432645243, val loss: 0.5280601374350429, acc: 0.7032493368700266, test loss: 0.5666629037067309, acc: 0.6918508742989112
epoch: 65, train loss: 0.38352039527189496, acc: 0.7518620983187912, val loss: 0.5267052223574894, acc: 0.7035809018567639, test loss: 0.5906523389023391, acc: 0.696799736060706
epoch: 66, train loss: 0.47161688502184795, acc: 0.7116407746329007, val loss: 0.48733251464777977, acc: 0.7294429708222812, test loss: 0.5246050778846464, acc: 0.7109864731111845
epoch: 67, train loss: 0.3875698224275579, acc: 0.7503014825849471, val loss: 0.5029496719729679, acc: 0.7165119363395226, test loss: 0.5342120641688237, acc: 0.7027383701748597
epoch: 68, train loss: 0.4111629595416247, acc: 0.7392352982904165, val loss: 0.49447126666810215, acc: 0.7317639257294429, test loss: 0.525427193953943, acc: 0.7301220719234576
epoch: 69, train loss: 0.3884431630819074, acc: 0.7461161949350926, val loss: 0.5217974927444357, acc: 0.7238063660477454, test loss: 0.5710505846505588, acc: 0.7152754866380733
epoch: 70, train loss: 0.3902251491049735, acc: 0.7467546286443925, val loss: 0.5209416881480331, acc: 0.7344164456233422, test loss: 0.5624448463335481, acc: 0.7086770042890135
epoch: 71, train loss: 0.36741615821175805, acc: 0.761438603958289, val loss: 0.4766439177313281, acc: 0.7314323607427056, test loss: 0.5243370985284728, acc: 0.7294622236885516
epoch: 72, train loss: 0.3961661842589498, acc: 0.7378874937930056, val loss: 0.5393446377164806, acc: 0.7161803713527851, test loss: 0.5764478540231748, acc: 0.6931705707687232
epoch: 73, train loss: 0.3713199559036549, acc: 0.7571114421508123, val loss: 0.48002376527938034, acc: 0.7327586206896551, test loss: 0.5276249756477958, acc: 0.7235235895743979
epoch: 74, train loss: 0.3415125668949461, acc: 0.772008228701142, val loss: 0.49905936819172664, acc: 0.7374005305039788, test loss: 0.5369775511965976, acc: 0.7281425272187397
epoch: 75, train loss: 0.35097158857463157, acc: 0.7693126197063205, val loss: 0.5200989598936997, acc: 0.7248010610079576, test loss: 0.5489572841159112, acc: 0.720224348399868
epoch: 76, train loss: 0.3269376878498412, acc: 0.7718663545435199, val loss: 0.5183952351147679, acc: 0.7327586206896551, test loss: 0.5618230288904204, acc: 0.7179148795776972
epoch: 77, train loss: 0.3414097361844288, acc: 0.7712988579130311, val loss: 0.515456932925419, acc: 0.7330901856763926, test loss: 0.5575651459000127, acc: 0.7179148795776972
epoch: 78, train loss: 0.32271261612912516, acc: 0.7791019365822516, val loss: 0.709544457870706, acc: 0.6535145888594165, test loss: 0.7303359195840748, acc: 0.6542395249092708
epoch: 79, train loss: 0.3702459277950754, acc: 0.7562601972050791, val loss: 0.5558480615641141, acc: 0.7025862068965517, test loss: 0.5801479668949194, acc: 0.6984493566479709
epoch: 80, train loss: 0.4406797860356572, acc: 0.729162233099241, val loss: 0.5330920478709497, acc: 0.705238726790451, test loss: 0.5738001913176398, acc: 0.6849224678323985
epoch: 81, train loss: 0.350920149425822, acc: 0.7633539050861885, val loss: 0.5375498254988491, acc: 0.7078912466843501, test loss: 0.5941934054222724, acc: 0.698119432530518
epoch: 82, train loss: 0.38221180586347414, acc: 0.7507980421366248, val loss: 0.5224344354093233, acc: 0.7294429708222812, test loss: 0.5365031158440343, acc: 0.7225338172220389
epoch: 83, train loss: 0.32608181683401727, acc: 0.7769738242179187, val loss: 0.48880271965375943, acc: 0.7374005305039788, test loss: 0.5307843370509989, acc: 0.7291322995710986
epoch: 84, train loss: 0.3257392939036642, acc: 0.7769028871391076, val loss: 0.47834866195521875, acc: 0.7420424403183024, test loss: 0.5169505473608548, acc: 0.7307819201583636
epoch: 85, train loss: 0.3085320234391726, acc: 0.7822231680499397, val loss: 0.5618067750563989, acc: 0.7112068965517241, test loss: 0.59968661493061, acc: 0.6918508742989112
epoch: 86, train loss: 0.30958797601957716, acc: 0.7805206781584735, val loss: 0.49836672806929533, acc: 0.7314323607427056, test loss: 0.5434010094913544, acc: 0.7192345760475091
epoch: 87, train loss: 0.3083933110591274, acc: 0.7843512804142725, val loss: 0.5224235525814228, acc: 0.7407161803713528, test loss: 0.5640294780356779, acc: 0.7245133619267569
epoch: 88, train loss: 0.4706076862558859, acc: 0.7138398240760445, val loss: 0.5184064913807876, acc: 0.7228116710875332, test loss: 0.5507919644412724, acc: 0.7126360936984494
epoch: 89, train loss: 0.3316440241526414, acc: 0.7730722848833085, val loss: 0.49709885427110706, acc: 0.7287798408488063, test loss: 0.5292496466274822, acc: 0.7165951831078852
epoch: 90, train loss: 0.31226017818118085, acc: 0.7780378804000851, val loss: 0.5075468956949857, acc: 0.7403846153846154, test loss: 0.5684372762058955, acc: 0.7149455625206202
epoch: 91, train loss: 0.3009046026474281, acc: 0.7867631410938497, val loss: 0.5121211703639448, acc: 0.7254641909814323, test loss: 0.5554887187319674, acc: 0.7215440448696799
epoch: 92, train loss: 0.3245556836048914, acc: 0.7797403702915514, val loss: 0.5183594407074015, acc: 0.7304376657824934, test loss: 0.5663978627946502, acc: 0.722203893104586
epoch: 93, train loss: 0.2923760167747522, acc: 0.7842094062566504, val loss: 0.5469284696351312, acc: 0.7214854111405835, test loss: 0.5835759544561343, acc: 0.7251732101616628
epoch: 94, train loss: 0.32228498706802744, acc: 0.7724338511740086, val loss: 0.4972939182972086, acc: 0.738395225464191, test loss: 0.5170511274163143, acc: 0.7274826789838337
epoch: 95, train loss: 0.2936991856835062, acc: 0.7873306377243385, val loss: 0.5255185351447654, acc: 0.7364058355437666, test loss: 0.5510648929494633, acc: 0.7301220719234576
epoch: 96, train loss: 0.33498701056394287, acc: 0.7669007590267433, val loss: 0.5592916593627525, acc: 0.7102122015915119, test loss: 0.5834563506471564, acc: 0.7119762454635434
epoch: 97, train loss: 0.32619305641133456, acc: 0.7747747747747747, val loss: 0.5418396473563318, acc: 0.7314323607427056, test loss: 0.5841335809808956, acc: 0.7192345760475091
epoch: 98, train loss: 0.27717647768677045, acc: 0.7977583883095695, val loss: 0.580038740875234, acc: 0.7264588859416445, test loss: 0.6008280225808531, acc: 0.7248432860442098
epoch: 99, train loss: 0.3191793158406221, acc: 0.7804497410796624, val loss: 0.5354654819010424, acc: 0.7231432360742706, test loss: 0.5613458234065832, acc: 0.7238535136918509
epoch: 100, train loss: 0.2877554441712685, acc: 0.7901681208767823, val loss: 0.5865511566953887, acc: 0.7141909814323607, test loss: 0.6142344341857015, acc: 0.7119762454635434
epoch: 101, train loss: 0.27873882837580344, acc: 0.8008796197772575, val loss: 0.5318370978459123, acc: 0.7307692307692307, test loss: 0.5876220847780812, acc: 0.7264929066314748
epoch: 102, train loss: 0.28351211640025875, acc: 0.7970490175214585, val loss: 0.634062595013282, acc: 0.7035809018567639, test loss: 0.6826752828946014, acc: 0.7010887495875948
epoch: 103, train loss: 0.33501594541190727, acc: 0.7678229410512875, val loss: 0.5179342061518358, acc: 0.7317639257294429, test loss: 0.5357821832116938, acc: 0.7297921478060047
epoch: 104, train loss: 0.268417045699203, acc: 0.8073348939490672, val loss: 0.5059475701114543, acc: 0.7453580901856764, test loss: 0.5411726036153583, acc: 0.7284724513361927
epoch: 105, train loss: 0.2571204494540384, acc: 0.8023692984322905, val loss: 0.5362194349658268, acc: 0.7407161803713528, test loss: 0.5633073339600722, acc: 0.7304519960409106
epoch: 106, train loss: 0.2735156901801758, acc: 0.8007377456196354, val loss: 0.5822553552430252, acc: 0.7122015915119363, test loss: 0.623656139638077, acc: 0.7070273837017486
epoch: 107, train loss: 0.26810994959092355, acc: 0.798184010782436, val loss: 0.5695847876824498, acc: 0.7204907161803713, test loss: 0.5900189045119781, acc: 0.7175849554602441
epoch: 108, train loss: 0.27972318510167976, acc: 0.7999574377527133, val loss: 0.544471042067682, acc: 0.7330901856763926, test loss: 0.6012351880251319, acc: 0.7248432860442098
epoch: 109, train loss: 0.28764983374463454, acc: 0.7910903029013265, val loss: 0.5142878651302752, acc: 0.746684350132626, test loss: 0.5319039376802234, acc: 0.7258330583965688
epoch: 110, train loss: 0.2582017461814009, acc: 0.8083280130524225, val loss: 0.5260967651792167, acc: 0.7354111405835544, test loss: 0.5518925801553729, acc: 0.7307819201583636
epoch: 111, train loss: 0.2702091842331666, acc: 0.8023692984322905, val loss: 0.5219611103401892, acc: 0.7377320954907162, test loss: 0.5628097794387098, acc: 0.7350709336852523
epoch: 112, train loss: 0.25321091135114593, acc: 0.8048520961906789, val loss: 0.5866421135414184, acc: 0.7277851458885941, test loss: 0.6411052176520519, acc: 0.7139557901682613
epoch: 113, train loss: 0.265585949833261, acc: 0.8003830602255799, val loss: 0.5623457904519706, acc: 0.7244694960212201, test loss: 0.610101530513367, acc: 0.7264929066314748
epoch: 114, train loss: 0.2668940350249481, acc: 0.8042845995601902, val loss: 0.5230582353290891, acc: 0.7390583554376657, test loss: 0.5426862924179988, acc: 0.7297921478060047
epoch: 115, train loss: 0.2506860323964328, acc: 0.8085408242888558, val loss: 0.6464173534504615, acc: 0.6936339522546419, test loss: 0.6931692357112063, acc: 0.6832728472451336
epoch: 116, train loss: 0.3330413787032794, acc: 0.7721501028587643, val loss: 0.556465377542005, acc: 0.7118700265251989, test loss: 0.5804925679137942, acc: 0.6898713295941933
epoch: 117, train loss: 0.2977281134796792, acc: 0.7881109455912606, val loss: 0.5282087194824725, acc: 0.7423740053050398, test loss: 0.5608722644055176, acc: 0.7268228307489277
epoch: 118, train loss: 0.25129654374605986, acc: 0.8112364332836773, val loss: 0.5108637275366947, acc: 0.735079575596817, test loss: 0.5394895396504769, acc: 0.7225338172220389
epoch: 119, train loss: 0.30593644099564654, acc: 0.7805916152372845, val loss: 0.530828674845101, acc: 0.7430371352785146, test loss: 0.5683719472340805, acc: 0.7255031342791158
epoch: 120, train loss: 0.25836834337488596, acc: 0.8050649074271121, val loss: 0.524864818910705, acc: 0.7423740053050398, test loss: 0.5561544370706307, acc: 0.7330913889805345
best val loss 0.4674166255983813 at epoch 61.
