seed:  18
save trained model at:  ../trained_models/trained_classifier_model_118.pt
save loss at:  ./results/train_classifier_results_118.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['3lcdA00', '2q7dB00', '2q7uA00', '4tqdA01', '2fqxA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['5o6mD00', '3cd0A00', '1b62A00', '4xdrA00', '1qkiF00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b9c8e5bc2b0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.041219954619369, acc: 0.38782999881614777; test loss: 1.8051719719623736, acc: 0.45355707870479794
epoch: 2, train loss: 1.7459189773269188, acc: 0.46040014206227065; test loss: 1.7688748046484604, acc: 0.44953911604821556
epoch: 3, train loss: 1.6367289645016736, acc: 0.5005919261276193; test loss: 1.5433732022407403, acc: 0.5095722051524463
epoch: 4, train loss: 1.5761523517021783, acc: 0.5223748076240086; test loss: 1.5589909643124982, acc: 0.5235168990782321
epoch: 5, train loss: 1.5257975182985433, acc: 0.5390079318101101; test loss: 1.5123209284428754, acc: 0.5308437721578823
epoch: 6, train loss: 1.4718215429558477, acc: 0.5566473304131644; test loss: 1.4473282558871228, acc: 0.5478610257622312
epoch: 7, train loss: 1.456606066959404, acc: 0.5613235468213567; test loss: 1.4286896035733783, acc: 0.5462065705506972
epoch: 8, train loss: 1.4198839070176859, acc: 0.5719782171185036; test loss: 1.3717024912368714, acc: 0.5769321673363271
epoch: 9, train loss: 1.405483300183191, acc: 0.5744051142417427; test loss: 1.4847102481911465, acc: 0.5407705034270858
epoch: 10, train loss: 1.3903475189767844, acc: 0.5778382857819344; test loss: 1.3813766401390521, acc: 0.5665327345781139
epoch: 11, train loss: 1.3506934911213755, acc: 0.592044512844797; test loss: 1.3524871890749444, acc: 0.5811864807374143
epoch: 12, train loss: 1.3536922042717294, acc: 0.5885521486918432; test loss: 1.3703695166846366, acc: 0.5655873316000946
epoch: 13, train loss: 1.3413433732260498, acc: 0.5937610986148929; test loss: 1.342410629056526, acc: 0.5750413613802884
epoch: 14, train loss: 1.3206100589892873, acc: 0.5983781224103232; test loss: 1.359125468444103, acc: 0.5752777121247932
epoch: 15, train loss: 1.3020915131152868, acc: 0.6043565762992779; test loss: 1.2832498060166484, acc: 0.6128574805010636
epoch: 16, train loss: 1.2761544035329782, acc: 0.611637267668995; test loss: 1.2970715481968418, acc: 0.5937130701961711
epoch: 17, train loss: 1.275088035567803, acc: 0.6119332307328046; test loss: 1.3667835583503127, acc: 0.5800047270148901
epoch: 18, train loss: 1.25957087132865, acc: 0.6178524920089973; test loss: 1.2411430027433614, acc: 0.6190025998581895
epoch: 19, train loss: 1.2354933650335278, acc: 0.6228246714809992; test loss: 1.2445920838500502, acc: 0.621366107303238
epoch: 20, train loss: 1.2279323131583408, acc: 0.6263170356339529; test loss: 1.363293661515613, acc: 0.5759867643583078
epoch: 21, train loss: 1.2251146943721318, acc: 0.6259026873446194; test loss: 1.2383536568377718, acc: 0.6180571968801701
epoch: 22, train loss: 1.215241040395423, acc: 0.629513436723097; test loss: 1.2170486963547535, acc: 0.6178208461356653
epoch: 23, train loss: 1.1990813463696985, acc: 0.6366757428672901; test loss: 1.2058500685722262, acc: 0.6154573386906169
epoch: 24, train loss: 1.1903081931658042, acc: 0.6363797798034805; test loss: 1.2875562465886814, acc: 0.6043488536988891
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9544642130213968, acc: 0.6425950041434829; test loss: 1.066902482968869, acc: 0.5982037343417632
epoch: 26, train loss: 0.9494878505762899, acc: 0.6448443234284361; test loss: 1.14893407095862, acc: 0.567950839045143
epoch: 27, train loss: 0.9606393739163022, acc: 0.6420030780158636; test loss: 1.001572101399623, acc: 0.6126211297565587
epoch: 28, train loss: 0.9479637996634181, acc: 0.6422398484669113; test loss: 0.9621416518829642, acc: 0.6364925549515481
epoch: 29, train loss: 0.935398317348665, acc: 0.6454954421688174; test loss: 0.9362186761760058, acc: 0.6383833609075868
epoch: 30, train loss: 0.9153921154951843, acc: 0.652302592636439; test loss: 1.0036017765702436, acc: 0.612148428267549
epoch: 31, train loss: 0.9140954839487506, acc: 0.6541375636320588; test loss: 0.9391888520644929, acc: 0.6374379579295675
epoch: 32, train loss: 0.8961102991565891, acc: 0.6601160175210133; test loss: 1.0417155885324498, acc: 0.6128574805010636
epoch: 33, train loss: 0.8957536485373826, acc: 0.6580442760743459; test loss: 1.0161780510859353, acc: 0.6133301819900733
epoch: 34, train loss: 0.9011635833821298, acc: 0.6540191784065349; test loss: 0.9798551919301952, acc: 0.6256204207043252
epoch: 35, train loss: 0.8798794647822284, acc: 0.6616550254528235; test loss: 0.9053935021324041, acc: 0.6502008981328291
epoch: 36, train loss: 0.87434299782887, acc: 0.6621877589676808; test loss: 0.9849501409532894, acc: 0.6234932640037816
epoch: 37, train loss: 0.8768003117237992, acc: 0.6654433526695869; test loss: 0.9469026156673193, acc: 0.6294020326164027
epoch: 38, train loss: 0.8678491484615045, acc: 0.6665680123120634; test loss: 0.8920540042702285, acc: 0.6556369652564406
epoch: 39, train loss: 0.8498669975295325, acc: 0.6716585770095892; test loss: 0.880422101576428, acc: 0.6591822264240133
epoch: 40, train loss: 0.8249832636381136, acc: 0.684029833076832; test loss: 1.1190585175520757, acc: 0.5670054360671236
epoch: 41, train loss: 0.8406493705420067, acc: 0.6773410678347342; test loss: 0.8967230024700665, acc: 0.641219569841645
epoch: 42, train loss: 0.8303971087648923, acc: 0.6811293950514976; test loss: 0.9534598335648394, acc: 0.6324745922949657
epoch: 43, train loss: 0.822709911022297, acc: 0.6818988990174026; test loss: 0.8776721729507573, acc: 0.6554006145119358
epoch: 44, train loss: 0.8016997586203225, acc: 0.6859831892979756; test loss: 0.9232876887419522, acc: 0.6424013235641692
epoch: 45, train loss: 0.791188318947864, acc: 0.6921392210252161; test loss: 0.8578887328952798, acc: 0.6705270621602458
epoch: 46, train loss: 0.8128198047805936, acc: 0.6898899017402628; test loss: 1.0522114394883741, acc: 0.6029307492318601
epoch: 47, train loss: 0.7905416040712946, acc: 0.6898307091275009; test loss: 0.8462197855555175, acc: 0.6646182935476247
epoch: 48, train loss: 0.7829179059785965, acc: 0.6954540073398839; test loss: 0.8136035838157566, acc: 0.674781375561333
epoch: 49, train loss: 0.776182072888299, acc: 0.6945069255356932; test loss: 0.9739320613452657, acc: 0.6220751595367525
epoch: 50, train loss: 0.7802989380280901, acc: 0.6916064875103587; test loss: 0.8829444696705439, acc: 0.6468919877097613
epoch: 51, train loss: 0.7627890858760851, acc: 0.6992423345566473; test loss: 0.8499216370705425, acc: 0.6740723233278185
epoch: 52, train loss: 0.7638525068865762, acc: 0.7021427725819818; test loss: 0.8366746633674371, acc: 0.6643819428031198
epoch: 53, train loss: 0.7561782058264442, acc: 0.7026163134840772; test loss: 0.915132282135814, acc: 0.6487827936658
epoch: 54, train loss: 0.7552030982104112, acc: 0.7045696697052208; test loss: 1.0133528569662607, acc: 0.6258567714488301
epoch: 55, train loss: 0.7557539186831042, acc: 0.7033266248372203; test loss: 0.8473228000630951, acc: 0.6577641219569842
epoch: 56, train loss: 0.7406670883151502, acc: 0.7071149520539837; test loss: 0.8997413492800073, acc: 0.6478373906877807
epoch: 57, train loss: 0.7397700008434898, acc: 0.7090683082751272; test loss: 0.928963636963172, acc: 0.6405105176081305
epoch: 58, train loss: 0.7325483238512177, acc: 0.7111400497217947; test loss: 0.8350057378217588, acc: 0.6766721815173717
epoch: 59, train loss: 0.734262601001539, acc: 0.7129750207174145; test loss: 0.8169038077669768, acc: 0.6832900023635075
epoch: 60, train loss: 0.7072746694772444, acc: 0.7208476382147508; test loss: 0.8307141989462484, acc: 0.6662727487591585
epoch: 61, train loss: 0.7251021906837594, acc: 0.7136853320705576; test loss: 0.8039284638833335, acc: 0.677853935239896
epoch: 62, train loss: 0.7010289684222043, acc: 0.7241624245294187; test loss: 0.8292609257383624, acc: 0.6752540770503427
epoch: 63, train loss: 0.7024280318243483, acc: 0.7202557120871316; test loss: 1.0119437937532523, acc: 0.6308201370834318
epoch: 64, train loss: 0.7073090512455886, acc: 0.7203740973126553; test loss: 0.8099987376697011, acc: 0.6825809501299929
epoch: 65, train loss: 0.6827321970647674, acc: 0.7309695749970404; test loss: 0.8563876424774552, acc: 0.6679272039706925
epoch: 66, train loss: 0.6805940603199436, acc: 0.7301408784183734; test loss: 0.8008497436399917, acc: 0.6984164500118175
epoch: 67, train loss: 0.6904047532407324, acc: 0.7275364034568486; test loss: 0.8486352155631779, acc: 0.6828173008744978
epoch: 68, train loss: 0.6829936682253571, acc: 0.7284834852610395; test loss: 0.7972398478774646, acc: 0.6847081068305365
epoch: 69, train loss: 0.6662386175061141, acc: 0.7349946726648514; test loss: 0.7847623424421837, acc: 0.6899078232096431
epoch: 70, train loss: 0.6802738965002195, acc: 0.7297265301290399; test loss: 0.8391067412580843, acc: 0.6754904277948476
epoch: 71, train loss: 0.6586142109176231, acc: 0.7363561027583757; test loss: 0.8517606447729708, acc: 0.6806901441739541
epoch: 72, train loss: 0.6661099445062008, acc: 0.733751627796851; test loss: 0.8275814872913906, acc: 0.6776175844953911
epoch: 73, train loss: 0.6655595899051614, acc: 0.735290635728661; test loss: 0.941532706177508, acc: 0.6476010399432758
epoch: 74, train loss: 0.660795093732881, acc: 0.7380726885284716; test loss: 0.7678225665925328, acc: 0.697707397778303
epoch: 75, train loss: 0.6577884263437401, acc: 0.7389013851071387; test loss: 0.7153839357241308, acc: 0.7170881588277003
epoch: 76, train loss: 0.652421036859765, acc: 0.7390789629454244; test loss: 0.8634342886938944, acc: 0.6632001890805956
epoch: 77, train loss: 0.6472247849770152, acc: 0.7435184089025689; test loss: 0.7465042054188277, acc: 0.7036161663909242
epoch: 78, train loss: 0.628331777815764, acc: 0.7474251213448562; test loss: 0.8247109929091314, acc: 0.6792720397069251
epoch: 79, train loss: 0.6476953609661193, acc: 0.7404995856517107; test loss: 0.8688137075405712, acc: 0.6750177263058379
epoch: 80, train loss: 0.6240237117575741, acc: 0.7493784775659997; test loss: 0.7915527598566549, acc: 0.6981800992673127
epoch: 81, train loss: 0.6301704927409233, acc: 0.7501479815319049; test loss: 0.7864710248353436, acc: 0.6906168754431576
epoch: 82, train loss: 0.6299755992770294, acc: 0.7444062980939978; test loss: 0.7742118842774772, acc: 0.6967619948002837
epoch: 83, train loss: 0.6207369407267349, acc: 0.7521605303658103; test loss: 0.9132658112728125, acc: 0.6606003308910423
epoch: 84, train loss: 0.6300433555255631, acc: 0.7482538179235232; test loss: 0.7932462257799091, acc: 0.6948711888442448
epoch: 85, train loss: 0.6308511627966573, acc: 0.7488457440511425; test loss: 0.7284159949980457, acc: 0.7135428976601277
epoch: 86, train loss: 0.628000169904243, acc: 0.7511542559488575; test loss: 0.801887601522429, acc: 0.6889624202316237
Epoch    86: reducing learning rate of group 0 to 1.5000e-03.
epoch: 87, train loss: 0.5530181773614641, acc: 0.7760151533088671; test loss: 0.71522276087648, acc: 0.7298510990309619
epoch: 88, train loss: 0.5232691402842875, acc: 0.7853675861252516; test loss: 0.6854012665582926, acc: 0.7407232332781848
epoch: 89, train loss: 0.512533652425391, acc: 0.791523617852492; test loss: 0.7390221311983503, acc: 0.7244150319073505
epoch: 90, train loss: 0.5041805581537325, acc: 0.7917603883035397; test loss: 0.6980444621994715, acc: 0.7419049870007091
epoch: 91, train loss: 0.502716770895532, acc: 0.795607908133065; test loss: 0.7076112640802811, acc: 0.7286693453084377
epoch: 92, train loss: 0.4941836749574511, acc: 0.795548715520303; test loss: 0.7259811136182164, acc: 0.7293783975419522
epoch: 93, train loss: 0.48968192405413913, acc: 0.7990410796732568; test loss: 0.8791701286516976, acc: 0.6842354053415268
epoch: 94, train loss: 0.496944454148971, acc: 0.8008760506688766; test loss: 0.7608375787115131, acc: 0.7289056960529425
epoch: 95, train loss: 0.4858132722999579, acc: 0.8020599029241151; test loss: 0.7232885605842051, acc: 0.725124084140865
epoch: 96, train loss: 0.48167734189443906, acc: 0.8024150586006866; test loss: 0.7438484504413898, acc: 0.7260694871188844
epoch: 97, train loss: 0.48465361890954295, acc: 0.8030069847283059; test loss: 0.7266721465124979, acc: 0.7260694871188844
epoch: 98, train loss: 0.4797192017496077, acc: 0.80129039895821; test loss: 0.7243248553063057, acc: 0.7324509572205152
epoch: 99, train loss: 0.46918944759214787, acc: 0.8022966733751627; test loss: 0.7617832793887415, acc: 0.7177972110612149
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.38373942253532334, acc: 0.7991002722860187; test loss: 0.5945592126055154, acc: 0.7369416213661073
epoch: 101, train loss: 0.3639993599050494, acc: 0.8112939505149758; test loss: 0.6468715526285084, acc: 0.7185062632947293
epoch: 102, train loss: 0.35666402933074554, acc: 0.8072688528471647; test loss: 0.6135530759188842, acc: 0.734341763176554
epoch: 103, train loss: 0.3675798944457686, acc: 0.8053154966260211; test loss: 0.6333810116094608, acc: 0.7170881588277003
epoch: 104, train loss: 0.3500634113246624, acc: 0.812714573221262; test loss: 0.6295116208625781, acc: 0.7322146064760104
epoch: 105, train loss: 0.3533654865660272, acc: 0.8096365573576417; test loss: 0.6729649459180244, acc: 0.7040888678799339
epoch: 106, train loss: 0.354211147013283, acc: 0.8093997869065941; test loss: 0.6415132996997098, acc: 0.7260694871188844
epoch: 107, train loss: 0.3511332079999275, acc: 0.8097549425831656; test loss: 0.5963693003202549, acc: 0.7376506735996219
epoch: 108, train loss: 0.3506941052637284, acc: 0.8126553806085001; test loss: 0.6209628882911915, acc: 0.7305601512644765
epoch: 109, train loss: 0.3570724543388234, acc: 0.8071504676216408; test loss: 0.7507679629230071, acc: 0.680926494918459
epoch: 110, train loss: 0.3581237407094475, acc: 0.805374689238783; test loss: 0.5964188821432411, acc: 0.7428503899787284
epoch: 111, train loss: 0.3477905589165004, acc: 0.8116491061915473; test loss: 0.6148502658202952, acc: 0.7369416213661073
epoch: 112, train loss: 0.3401896712186422, acc: 0.8113531431277377; test loss: 0.6241613731573614, acc: 0.735759867643583
epoch: 113, train loss: 0.33982183651499626, acc: 0.8162661299869777; test loss: 0.640866828821309, acc: 0.7128338454266131
epoch: 114, train loss: 0.33216189922870965, acc: 0.8165620930507873; test loss: 0.7434896699814784, acc: 0.6825809501299929
epoch: 115, train loss: 0.34316150101244175, acc: 0.810228483485261; test loss: 0.6595544915085452, acc: 0.7211061214842827
epoch: 116, train loss: 0.33853410437175674, acc: 0.811885876642595; test loss: 0.6698327054371087, acc: 0.7092885842590404
epoch: 117, train loss: 0.3459574710522599, acc: 0.8083343198768793; test loss: 0.6078257799655713, acc: 0.7319782557315055
epoch: 118, train loss: 0.32603905932556404, acc: 0.8178643305315496; test loss: 0.6711031177933687, acc: 0.7163791065941858
epoch: 119, train loss: 0.33643988181997986, acc: 0.810287676098023; test loss: 0.6027742403670987, acc: 0.735759867643583
epoch: 120, train loss: 0.3232342437574733, acc: 0.8182194862081212; test loss: 0.6293288194042829, acc: 0.7333963601985346
epoch: 121, train loss: 0.3257128404832894, acc: 0.8187522197229786; test loss: 0.6583046725500111, acc: 0.7107066887260695
epoch: 122, train loss: 0.3291073101955828, acc: 0.8160293595359299; test loss: 0.6146002420717536, acc: 0.7298510990309619
epoch: 123, train loss: 0.31578195772820766, acc: 0.8179235231443116; test loss: 0.6503378612384872, acc: 0.7248877333963601
epoch: 124, train loss: 0.3283052054208821, acc: 0.8123002249319285; test loss: 0.6972795639216153, acc: 0.7196880170172536
epoch: 125, train loss: 0.3228013875026653, acc: 0.8176867526932639; test loss: 0.6312270600078406, acc: 0.7270148900969038
epoch: 126, train loss: 0.3171880095826672, acc: 0.818397064046407; test loss: 0.6369966385466527, acc: 0.7348144646655637
epoch: 127, train loss: 0.32030612707307415, acc: 0.8175091748549781; test loss: 0.6769842912727596, acc: 0.7095249350035453
epoch: 128, train loss: 0.3114746124790705, acc: 0.8213566946845033; test loss: 0.6701139433393521, acc: 0.7258331363743796
epoch: 129, train loss: 0.3331365403662465, acc: 0.8107020243873565; test loss: 0.6493718939959031, acc: 0.7182699125502245
epoch: 130, train loss: 0.316621948963681, acc: 0.8196401089144075; test loss: 0.6265951864715447, acc: 0.7286693453084377
epoch: 131, train loss: 0.3088560352293545, acc: 0.8213566946845033; test loss: 0.6390246918961614, acc: 0.7291420467974474
epoch: 132, train loss: 0.29707359585646864, acc: 0.8270983781224103; test loss: 0.6564269650210671, acc: 0.7270148900969038
epoch: 133, train loss: 0.30853465050582396, acc: 0.8206463833313602; test loss: 0.641704477449028, acc: 0.7260694871188844
epoch: 134, train loss: 0.29101862389264876, acc: 0.8302355865987925; test loss: 0.6394055563629842, acc: 0.7319782557315055
epoch: 135, train loss: 0.3003918262604526, acc: 0.8241979400970759; test loss: 0.6986641741847068, acc: 0.711415740959584
epoch: 136, train loss: 0.30452614830727687, acc: 0.82248135432698; test loss: 0.7232619843790644, acc: 0.6967619948002837
epoch: 137, train loss: 0.31265726037049163, acc: 0.8166804782763111; test loss: 0.6566677405687124, acc: 0.7180335618057196
Epoch   137: reducing learning rate of group 0 to 7.5000e-04.
epoch: 138, train loss: 0.2616709055026438, acc: 0.8422516869894637; test loss: 0.6225387991193169, acc: 0.7482864571023399
epoch: 139, train loss: 0.22995190859737094, acc: 0.8569314549544217; test loss: 0.6561609960509598, acc: 0.7376506735996219
epoch: 140, train loss: 0.2176171247036454, acc: 0.8624955605540429; test loss: 0.6681100170371719, acc: 0.7421413377452138
epoch: 141, train loss: 0.22736243313376658, acc: 0.8621404048774713; test loss: 0.6531468175934494, acc: 0.7421413377452138
epoch: 142, train loss: 0.21383210858417231, acc: 0.8637386054220433; test loss: 0.6837154072640044, acc: 0.738832427322146
epoch: 143, train loss: 0.2119617205943612, acc: 0.8661655025452824; test loss: 0.7327539345469212, acc: 0.7355235168990782
epoch: 144, train loss: 0.2261999593982615, acc: 0.8563395288268024; test loss: 0.6812351556907335, acc: 0.7326873079650201
epoch: 145, train loss: 0.22157288372791828, acc: 0.8612525156860423; test loss: 0.7273360029570856, acc: 0.725124084140865
epoch: 146, train loss: 0.2117052585207613, acc: 0.8642121463241388; test loss: 0.6978430212093454, acc: 0.7445048451902624
epoch: 147, train loss: 0.20402335707105743, acc: 0.8693027110216645; test loss: 0.7009538255158229, acc: 0.7452138974237769
epoch: 148, train loss: 0.20226587230945828, acc: 0.8730910382384278; test loss: 0.7549393916349832, acc: 0.7248877333963601
epoch: 149, train loss: 0.2137847760361695, acc: 0.8647448798389961; test loss: 0.7124347860326837, acc: 0.735759867643583
epoch: 150, train loss: 0.2030829046306574, acc: 0.8706049485024269; test loss: 0.7015778272886193, acc: 0.7414322855116994
epoch: 151, train loss: 0.20181845887265884, acc: 0.8700722149875696; test loss: 0.7282886832522828, acc: 0.7315055542424959
epoch: 152, train loss: 0.20399120455488476, acc: 0.8679412809281402; test loss: 0.7497947456086677, acc: 0.7289056960529425
epoch: 153, train loss: 0.2052805684273153, acc: 0.868000473540902; test loss: 0.7169324816949709, acc: 0.7348144646655637
epoch: 154, train loss: 0.1952735056933728, acc: 0.8712560672428081; test loss: 0.7247245343856793, acc: 0.7383597258331364
epoch: 155, train loss: 0.19332043589311648, acc: 0.8723807268852847; test loss: 0.7360799636606402, acc: 0.7407232332781848
epoch: 156, train loss: 0.198934202825683, acc: 0.8674677400260448; test loss: 0.6982155253604861, acc: 0.743559442212243
epoch: 157, train loss: 0.20694603969518202, acc: 0.8693027110216645; test loss: 0.7090072800323659, acc: 0.7253604348853699
epoch: 158, train loss: 0.20098925580269375, acc: 0.8733278086894756; test loss: 0.7377781787963377, acc: 0.7322146064760104
epoch: 159, train loss: 0.19242370248236035, acc: 0.8731502308511898; test loss: 0.7078818199736546, acc: 0.7423776884897187
epoch: 160, train loss: 0.18711981957934234, acc: 0.8781816029359536; test loss: 0.7169697426701516, acc: 0.7352871661545733
epoch: 161, train loss: 0.1820969981537975, acc: 0.8790694921273825; test loss: 0.7277306341104997, acc: 0.743559442212243
epoch: 162, train loss: 0.18663806854316692, acc: 0.8782407955487155; test loss: 0.7945416686214247, acc: 0.7274875915859135
epoch: 163, train loss: 0.1932947390673216, acc: 0.8714336450810939; test loss: 0.7467638810625935, acc: 0.7350508154100686
epoch: 164, train loss: 0.18397221692824656, acc: 0.8798389960932875; test loss: 0.7375988280680185, acc: 0.7378870243441267
epoch: 165, train loss: 0.18136652864491967, acc: 0.8795430330294779; test loss: 0.8016797981485355, acc: 0.7260694871188844
epoch: 166, train loss: 0.18452816578013045, acc: 0.8776488694210962; test loss: 0.7363581524986208, acc: 0.7419049870007091
epoch: 167, train loss: 0.1711680717193842, acc: 0.8838049011483367; test loss: 0.7644449995640913, acc: 0.7369416213661073
epoch: 168, train loss: 0.18074187791614232, acc: 0.8785367586125251; test loss: 0.7982277910187012, acc: 0.7239423304183408
epoch: 169, train loss: 0.19155307003163835, acc: 0.8742748904936664; test loss: 0.7581868592065943, acc: 0.7258331363743796
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.143194029989067, acc: 0.8739789274298567; test loss: 0.6308289909723436, acc: 0.7326873079650201
epoch: 171, train loss: 0.13202366457086237, acc: 0.8779448324849058; test loss: 0.6702782397178915, acc: 0.7352871661545733
epoch: 172, train loss: 0.12911883899061105, acc: 0.8837457085355748; test loss: 0.690395097196088, acc: 0.7322146064760104
epoch: 173, train loss: 0.13953316272561106, acc: 0.8741565052681425; test loss: 0.6501347626729148, acc: 0.7277239423304184
epoch: 174, train loss: 0.13602575021949734, acc: 0.8774712915828105; test loss: 0.6548372698686726, acc: 0.7341054124320492
epoch: 175, train loss: 0.12871279601262167, acc: 0.8825026636675742; test loss: 0.6410835054686548, acc: 0.7445048451902624
epoch: 176, train loss: 0.12279848930108077, acc: 0.883568130697289; test loss: 0.7095804140095, acc: 0.7225242259513117
epoch: 177, train loss: 0.12523981595414763, acc: 0.8846335977270037; test loss: 0.6667303489866733, acc: 0.7326873079650201
epoch: 178, train loss: 0.12760207271334603, acc: 0.8811412335740499; test loss: 0.665132867193932, acc: 0.734341763176554
epoch: 179, train loss: 0.1363602899931388, acc: 0.8742156978809045; test loss: 0.6630225088319663, acc: 0.7350508154100686
epoch: 180, train loss: 0.13486265230756744, acc: 0.8791878773529064; test loss: 0.652278233549325, acc: 0.7369416213661073
epoch: 181, train loss: 0.13150397989895832, acc: 0.8738605422043328; test loss: 0.6732180563165515, acc: 0.734341763176554
epoch: 182, train loss: 0.1310821635470216, acc: 0.8759914762637623; test loss: 0.6542083908834798, acc: 0.7274875915859135
epoch: 183, train loss: 0.13030349434381747, acc: 0.8772937137445247; test loss: 0.6733380775591409, acc: 0.7319782557315055
epoch: 184, train loss: 0.14029635145344704, acc: 0.87131525985557; test loss: 0.6839202269682843, acc: 0.7125974946821082
epoch: 185, train loss: 0.13090243016677142, acc: 0.8796022256422399; test loss: 0.7019545960105134, acc: 0.711415740959584
epoch: 186, train loss: 0.12150706003632956, acc: 0.8799573813188114; test loss: 0.6780614869246891, acc: 0.7296147482864571
epoch: 187, train loss: 0.13247825082458095, acc: 0.8732094234639517; test loss: 0.6753657784368433, acc: 0.7312692034979911
epoch: 188, train loss: 0.1268286871440145, acc: 0.8790102995146206; test loss: 0.6968872646889882, acc: 0.7411959347671945
Epoch   188: reducing learning rate of group 0 to 3.7500e-04.
epoch: 189, train loss: 0.10255287568590635, acc: 0.8923878299988162; test loss: 0.68048958661736, acc: 0.7393051288111557
epoch: 190, train loss: 0.08794896994626239, acc: 0.9057653604830117; test loss: 0.6975072122189317, acc: 0.7423776884897187
epoch: 191, train loss: 0.08799231981185394, acc: 0.9049366639043447; test loss: 0.7106547691500291, acc: 0.7371779721106122
epoch: 192, train loss: 0.08683369845607034, acc: 0.9070084053510122; test loss: 0.6933786049150626, acc: 0.737414322855117
epoch: 193, train loss: 0.08539791166433092, acc: 0.9069492127382502; test loss: 0.7030120013143909, acc: 0.7440321437012527
epoch: 194, train loss: 0.08355065612284492, acc: 0.9097904581508228; test loss: 0.6999278310003981, acc: 0.737414322855117
epoch: 195, train loss: 0.08863983050780817, acc: 0.9035160411980585; test loss: 0.7283451214548939, acc: 0.7416686362562042
epoch: 196, train loss: 0.09595335220601187, acc: 0.9013259145258672; test loss: 0.7111636624215653, acc: 0.7428503899787284
epoch: 197, train loss: 0.08585529401413262, acc: 0.9082514502190127; test loss: 0.7241972897583022, acc: 0.7385960765776413
epoch: 198, train loss: 0.08094006385713749, acc: 0.9105599621167279; test loss: 0.7173706630363275, acc: 0.7383597258331364
epoch: 199, train loss: 0.08139393164890821, acc: 0.910441576891204; test loss: 0.719797332809203, acc: 0.7395414795556606
epoch: 200, train loss: 0.08156014497336642, acc: 0.9073043684148219; test loss: 0.7337983058603963, acc: 0.7381233750886316
best test acc 0.7482864571023399 at epoch 138.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9122    0.9489    0.9302      6100
           1     0.9604    0.8898    0.9238       926
           2     0.8239    0.9454    0.8805      2400
           3     0.9349    0.8683    0.9004       843
           4     0.8884    0.9561    0.9210       774
           5     0.9150    0.9398    0.9272      1512
           6     0.7787    0.7910    0.7848      1330
           7     0.9079    0.7173    0.8014       481
           8     0.8034    0.7314    0.7657       458
           9     0.9355    0.9624    0.9487       452
          10     0.9204    0.8550    0.8865       717
          11     0.9516    0.7087    0.8124       333
          12     1.0000    0.0067    0.0133       299
          13     0.8117    0.7212    0.7638       269

    accuracy                         0.8871     16894
   macro avg     0.8960    0.7887    0.8043     16894
weighted avg     0.8907    0.8871    0.8785     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8091    0.8590    0.8333      1525
           1     0.8178    0.7543    0.7848       232
           2     0.6745    0.8070    0.7348       601
           3     0.8054    0.7062    0.7525       211
           4     0.8093    0.8093    0.8093       194
           5     0.7690    0.8280    0.7975       378
           6     0.5569    0.5736    0.5651       333
           7     0.8125    0.5372    0.6468       121
           8     0.5446    0.5304    0.5374       115
           9     0.7321    0.7193    0.7257       114
          10     0.7469    0.6722    0.7076       180
          11     0.6667    0.2619    0.3761        84
          12     0.0000    0.0000    0.0000        75
          13     0.7143    0.5147    0.5983        68

    accuracy                         0.7483      4231
   macro avg     0.6757    0.6124    0.6335      4231
weighted avg     0.7364    0.7483    0.7380      4231

---------------------------------------
program finished.
