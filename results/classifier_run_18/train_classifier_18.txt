seed:  666
save trained model at:  ../trained_models/trained_classifier_model_18.pt
save loss at:  ./results/train_classifier_results_18.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  15
number of pockets in training set:  17282
number of pockets in validation set:  3698
number of pockets in test set:  3720
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=15, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2ba8091dfac0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.1002366490116304, acc: 0.3497280407360259, val loss: 1.921502070016897, acc: 0.40048674959437536, test loss: 1.9503566547106672, acc: 0.38467741935483873
epoch: 2, train loss: 1.8166065007748255, acc: 0.42842263626895033, val loss: 1.6985400803608788, acc: 0.4670091941590049, test loss: 1.7091459976729526, acc: 0.4629032258064516
epoch: 3, train loss: 1.7239072726095954, acc: 0.46024765652123595, val loss: 1.6144974311665627, acc: 0.4881016765819362, test loss: 1.6166636026033792, acc: 0.49946236559139784
epoch: 4, train loss: 1.6329024883487022, acc: 0.4898159935192686, val loss: 1.6172410045332881, acc: 0.4808004326663061, test loss: 1.6243390544768304, acc: 0.48575268817204303
epoch: 5, train loss: 1.5660879588102303, acc: 0.5115148709640088, val loss: 1.5339610819045502, acc: 0.5235262303948081, test loss: 1.5274735153362315, acc: 0.5241935483870968
epoch: 6, train loss: 1.5008042266614154, acc: 0.5273116537437796, val loss: 1.5216339218610038, acc: 0.5094645754461872, test loss: 1.5346107354728125, acc: 0.510752688172043
epoch: 7, train loss: 1.459840305328038, acc: 0.5407360259229256, val loss: 1.5577827986670287, acc: 0.4983775013520822, test loss: 1.5386837584998019, acc: 0.5053763440860215
epoch: 8, train loss: 1.4399465400578593, acc: 0.5497048952667515, val loss: 1.4831705954733767, acc: 0.5283937263385614, test loss: 1.4799321400221959, acc: 0.5287634408602151
epoch: 9, train loss: 1.4083290328003855, acc: 0.5594838560351811, val loss: 1.4974030663091598, acc: 0.5348837209302325, test loss: 1.5008152074711296, acc: 0.5370967741935484
epoch: 10, train loss: 1.3831927916363784, acc: 0.5651544960074065, val loss: 1.4317420292313257, acc: 0.5429962141698216, test loss: 1.4301104955775763, acc: 0.5438172043010753
epoch: 11, train loss: 1.3487891655748552, acc: 0.575396366161324, val loss: 1.4448360449046687, acc: 0.5421849648458626, test loss: 1.4443340803987237, acc: 0.5440860215053763
epoch: 12, train loss: 1.3358893188085426, acc: 0.5830343710218725, val loss: 1.3303345901505763, acc: 0.5789616008653327, test loss: 1.3121803109363843, acc: 0.5755376344086022
epoch: 13, train loss: 1.2953658667224355, acc: 0.5947806966786252, val loss: 1.358439200887685, acc: 0.5738236884802596, test loss: 1.362091548981205, acc: 0.571505376344086
epoch: 14, train loss: 1.274922836338697, acc: 0.5977317440111098, val loss: 1.47990700849138, acc: 0.533802055164954, test loss: 1.4790451003659155, acc: 0.5346774193548387
epoch: 15, train loss: 1.2533962954085343, acc: 0.6031130656174054, val loss: 1.277420446123543, acc: 0.5884261763115197, test loss: 1.2782544448811521, acc: 0.5889784946236559
epoch: 16, train loss: 1.242455338013557, acc: 0.610751070477954, val loss: 1.3850635373315145, acc: 0.5502974580854516, test loss: 1.3786380967786236, acc: 0.5612903225806452
epoch: 17, train loss: 1.2287769297347164, acc: 0.6135285267908807, val loss: 1.2663373461022385, acc: 0.5927528393726339, test loss: 1.2601301290655649, acc: 0.5956989247311828
epoch: 18, train loss: 1.184546243467067, acc: 0.6259113528526791, val loss: 1.2336971403909025, acc: 0.6095186587344511, test loss: 1.223183105325186, acc: 0.6091397849462366
epoch: 19, train loss: 1.1737721619707002, acc: 0.6282837634533041, val loss: 1.2166012656437248, acc: 0.6168199026500811, test loss: 1.2306368761165167, acc: 0.6150537634408603
epoch: 20, train loss: 1.162902013196836, acc: 0.637368360143502, val loss: 1.2114148542260788, acc: 0.6116819902650081, test loss: 1.2174364956476356, acc: 0.6206989247311828
epoch: 21, train loss: 1.1423229496176355, acc: 0.6411873625737762, val loss: 1.2223201079133912, acc: 0.6060032449972959, test loss: 1.2238646625190652, acc: 0.6120967741935484
epoch: 22, train loss: 1.137467524586225, acc: 0.6405508621687305, val loss: 1.3103928670424136, acc: 0.5800432666306111, test loss: 1.308565281283471, acc: 0.5798387096774194
epoch: 23, train loss: 1.1047959304796768, acc: 0.6541488253674344, val loss: 1.362329443446232, acc: 0.5781503515413737, test loss: 1.3904913589518557, acc: 0.5790322580645161
epoch: 24, train loss: 1.1140872439001601, acc: 0.6518921421131814, val loss: 1.1699280358443975, acc: 0.6400757166035695, test loss: 1.1675017146654028, acc: 0.6427419354838709
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.8693260513457307, acc: 0.6587779192223122, val loss: 0.924807638268396, acc: 0.638182801514332, test loss: 0.924155455763622, acc: 0.6381720430107527
epoch: 26, train loss: 0.8450586854438927, acc: 0.6668788334683485, val loss: 0.9571025853804346, acc: 0.6333153055705787, test loss: 0.9741578819931195, acc: 0.6225806451612903
epoch: 27, train loss: 0.848958611074469, acc: 0.6625969216525865, val loss: 0.9780948436730871, acc: 0.6371011357490536, test loss: 1.0137469322450698, acc: 0.6174731182795699
epoch: 28, train loss: 0.840556491388379, acc: 0.6627705126721444, val loss: 0.872120457381542, acc: 0.65440778799351, test loss: 0.8918112170311713, acc: 0.6532258064516129
epoch: 29, train loss: 0.8215252029207374, acc: 0.6762527485244764, val loss: 0.9647545523099348, acc: 0.6270957274202271, test loss: 0.9712097552514846, acc: 0.632258064516129
epoch: 30, train loss: 0.8142693070070761, acc: 0.674806156694827, val loss: 0.9562977817395109, acc: 0.6371011357490536, test loss: 0.9677463823749173, acc: 0.6295698924731182
epoch: 31, train loss: 0.8202244008033036, acc: 0.6705242448790649, val loss: 0.9544933701025724, acc: 0.6265548945375878, test loss: 0.9751124100018573, acc: 0.6163978494623656
epoch: 32, train loss: 0.8159259625116358, acc: 0.6752112024071288, val loss: 1.1319587709066867, acc: 0.5881557598702001, test loss: 1.1480764430056336, acc: 0.5811827956989247
epoch: 33, train loss: 0.8021280861069188, acc: 0.6801296146279365, val loss: 0.9154780573816542, acc: 0.6430502974580855, test loss: 0.9295230168168263, acc: 0.638978494623656
epoch: 34, train loss: 0.7835553554925166, acc: 0.688230528873973, val loss: 1.023361371103785, acc: 0.6030286641427799, test loss: 1.0278221971245223, acc: 0.6048387096774194
epoch: 35, train loss: 0.7957787406746677, acc: 0.6798981599351926, val loss: 0.9182891501421023, acc: 0.6471065440778799, test loss: 0.9413412970881309, acc: 0.6446236559139785
epoch: 36, train loss: 0.7797096745171297, acc: 0.6910079851868997, val loss: 1.063177626219358, acc: 0.591941590048675, test loss: 1.068133290095996, acc: 0.5983870967741935
epoch: 37, train loss: 0.7657390435858604, acc: 0.6941326235389422, val loss: 0.9143825614046187, acc: 0.6508923742563548, test loss: 0.9150919765554448, acc: 0.6532258064516129
epoch: 38, train loss: 0.7473879169085114, acc: 0.7024071288045365, val loss: 0.9883064626680316, acc: 0.6392644672796106, test loss: 1.000873916892595, acc: 0.6341397849462366
epoch: 39, train loss: 0.7505285095153149, acc: 0.6984145353547043, val loss: 0.9266306020686148, acc: 0.6449432125473229, test loss: 0.915873178359001, acc: 0.6422043010752688
Epoch    39: reducing learning rate of group 0 to 1.5000e-03.
epoch: 40, train loss: 0.675240281706758, acc: 0.7275778266404351, val loss: 0.8295135675990434, acc: 0.688209843158464, test loss: 0.8384083009535267, acc: 0.6793010752688172
epoch: 41, train loss: 0.6333648437441616, acc: 0.7408864714732091, val loss: 0.8751979475088414, acc: 0.6765819361817198, test loss: 0.9081272468771986, acc: 0.6658602150537635
epoch: 42, train loss: 0.6266019977969768, acc: 0.7431431547274621, val loss: 0.8602312223662423, acc: 0.683071930773391, test loss: 0.8512899157821491, acc: 0.6733870967741935
epoch: 43, train loss: 0.6178760531737482, acc: 0.7441268371716236, val loss: 0.8391939983811489, acc: 0.6784748512709573, test loss: 0.839949196128435, acc: 0.6755376344086022
epoch: 44, train loss: 0.6220510695670686, acc: 0.7445897465571114, val loss: 0.9205925521365754, acc: 0.6584640346133045, test loss: 0.8979297227756952, acc: 0.6610215053763441
epoch: 45, train loss: 0.6167763763039135, acc: 0.7475986575627821, val loss: 0.8045105740726155, acc: 0.6946998377501352, test loss: 0.8055820398433234, acc: 0.6913978494623656
epoch: 46, train loss: 0.6081884383824174, acc: 0.7480615669482699, val loss: 0.8116661822750737, acc: 0.6976744186046512, test loss: 0.8326244067120295, acc: 0.6900537634408602
epoch: 47, train loss: 0.6115120909043228, acc: 0.7463256567526907, val loss: 0.83894999044015, acc: 0.6825310978907517, test loss: 0.8239627925298547, acc: 0.6836021505376344
epoch: 48, train loss: 0.589144827142494, acc: 0.7581877097558153, val loss: 0.8766437078437784, acc: 0.674959437533802, test loss: 0.8738068442190847, acc: 0.6688172043010753
epoch: 49, train loss: 0.5846166306038089, acc: 0.7618331211665317, val loss: 0.8414366102141261, acc: 0.6995673336938886, test loss: 0.8366546271949686, acc: 0.689247311827957
epoch: 50, train loss: 0.578951927512184, acc: 0.7574354820043976, val loss: 0.8423317759149199, acc: 0.6822606814494321, test loss: 0.8368229691700269, acc: 0.6865591397849462
epoch: 51, train loss: 0.5686122126469471, acc: 0.7614280754542299, val loss: 0.8406644158907488, acc: 0.6895619253650622, test loss: 0.8413793815079555, acc: 0.6790322580645162
epoch: 52, train loss: 0.5735584656724002, acc: 0.7591135285267909, val loss: 0.8203351576306357, acc: 0.6909140075716603, test loss: 0.8196978215248354, acc: 0.6895161290322581
epoch: 53, train loss: 0.5714400168329378, acc: 0.7617752574933456, val loss: 0.8621013022681195, acc: 0.6855056787452677, test loss: 0.8498745420927643, acc: 0.6793010752688172
epoch: 54, train loss: 0.5583181390399776, acc: 0.7681402615438028, val loss: 0.8759354402594723, acc: 0.6857760951865873, test loss: 0.8845351393504809, acc: 0.6865591397849462
epoch: 55, train loss: 0.5785893678334062, acc: 0.760907302395556, val loss: 0.8234817306050228, acc: 0.69118442401298, test loss: 0.8257563365403042, acc: 0.6948924731182796
epoch: 56, train loss: 0.5284783493422646, acc: 0.7768776761948849, val loss: 0.8505601225317072, acc: 0.6944294213088156, test loss: 0.8535407035581527, acc: 0.6940860215053763
epoch: 57, train loss: 0.5284630806823252, acc: 0.776530494155769, val loss: 0.858508630853656, acc: 0.6960519199567333, test loss: 0.8505569965608658, acc: 0.6884408602150538
epoch: 58, train loss: 0.5257727379269683, acc: 0.7778034949658604, val loss: 0.8621802710403682, acc: 0.6887506760411033, test loss: 0.8535433605153073, acc: 0.6887096774193548
epoch: 59, train loss: 0.5449542138342255, acc: 0.7726536280523087, val loss: 0.8135252268395596, acc: 0.7011898323418064, test loss: 0.8239811128185641, acc: 0.6994623655913978
epoch: 60, train loss: 0.5064081526743485, acc: 0.7839370443235737, val loss: 0.9242538554788345, acc: 0.678745267712277, test loss: 0.9304493699022519, acc: 0.6771505376344086
epoch: 61, train loss: 0.5513712836559028, acc: 0.7716120819349612, val loss: 0.817678188748976, acc: 0.7071389940508382, test loss: 0.8397369615493282, acc: 0.6946236559139785
epoch: 62, train loss: 0.4941409866251944, acc: 0.7885082745052656, val loss: 0.7963460198346185, acc: 0.7084910762574365, test loss: 0.7999222863105035, acc: 0.703763440860215
epoch: 63, train loss: 0.49417923220315063, acc: 0.7909385487790765, val loss: 0.8685882385515922, acc: 0.6936181719848566, test loss: 0.8923668569134128, acc: 0.6841397849462365
epoch: 64, train loss: 0.4954827426622455, acc: 0.789897002661729, val loss: 0.8562867182406172, acc: 0.7041644131963224, test loss: 0.8752794614402196, acc: 0.693010752688172
epoch: 65, train loss: 0.5031277589011063, acc: 0.7878717741002199, val loss: 0.8293461099452364, acc: 0.7030827474310438, test loss: 0.8556399991435389, acc: 0.693010752688172
epoch: 66, train loss: 0.49139004962309935, acc: 0.791922231223238, val loss: 0.8602287740949813, acc: 0.7028123309897242, test loss: 0.8545738702179283, acc: 0.693010752688172
epoch: 67, train loss: 0.4817719932935702, acc: 0.7960305520194422, val loss: 0.8545185739507928, acc: 0.6976744186046512, test loss: 0.8563298999622304, acc: 0.6948924731182796
epoch: 68, train loss: 0.4905525199959447, acc: 0.7942367781506771, val loss: 0.8717473676360704, acc: 0.686046511627907, test loss: 0.877301303801998, acc: 0.6854838709677419
epoch: 69, train loss: 0.4767334949755031, acc: 0.7952783242680246, val loss: 0.8681171921667632, acc: 0.6979448350459708, test loss: 0.878436924821587, acc: 0.6922043010752689
epoch: 70, train loss: 0.4727944367952251, acc: 0.8000231454692744, val loss: 0.9013810398902294, acc: 0.6795565170362358, test loss: 0.904046078138454, acc: 0.6790322580645162
epoch: 71, train loss: 0.4703043144363977, acc: 0.7988658720055549, val loss: 0.8603178216935236, acc: 0.6982152514872905, test loss: 0.8675166550502982, acc: 0.6973118279569892
epoch: 72, train loss: 0.5113147184150995, acc: 0.7870616826756163, val loss: 0.8151033685425283, acc: 0.7041644131963224, test loss: 0.8380080715302498, acc: 0.693010752688172
epoch: 73, train loss: 0.45600587368949586, acc: 0.8048836940168962, val loss: 0.9127917261366073, acc: 0.6771227690643591, test loss: 0.9336957244462865, acc: 0.6771505376344086
epoch: 74, train loss: 0.4478891250429406, acc: 0.8034371021872468, val loss: 0.8634054204333209, acc: 0.7017306652244456, test loss: 0.8981377309368502, acc: 0.6997311827956989
epoch: 75, train loss: 0.43920945599045746, acc: 0.8114801527600972, val loss: 0.8948054610232266, acc: 0.6965927528393726, test loss: 0.9214128571171915, acc: 0.6940860215053763
epoch: 76, train loss: 0.44731710545098724, acc: 0.8050572850364541, val loss: 0.8926729410644993, acc: 0.6906435911303407, test loss: 0.9020549440896639, acc: 0.6766129032258065
epoch: 77, train loss: 0.4397862628231638, acc: 0.8110172433746095, val loss: 0.8629087860227083, acc: 0.7074094104921579, test loss: 0.8755174672731789, acc: 0.696236559139785
epoch: 78, train loss: 0.4227310832128137, acc: 0.816572156000463, val loss: 0.9261287090390228, acc: 0.6928069226608978, test loss: 0.9208883413704493, acc: 0.6908602150537635
epoch: 79, train loss: 0.452237942768956, acc: 0.8044786483045944, val loss: 0.9274724246747691, acc: 0.6952406706327745, test loss: 0.905416460447414, acc: 0.6876344086021505
epoch: 80, train loss: 0.4574191936594291, acc: 0.8067353315588474, val loss: 0.915169494096623, acc: 0.6941590048674959, test loss: 0.904750717327159, acc: 0.685752688172043
epoch: 81, train loss: 0.4323544677395661, acc: 0.8161671102881611, val loss: 0.8634040213843305, acc: 0.7144402379664684, test loss: 0.8703353604962749, acc: 0.6970430107526882
epoch: 82, train loss: 0.41596666786174313, acc: 0.8168614743663928, val loss: 0.8326201869578798, acc: 0.7098431584640346, test loss: 0.8564705023201563, acc: 0.7008064516129032
epoch: 83, train loss: 0.40457260914249504, acc: 0.8226478416849902, val loss: 0.972249275932704, acc: 0.6838831800973499, test loss: 0.9733088816365888, acc: 0.6798387096774193
epoch: 84, train loss: 0.4336806035555012, acc: 0.8081819233884967, val loss: 0.8267458906168548, acc: 0.7171444023796647, test loss: 0.8379186732794649, acc: 0.7077956989247312
epoch: 85, train loss: 0.4128912780871478, acc: 0.8161092466149751, val loss: 0.9011202710844363, acc: 0.6887506760411033, test loss: 0.8921778863476169, acc: 0.6946236559139785
epoch: 86, train loss: 0.41148003503316366, acc: 0.8211433861821549, val loss: 0.8989348833208924, acc: 0.7044348296376419, test loss: 0.9064300290999874, acc: 0.7016129032258065
epoch: 87, train loss: 0.4075318429911803, acc: 0.8223006596458743, val loss: 0.8890664706428738, acc: 0.6855056787452677, test loss: 0.8904872971196328, acc: 0.6860215053763441
epoch: 88, train loss: 0.3961806281506084, acc: 0.8245573429001273, val loss: 0.8753382090686656, acc: 0.7049756625202812, test loss: 0.8886511664236746, acc: 0.6900537634408602
epoch: 89, train loss: 0.3962981324479503, acc: 0.8270454808471241, val loss: 0.9924960306620456, acc: 0.6806381828015143, test loss: 0.9836720138467768, acc: 0.6870967741935484
epoch: 90, train loss: 0.39452963342491143, acc: 0.8287235273695174, val loss: 0.943195637656006, acc: 0.7025419145484045, test loss: 0.9462983669773225, acc: 0.693010752688172
epoch: 91, train loss: 0.3974705409905712, acc: 0.8249623886124291, val loss: 0.9383507482808239, acc: 0.6917252568956193, test loss: 0.9557959223306307, acc: 0.6870967741935484
epoch: 92, train loss: 0.39762575966646074, acc: 0.8233422057632218, val loss: 0.8795552543848512, acc: 0.7017306652244456, test loss: 0.8703496733019429, acc: 0.7045698924731183
epoch: 93, train loss: 0.38143322729509804, acc: 0.8313273926628862, val loss: 0.8797828733373037, acc: 0.7076798269334775, test loss: 0.885881084011447, acc: 0.6959677419354838
epoch: 94, train loss: 0.36452816477281025, acc: 0.8390232611966207, val loss: 0.9071656704722643, acc: 0.7065981611681991, test loss: 0.8881697987997403, acc: 0.7112903225806452
epoch: 95, train loss: 0.3869794441958073, acc: 0.8283763453304016, val loss: 0.9125167655583779, acc: 0.7055164954029205, test loss: 0.8872529188791911, acc: 0.7134408602150538
Epoch    95: reducing learning rate of group 0 to 7.5000e-04.
epoch: 96, train loss: 0.3116880010957037, acc: 0.8577710913088763, val loss: 0.8808131326270013, acc: 0.7239048134126554, test loss: 0.8581143220265707, acc: 0.7228494623655914
epoch: 97, train loss: 0.2746783045034637, acc: 0.8748987385719246, val loss: 0.902843940702885, acc: 0.7341806381828015, test loss: 0.877552757981003, acc: 0.7306451612903225
epoch: 98, train loss: 0.26183456860542187, acc: 0.8769239671334337, val loss: 0.9239678693630039, acc: 0.7274202271498107, test loss: 0.8993785699208577, acc: 0.728494623655914
epoch: 99, train loss: 0.26881975057969115, acc: 0.8764031940747599, val loss: 0.9074056521178968, acc: 0.7255273120605733, test loss: 0.9103788263054304, acc: 0.7244623655913979
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.1836119872744113, acc: 0.8853141997453998, val loss: 0.8022234402455918, acc: 0.7255273120605733, test loss: 0.7678044344789239, acc: 0.728763440860215
epoch: 101, train loss: 0.1952371575354757, acc: 0.8794699687536165, val loss: 0.8076228232819561, acc: 0.7274202271498107, test loss: 0.7812622239512782, acc: 0.7266129032258064
epoch: 102, train loss: 0.1763101755665651, acc: 0.8887281564633723, val loss: 0.8282202828439782, acc: 0.7290427257977286, test loss: 0.8161353147158059, acc: 0.7233870967741935
epoch: 103, train loss: 0.18010249674603035, acc: 0.8846776993403541, val loss: 0.8411369122576882, acc: 0.725797728501893, test loss: 0.8272276447665307, acc: 0.7217741935483871
epoch: 104, train loss: 0.17517415458868257, acc: 0.8864714732091193, val loss: 0.8430409323144049, acc: 0.7276906435911303, test loss: 0.7848568572792956, acc: 0.7260752688172043
epoch: 105, train loss: 0.17897360207733493, acc: 0.887281564633723, val loss: 0.8507469588837281, acc: 0.72065981611682, test loss: 0.8385540367454611, acc: 0.706989247311828
epoch: 106, train loss: 0.279259291003595, acc: 0.8379817150792732, val loss: 0.7491771301880213, acc: 0.722011898323418, test loss: 0.7464270032862181, acc: 0.7239247311827957
epoch: 107, train loss: 0.20187505565953606, acc: 0.8707325541025345, val loss: 0.8099421045469425, acc: 0.7233639805300163, test loss: 0.7716508050118723, acc: 0.7228494623655914
epoch: 108, train loss: 0.18240859399424317, acc: 0.882768198125217, val loss: 0.8330071249673533, acc: 0.7266089778258518, test loss: 0.808528374087426, acc: 0.7258064516129032
epoch: 109, train loss: 0.1785162128003683, acc: 0.8818423793542414, val loss: 0.8325496591575214, acc: 0.7266089778258518, test loss: 0.809141900462489, acc: 0.7193548387096774
epoch: 110, train loss: 0.18057997509218066, acc: 0.8813216062955677, val loss: 0.8329300688304664, acc: 0.7138994050838291, test loss: 0.8095825338876376, acc: 0.7158602150537634
epoch: 111, train loss: 0.18106045659569842, acc: 0.8809744242564518, val loss: 0.8413553397806353, acc: 0.7195781503515414, test loss: 0.8228603798856018, acc: 0.7196236559139785
epoch: 112, train loss: 0.16089269979286216, acc: 0.8896539752343479, val loss: 0.8724676192032447, acc: 0.7233639805300163, test loss: 0.8396198595723798, acc: 0.714247311827957
epoch: 113, train loss: 0.17117748300436814, acc: 0.8866450642286773, val loss: 0.8798924005631178, acc: 0.72065981611682, test loss: 0.8493544963098342, acc: 0.7260752688172043
epoch: 114, train loss: 0.1725482407363666, acc: 0.8839254715889364, val loss: 0.8736130356982053, acc: 0.7312060573282856, test loss: 0.8267516284860591, acc: 0.728225806451613
epoch: 115, train loss: 0.17209982120312, acc: 0.8864714732091193, val loss: 0.8684791180170698, acc: 0.73526230394808, test loss: 0.8225346288373393, acc: 0.7295698924731183
epoch: 116, train loss: 0.17222768512365097, acc: 0.8851406087258419, val loss: 0.8732914769888053, acc: 0.7155219037317468, test loss: 0.8472486516480805, acc: 0.7120967741935483
epoch: 117, train loss: 0.1742615190169392, acc: 0.8846198356671682, val loss: 0.8757878554196149, acc: 0.7195781503515414, test loss: 0.8699678262074788, acc: 0.7126344086021505
epoch: 118, train loss: 0.1616848164731989, acc: 0.8885545654438144, val loss: 0.8734839542805665, acc: 0.7314764737696052, test loss: 0.8491477110052621, acc: 0.7161290322580646
epoch: 119, train loss: 0.1582571844951986, acc: 0.896076842957991, val loss: 0.8467224510764354, acc: 0.7290427257977286, test loss: 0.8237868878149217, acc: 0.7220430107526882
epoch: 120, train loss: 0.15525829014975692, acc: 0.8950352968406434, val loss: 0.8701254689674109, acc: 0.7247160627366144, test loss: 0.8775014308191115, acc: 0.7209677419354839
epoch: 121, train loss: 0.1612227679906158, acc: 0.8906955213516954, val loss: 0.867985606515904, acc: 0.725797728501893, test loss: 0.8449532606268442, acc: 0.7193548387096774
epoch: 122, train loss: 0.16080648779089618, acc: 0.8919685221617868, val loss: 0.8824895913695051, acc: 0.7263385613845322, test loss: 0.8579105710470548, acc: 0.7220430107526882
epoch: 123, train loss: 0.16021290624456236, acc: 0.8919685221617868, val loss: 0.899309677714074, acc: 0.7120064899945917, test loss: 0.8789775740715765, acc: 0.6997311827956989
epoch: 124, train loss: 0.1692775799287661, acc: 0.887281564633723, val loss: 0.8717640321792429, acc: 0.7125473228772309, test loss: 0.8839821384799096, acc: 0.7045698924731183
epoch: 125, train loss: 0.18067974955474678, acc: 0.8801064691586622, val loss: 0.846588668467355, acc: 0.7217414818820984, test loss: 0.8302057045762257, acc: 0.7155913978494624
epoch: 126, train loss: 0.15766200346329218, acc: 0.8932415229718783, val loss: 0.8687797535297482, acc: 0.733098972417523, test loss: 0.8221916080803, acc: 0.7303763440860215
epoch: 127, train loss: 0.1462038357098364, acc: 0.8970026617289666, val loss: 0.9313126115814424, acc: 0.72065981611682, test loss: 0.890543657733548, acc: 0.714516129032258
epoch: 128, train loss: 0.14535357513826155, acc: 0.8965397523434787, val loss: 0.9303807147197817, acc: 0.7209302325581395, test loss: 0.8901071753553165, acc: 0.7198924731182795
epoch: 129, train loss: 0.14558260690323002, acc: 0.8995486633491494, val loss: 0.8940315985434631, acc: 0.7252568956192537, test loss: 0.8780467576878045, acc: 0.7174731182795699
epoch: 130, train loss: 0.15374295086189355, acc: 0.893820159703738, val loss: 0.9103352579959604, acc: 0.7230935640886966, test loss: 0.8951178243083339, acc: 0.7091397849462365
epoch: 131, train loss: 0.1485797240010926, acc: 0.8962504339775489, val loss: 0.9253817046894262, acc: 0.7222823147647377, test loss: 0.8764769600283715, acc: 0.717741935483871
epoch: 132, train loss: 0.14703630231697656, acc: 0.8963661613239209, val loss: 0.921073650398275, acc: 0.717685235262304, test loss: 0.8834784456478653, acc: 0.7163978494623656
epoch: 133, train loss: 0.1502465601752249, acc: 0.8964240249971068, val loss: 0.9338109775644305, acc: 0.6987560843699296, test loss: 0.8949004624479561, acc: 0.7064516129032258
epoch: 134, train loss: 0.14254478070211968, acc: 0.8971762527485245, val loss: 0.9179392469580461, acc: 0.7247160627366144, test loss: 0.8831239992572415, acc: 0.7166666666666667
epoch: 135, train loss: 0.14075236895572477, acc: 0.8998379817150792, val loss: 0.9311052812892981, acc: 0.7163331530557058, test loss: 0.8879613871215493, acc: 0.7120967741935483
epoch: 136, train loss: 0.15391702046032624, acc: 0.8948038421478995, val loss: 0.9579977111084, acc: 0.7122769064359114, test loss: 0.9082148695504794, acc: 0.7118279569892473
epoch: 137, train loss: 0.1541706940605193, acc: 0.891679203795857, val loss: 0.9113368944195943, acc: 0.7182260681449432, test loss: 0.9012161321537469, acc: 0.7131720430107527
epoch: 138, train loss: 0.15482913366885473, acc: 0.8959611156116191, val loss: 0.8990362805892095, acc: 0.7244456462952947, test loss: 0.8703374375579178, acc: 0.7155913978494624
epoch: 139, train loss: 0.16803431382624587, acc: 0.8857192454577016, val loss: 0.9024173569975833, acc: 0.7074094104921579, test loss: 0.8649610268172397, acc: 0.7043010752688172
epoch: 140, train loss: 0.1638718035458801, acc: 0.8866450642286773, val loss: 0.9308149463876381, acc: 0.7114656571119524, test loss: 0.8675131474771808, acc: 0.7174731182795699
epoch: 141, train loss: 0.143938263455084, acc: 0.8991436176368476, val loss: 0.9134712537731462, acc: 0.7222823147647377, test loss: 0.88921798018999, acc: 0.7209677419354839
epoch: 142, train loss: 0.13459274997942675, acc: 0.9000694364078232, val loss: 0.9413333291166728, acc: 0.717685235262304, test loss: 0.9106627956513436, acc: 0.7147849462365592
epoch: 143, train loss: 0.12628587979955705, acc: 0.9066080314778382, val loss: 0.9670528501610681, acc: 0.7225527312060573, test loss: 0.9456661706329674, acc: 0.7104838709677419
epoch: 144, train loss: 0.1520250458509582, acc: 0.8932415229718783, val loss: 0.9125372667193993, acc: 0.7222823147647377, test loss: 0.8735335924292124, acc: 0.728763440860215
epoch: 145, train loss: 0.13994398301825342, acc: 0.9025575743548201, val loss: 0.9483271724407966, acc: 0.7117360735532721, test loss: 0.9167550815049038, acc: 0.710752688172043
epoch: 146, train loss: 0.1483087906502502, acc: 0.8953824788797593, val loss: 0.9232949502665394, acc: 0.722823147647377, test loss: 0.8908628863673056, acc: 0.7236559139784946
Epoch   146: reducing learning rate of group 0 to 3.7500e-04.
epoch: 147, train loss: 0.11575776903428023, acc: 0.9130308992014813, val loss: 0.9439188372837395, acc: 0.7287723093564089, test loss: 0.869380417177754, acc: 0.7298387096774194
epoch: 148, train loss: 0.09366665100843705, acc: 0.9272074991320449, val loss: 0.9626302565672128, acc: 0.72796106003245, test loss: 0.9042172206345425, acc: 0.7330645161290322
epoch: 149, train loss: 0.08653741760767661, acc: 0.9309686378891332, val loss: 0.9862843739657611, acc: 0.73526230394808, test loss: 0.9227135612118629, acc: 0.7352150537634409
epoch: 150, train loss: 0.08922001624283947, acc: 0.9296956370790418, val loss: 1.002330692346964, acc: 0.7209302325581395, test loss: 1.0108464235900552, acc: 0.706989247311828
epoch: 151, train loss: 0.11430594839216893, acc: 0.915866219187594, val loss: 0.99650407946387, acc: 0.7255273120605733, test loss: 0.9643836364951185, acc: 0.7198924731182795
epoch: 152, train loss: 0.08543265167744041, acc: 0.9309686378891332, val loss: 0.9982216559338402, acc: 0.72796106003245, test loss: 0.9543580003964004, acc: 0.728225806451613
epoch: 153, train loss: 0.08401514322320947, acc: 0.9333989121629441, val loss: 1.03950972386861, acc: 0.7293131422390481, test loss: 1.0073326890186598, acc: 0.725268817204301
epoch: 154, train loss: 0.08214314404895302, acc: 0.933630366855688, val loss: 1.0707760402613424, acc: 0.7225527312060573, test loss: 1.0113362040570988, acc: 0.7212365591397849
epoch: 155, train loss: 0.07993870374850412, acc: 0.9359449137831269, val loss: 1.0617229772168537, acc: 0.7276906435911303, test loss: 1.0019530721890029, acc: 0.7268817204301076
epoch: 156, train loss: 0.07779454999297129, acc: 0.9370443235736604, val loss: 1.0691220505035137, acc: 0.72796106003245, test loss: 1.0357677223861859, acc: 0.7201612903225807
epoch: 157, train loss: 0.08133065665179333, acc: 0.9352505497048953, val loss: 1.0749376360437817, acc: 0.7225527312060573, test loss: 1.0205271828559137, acc: 0.7239247311827957
epoch: 158, train loss: 0.08230218875447491, acc: 0.9332831848165721, val loss: 1.0628942379505328, acc: 0.7295835586803677, test loss: 1.0240761121114095, acc: 0.7295698924731183
epoch: 159, train loss: 0.07815185473526315, acc: 0.9366971415345446, val loss: 1.0807688523783048, acc: 0.724986479177934, test loss: 1.0246432258236793, acc: 0.7244623655913979
epoch: 160, train loss: 0.07836189261164368, acc: 0.9364078231686147, val loss: 1.095779691018434, acc: 0.7260681449432126, test loss: 1.0287030317450083, acc: 0.728763440860215
epoch: 161, train loss: 0.0796970205139871, acc: 0.9368707325541026, val loss: 1.0471338421154692, acc: 0.7306652244456463, test loss: 1.0023069889314713, acc: 0.7276881720430107
epoch: 162, train loss: 0.0824134295864274, acc: 0.9371021872468465, val loss: 1.0543603790070701, acc: 0.7293131422390481, test loss: 1.0097558498382568, acc: 0.7247311827956989
epoch: 163, train loss: 0.07526678772518831, acc: 0.9399953709061452, val loss: 1.090553765944238, acc: 0.7241752298539751, test loss: 1.02305816219699, acc: 0.7293010752688172
epoch: 164, train loss: 0.0756540131084677, acc: 0.9408054623307488, val loss: 1.1011754122858373, acc: 0.7282314764737696, test loss: 1.0575894842865647, acc: 0.7258064516129032
epoch: 165, train loss: 0.07359472533047523, acc: 0.942483508853142, val loss: 1.0887556878085263, acc: 0.722823147647377, test loss: 1.0271088533504036, acc: 0.7239247311827957
epoch: 166, train loss: 0.07884225707051518, acc: 0.9391852794815415, val loss: 1.045466744880408, acc: 0.7314764737696052, test loss: 1.0158487248164352, acc: 0.7306451612903225
epoch: 167, train loss: 0.07582199491345762, acc: 0.9391852794815415, val loss: 1.0669446794841018, acc: 0.7314764737696052, test loss: 1.0309575209053614, acc: 0.7244623655913979
epoch: 168, train loss: 0.07481981848310837, acc: 0.9398217798865872, val loss: 1.109705462205597, acc: 0.7212006489994591, test loss: 1.0590817415586082, acc: 0.7185483870967742
epoch: 169, train loss: 0.07792729155020697, acc: 0.937854414998264, val loss: 1.081585808274809, acc: 0.7287723093564089, test loss: 1.032139632522419, acc: 0.7276881720430107
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.052827933492580234, acc: 0.9403425529452609, val loss: 1.038851577053204, acc: 0.7241752298539751, test loss: 0.9614255443696053, acc: 0.7201612903225807
epoch: 171, train loss: 0.058630198289858715, acc: 0.9325309570651545, val loss: 1.0118831267803021, acc: 0.7293131422390481, test loss: 0.9816673166008406, acc: 0.7201612903225807
epoch: 172, train loss: 0.061899439354880154, acc: 0.9297535007522277, val loss: 0.9409228107489529, acc: 0.724986479177934, test loss: 0.8932812695862145, acc: 0.7215053763440861
epoch: 173, train loss: 0.051798281877519024, acc: 0.9399953709061452, val loss: 0.9525755536175857, acc: 0.7255273120605733, test loss: 0.9334573499618039, acc: 0.7206989247311828
epoch: 174, train loss: 0.046268252985884564, acc: 0.9432936002777457, val loss: 0.9691808785922853, acc: 0.7320173066522444, test loss: 0.9327154472310056, acc: 0.7193548387096774
epoch: 175, train loss: 0.05064877499653266, acc: 0.9430621455850018, val loss: 0.970076445052919, acc: 0.7295835586803677, test loss: 0.9328795561226465, acc: 0.7139784946236559
epoch: 176, train loss: 0.04865777476744276, acc: 0.9420205994676542, val loss: 0.9599770309475063, acc: 0.7298539751216874, test loss: 0.931086132603307, acc: 0.7190860215053764
epoch: 177, train loss: 0.049337681184149135, acc: 0.9405161439648189, val loss: 0.99735496532859, acc: 0.7212006489994591, test loss: 0.9374573123070502, acc: 0.7123655913978495
epoch: 178, train loss: 0.049969600320221054, acc: 0.9396481888670293, val loss: 0.9471108214541859, acc: 0.7276906435911303, test loss: 0.8971818313803724, acc: 0.7271505376344086
epoch: 179, train loss: 0.04305675895767153, acc: 0.9456660108783705, val loss: 0.9785038974879561, acc: 0.7320173066522444, test loss: 0.9219204528357393, acc: 0.7260752688172043
epoch: 180, train loss: 0.05842753134244515, acc: 0.9356555954171971, val loss: 0.9598400049947158, acc: 0.7225527312060573, test loss: 0.9157279604224748, acc: 0.7225806451612903
epoch: 181, train loss: 0.052530755148677825, acc: 0.9390116884619836, val loss: 0.9280262778680862, acc: 0.7230935640886966, test loss: 0.9027642260315598, acc: 0.7260752688172043
epoch: 182, train loss: 0.052441752310413314, acc: 0.9369285962272885, val loss: 0.9900891437860616, acc: 0.7117360735532721, test loss: 0.9371371571735669, acc: 0.714247311827957
epoch: 183, train loss: 0.05328610668599281, acc: 0.9379122786714501, val loss: 0.9679718480489266, acc: 0.7138994050838291, test loss: 0.9517647450970065, acc: 0.714247311827957
epoch: 184, train loss: 0.05747180452602437, acc: 0.9344404582802917, val loss: 0.941232987388912, acc: 0.7244456462952947, test loss: 0.9036608449874386, acc: 0.7201612903225807
epoch: 185, train loss: 0.0545715202711382, acc: 0.9372757782664044, val loss: 0.9755479862652578, acc: 0.7217414818820984, test loss: 0.9223630515477991, acc: 0.7209677419354839
epoch: 186, train loss: 0.04507420420993286, acc: 0.9413840990626084, val loss: 0.9480112666629855, acc: 0.7312060573282856, test loss: 0.906511216522545, acc: 0.7274193548387097
epoch: 187, train loss: 0.04732430126734001, acc: 0.9420784631408402, val loss: 0.9746287494430418, acc: 0.7295835586803677, test loss: 0.9405187519647742, acc: 0.7188172043010753
epoch: 188, train loss: 0.046810071814075385, acc: 0.9441615553755353, val loss: 0.9594738283564813, acc: 0.7298539751216874, test loss: 0.9010703871327062, acc: 0.7274193548387097
epoch: 189, train loss: 0.04333908033945566, acc: 0.9458974655711144, val loss: 0.9547484919082286, acc: 0.7355327203893997, test loss: 0.9221402378492458, acc: 0.7293010752688172
epoch: 190, train loss: 0.049026476079398675, acc: 0.9435829186436755, val loss: 0.9703364958951382, acc: 0.7241752298539751, test loss: 0.9302114312366773, acc: 0.7260752688172043
epoch: 191, train loss: 0.06259418253065493, acc: 0.9306793195232034, val loss: 0.9754093144634983, acc: 0.7236343969713358, test loss: 0.9059393241841306, acc: 0.7201612903225807
epoch: 192, train loss: 0.057534910848834366, acc: 0.9339196852216178, val loss: 0.9361217036126045, acc: 0.7239048134126554, test loss: 0.9062610436511296, acc: 0.7163978494623656
epoch: 193, train loss: 0.05596179805093308, acc: 0.9339775488948039, val loss: 0.953693066757392, acc: 0.7241752298539751, test loss: 0.9195391695986512, acc: 0.718010752688172
epoch: 194, train loss: 0.06777145951027506, acc: 0.926976044439301, val loss: 1.011996089606236, acc: 0.7041644131963224, test loss: 0.9561625834434263, acc: 0.706989247311828
epoch: 195, train loss: 0.057896873004154234, acc: 0.9334567758361301, val loss: 0.910925172096075, acc: 0.7306652244456463, test loss: 0.8864638036297213, acc: 0.7244623655913979
epoch: 196, train loss: 0.04600674553950301, acc: 0.9434671912973035, val loss: 0.9434844054293801, acc: 0.7230935640886966, test loss: 0.903251826378607, acc: 0.7225806451612903
epoch: 197, train loss: 0.04371896031336464, acc: 0.9441615553755353, val loss: 0.9716860694843992, acc: 0.7276906435911303, test loss: 0.9310350305290632, acc: 0.7190860215053764
epoch: 198, train loss: 0.048261605749809276, acc: 0.9398796435597732, val loss: 0.9485333133478819, acc: 0.7282314764737696, test loss: 0.9199620528887676, acc: 0.7215053763440861
epoch: 199, train loss: 0.04676117909799094, acc: 0.9449137831269528, val loss: 0.981010916814603, acc: 0.7274202271498107, test loss: 0.950915789347823, acc: 0.7204301075268817
epoch: 200, train loss: 0.04820098525445407, acc: 0.9412683717162366, val loss: 0.960565421696029, acc: 0.7295835586803677, test loss: 0.9336911529623052, acc: 0.7241935483870968
Epoch   200: reducing learning rate of group 0 to 1.8750e-04.
best val acc 0.7355327203893997 at epoch 189.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9957    0.9968    0.9963      5337
           1     0.9460    0.9173    0.9314      2502
           2     0.9888    0.9802    0.9845       810
           3     0.9667    0.9948    0.9805      2100
           4     0.9863    0.9796    0.9830       737
           5     0.9767    0.9897    0.9831       677
           6     0.8848    0.9932    0.9359      1323
           7     0.9663    0.9613    0.9638      1164
           8     0.9904    0.9810    0.9857       421
           9     0.9900    0.9925    0.9913       401
          10     0.9849    0.9899    0.9874       396
          11     0.9967    0.9665    0.9814       627
          12     0.9896    0.9794    0.9845       291
          13     0.9187    0.5632    0.6983       261
          14     0.9767    0.8936    0.9333       235

    accuracy                         0.9706     17282
   macro avg     0.9706    0.9453    0.9547     17282
weighted avg     0.9711    0.9706    0.9699     17282

train confusion matrix:
[[9.96814690e-01 7.49484729e-04 0.00000000e+00 1.87371182e-04
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.74742365e-04 3.74742365e-04 1.87371182e-04 0.00000000e+00
  0.00000000e+00 1.31159828e-03 0.00000000e+00]
 [7.99360512e-04 9.17266187e-01 0.00000000e+00 1.35891287e-02
  0.00000000e+00 0.00000000e+00 6.67466027e-02 3.99680256e-04
  0.00000000e+00 3.99680256e-04 0.00000000e+00 0.00000000e+00
  0.00000000e+00 7.99360512e-04 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 9.80246914e-01 0.00000000e+00
  0.00000000e+00 1.97530864e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [4.76190476e-04 4.28571429e-03 0.00000000e+00 9.94761905e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 4.76190476e-04 0.00000000e+00]
 [0.00000000e+00 1.08548168e-02 0.00000000e+00 0.00000000e+00
  9.79647218e-01 0.00000000e+00 4.07055631e-03 2.71370421e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.35685210e-03
  1.35685210e-03 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 8.86262925e-03 0.00000000e+00
  0.00000000e+00 9.89660266e-01 1.47710487e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 6.80272109e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.93197279e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [5.15463918e-03 2.06185567e-02 0.00000000e+00 2.57731959e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.61340206e-01
  0.00000000e+00 0.00000000e+00 3.43642612e-03 8.59106529e-04
  0.00000000e+00 2.57731959e-03 3.43642612e-03]
 [1.90023753e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.80997625e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 2.49376559e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.92518703e-01 0.00000000e+00 0.00000000e+00
  4.98753117e-03 0.00000000e+00 0.00000000e+00]
 [2.52525253e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.05050505e-03
  0.00000000e+00 0.00000000e+00 9.89898990e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.52525253e-03]
 [0.00000000e+00 0.00000000e+00 3.18979266e-03 0.00000000e+00
  1.59489633e-02 0.00000000e+00 0.00000000e+00 1.43540670e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.66507177e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [3.43642612e-03 0.00000000e+00 3.43642612e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  6.87285223e-03 3.43642612e-03 3.43642612e-03 0.00000000e+00
  9.79381443e-01 0.00000000e+00 0.00000000e+00]
 [1.53256705e-02 2.91187739e-01 0.00000000e+00 1.30268199e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.63218391e-01 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.06382979e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 8.93617021e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8513    0.8714    0.8612      1143
           1     0.5719    0.6381    0.6032       536
           2     0.8544    0.7803    0.8157       173
           3     0.6847    0.7044    0.6944       450
           4     0.8227    0.7342    0.7759       158
           5     0.8000    0.8000    0.8000       145
           6     0.7360    0.7880    0.7611       283
           7     0.5662    0.4980    0.5299       249
           8     0.7500    0.6667    0.7059        90
           9     0.6143    0.5059    0.5548        85
          10     0.8193    0.8095    0.8144        84
          11     0.8559    0.7537    0.8016       134
          12     0.7231    0.7581    0.7402        62
          13     0.1304    0.1071    0.1176        56
          14     0.6667    0.5200    0.5843        50

    accuracy                         0.7355      3698
   macro avg     0.6965    0.6624    0.6773      3698
weighted avg     0.7354    0.7355    0.7343      3698

validation confusion matrix:
[[0.87139108 0.02624672 0.00437445 0.03062117 0.00174978 0.00174978
  0.00174978 0.02099738 0.01049869 0.00612423 0.00349956 0.00262467
  0.00524934 0.0096238  0.00349956]
 [0.06343284 0.6380597  0.00932836 0.12126866 0.0130597  0.00373134
  0.07276119 0.03171642 0.00373134 0.01119403 0.         0.01119403
  0.         0.01679104 0.00373134]
 [0.02312139 0.05202312 0.78034682 0.02312139 0.         0.06358382
  0.02890173 0.         0.         0.00578035 0.         0.01734104
  0.00578035 0.         0.        ]
 [0.06888889 0.16       0.         0.70444444 0.01111111 0.00222222
  0.00444444 0.02666667 0.00222222 0.         0.         0.
  0.00222222 0.01777778 0.        ]
 [0.03164557 0.14556962 0.         0.01265823 0.73417722 0.
  0.03164557 0.02531646 0.         0.         0.         0.00632911
  0.00632911 0.00632911 0.        ]
 [0.0137931  0.04137931 0.03448276 0.00689655 0.         0.8
  0.08275862 0.         0.         0.00689655 0.         0.
  0.0137931  0.         0.        ]
 [0.02473498 0.08480565 0.         0.01766784 0.         0.03533569
  0.78798587 0.01060071 0.00353357 0.02473498 0.         0.
  0.00706714 0.00353357 0.        ]
 [0.09638554 0.16465863 0.         0.06827309 0.03212851 0.00803213
  0.01204819 0.49799197 0.00401606 0.00803213 0.02811245 0.01204819
  0.00401606 0.03614458 0.02811245]
 [0.25555556 0.01111111 0.01111111 0.         0.01111111 0.
  0.01111111 0.01111111 0.66666667 0.         0.01111111 0.
  0.         0.01111111 0.        ]
 [0.2        0.12941176 0.05882353 0.01176471 0.         0.
  0.05882353 0.         0.01176471 0.50588235 0.         0.
  0.02352941 0.         0.        ]
 [0.02380952 0.02380952 0.         0.02380952 0.         0.
  0.         0.10714286 0.         0.         0.80952381 0.
  0.01190476 0.         0.        ]
 [0.02985075 0.08955224 0.00746269 0.02238806 0.01492537 0.
  0.01492537 0.05223881 0.00746269 0.00746269 0.         0.75373134
  0.         0.         0.        ]
 [0.06451613 0.0483871  0.01612903 0.         0.         0.
  0.0483871  0.01612903 0.         0.03225806 0.01612903 0.
  0.75806452 0.         0.        ]
 [0.16071429 0.32142857 0.         0.17857143 0.         0.
  0.01785714 0.14285714 0.01785714 0.         0.01785714 0.01785714
  0.01785714 0.10714286 0.        ]
 [0.16       0.08       0.         0.02       0.         0.02
  0.         0.18       0.         0.         0.02       0.
  0.         0.         0.52      ]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8432    0.8594    0.8512      1145
           1     0.5710    0.6369    0.6021       537
           2     0.8182    0.8229    0.8205       175
           3     0.7277    0.7406    0.7341       451
           4     0.8175    0.7044    0.7568       159
           5     0.8125    0.8014    0.8069       146
           6     0.7077    0.8099    0.7553       284
           7     0.5424    0.5120    0.5267       250
           8     0.7558    0.7143    0.7345        91
           9     0.5325    0.4713    0.5000        87
          10     0.8333    0.7558    0.7927        86
          11     0.8922    0.6691    0.7647       136
          12     0.6744    0.4531    0.5421        64
          13     0.1400    0.1228    0.1308        57
          14     0.5854    0.4615    0.5161        52

    accuracy                         0.7293      3720
   macro avg     0.6836    0.6357    0.6556      3720
weighted avg     0.7307    0.7293    0.7281      3720

test confusion matrix:
[[0.85938865 0.03406114 0.00349345 0.01921397 0.00174672 0.
  0.00436681 0.02358079 0.01135371 0.01484716 0.00524017 0.00262009
  0.00611354 0.01135371 0.00262009]
 [0.05959032 0.63687151 0.00931099 0.09310987 0.00744879 0.00931099
  0.09310987 0.04283054 0.         0.01303538 0.00372439 0.00372439
  0.0018622  0.02234637 0.00372439]
 [0.02857143 0.01142857 0.82285714 0.01714286 0.         0.08
  0.02285714 0.         0.         0.01714286 0.         0.
  0.         0.         0.        ]
 [0.05099778 0.13525499 0.00221729 0.7405765  0.00221729 0.
  0.00443459 0.03104213 0.00221729 0.         0.         0.00221729
  0.00443459 0.01773836 0.00665188]
 [0.03144654 0.16981132 0.         0.03773585 0.70440252 0.
  0.01886792 0.01886792 0.00628931 0.         0.         0.00628931
  0.00628931 0.         0.        ]
 [0.         0.01369863 0.10958904 0.         0.         0.80136986
  0.06849315 0.00684932 0.         0.         0.         0.
  0.         0.         0.        ]
 [0.01760563 0.11619718 0.00704225 0.00704225 0.00704225 0.01408451
  0.80985915 0.         0.00352113 0.00704225 0.         0.00704225
  0.         0.         0.00352113]
 [0.132      0.152      0.         0.068      0.016      0.
  0.044      0.512      0.         0.004      0.016      0.004
  0.008      0.024      0.02      ]
 [0.20879121 0.02197802 0.         0.01098901 0.01098901 0.
  0.01098901 0.         0.71428571 0.         0.         0.01098901
  0.         0.01098901 0.        ]
 [0.28735632 0.06896552 0.03448276 0.01149425 0.03448276 0.01149425
  0.03448276 0.         0.03448276 0.47126437 0.         0.
  0.01149425 0.         0.        ]
 [0.06976744 0.01162791 0.         0.01162791 0.         0.
  0.         0.11627907 0.         0.         0.75581395 0.
  0.         0.02325581 0.01162791]
 [0.00735294 0.125      0.         0.02205882 0.02941176 0.
  0.02941176 0.08823529 0.         0.00735294 0.         0.66911765
  0.         0.00735294 0.01470588]
 [0.171875   0.046875   0.015625   0.046875   0.046875   0.046875
  0.03125    0.03125    0.03125    0.078125   0.         0.
  0.453125   0.         0.        ]
 [0.1754386  0.36842105 0.         0.22807018 0.01754386 0.
  0.         0.07017544 0.         0.         0.01754386 0.
  0.         0.12280702 0.        ]
 [0.15384615 0.09615385 0.         0.05769231 0.         0.
  0.         0.23076923 0.         0.         0.         0.
  0.         0.         0.46153846]]
---------------------------------------
program finished.
