seed:  666
save trained model at:  ../trained_models/trained_classifier_model_29.pt
save loss at:  ./results/train_classifier_results_29.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  14780
number of pockets in validation set:  3162
number of pockets in test set:  3183
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKMCNMMEmbeddingNet(
    (conv0): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=22, out_features=64, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0006
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b53258b03a0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.1074781300089196, acc: 0.3692828146143437, val loss: 1.8754901625098193, acc: 0.44086021505376344, test loss: 1.8824548853144045, acc: 0.44077913917687717
epoch: 2, train loss: 1.8013718256931022, acc: 0.451894451962111, val loss: 1.6736619303604985, acc: 0.45983554712207464, test loss: 1.6848198975776674, acc: 0.4630851398052152
epoch: 3, train loss: 1.6885132364717002, acc: 0.4845060893098782, val loss: 1.628215027941246, acc: 0.4911448450347881, test loss: 1.621472501395223, acc: 0.4992145774426642
epoch: 4, train loss: 1.6058436935744846, acc: 0.5048037889039242, val loss: 1.5950112054801908, acc: 0.48355471220746366, test loss: 1.6001434246024882, acc: 0.49952874646559847
epoch: 5, train loss: 1.528772521115769, acc: 0.534573748308525, val loss: 1.500608465778609, acc: 0.5376344086021505, test loss: 1.50388254903601, acc: 0.5309456487590324
epoch: 6, train loss: 1.482975805338084, acc: 0.5510825439783491, val loss: 1.4244616910824663, acc: 0.5597722960151803, test loss: 1.4295906341941036, acc: 0.5664467483506126
epoch: 7, train loss: 1.4290431366237801, acc: 0.5673207036535859, val loss: 1.4710166071277113, acc: 0.5540796963946869, test loss: 1.4686894834771906, acc: 0.5516808042726987
epoch: 8, train loss: 1.398458578331706, acc: 0.5772665764546685, val loss: 1.5352511629132362, acc: 0.5249841872232764, test loss: 1.5470002736455046, acc: 0.5221489161168709
epoch: 9, train loss: 1.3488690101084109, acc: 0.593978349120433, val loss: 1.3102614751426123, acc: 0.5945604048070842, test loss: 1.3118986959094863, acc: 0.5962928055293748
epoch: 10, train loss: 1.2882587513839763, acc: 0.6159675236806496, val loss: 1.2593082865908658, acc: 0.603415559772296, test loss: 1.2726541494297, acc: 0.6060320452403393
epoch: 11, train loss: 1.2907913409969964, acc: 0.6132611637347767, val loss: 1.3524835735540612, acc: 0.6046805819101835, test loss: 1.385423372198567, acc: 0.6135721017907634
epoch: 12, train loss: 1.23890468112832, acc: 0.6335588633288227, val loss: 1.2171686016857284, acc: 0.622707147375079, test loss: 1.1949010636122617, acc: 0.6390197926484449
epoch: 13, train loss: 1.3495398113304933, acc: 0.6002706359945873, val loss: 1.621657429429114, acc: 0.551549652118912, test loss: 1.6130203523614892, acc: 0.5611058749607288
epoch: 14, train loss: 1.2486206331498246, acc: 0.6333558863328823, val loss: 1.1942207451337203, acc: 0.6382036685641999, test loss: 1.181253366713025, acc: 0.6443606660383286
epoch: 15, train loss: 1.1699433727741886, acc: 0.6535182679296346, val loss: 1.8392032126246338, acc: 0.48323845667299176, test loss: 1.8494369546984337, acc: 0.5032987747408105
epoch: 16, train loss: 1.1642668201087776, acc: 0.6546008119079838, val loss: 1.208801583200222, acc: 0.6397849462365591, test loss: 1.230075364737551, acc: 0.644674835061263
epoch: 17, train loss: 1.14318236201316, acc: 0.6653585926928282, val loss: 1.1402526304436815, acc: 0.6518026565464896, test loss: 1.1320858527533184, acc: 0.6603832862079799
epoch: 18, train loss: 1.1063547282483484, acc: 0.6746955345060893, val loss: 1.1333705968754266, acc: 0.6524351676154333, test loss: 1.1112234671086663, acc: 0.6707508639648131
epoch: 19, train loss: 1.1421082950251351, acc: 0.663531799729364, val loss: 1.2710560286520103, acc: 0.6151170145477546, test loss: 1.2473173261325463, acc: 0.629908890983349
epoch: 20, train loss: 1.0716571800763617, acc: 0.6866035182679296, val loss: 1.1883601709531124, acc: 0.631562302340291, test loss: 1.1607865530703751, acc: 0.6544140747722275
epoch: 21, train loss: 1.0649224196138498, acc: 0.6833558863328822, val loss: 1.271658162389052, acc: 0.6087919038583175, test loss: 1.225918451944987, acc: 0.6258246936852027
epoch: 22, train loss: 1.0365326709773124, acc: 0.6914749661705006, val loss: 1.0846943117861956, acc: 0.6691967109424415, test loss: 1.0738069636620255, acc: 0.6792334275840403
epoch: 23, train loss: 1.0534337041826467, acc: 0.6889715832205683, val loss: 1.1465954117048096, acc: 0.635673624288425, test loss: 1.1253692035663063, acc: 0.6547282437951618
epoch: 24, train loss: 1.0124207455347292, acc: 0.7028416779431664, val loss: 1.2102011777719768, acc: 0.6129032258064516, test loss: 1.2086214148844707, acc: 0.6123154256990261
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.8038941537575728, acc: 0.703044654939107, val loss: 0.8710014767318041, acc: 0.6736242884250474, test loss: 0.8516816304908247, acc: 0.6848884699968583
epoch: 26, train loss: 0.7588478846220912, acc: 0.7185385656292287, val loss: 1.1206456164179688, acc: 0.6065781151170145, test loss: 1.109964431399562, acc: 0.6167137920201068
epoch: 27, train loss: 0.755657364809142, acc: 0.716914749661705, val loss: 0.870255331990087, acc: 0.6736242884250474, test loss: 0.8522478096746552, acc: 0.685516808042727
epoch: 28, train loss: 0.7497740582942317, acc: 0.7148173207036536, val loss: 1.0349652684240083, acc: 0.6363061353573688, test loss: 0.9754008579433919, acc: 0.6566132579327678
epoch: 29, train loss: 0.7365156789432521, acc: 0.7224627875507442, val loss: 0.9007292372904421, acc: 0.6612903225806451, test loss: 0.9167306807443402, acc: 0.6584982720703738
epoch: 30, train loss: 0.7454517873924059, acc: 0.7200947225981056, val loss: 0.8767779621423459, acc: 0.6739405439595193, test loss: 0.8616590982257964, acc: 0.6738925541941565
epoch: 31, train loss: 0.7276701383887512, acc: 0.7242219215155615, val loss: 0.8953513535120154, acc: 0.6710942441492727, test loss: 0.8657489973458335, acc: 0.6745208922400251
epoch: 32, train loss: 0.72384275825485, acc: 0.7265899864682003, val loss: 0.9716654865897056, acc: 0.6625553447185326, test loss: 0.9615325117275995, acc: 0.6666666666666666
epoch: 33, train loss: 0.71465047207511, acc: 0.7314614343707713, val loss: 0.7946829975292247, acc: 0.7039848197343453, test loss: 0.7750384030085932, acc: 0.7200754005655042
epoch: 34, train loss: 0.6933535116128573, acc: 0.7371447902571042, val loss: 0.8847849409765718, acc: 0.683111954459203, test loss: 0.8965182663171201, acc: 0.6886584982720704
epoch: 35, train loss: 0.6833963158166134, acc: 0.741745602165088, val loss: 0.9356408788764273, acc: 0.6562302340290955, test loss: 0.9157603260409409, acc: 0.6776625824693685
epoch: 36, train loss: 0.7232005939432022, acc: 0.7244925575101488, val loss: 0.8920008342979379, acc: 0.6691967109424415, test loss: 0.8928301242249513, acc: 0.6779767514923029
epoch: 37, train loss: 0.7371351460803991, acc: 0.719553450608931, val loss: 0.9492330834655954, acc: 0.6685641998734978, test loss: 0.9622342347574429, acc: 0.671693371033616
epoch: 38, train loss: 0.6864739761784854, acc: 0.7347090663058187, val loss: 0.837982619736181, acc: 0.6774193548387096, test loss: 0.8189131236098827, acc: 0.6889726672950047
epoch: 39, train loss: 0.6741944601474499, acc: 0.742083897158322, val loss: 0.8356559205703687, acc: 0.6821631878557874, test loss: 0.8270600985851522, acc: 0.700282752120641
epoch: 40, train loss: 0.6529614197546477, acc: 0.7487821380243572, val loss: 0.760016189505223, acc: 0.7179000632511069, test loss: 0.7525338320622758, acc: 0.7197612315425699
epoch: 41, train loss: 0.6545857606301934, acc: 0.7479702300405954, val loss: 0.8016184919020709, acc: 0.6944971537001897, test loss: 0.7755579436083766, acc: 0.7150486961985548
epoch: 42, train loss: 0.6539464279989106, acc: 0.7497970230040596, val loss: 0.8471219521546047, acc: 0.6815306767868438, test loss: 0.8388788103121015, acc: 0.6974552309142319
epoch: 43, train loss: 0.6507524910089288, acc: 0.7489174560216508, val loss: 0.8612861450829888, acc: 0.6748893105629349, test loss: 0.8384837120013008, acc: 0.688344329249136
epoch: 44, train loss: 0.6348616112552896, acc: 0.7554803788903924, val loss: 0.9831865513950567, acc: 0.6287160025300442, test loss: 0.9730421314065725, acc: 0.6374489475337731
epoch: 45, train loss: 0.6091394819168019, acc: 0.7642760487144791, val loss: 0.7683093543296973, acc: 0.7014547754585705, test loss: 0.7431764357725928, acc: 0.7106503298774741
epoch: 46, train loss: 0.6037618484322854, acc: 0.7687415426251691, val loss: 0.7843550002552301, acc: 0.6973434535104365, test loss: 0.7928889904088132, acc: 0.6958843857995601
epoch: 47, train loss: 0.6447886686202476, acc: 0.7532476319350474, val loss: 0.7841944851353528, acc: 0.7049335863377609, test loss: 0.7729227217045963, acc: 0.7150486961985548
epoch: 48, train loss: 0.6544515838319781, acc: 0.7448579161028417, val loss: 0.8055950266737337, acc: 0.6925996204933587, test loss: 0.7975842232154369, acc: 0.6974552309142319
epoch: 49, train loss: 0.6290127347867447, acc: 0.756765899864682, val loss: 0.796130253498046, acc: 0.6973434535104365, test loss: 0.8059179187382762, acc: 0.7012252591894439
epoch: 50, train loss: 0.5898390249889823, acc: 0.7696887686062246, val loss: 0.8151864295213905, acc: 0.7103099304237824, test loss: 0.7968243954131485, acc: 0.7090794847628024
epoch: 51, train loss: 0.5890361200972726, acc: 0.7709066305818674, val loss: 0.7518938595098005, acc: 0.7122074636306135, test loss: 0.7234628714370608, acc: 0.7141061891297518
Epoch    51: reducing learning rate of group 0 to 1.5000e-03.
epoch: 52, train loss: 0.5207793734231724, acc: 0.7945872801082544, val loss: 0.7353534031939762, acc: 0.7397216951296648, test loss: 0.7242295140557944, acc: 0.7389255419415646
epoch: 53, train loss: 0.48562472573152576, acc: 0.8050744248985116, val loss: 0.6911253911041897, acc: 0.7450980392156863, test loss: 0.6918378279263817, acc: 0.7404963870562362
epoch: 54, train loss: 0.48122905989622394, acc: 0.8053450608930988, val loss: 0.7452949290785286, acc: 0.7299177735610373, test loss: 0.7403782381788201, acc: 0.7335846685516808
epoch: 55, train loss: 0.46768346945874906, acc: 0.8119756427604872, val loss: 0.7071322800613973, acc: 0.7381404174573055, test loss: 0.6991949172054415, acc: 0.7414388941250393
epoch: 56, train loss: 0.458973792755878, acc: 0.8161028416779431, val loss: 0.75415008464695, acc: 0.732764073371284, test loss: 0.7314213756641577, acc: 0.7335846685516808
epoch: 57, train loss: 0.4410062582792868, acc: 0.8194857916102841, val loss: 0.7217781077752303, acc: 0.7441492726122707, test loss: 0.738617686104782, acc: 0.7357838517122212
epoch: 58, train loss: 0.4562399428252116, acc: 0.8181326116373477, val loss: 0.743297973363781, acc: 0.7419354838709677, test loss: 0.7450402435401008, acc: 0.7401822180333019
epoch: 59, train loss: 0.4434055836904033, acc: 0.8190798376184032, val loss: 0.7486691728262567, acc: 0.7330803289057558, test loss: 0.7569221152923709, acc: 0.723531259817782
epoch: 60, train loss: 0.45629749361813793, acc: 0.8197564276048714, val loss: 0.8363773519671619, acc: 0.7204301075268817, test loss: 0.8467343247539774, acc: 0.7141061891297518
epoch: 61, train loss: 0.4673347265939751, acc: 0.8080514208389716, val loss: 0.7368558923478823, acc: 0.7397216951296648, test loss: 0.7308688982455116, acc: 0.7373546968268929
epoch: 62, train loss: 0.44093952193473124, acc: 0.8213125845737483, val loss: 0.8502295679264925, acc: 0.7115749525616698, test loss: 0.8414676563941268, acc: 0.7238454288407163
epoch: 63, train loss: 0.46839568997268266, acc: 0.8090663058186739, val loss: 0.7909135421871158, acc: 0.7258064516129032, test loss: 0.7513998347260538, acc: 0.7298146402764687
epoch: 64, train loss: 0.44599471428558857, acc: 0.8182002706359945, val loss: 0.9473425886587279, acc: 0.6881720430107527, test loss: 0.9373258178877524, acc: 0.690229343386742
epoch: 65, train loss: 0.41443919819327263, acc: 0.8290933694181326, val loss: 0.7396613429574406, acc: 0.7422517394054396, test loss: 0.7478102399536621, acc: 0.7392397109644989
epoch: 66, train loss: 0.398699345309615, acc: 0.8376860622462787, val loss: 0.9045205680272912, acc: 0.7371916508538899, test loss: 0.8986154474479539, acc: 0.7320138234370092
epoch: 67, train loss: 0.403953173091189, acc: 0.8335588633288228, val loss: 0.745456867622168, acc: 0.7394054395951929, test loss: 0.7478124063194004, acc: 0.7433239082626453
epoch: 68, train loss: 0.390669859552577, acc: 0.841339648173207, val loss: 0.8304928045797619, acc: 0.7239089184060721, test loss: 0.8499675629238569, acc: 0.7225887527489789
epoch: 69, train loss: 0.41368114219950725, acc: 0.8301082543978349, val loss: 0.7479494474569052, acc: 0.7409867172675522, test loss: 0.7343552004717971, acc: 0.7540056550424128
epoch: 70, train loss: 0.3803713858046938, acc: 0.8427604871447902, val loss: 0.8380821066367181, acc: 0.7422517394054396, test loss: 0.8219169744185371, acc: 0.7417530631479736
epoch: 71, train loss: 0.4143988024236707, acc: 0.8295669824086603, val loss: 0.8842365459729267, acc: 0.7087286527514232, test loss: 0.9028494494387537, acc: 0.7078228086710651
epoch: 72, train loss: 0.38873396398895327, acc: 0.8388362652232747, val loss: 0.7808566067808418, acc: 0.743516761543327, test loss: 0.7809219313911249, acc: 0.7518064718818724
epoch: 73, train loss: 0.3862483495788419, acc: 0.841745602165088, val loss: 0.7393309484924567, acc: 0.7428842504743833, test loss: 0.7249508370702985, acc: 0.7408105560791706
epoch: 74, train loss: 0.36290987011382964, acc: 0.8464817320703654, val loss: 0.7654501515653298, acc: 0.7536369386464263, test loss: 0.7657077640547829, acc: 0.7543198240653471
epoch: 75, train loss: 0.36599781520473135, acc: 0.8476995940460081, val loss: 0.7766873845565477, acc: 0.7507906388361796, test loss: 0.7759376277048708, acc: 0.7477222745837261
epoch: 76, train loss: 0.35431408110264995, acc: 0.8556833558863329, val loss: 0.7818842618846954, acc: 0.7495256166982922, test loss: 0.8086670947606707, acc: 0.7395538799874333
epoch: 77, train loss: 0.371790545654555, acc: 0.846617050067659, val loss: 0.9236227820599705, acc: 0.7131562302340291, test loss: 0.938996428331788, acc: 0.7093936537857367
epoch: 78, train loss: 0.3949913957606149, acc: 0.839106901217862, val loss: 0.8360396481103192, acc: 0.7179000632511069, test loss: 0.8310858112740285, acc: 0.7276154571159283
epoch: 79, train loss: 0.35457808489889836, acc: 0.8537212449255751, val loss: 0.7852035354467075, acc: 0.7466793168880456, test loss: 0.8012848663659654, acc: 0.7470939365378574
epoch: 80, train loss: 0.35195578268964817, acc: 0.8532476319350474, val loss: 0.8318096688093829, acc: 0.7308665401644528, test loss: 0.8333104896425564, acc: 0.7342130065975495
epoch: 81, train loss: 0.37099119685338217, acc: 0.8470906630581867, val loss: 0.7807771749393915, acc: 0.7403542061986085, test loss: 0.8073537038101403, acc: 0.7452089224002514
epoch: 82, train loss: 0.3548116440782689, acc: 0.8537889039242219, val loss: 0.8115886461124022, acc: 0.7492093611638204, test loss: 0.8127299043467836, acc: 0.7492931196983977
epoch: 83, train loss: 0.3580761144545146, acc: 0.8508795669824086, val loss: 0.8341437170280828, acc: 0.7387729285262492, test loss: 0.8657585203816147, acc: 0.7426955702167767
epoch: 84, train loss: 0.32754138521799697, acc: 0.8613667117726658, val loss: 0.7557437600854225, acc: 0.7517394054395952, test loss: 0.7602777374715802, acc: 0.7514923028589381
epoch: 85, train loss: 0.3228083931544798, acc: 0.8625169147496617, val loss: 0.8383983493529868, acc: 0.7390891840607211, test loss: 0.8643540320364964, acc: 0.7401822180333019
epoch: 86, train loss: 0.3383290721695865, acc: 0.8576454668470906, val loss: 0.9446266688250953, acc: 0.7182163187855788, test loss: 0.9600019950069724, acc: 0.7207037386113729
epoch: 87, train loss: 0.33816520591872634, acc: 0.8600135317997294, val loss: 0.8933432443016771, acc: 0.7384566729917773, test loss: 0.8884606744147231, acc: 0.7351555136663525
epoch: 88, train loss: 0.3667954201627326, acc: 0.8443166441136671, val loss: 0.8083699126548212, acc: 0.7318153067678684, test loss: 0.816691910287999, acc: 0.7288721332076658
epoch: 89, train loss: 0.30935577484041815, acc: 0.8647496617050068, val loss: 0.8874993978762159, acc: 0.7378241619228336, test loss: 0.8708973041016689, acc: 0.7326421614828778
epoch: 90, train loss: 0.32650929112718297, acc: 0.8625845737483085, val loss: 0.8429977100608773, acc: 0.7425679949399114, test loss: 0.8350585298720973, acc: 0.748664781652529
epoch: 91, train loss: 0.3137315253604894, acc: 0.8656292286874154, val loss: 0.7997126059016422, acc: 0.7545857052498419, test loss: 0.8157903766841811, acc: 0.7596606974552309
epoch: 92, train loss: 0.30225981623135983, acc: 0.8705006765899864, val loss: 0.8084926302111805, acc: 0.7526881720430108, test loss: 0.8269478897234946, acc: 0.7577756833176249
epoch: 93, train loss: 0.29870334372872104, acc: 0.8719215155615697, val loss: 0.9131174760103075, acc: 0.7308665401644528, test loss: 0.920825848967408, acc: 0.7295004712535345
epoch: 94, train loss: 0.29116397419382334, acc: 0.8749661705006766, val loss: 0.9758364440971051, acc: 0.7277039848197343, test loss: 0.9393463087576274, acc: 0.730128809299403
epoch: 95, train loss: 0.3258588255984212, acc: 0.8615020297699594, val loss: 0.7982023802835078, acc: 0.7450980392156863, test loss: 0.8016356185214384, acc: 0.7511781338360037
epoch: 96, train loss: 0.3551649966888402, acc: 0.8490527740189445, val loss: 0.77233284474324, acc: 0.7555344718532574, test loss: 0.8038960784776683, acc: 0.7499214577442664
epoch: 97, train loss: 0.3046297400907509, acc: 0.8705006765899864, val loss: 0.8165904849786535, acc: 0.7333965844402277, test loss: 0.8427294384485665, acc: 0.7291863022306001
epoch: 98, train loss: 0.30538357026844776, acc: 0.8722598105548038, val loss: 0.7952678972674955, acc: 0.7425679949399114, test loss: 0.8115744417731656, acc: 0.7530631479736098
epoch: 99, train loss: 0.2877945995298548, acc: 0.8746278755074425, val loss: 0.8451200689717534, acc: 0.7432005060088551, test loss: 0.8360100038913747, acc: 0.7499214577442664
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.26513821975465396, acc: 0.860893098782138, val loss: 0.6694904800308874, acc: 0.7507906388361796, test loss: 0.678873651139286, acc: 0.748664781652529
epoch: 101, train loss: 0.23215804924339015, acc: 0.8659675236806496, val loss: 0.6894934332725143, acc: 0.7507906388361796, test loss: 0.6906310935234672, acc: 0.7571473452717562
epoch: 102, train loss: 0.1990655515690779, acc: 0.8824763193504737, val loss: 0.689785134860189, acc: 0.7444655281467426, test loss: 0.719162658329771, acc: 0.7464655984919887
epoch: 103, train loss: 0.18672300579180093, acc: 0.8897158322056834, val loss: 0.7390367665070661, acc: 0.7552182163187856, test loss: 0.725166081296098, acc: 0.7590323594093622
epoch: 104, train loss: 0.1799663886931978, acc: 0.8910690121786198, val loss: 0.7137431655029645, acc: 0.7549019607843137, test loss: 0.746161816669192, acc: 0.7540056550424128
epoch: 105, train loss: 0.2023906473982802, acc: 0.8845737483085251, val loss: 0.759900825902829, acc: 0.7387729285262492, test loss: 0.7777734765308367, acc: 0.7417530631479736
epoch: 106, train loss: 0.22197719035677724, acc: 0.8720568335588633, val loss: 0.6716451774587516, acc: 0.7609108159392789, test loss: 0.7087170754893186, acc: 0.7524348099277411
epoch: 107, train loss: 0.20448688205749965, acc: 0.8809878213802436, val loss: 0.7166351131967621, acc: 0.7381404174573055, test loss: 0.6940143621458484, acc: 0.744894753377317
epoch: 108, train loss: 0.21783312588970458, acc: 0.8735453315290934, val loss: 0.6854689124262385, acc: 0.7533206831119544, test loss: 0.7396999395683881, acc: 0.746779767514923
epoch: 109, train loss: 0.18963718942602206, acc: 0.8880920162381597, val loss: 0.7238828241636601, acc: 0.7511068943706515, test loss: 0.7734525257635521, acc: 0.7483506126295947
epoch: 110, train loss: 0.22871520329715433, acc: 0.8719215155615697, val loss: 0.7912635707463138, acc: 0.7277039848197343, test loss: 0.800075408126287, acc: 0.7320138234370092
epoch: 111, train loss: 0.20412270788894776, acc: 0.879702300405954, val loss: 0.7493593989106238, acc: 0.7359266287160026, test loss: 0.7660679795477625, acc: 0.7442664153314483
epoch: 112, train loss: 0.20143950257314236, acc: 0.8841001353179972, val loss: 0.6710510227515831, acc: 0.7511068943706515, test loss: 0.7147854624999007, acc: 0.7489789506754634
epoch: 113, train loss: 0.21447171749539562, acc: 0.8755751014884979, val loss: 0.6676621530570236, acc: 0.7517394054395952, test loss: 0.6750816413152528, acc: 0.7499214577442664
epoch: 114, train loss: 0.1972533763100233, acc: 0.8828146143437077, val loss: 0.6981779802154695, acc: 0.7590132827324478, test loss: 0.6594604474360412, acc: 0.764373232799246
epoch: 115, train loss: 0.1906769757422446, acc: 0.8889039242219215, val loss: 0.6903400908234298, acc: 0.7488931056293485, test loss: 0.7253444861062398, acc: 0.7502356267672008
epoch: 116, train loss: 0.2527277827262878, acc: 0.8616373477672531, val loss: 0.676828687286015, acc: 0.7400379506641366, test loss: 0.6842248728542706, acc: 0.7452089224002514
epoch: 117, train loss: 0.2257716234546553, acc: 0.8693504736129906, val loss: 0.6700134787809842, acc: 0.7460468058191019, test loss: 0.700403920067881, acc: 0.7492931196983977
Epoch   117: reducing learning rate of group 0 to 7.5000e-04.
epoch: 118, train loss: 0.1603384101786052, acc: 0.9, val loss: 0.6830466846964316, acc: 0.7659709044908286, test loss: 0.7061584133641958, acc: 0.7728557964184731
epoch: 119, train loss: 0.120327897458583, acc: 0.9200270635994587, val loss: 0.731321406620805, acc: 0.7618595825426945, test loss: 0.7638104244331649, acc: 0.7712849513038015
epoch: 120, train loss: 0.11281088437174266, acc: 0.9208389715832206, val loss: 0.7437383928003377, acc: 0.7675521821631879, test loss: 0.7532158748855615, acc: 0.7728557964184731
epoch: 121, train loss: 0.11386502458536255, acc: 0.9223274695534506, val loss: 0.7415007929648122, acc: 0.7770398481973435, test loss: 0.7787938889114878, acc: 0.7769399937166196
epoch: 122, train loss: 0.11770111960793703, acc: 0.9209066305818674, val loss: 0.7904458337008086, acc: 0.7533206831119544, test loss: 0.8031349096738651, acc: 0.7675149230285894
epoch: 123, train loss: 0.12014262700839036, acc: 0.9167117726657645, val loss: 0.7515586330346259, acc: 0.7615433270082227, test loss: 0.7740678817941975, acc: 0.7728557964184731
epoch: 124, train loss: 0.11045519120396394, acc: 0.9219215155615696, val loss: 0.8040149861841848, acc: 0.7552182163187856, test loss: 0.8229005427769509, acc: 0.7712849513038015
epoch: 125, train loss: 0.10978115696021443, acc: 0.9228010825439783, val loss: 0.7843823254975543, acc: 0.7659709044908286, test loss: 0.8252904457076705, acc: 0.7744266415331448
epoch: 126, train loss: 0.1018490633679017, acc: 0.9263870094722598, val loss: 0.831516079024979, acc: 0.7574320050600886, test loss: 0.8619160107235994, acc: 0.7693999371661954
epoch: 127, train loss: 0.09468183354128036, acc: 0.9311231393775372, val loss: 0.8312337608144376, acc: 0.7653383934218849, test loss: 0.8714992179834652, acc: 0.7753691486019478
epoch: 128, train loss: 0.1004273520573389, acc: 0.9294993234100135, val loss: 0.864139460823643, acc: 0.7596457938013915, test loss: 0.9219319344016317, acc: 0.7508639648130694
epoch: 129, train loss: 0.10696288151049001, acc: 0.9261163734776725, val loss: 0.8164960084875349, acc: 0.7624920936116382, test loss: 0.8606076806437246, acc: 0.7697141061891297
epoch: 130, train loss: 0.10476478066028214, acc: 0.9307171853856563, val loss: 0.8030916113856471, acc: 0.7691334598355472, test loss: 0.8106114907419011, acc: 0.7678290920515237
epoch: 131, train loss: 0.09779416371303257, acc: 0.9298376184032476, val loss: 0.8460524264654731, acc: 0.7647058823529411, test loss: 0.8398765742760053, acc: 0.7700282752120641
epoch: 132, train loss: 0.09130509126533191, acc: 0.9337618403247632, val loss: 0.7850339513726811, acc: 0.7700822264389627, test loss: 0.8443434918185206, acc: 0.7744266415331448
epoch: 133, train loss: 0.08882071645842192, acc: 0.9356562922868742, val loss: 0.8741507052168825, acc: 0.7675521821631879, test loss: 0.8842192482656178, acc: 0.7709707822808671
epoch: 134, train loss: 0.08937845363587907, acc: 0.935723951285521, val loss: 0.8885995507768104, acc: 0.7647058823529411, test loss: 0.9128645316182986, acc: 0.765315739868049
epoch: 135, train loss: 0.10711579664190664, acc: 0.9278755074424898, val loss: 0.8561193383546813, acc: 0.7526881720430108, test loss: 0.8656238316366068, acc: 0.7646874018221803
epoch: 136, train loss: 0.1003365552070499, acc: 0.9293640054127199, val loss: 0.8014044714908973, acc: 0.7722960151802657, test loss: 0.8638190419427486, acc: 0.7681432610744581
epoch: 137, train loss: 0.09706190678924927, acc: 0.9315290933694181, val loss: 0.849220625438847, acc: 0.7561669829222012, test loss: 0.8921047459223942, acc: 0.7612315425699026
epoch: 138, train loss: 0.13868825129347984, acc: 0.9150879566982408, val loss: 0.8259310674094309, acc: 0.7539531941808981, test loss: 0.8188757206185449, acc: 0.7615457115928369
epoch: 139, train loss: 0.12034262782907938, acc: 0.9191474966170501, val loss: 0.7810751826306221, acc: 0.7745098039215687, test loss: 0.8269455086836603, acc: 0.7728557964184731
epoch: 140, train loss: 0.09616702269959353, acc: 0.9318673883626523, val loss: 0.808841371671977, acc: 0.7716635041113219, test loss: 0.8435756081198508, acc: 0.7697141061891297
epoch: 141, train loss: 0.14981950226672125, acc: 0.9111637347767253, val loss: 0.8510025440327055, acc: 0.7239089184060721, test loss: 0.8856302714370311, acc: 0.7288721332076658
epoch: 142, train loss: 0.15439362866912384, acc: 0.9024357239512856, val loss: 0.7569194517129588, acc: 0.7615433270082227, test loss: 0.7618474984296343, acc: 0.760603204524034
epoch: 143, train loss: 0.09940086464839956, acc: 0.9320027063599459, val loss: 0.8553949109667995, acc: 0.7450980392156863, test loss: 0.8757904494966009, acc: 0.7584040213634936
epoch: 144, train loss: 0.09756117382052787, acc: 0.9298376184032476, val loss: 0.8457298150626562, acc: 0.7520556609740671, test loss: 0.8729427908712537, acc: 0.7612315425699026
epoch: 145, train loss: 0.09915458810224585, acc: 0.9285520974289581, val loss: 0.7975161974374287, acc: 0.7688172043010753, test loss: 0.8393448468015661, acc: 0.7715991203267358
epoch: 146, train loss: 0.09257524689616628, acc: 0.9322733423545332, val loss: 0.8366811151824217, acc: 0.7659709044908286, test loss: 0.8919048999938581, acc: 0.7662582469368521
epoch: 147, train loss: 0.09146022752875081, acc: 0.9318673883626523, val loss: 0.8695200098835163, acc: 0.756483238456673, test loss: 0.9216185004839237, acc: 0.7659440779139177
epoch: 148, train loss: 0.11126258292201407, acc: 0.9242895805142084, val loss: 0.7947187536807847, acc: 0.773877292852625, test loss: 0.8506857287835071, acc: 0.7668865849827207
epoch: 149, train loss: 0.08352441109001071, acc: 0.9373477672530447, val loss: 0.8621245673153688, acc: 0.7688172043010753, test loss: 0.9269344053514117, acc: 0.760603204524034
epoch: 150, train loss: 0.08567764666712817, acc: 0.9399864682002707, val loss: 0.8477797286560232, acc: 0.765022137887413, test loss: 0.9254759255348419, acc: 0.7621740496387056
epoch: 151, train loss: 0.1111213653795774, acc: 0.9232746955345061, val loss: 0.8158165511264596, acc: 0.7536369386464263, test loss: 0.8907048404497655, acc: 0.7562048382029531
epoch: 152, train loss: 0.09088692720550968, acc: 0.9356562922868742, val loss: 0.8526311324865739, acc: 0.7593295382669196, test loss: 0.9039616261193709, acc: 0.764373232799246
epoch: 153, train loss: 0.09403862717267786, acc: 0.9353179972936401, val loss: 0.839574802080187, acc: 0.7605945604048071, test loss: 0.878297004016585, acc: 0.7565190072258875
epoch: 154, train loss: 0.09506875959014538, acc: 0.9321380243572395, val loss: 0.8530292726633746, acc: 0.7659709044908286, test loss: 0.921657863705929, acc: 0.7662582469368521
epoch: 155, train loss: 0.09532172634733868, acc: 0.9333558863328822, val loss: 0.8653385162655121, acc: 0.7624920936116382, test loss: 0.939753824534223, acc: 0.7596606974552309
epoch: 156, train loss: 0.08726100378615607, acc: 0.9363328822733423, val loss: 0.8881028120763539, acc: 0.7647058823529411, test loss: 0.935419971187123, acc: 0.7675149230285894
epoch: 157, train loss: 0.10571105820158337, acc: 0.926725304465494, val loss: 0.846874322113095, acc: 0.7567994939911449, test loss: 0.8671728485303969, acc: 0.7659440779139177
epoch: 158, train loss: 0.0889392011986695, acc: 0.936874154262517, val loss: 0.8652185193350767, acc: 0.7444655281467426, test loss: 0.9241165067652954, acc: 0.7530631479736098
epoch: 159, train loss: 0.08749655983807754, acc: 0.9355209742895805, val loss: 0.8595261300839178, acc: 0.7621758380771664, test loss: 0.8986743397582375, acc: 0.7693999371661954
epoch: 160, train loss: 0.08109872941840163, acc: 0.9408660351826793, val loss: 0.8490769665577531, acc: 0.756483238456673, test loss: 0.8878165174496837, acc: 0.7618598806157713
epoch: 161, train loss: 0.08356140001458955, acc: 0.9376184032476319, val loss: 0.904918711672547, acc: 0.7640733712839974, test loss: 0.9186153683765782, acc: 0.7640590637763116
epoch: 162, train loss: 0.08261998988819058, acc: 0.9390392422192152, val loss: 0.8737328664778456, acc: 0.7624920936116382, test loss: 0.9188036470341301, acc: 0.7618598806157713
epoch: 163, train loss: 0.08013785625826844, acc: 0.9401217861975643, val loss: 0.895049731992303, acc: 0.7596457938013915, test loss: 0.9481544139585216, acc: 0.765315739868049
epoch: 164, train loss: 0.0740584925784955, acc: 0.9443166441136671, val loss: 0.9070232288510192, acc: 0.7647058823529411, test loss: 0.9777262242305514, acc: 0.765315739868049
epoch: 165, train loss: 0.08537799695428556, acc: 0.9395128552097429, val loss: 0.9388885770999201, acc: 0.7545857052498419, test loss: 0.9263870172293877, acc: 0.7533773169965441
epoch: 166, train loss: 0.104200378953604, acc: 0.9316644113667117, val loss: 0.853532453340341, acc: 0.7501581277672359, test loss: 0.8776878398280454, acc: 0.7590323594093622
epoch: 167, train loss: 0.09712638487189967, acc: 0.9305818673883627, val loss: 0.8350505092941757, acc: 0.7555344718532574, test loss: 0.8793896279018928, acc: 0.7621740496387056
epoch: 168, train loss: 0.08560446728832506, acc: 0.9393098782138024, val loss: 0.924144285528966, acc: 0.7526881720430108, test loss: 0.9511921296432787, acc: 0.7640590637763116
Epoch   168: reducing learning rate of group 0 to 3.7500e-04.
epoch: 169, train loss: 0.07880383794156882, acc: 0.940595399188092, val loss: 0.8547525870355152, acc: 0.7628083491461101, test loss: 0.8882767960892658, acc: 0.7712849513038015
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.04136082377834233, acc: 0.953044654939107, val loss: 0.8083855617204698, acc: 0.7681846932321316, test loss: 0.8454543687321276, acc: 0.7769399937166196
epoch: 171, train loss: 0.036225963244781764, acc: 0.957510148849797, val loss: 0.7908374800250773, acc: 0.7729285262492094, test loss: 0.8542686019196961, acc: 0.7769399937166196
epoch: 172, train loss: 0.03251378439235752, acc: 0.9580514208389715, val loss: 0.8185206151777699, acc: 0.7662871600253004, test loss: 0.854512277130357, acc: 0.7807100219918316
epoch: 173, train loss: 0.030186499233820118, acc: 0.9616373477672531, val loss: 0.8216025322619154, acc: 0.7707147375079064, test loss: 0.8601748899135655, acc: 0.7822808671065034
epoch: 174, train loss: 0.029272970877315584, acc: 0.9650202976995941, val loss: 0.8175282247604863, acc: 0.7703984819734345, test loss: 0.8708285110160835, acc: 0.7731699654414075
epoch: 175, train loss: 0.026220916934482783, acc: 0.96468200270636, val loss: 0.8327754771385217, acc: 0.7656546489563567, test loss: 0.8778654805452165, acc: 0.7775683317624882
epoch: 176, train loss: 0.027818509897517094, acc: 0.9659675236806495, val loss: 0.84527808574843, acc: 0.7713472485768501, test loss: 0.8868504850661422, acc: 0.7794533459000943
epoch: 177, train loss: 0.02579329834094067, acc: 0.9639377537212449, val loss: 0.840638208811529, acc: 0.7685009487666035, test loss: 0.87920015082208, acc: 0.7778825007854225
epoch: 178, train loss: 0.026317800282524628, acc: 0.9667794316644114, val loss: 0.8681199593607344, acc: 0.7666034155597723, test loss: 0.920111104897497, acc: 0.7769399937166196
epoch: 179, train loss: 0.02758040682101403, acc: 0.9645466847090663, val loss: 0.8523129883029655, acc: 0.7659709044908286, test loss: 0.8933796773269945, acc: 0.7719132893496701
epoch: 180, train loss: 0.026650214785551185, acc: 0.9652909336941813, val loss: 0.8640006214361414, acc: 0.7659709044908286, test loss: 0.9065825632447684, acc: 0.7741124725102105
epoch: 181, train loss: 0.02779588167314923, acc: 0.9656292286874154, val loss: 0.8667915205198478, acc: 0.7685009487666035, test loss: 0.9267638358461606, acc: 0.7719132893496701
epoch: 182, train loss: 0.027210762001906745, acc: 0.9673207036535859, val loss: 0.8717364518420746, acc: 0.7643896268184693, test loss: 0.9046796534445688, acc: 0.7737983034872762
epoch: 183, train loss: 0.02607047105553347, acc: 0.9653585926928282, val loss: 0.882841950500411, acc: 0.7624920936116382, test loss: 0.9393987129066412, acc: 0.7712849513038015
epoch: 184, train loss: 0.029047479365570457, acc: 0.9650202976995941, val loss: 0.9092118145920369, acc: 0.7517394054395952, test loss: 0.9558146129117385, acc: 0.7599748664781653
epoch: 185, train loss: 0.04570939762767339, acc: 0.9508795669824086, val loss: 0.8428470358525855, acc: 0.7609108159392789, test loss: 0.878422923032645, acc: 0.7675149230285894
epoch: 186, train loss: 0.03964679432334693, acc: 0.9580514208389715, val loss: 0.8445866647813232, acc: 0.7612270714737508, test loss: 0.9049071282513517, acc: 0.7734841344643418
epoch: 187, train loss: 0.031730264728745204, acc: 0.961299052774019, val loss: 0.8242383662995321, acc: 0.7643896268184693, test loss: 0.8778967970839994, acc: 0.7750549795790135
epoch: 188, train loss: 0.02510783811262399, acc: 0.9682679296346414, val loss: 0.8621499678667854, acc: 0.7659709044908286, test loss: 0.9240005434289729, acc: 0.7759974866478165
epoch: 189, train loss: 0.02470793591986167, acc: 0.9685385656292287, val loss: 0.884965869462015, acc: 0.7615433270082227, test loss: 0.9337735625883337, acc: 0.7747408105560791
epoch: 190, train loss: 0.028924544181306727, acc: 0.9650202976995941, val loss: 0.8739190048842699, acc: 0.7612270714737508, test loss: 0.9406040781442376, acc: 0.7656299088909834
epoch: 191, train loss: 0.029671678002495244, acc: 0.9630581867388363, val loss: 0.9006132204874945, acc: 0.7545857052498419, test loss: 0.931113282352583, acc: 0.7665724159597863
epoch: 192, train loss: 0.03302869046452242, acc: 0.960893098782138, val loss: 0.8713488459662498, acc: 0.767235926628716, test loss: 0.9173197335055366, acc: 0.7709707822808671
epoch: 193, train loss: 0.03054997063114614, acc: 0.9613667117726657, val loss: 0.9006122123735152, acc: 0.756483238456673, test loss: 0.9198649571146412, acc: 0.7628023876845743
epoch: 194, train loss: 0.03142965411079269, acc: 0.9615020297699594, val loss: 0.8923513913441127, acc: 0.7599620493358634, test loss: 0.9237993975836191, acc: 0.7712849513038015
epoch: 195, train loss: 0.03170566166144908, acc: 0.9608254397834912, val loss: 0.9090191269886033, acc: 0.7514231499051234, test loss: 0.95311131317811, acc: 0.767200754005655
epoch: 196, train loss: 0.03321050596354134, acc: 0.9602165087956698, val loss: 0.9030556048417981, acc: 0.7615433270082227, test loss: 0.9533448652617407, acc: 0.7659440779139177
epoch: 197, train loss: 0.0335451386737888, acc: 0.9604194857916103, val loss: 0.8643827114129353, acc: 0.7621758380771664, test loss: 0.8989386433443483, acc: 0.7719132893496701
epoch: 198, train loss: 0.030615310090643142, acc: 0.9612313937753721, val loss: 0.8810245247297389, acc: 0.7571157495256167, test loss: 0.9304524645129727, acc: 0.7646874018221803
epoch: 199, train loss: 0.0327832598869307, acc: 0.9604871447902571, val loss: 0.8923736280613802, acc: 0.7485768500948766, test loss: 0.9223760564560191, acc: 0.7577756833176249
epoch: 200, train loss: 0.038309421879774985, acc: 0.9562246278755074, val loss: 0.8805217293528799, acc: 0.7599620493358634, test loss: 0.940234312500326, acc: 0.7709707822808671
best val acc 0.7770398481973435 at epoch 121.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9864    0.9940    0.9902      5337
           1     0.9832    0.9407    0.9615       810
           2     0.8994    0.9919    0.9434      2100
           3     0.9521    0.9430    0.9475       737
           4     0.9304    0.9867    0.9577       677
           5     0.9713    0.9713    0.9713      1323
           6     0.9046    0.8557    0.8795      1164
           7     0.9826    0.9406    0.9612       421
           8     0.9716    0.9401    0.9556       401
           9     0.9402    0.9924    0.9656       396
          10     0.9686    0.9362    0.9521       627
          11     0.8779    0.9141    0.8956       291
          12     0.7826    0.1379    0.2345       261
          13     0.7236    0.8468    0.7804       235

    accuracy                         0.9505     14780
   macro avg     0.9196    0.8851    0.8854     14780
weighted avg     0.9494    0.9505    0.9454     14780

train confusion matrix:
[[9.94004122e-01 1.87371182e-04 9.36855912e-04 0.00000000e+00
  0.00000000e+00 1.87371182e-04 1.87371182e-03 1.31159828e-03
  3.74742365e-04 1.87371182e-04 0.00000000e+00 9.36855912e-04
  0.00000000e+00 0.00000000e+00]
 [1.23456790e-03 9.40740741e-01 0.00000000e+00 0.00000000e+00
  4.69135802e-02 4.93827160e-03 0.00000000e+00 0.00000000e+00
  1.23456790e-03 0.00000000e+00 4.93827160e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.33333333e-03 0.00000000e+00 9.91904762e-01 4.76190476e-04
  9.52380952e-04 9.52380952e-04 9.52380952e-04 0.00000000e+00
  0.00000000e+00 9.52380952e-04 0.00000000e+00 0.00000000e+00
  4.76190476e-04 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 1.35685210e-02 9.43012212e-01
  0.00000000e+00 4.07055631e-03 2.17096336e-02 0.00000000e+00
  0.00000000e+00 2.71370421e-03 1.35685210e-02 0.00000000e+00
  1.35685210e-03 0.00000000e+00]
 [0.00000000e+00 1.18168390e-02 0.00000000e+00 0.00000000e+00
  9.86706056e-01 1.47710487e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.77928949e-03 1.51171580e-03 6.04686319e-03 4.53514739e-03
  6.04686319e-03 9.71277400e-01 2.26757370e-03 0.00000000e+00
  7.55857899e-04 0.00000000e+00 7.55857899e-04 0.00000000e+00
  3.02343159e-03 0.00000000e+00]
 [1.28865979e-02 0.00000000e+00 9.45017182e-03 1.37457045e-02
  8.59106529e-04 1.89003436e-02 8.55670103e-01 0.00000000e+00
  0.00000000e+00 1.71821306e-02 2.57731959e-03 0.00000000e+00
  3.43642612e-03 6.52920962e-02]
 [3.08788599e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.40617577e-01
  0.00000000e+00 0.00000000e+00 2.37529691e-03 2.61282660e-02
  0.00000000e+00 0.00000000e+00]
 [2.49376559e-03 2.49376559e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.49376559e-03 0.00000000e+00
  9.40149626e-01 0.00000000e+00 0.00000000e+00 5.23690773e-02
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 7.57575758e-03 0.00000000e+00
  0.00000000e+00 9.92424242e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.59489633e-03 0.00000000e+00 1.75438596e-02
  0.00000000e+00 1.59489633e-03 4.14673046e-02 0.00000000e+00
  1.59489633e-03 0.00000000e+00 9.36204147e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.18556701e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 3.43642612e-03 0.00000000e+00 0.00000000e+00
  2.06185567e-02 0.00000000e+00 0.00000000e+00 9.14089347e-01
  0.00000000e+00 0.00000000e+00]
 [4.59770115e-02 0.00000000e+00 7.62452107e-01 3.83141762e-03
  0.00000000e+00 1.14942529e-02 3.83141762e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.37931034e-01 0.00000000e+00]
 [4.25531915e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.25531915e-03 0.00000000e+00 1.44680851e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 8.46808511e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8648    0.8898    0.8771      1143
           1     0.8588    0.8439    0.8513       173
           2     0.7602    0.7889    0.7743       450
           3     0.7905    0.7405    0.7647       158
           4     0.8355    0.8759    0.8552       145
           5     0.8000    0.7915    0.7957       283
           6     0.4797    0.5703    0.5211       249
           7     0.8421    0.7111    0.7711        90
           8     0.6410    0.5882    0.6135        85
           9     0.7857    0.7857    0.7857        84
          10     0.7638    0.7239    0.7433       134
          11     0.5472    0.4677    0.5043        62
          12     0.0952    0.0357    0.0519        56
          13     0.6176    0.4200    0.5000        50

    accuracy                         0.7770      3162
   macro avg     0.6916    0.6595    0.6721      3162
weighted avg     0.7716    0.7770    0.7730      3162

validation confusion matrix:
[[0.88976378 0.00262467 0.02887139 0.00174978 0.         0.01137358
  0.03499563 0.00874891 0.00262467 0.00174978 0.00262467 0.00699913
  0.00612423 0.00174978]
 [0.02312139 0.84393064 0.         0.         0.06358382 0.01734104
  0.01734104 0.         0.02312139 0.         0.01156069 0.
  0.         0.        ]
 [0.07555556 0.00444444 0.78888889 0.02444444 0.         0.01555556
  0.06222222 0.         0.         0.01111111 0.00888889 0.
  0.00666667 0.00222222]
 [0.02531646 0.         0.0443038  0.74050633 0.00632911 0.03797468
  0.09493671 0.00632911 0.01265823 0.         0.01898734 0.
  0.01265823 0.        ]
 [0.00689655 0.04137931 0.00689655 0.         0.87586207 0.06206897
  0.         0.         0.         0.         0.00689655 0.
  0.         0.        ]
 [0.03533569 0.01766784 0.03180212 0.00706714 0.04240283 0.79151943
  0.02473498 0.         0.02826855 0.         0.00353357 0.00706714
  0.01060071 0.        ]
 [0.12048193 0.00401606 0.13253012 0.02811245 0.         0.04417671
  0.57028112 0.         0.         0.02811245 0.02409639 0.00803213
  0.01204819 0.02811245]
 [0.22222222 0.         0.         0.01111111 0.         0.
  0.02222222 0.71111111 0.         0.         0.         0.02222222
  0.01111111 0.        ]
 [0.16470588 0.05882353 0.01176471 0.         0.01176471 0.04705882
  0.01176471 0.         0.58823529 0.         0.01176471 0.09411765
  0.         0.        ]
 [0.02380952 0.         0.02380952 0.01190476 0.         0.01190476
  0.10714286 0.         0.         0.78571429 0.         0.
  0.         0.03571429]
 [0.02238806 0.01492537 0.02238806 0.02985075 0.         0.00746269
  0.14179104 0.         0.01492537 0.00746269 0.7238806  0.01492537
  0.         0.        ]
 [0.22580645 0.         0.01612903 0.01612903 0.         0.
  0.01612903 0.01612903 0.14516129 0.01612903 0.08064516 0.46774194
  0.         0.        ]
 [0.30357143 0.         0.35714286 0.01785714 0.         0.01785714
  0.19642857 0.         0.         0.         0.07142857 0.
  0.03571429 0.        ]
 [0.12       0.         0.04       0.02       0.         0.
  0.36       0.         0.         0.04       0.         0.
  0.         0.42      ]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8656    0.8891    0.8772      1145
           1     0.7941    0.7714    0.7826       175
           2     0.7653    0.8027    0.7835       451
           3     0.7987    0.7484    0.7727       159
           4     0.8092    0.8425    0.8255       146
           5     0.8521    0.8521    0.8521       284
           6     0.4534    0.5640    0.5027       250
           7     0.8831    0.7473    0.8095        91
           8     0.7600    0.6552    0.7037        87
           9     0.7952    0.7674    0.7811        86
          10     0.7830    0.6103    0.6860       136
          11     0.5645    0.5469    0.5556        64
          12     0.1071    0.0526    0.0706        57
          13     0.5676    0.4038    0.4719        52

    accuracy                         0.7769      3183
   macro avg     0.6999    0.6610    0.6768      3183
weighted avg     0.7756    0.7769    0.7745      3183

test confusion matrix:
[[8.89082969e-01 7.86026201e-03 2.27074236e-02 2.62008734e-03
  8.73362445e-04 5.24017467e-03 3.75545852e-02 5.24017467e-03
  6.98689956e-03 0.00000000e+00 8.73362445e-04 8.73362445e-03
  6.98689956e-03 5.24017467e-03]
 [3.42857143e-02 7.71428571e-01 5.71428571e-03 5.71428571e-03
  1.08571429e-01 4.57142857e-02 1.71428571e-02 0.00000000e+00
  0.00000000e+00 5.71428571e-03 0.00000000e+00 5.71428571e-03
  0.00000000e+00 0.00000000e+00]
 [7.53880266e-02 2.21729490e-03 8.02660754e-01 4.43458980e-03
  2.21729490e-03 4.43458980e-03 8.42572062e-02 0.00000000e+00
  0.00000000e+00 4.43458980e-03 8.86917960e-03 0.00000000e+00
  1.10864745e-02 0.00000000e+00]
 [3.77358491e-02 6.28930818e-03 4.40251572e-02 7.48427673e-01
  0.00000000e+00 1.88679245e-02 7.54716981e-02 0.00000000e+00
  6.28930818e-03 6.28930818e-03 3.14465409e-02 6.28930818e-03
  1.88679245e-02 0.00000000e+00]
 [1.36986301e-02 4.79452055e-02 0.00000000e+00 0.00000000e+00
  8.42465753e-01 8.21917808e-02 0.00000000e+00 0.00000000e+00
  6.84931507e-03 0.00000000e+00 0.00000000e+00 6.84931507e-03
  0.00000000e+00 0.00000000e+00]
 [4.92957746e-02 1.40845070e-02 1.76056338e-02 1.40845070e-02
  2.46478873e-02 8.52112676e-01 1.76056338e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.52112676e-03 3.52112676e-03
  0.00000000e+00 3.52112676e-03]
 [1.00000000e-01 1.60000000e-02 1.28000000e-01 4.40000000e-02
  0.00000000e+00 2.40000000e-02 5.64000000e-01 4.00000000e-03
  4.00000000e-03 3.60000000e-02 2.40000000e-02 8.00000000e-03
  2.40000000e-02 2.40000000e-02]
 [1.97802198e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.29670330e-02 7.47252747e-01
  0.00000000e+00 0.00000000e+00 1.09890110e-02 1.09890110e-02
  0.00000000e+00 0.00000000e+00]
 [1.37931034e-01 5.74712644e-02 0.00000000e+00 0.00000000e+00
  1.14942529e-02 2.29885057e-02 2.29885057e-02 1.14942529e-02
  6.55172414e-01 0.00000000e+00 0.00000000e+00 8.04597701e-02
  0.00000000e+00 0.00000000e+00]
 [8.13953488e-02 0.00000000e+00 3.48837209e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 8.13953488e-02 0.00000000e+00
  0.00000000e+00 7.67441860e-01 0.00000000e+00 0.00000000e+00
  1.16279070e-02 2.32558140e-02]
 [4.41176471e-02 1.47058824e-02 8.08823529e-02 5.14705882e-02
  0.00000000e+00 2.20588235e-02 1.10294118e-01 0.00000000e+00
  2.20588235e-02 0.00000000e+00 6.10294118e-01 2.20588235e-02
  1.47058824e-02 7.35294118e-03]
 [2.34375000e-01 1.56250000e-02 6.25000000e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.56250000e-02 0.00000000e+00
  6.25000000e-02 1.56250000e-02 4.68750000e-02 5.46875000e-01
  0.00000000e+00 0.00000000e+00]
 [2.10526316e-01 1.75438596e-02 3.85964912e-01 1.75438596e-02
  0.00000000e+00 0.00000000e+00 2.45614035e-01 1.75438596e-02
  0.00000000e+00 1.75438596e-02 3.50877193e-02 0.00000000e+00
  5.26315789e-02 0.00000000e+00]
 [1.92307692e-02 0.00000000e+00 0.00000000e+00 1.92307692e-02
  0.00000000e+00 0.00000000e+00 5.19230769e-01 0.00000000e+00
  0.00000000e+00 3.84615385e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 4.03846154e-01]]
---------------------------------------
program finished.
