seed:  666
save trained model at:  ../trained_models/trained_classifier_model_23.pt
save loss at:  ./results/train_classifier_results_23.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  14780
number of pockets in validation set:  3162
number of pockets in test set:  3183
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b77ad4529a0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 1.9948457624980014, acc: 0.4057510148849797, val loss: 1.8374032651526049, acc: 0.40860215053763443, test loss: 1.839861555291241, acc: 0.4143889412503927
epoch: 2, train loss: 1.734153823794467, acc: 0.47151556156968877, val loss: 1.657939230166682, acc: 0.5006325110689437, test loss: 1.6579987998282875, acc: 0.5130380144517751
epoch: 3, train loss: 1.600164501657344, acc: 0.5136671177266576, val loss: 1.6062601440545503, acc: 0.476280834914611, test loss: 1.5927615071331747, acc: 0.48539114043355325
epoch: 4, train loss: 1.5485307288266648, acc: 0.5288903924221922, val loss: 1.4468545429620616, acc: 0.549652118912081, test loss: 1.4282090344070977, acc: 0.5541941564561734
epoch: 5, train loss: 1.4677135680783262, acc: 0.5571718538565629, val loss: 1.4263664584307638, acc: 0.5585072738772928, test loss: 1.4385283646477194, acc: 0.5560791705937794
epoch: 6, train loss: 1.3961621605817616, acc: 0.5790257104194858, val loss: 1.421947403997654, acc: 0.5493358633776091, test loss: 1.4088675367581256, acc: 0.5535658184103047
epoch: 7, train loss: 1.349984798895974, acc: 0.5945196211096075, val loss: 1.4343563509017587, acc: 0.5471220746363061, test loss: 1.4165672328612207, acc: 0.5576500157084512
epoch: 8, train loss: 1.324908388741768, acc: 0.6049391069012179, val loss: 1.3849913610980753, acc: 0.5721062618595826, test loss: 1.364863127610461, acc: 0.5786993402450519
epoch: 9, train loss: 1.2917254754106473, acc: 0.607510148849797, val loss: 1.419428850562417, acc: 0.551549652118912, test loss: 1.4209408260462908, acc: 0.5614200439836632
epoch: 10, train loss: 1.2525098189288129, acc: 0.6218538565629229, val loss: 1.3261600339963118, acc: 0.5910815939278937, test loss: 1.2913950203426072, acc: 0.6038328620797989
epoch: 11, train loss: 1.220638321410658, acc: 0.6311907983761841, val loss: 1.3891339273410535, acc: 0.5585072738772928, test loss: 1.380829669752849, acc: 0.5749293119698398
epoch: 12, train loss: 1.1871551275898542, acc: 0.6416102841677943, val loss: 1.272805907920823, acc: 0.6081593927893738, test loss: 1.2582563986165547, acc: 0.6154571159283695
epoch: 13, train loss: 1.1728238681011174, acc: 0.6430311231393775, val loss: 1.2659189277325607, acc: 0.5910815939278937, test loss: 1.2593978739668654, acc: 0.604775369148602
epoch: 14, train loss: 1.1290216339134558, acc: 0.6610284167794317, val loss: 1.141179636111371, acc: 0.6416824794433903, test loss: 1.1216312509417496, acc: 0.6572415959786365
epoch: 15, train loss: 1.097926472746471, acc: 0.6728687415426252, val loss: 1.1809328407671238, acc: 0.6426312460468058, test loss: 1.162370832543908, acc: 0.644674835061263
epoch: 16, train loss: 1.092591791191669, acc: 0.6702300405953991, val loss: 1.153093503292115, acc: 0.6438962681846933, test loss: 1.1156554766657065, acc: 0.6534715677034244
epoch: 17, train loss: 1.056842594249968, acc: 0.6842354533152909, val loss: 1.5396462378058533, acc: 0.5553447185325743, test loss: 1.5199083469964931, acc: 0.5661325793276782
epoch: 18, train loss: 1.02921458700513, acc: 0.6866711772665764, val loss: 1.1697506690462656, acc: 0.6337760910815939, test loss: 1.1941463314359606, acc: 0.628023876845743
epoch: 19, train loss: 1.0277726592165852, acc: 0.6875507442489851, val loss: 1.197266042571819, acc: 0.6451612903225806, test loss: 1.1756286226351083, acc: 0.6515865535658184
epoch: 20, train loss: 1.0008081211288824, acc: 0.6991880920162381, val loss: 1.0850741355046678, acc: 0.6685641998734978, test loss: 1.090835484999963, acc: 0.6676091737354697
epoch: 21, train loss: 0.98594235660904, acc: 0.7040595399188092, val loss: 1.103440164991906, acc: 0.6691967109424415, test loss: 1.0993955270921214, acc: 0.666038328620798
epoch: 22, train loss: 0.972252608926435, acc: 0.7106901217861976, val loss: 1.0295111410682674, acc: 0.6780518659076534, test loss: 1.0078647120958448, acc: 0.6961985548224945
epoch: 23, train loss: 0.9430287532941898, acc: 0.7121109607577808, val loss: 1.0416744678251204, acc: 0.6869070208728653, test loss: 1.0154295909940316, acc: 0.68645931511153
epoch: 24, train loss: 0.9256009181232995, acc: 0.7178619756427604, val loss: 1.103776776285431, acc: 0.6717267552182163, test loss: 1.1080486267346314, acc: 0.66980835689601
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.7278074906546628, acc: 0.7227334235453315, val loss: 0.806672941047432, acc: 0.6903858317520557, test loss: 0.7846930343850843, acc: 0.7009110901665095
epoch: 26, train loss: 0.7026692821950487, acc: 0.7307848443843031, val loss: 0.941251074340357, acc: 0.633459835547122, test loss: 0.9168821286001634, acc: 0.649387370405278
epoch: 27, train loss: 0.7023211052840391, acc: 0.7310554803788903, val loss: 0.9728337968926427, acc: 0.6407337128399747, test loss: 0.9810542087303256, acc: 0.6449890040841973
epoch: 28, train loss: 0.6885790308370319, acc: 0.7359945872801082, val loss: 0.8260120553807471, acc: 0.6891208096141682, test loss: 0.8108192013750427, acc: 0.685516808042727
epoch: 29, train loss: 0.6739815656161277, acc: 0.7402571041948579, val loss: 0.8598973045312929, acc: 0.6827956989247311, test loss: 0.8543641440642917, acc: 0.6792334275840403
epoch: 30, train loss: 0.6650455649392691, acc: 0.7408660351826793, val loss: 0.7952135835245845, acc: 0.6843769765970904, test loss: 0.7928512959296022, acc: 0.6983977379830348
epoch: 31, train loss: 0.6570234765864515, acc: 0.7428958051420839, val loss: 0.8531238399375322, acc: 0.683111954459203, test loss: 0.8533964301668695, acc: 0.690229343386742
epoch: 32, train loss: 0.6496740184391626, acc: 0.7459404600811907, val loss: 0.797193712407015, acc: 0.7043010752688172, test loss: 0.8114264876521162, acc: 0.6936852026390198
epoch: 33, train loss: 0.6708497356981321, acc: 0.7374154262516914, val loss: 0.8826811530784593, acc: 0.6619228336495888, test loss: 0.8953842730256997, acc: 0.6562990889098335
epoch: 34, train loss: 0.6522145658126542, acc: 0.745466847090663, val loss: 0.8966390138031334, acc: 0.6729917773561037, test loss: 0.8849915943541243, acc: 0.6779767514923029
epoch: 35, train loss: 0.6255839444142395, acc: 0.7560893098782138, val loss: 0.7428526564687962, acc: 0.7043010752688172, test loss: 0.739668249072723, acc: 0.7163053722902922
epoch: 36, train loss: 0.6052649928687874, acc: 0.7605548037889039, val loss: 0.802715430600518, acc: 0.7074636306135358, test loss: 0.8179933537936532, acc: 0.7097078228086711
epoch: 37, train loss: 0.5936074704536727, acc: 0.7665087956698241, val loss: 0.8357672881356226, acc: 0.6657179000632512, test loss: 0.8292051127149442, acc: 0.6770342444234998
epoch: 38, train loss: 0.5820143204258001, acc: 0.7723274695534507, val loss: 0.7594797762038963, acc: 0.7071473750790639, test loss: 0.7641191359701824, acc: 0.706880301602262
epoch: 39, train loss: 0.5921058813354482, acc: 0.7627198917456022, val loss: 0.7640080488157905, acc: 0.7049335863377609, test loss: 0.7750376156205618, acc: 0.7134778510838832
epoch: 40, train loss: 0.591454484433702, acc: 0.7693504736129906, val loss: 0.8131957919743904, acc: 0.6884882985452245, test loss: 0.8484121566668195, acc: 0.6745208922400251
epoch: 41, train loss: 0.5697800306247923, acc: 0.7767253044654939, val loss: 0.7583576713009472, acc: 0.7125237191650854, test loss: 0.7679804751905225, acc: 0.7081369776939994
epoch: 42, train loss: 0.5802961422722782, acc: 0.7730717185385656, val loss: 0.8423770988085539, acc: 0.6907020872865275, test loss: 0.8636622133024301, acc: 0.6839459629280553
epoch: 43, train loss: 0.5629888145300952, acc: 0.778552097428958, val loss: 0.7792958180260462, acc: 0.7043010752688172, test loss: 0.7986346760748918, acc: 0.7046811184417217
epoch: 44, train loss: 0.547342853455808, acc: 0.7801082543978349, val loss: 0.7996897869363455, acc: 0.7077798861480076, test loss: 0.804233332606108, acc: 0.7027961043041157
epoch: 45, train loss: 0.5686653276580274, acc: 0.777063599458728, val loss: 0.9941074279650037, acc: 0.6413662239089184, test loss: 0.9883583100606839, acc: 0.6421614828777883
epoch: 46, train loss: 0.5445400191579362, acc: 0.7826116373477673, val loss: 0.7322297085696274, acc: 0.7197975964579381, test loss: 0.7290886441679297, acc: 0.7141061891297518
epoch: 47, train loss: 0.5151351500428578, acc: 0.793978349120433, val loss: 0.8066565798928962, acc: 0.7024035420619861, test loss: 0.8364097931232986, acc: 0.6977693999371662
epoch: 48, train loss: 0.5286755080152107, acc: 0.7906630581867389, val loss: 0.7951629888702239, acc: 0.6970271979759646, test loss: 0.806535847262695, acc: 0.6936852026390198
epoch: 49, train loss: 0.5207120454037141, acc: 0.7924221921515562, val loss: 0.8486746443290638, acc: 0.6884882985452245, test loss: 0.8522350943916218, acc: 0.688344329249136
epoch: 50, train loss: 0.5229672170459659, acc: 0.7949932341001353, val loss: 0.851807123946661, acc: 0.687539531941809, test loss: 0.849990046215327, acc: 0.6782909205152372
epoch: 51, train loss: 0.5076387600743884, acc: 0.7951962110960757, val loss: 0.7981362428791835, acc: 0.711258697027198, test loss: 0.8007073026388051, acc: 0.7005969211435752
epoch: 52, train loss: 0.5074435511203192, acc: 0.7980378890392422, val loss: 0.8602948795007347, acc: 0.6986084756483238, test loss: 0.879639700303241, acc: 0.7018535972353126
epoch: 53, train loss: 0.5115561659184458, acc: 0.7951962110960757, val loss: 0.7477842830993343, acc: 0.715370018975332, test loss: 0.7554395404807285, acc: 0.7137920201068174
epoch: 54, train loss: 0.4824227111910612, acc: 0.8045331529093369, val loss: 0.7987555466747827, acc: 0.7052498418722327, test loss: 0.8071732314322828, acc: 0.6996544140747722
epoch: 55, train loss: 0.4999488425593576, acc: 0.8001353179972936, val loss: 0.8398138623234593, acc: 0.6995572422517394, test loss: 0.8478922679144895, acc: 0.6918001885014138
epoch: 56, train loss: 0.4814348092905078, acc: 0.8071718538565629, val loss: 0.7478343252137369, acc: 0.7248576850094877, test loss: 0.7711347586098161, acc: 0.7090794847628024
epoch: 57, train loss: 0.48260586927159715, acc: 0.8059539918809202, val loss: 0.9883983702240376, acc: 0.6742567994939912, test loss: 0.9672780484850587, acc: 0.6685516808042727
epoch: 58, train loss: 0.48127966338468664, acc: 0.8051420838971584, val loss: 0.7682588771503986, acc: 0.7118912080961417, test loss: 0.7675293953284654, acc: 0.7134778510838832
epoch: 59, train loss: 0.4823122513794286, acc: 0.8077807848443843, val loss: 0.7365103381710064, acc: 0.7286527514231499, test loss: 0.760549209269944, acc: 0.7229029217719133
epoch: 60, train loss: 0.4626449897744175, acc: 0.8146143437077131, val loss: 0.7649267988066217, acc: 0.7182163187855788, test loss: 0.814359487170436, acc: 0.7071944706251964
epoch: 61, train loss: 0.4502225081798027, acc: 0.816508795669824, val loss: 0.7438474178012604, acc: 0.724225173940544, test loss: 0.7928106022300564, acc: 0.7185045554508326
epoch: 62, train loss: 0.4523569553082302, acc: 0.816576454668471, val loss: 0.7463043304578478, acc: 0.7397216951296648, test loss: 0.7572168507667372, acc: 0.7269871190700596
epoch: 63, train loss: 0.45151070725772635, acc: 0.8178619756427605, val loss: 0.8109273171590748, acc: 0.7080961416824795, test loss: 0.8347814332528793, acc: 0.7125353440150801
epoch: 64, train loss: 0.49574293204670183, acc: 0.8068335588633289, val loss: 0.8269853576838706, acc: 0.6793168880455408, test loss: 0.8380062544903889, acc: 0.6880301602262017
epoch: 65, train loss: 0.45571480852341295, acc: 0.8181326116373477, val loss: 0.8302689010323339, acc: 0.6925996204933587, test loss: 0.8652227036502801, acc: 0.6927426955702167
epoch: 66, train loss: 0.44009747007541633, acc: 0.8214479025710419, val loss: 0.7117704877666399, acc: 0.7359266287160026, test loss: 0.7114977635866885, acc: 0.7445805843543827
epoch: 67, train loss: 0.4257835039911799, acc: 0.8278755074424898, val loss: 0.7857809435484305, acc: 0.7106261859582542, test loss: 0.8228627792739509, acc: 0.6996544140747722
epoch: 68, train loss: 0.424279964091168, acc: 0.8235453315290934, val loss: 0.8181674643304814, acc: 0.7204301075268817, test loss: 0.8058207222324028, acc: 0.7244737668865849
epoch: 69, train loss: 0.42951355213564046, acc: 0.8223274695534506, val loss: 0.7660075061913912, acc: 0.7330803289057558, test loss: 0.7855767196280156, acc: 0.7266729500471254
epoch: 70, train loss: 0.4177063493509254, acc: 0.8294993234100135, val loss: 0.8906526738974506, acc: 0.7039848197343453, test loss: 0.8748811945164687, acc: 0.70311027332705
epoch: 71, train loss: 0.4397771191693772, acc: 0.8215155615696887, val loss: 0.8277528338761061, acc: 0.7020872865275142, test loss: 0.8777635242067865, acc: 0.702167766258247
epoch: 72, train loss: 0.41102999401995877, acc: 0.8316644113667118, val loss: 0.7512229010698389, acc: 0.736875395319418, test loss: 0.7936216381934691, acc: 0.7332704995287465
epoch: 73, train loss: 0.3918026807988932, acc: 0.8351826792963464, val loss: 0.9250809555186417, acc: 0.6970271979759646, test loss: 0.9237576330228681, acc: 0.6823751178133836
Epoch    73: reducing learning rate of group 0 to 1.5000e-03.
epoch: 74, train loss: 0.3342375787572383, acc: 0.8596752368064953, val loss: 0.7404170386177463, acc: 0.7590132827324478, test loss: 0.7791555096733843, acc: 0.748664781652529
epoch: 75, train loss: 0.29396663864668715, acc: 0.8764546684709066, val loss: 0.7165566820799739, acc: 0.7615433270082227, test loss: 0.738587718394954, acc: 0.7571473452717562
epoch: 76, train loss: 0.2707272414667519, acc: 0.8843707713125846, val loss: 0.8055204283504981, acc: 0.7381404174573055, test loss: 0.8222240920041366, acc: 0.742067232170908
epoch: 77, train loss: 0.28736728414872986, acc: 0.8778755074424899, val loss: 0.7313107979742505, acc: 0.7612270714737508, test loss: 0.7389827212034963, acc: 0.7511781338360037
epoch: 78, train loss: 0.26254072703511855, acc: 0.8858592692828147, val loss: 0.7709637201714561, acc: 0.7580645161290323, test loss: 0.7896486672447269, acc: 0.7518064718818724
epoch: 79, train loss: 0.2529714548095799, acc: 0.8896481732070365, val loss: 0.7545303475019828, acc: 0.763124604680582, test loss: 0.785091022294608, acc: 0.7562048382029531
epoch: 80, train loss: 0.24930768296102387, acc: 0.8909336941813261, val loss: 0.7301074966156806, acc: 0.7621758380771664, test loss: 0.7796124822120415, acc: 0.7518064718818724
epoch: 81, train loss: 0.24547576779280367, acc: 0.8937077131258457, val loss: 0.7807510770777221, acc: 0.7624920936116382, test loss: 0.8016997744667803, acc: 0.7521206409048068
epoch: 82, train loss: 0.2631009838497203, acc: 0.8870094722598105, val loss: 0.756319039536005, acc: 0.7580645161290323, test loss: 0.7949555548638845, acc: 0.7474081055607917
epoch: 83, train loss: 0.2394622733854312, acc: 0.8947902571041949, val loss: 0.7766746278807455, acc: 0.7615433270082227, test loss: 0.8114051676156047, acc: 0.7571473452717562
epoch: 84, train loss: 0.2362143190933016, acc: 0.8956698240866036, val loss: 0.7740346445907602, acc: 0.7586970271979759, test loss: 0.810495570887195, acc: 0.7505497957901351
epoch: 85, train loss: 0.23639312665581866, acc: 0.8960757780784845, val loss: 0.7387079102264635, acc: 0.7634408602150538, test loss: 0.7702189871548033, acc: 0.7558906691800189
epoch: 86, train loss: 0.23606630812155863, acc: 0.8979702300405954, val loss: 0.8279366113957086, acc: 0.7466793168880456, test loss: 0.8327414844907233, acc: 0.7401822180333019
epoch: 87, train loss: 0.2398336870825178, acc: 0.8957374830852504, val loss: 0.8924211069875545, acc: 0.7131562302340291, test loss: 0.8968559605049855, acc: 0.7219604147031102
epoch: 88, train loss: 0.27125569547303474, acc: 0.8827469553450609, val loss: 0.7457365230395023, acc: 0.7605945604048071, test loss: 0.783602725774134, acc: 0.7530631479736098
epoch: 89, train loss: 0.22604926155529745, acc: 0.8983761840324763, val loss: 0.7207735507718979, acc: 0.7697659709044908, test loss: 0.7727000535826544, acc: 0.7552623311341502
epoch: 90, train loss: 0.21068554245973956, acc: 0.9051420838971583, val loss: 0.8253642766253998, acc: 0.7605945604048071, test loss: 0.866066700027231, acc: 0.7549481621112158
epoch: 91, train loss: 0.21919609875253154, acc: 0.9018944519621109, val loss: 0.8303248823480769, acc: 0.7495256166982922, test loss: 0.8318715551683747, acc: 0.7527489789506755
epoch: 92, train loss: 0.22962448516701814, acc: 0.9002029769959404, val loss: 0.7579371037926574, acc: 0.7647058823529411, test loss: 0.8013762286950536, acc: 0.7552623311341502
epoch: 93, train loss: 0.20407818532038444, acc: 0.906765899864682, val loss: 0.9081311317880572, acc: 0.7454142947501581, test loss: 0.9740814310254221, acc: 0.7329563305058121
epoch: 94, train loss: 0.20457307316856874, acc: 0.9080514208389716, val loss: 0.8572702297751169, acc: 0.7526881720430108, test loss: 0.9093612978902583, acc: 0.7508639648130694
epoch: 95, train loss: 0.20343074265852348, acc: 0.9095399188092016, val loss: 0.7800282539256685, acc: 0.7545857052498419, test loss: 0.8515306693594522, acc: 0.7436380772855796
epoch: 96, train loss: 0.19767310868983984, acc: 0.9111637347767253, val loss: 0.8528580502721798, acc: 0.7599620493358634, test loss: 0.9207896493419775, acc: 0.7496072887213321
epoch: 97, train loss: 0.217908917171868, acc: 0.9055480378890393, val loss: 0.7739131287786495, acc: 0.7609108159392789, test loss: 0.8056109601732998, acc: 0.7596606974552309
epoch: 98, train loss: 0.1970111263419359, acc: 0.9133288227334235, val loss: 0.8058469283437518, acc: 0.7713472485768501, test loss: 0.8362973200012744, acc: 0.7646874018221803
epoch: 99, train loss: 0.1751840677167792, acc: 0.9214479025710419, val loss: 0.8870732938089678, acc: 0.7586970271979759, test loss: 0.9129243894002365, acc: 0.7524348099277411
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.1433573505955878, acc: 0.9153585926928282, val loss: 0.6971111540399271, acc: 0.7517394054395952, test loss: 0.7332978501770806, acc: 0.7530631479736098
epoch: 101, train loss: 0.1354739383205832, acc: 0.9141407307171854, val loss: 0.6830649855467129, acc: 0.7555344718532574, test loss: 0.7265279390435154, acc: 0.7565190072258875
epoch: 102, train loss: 0.12259115561419476, acc: 0.9239512855209743, val loss: 0.7353966028761215, acc: 0.7514231499051234, test loss: 0.7856268632947518, acc: 0.7496072887213321
epoch: 103, train loss: 0.1259699861719418, acc: 0.9194181326116373, val loss: 0.7394966294236156, acc: 0.7460468058191019, test loss: 0.7618305968519206, acc: 0.7389255419415646
epoch: 104, train loss: 0.135328508056549, acc: 0.9132611637347767, val loss: 0.7693141820233205, acc: 0.7514231499051234, test loss: 0.7719914458511836, acc: 0.7505497957901351
epoch: 105, train loss: 0.12926646369800515, acc: 0.918403247631935, val loss: 0.7117892002320455, acc: 0.7618595825426945, test loss: 0.787167220642984, acc: 0.7514923028589381
epoch: 106, train loss: 0.1415540062768211, acc: 0.9134641407307171, val loss: 0.7220306557842933, acc: 0.7479443390259329, test loss: 0.7512594468931413, acc: 0.7530631479736098
epoch: 107, train loss: 0.13740571258597187, acc: 0.9146820027063599, val loss: 0.7046157009310544, acc: 0.7618595825426945, test loss: 0.7276571526154384, acc: 0.7596606974552309
epoch: 108, train loss: 0.126661470229958, acc: 0.9218538565629228, val loss: 0.7006496929202481, acc: 0.7501581277672359, test loss: 0.7103697929298433, acc: 0.7502356267672008
epoch: 109, train loss: 0.12395207687546984, acc: 0.9185385656292286, val loss: 0.7710850521554531, acc: 0.7533206831119544, test loss: 0.8248068833927894, acc: 0.7464655984919887
epoch: 110, train loss: 0.14025684854013187, acc: 0.9155615696887686, val loss: 0.734103284253115, acc: 0.7580645161290323, test loss: 0.7499150283824263, acc: 0.7568331762488218
epoch: 111, train loss: 0.13879192597327278, acc: 0.9095399188092016, val loss: 0.7740055275144342, acc: 0.7384566729917773, test loss: 0.8457796211853891, acc: 0.7291863022306001
epoch: 112, train loss: 0.1473330077867708, acc: 0.904871447902571, val loss: 0.6976326032954632, acc: 0.7533206831119544, test loss: 0.7355681937931096, acc: 0.7536914860194784
epoch: 113, train loss: 0.12399334670202013, acc: 0.9192828146143437, val loss: 0.7674947457551805, acc: 0.7469955724225174, test loss: 0.7887556280611897, acc: 0.7502356267672008
epoch: 114, train loss: 0.12632992981815855, acc: 0.9191474966170501, val loss: 0.7310595118041886, acc: 0.7599620493358634, test loss: 0.7546290167839393, acc: 0.7530631479736098
epoch: 115, train loss: 0.12655350158396853, acc: 0.9200270635994587, val loss: 0.7027451255365236, acc: 0.7643896268184693, test loss: 0.7222435951532525, acc: 0.7637448947533774
epoch: 116, train loss: 0.11752919535756272, acc: 0.9211096075778078, val loss: 0.7463156303071584, acc: 0.7428842504743833, test loss: 0.79300560110064, acc: 0.74583726044612
epoch: 117, train loss: 0.13886091044533075, acc: 0.9103518267929634, val loss: 0.6714511087316263, acc: 0.7580645161290323, test loss: 0.7341808691887671, acc: 0.7593465284322966
epoch: 118, train loss: 0.1282467543554564, acc: 0.9190798376184033, val loss: 0.8146145718524458, acc: 0.7229601518026565, test loss: 0.8371980880889509, acc: 0.7304429783223374
epoch: 119, train loss: 0.141612074436451, acc: 0.9095399188092016, val loss: 0.73802204316059, acc: 0.763124604680582, test loss: 0.7451703149569023, acc: 0.7640590637763116
epoch: 120, train loss: 0.13207975620345913, acc: 0.915020297699594, val loss: 0.6591744899448151, acc: 0.7694497153700189, test loss: 0.6879282484554923, acc: 0.7596606974552309
epoch: 121, train loss: 0.10847581666119205, acc: 0.9273342354533153, val loss: 0.7594557806783504, acc: 0.7567994939911449, test loss: 0.8163779981474137, acc: 0.7590323594093622
epoch: 122, train loss: 0.1163272124141737, acc: 0.921786197564276, val loss: 0.8660878133803964, acc: 0.7321315623023403, test loss: 0.8578157219690532, acc: 0.7257304429783223
epoch: 123, train loss: 0.1248735597112666, acc: 0.9194857916102842, val loss: 0.7146145236560018, acc: 0.7618595825426945, test loss: 0.7629783346036527, acc: 0.7496072887213321
epoch: 124, train loss: 0.11300084309019803, acc: 0.9240189445196211, val loss: 0.7603366632841569, acc: 0.7615433270082227, test loss: 0.7756875294617351, acc: 0.7533773169965441
Epoch   124: reducing learning rate of group 0 to 7.5000e-04.
epoch: 125, train loss: 0.08071601231039296, acc: 0.9451962110960758, val loss: 0.7325251070779008, acc: 0.7729285262492094, test loss: 0.7625945850321979, acc: 0.7737983034872762
epoch: 126, train loss: 0.056134554477602606, acc: 0.9579837618403247, val loss: 0.7631349109683438, acc: 0.7760910815939279, test loss: 0.7949976156688957, acc: 0.7712849513038015
epoch: 127, train loss: 0.04903495841847382, acc: 0.9615696887686063, val loss: 0.8425918071802322, acc: 0.7640733712839974, test loss: 0.8652880437112701, acc: 0.7609173735469683
epoch: 128, train loss: 0.047531738775468485, acc: 0.962043301759134, val loss: 0.8448921417149466, acc: 0.7656546489563567, test loss: 0.8934351522668592, acc: 0.7678290920515237
epoch: 129, train loss: 0.05081694252960414, acc: 0.961299052774019, val loss: 0.8459159290390932, acc: 0.7700822264389627, test loss: 0.9149590757632458, acc: 0.7631165567075087
epoch: 130, train loss: 0.049491021281085575, acc: 0.9634641407307172, val loss: 0.872117054530294, acc: 0.7691334598355472, test loss: 0.922971563372191, acc: 0.76248821866164
epoch: 131, train loss: 0.05011161809209961, acc: 0.9616373477672531, val loss: 0.8338600194280771, acc: 0.7688172043010753, test loss: 0.8728284084156479, acc: 0.7715991203267358
epoch: 132, train loss: 0.04299993590125373, acc: 0.9675236806495264, val loss: 0.8595555541939105, acc: 0.7741935483870968, test loss: 0.9089467055835778, acc: 0.7690857681432611
epoch: 133, train loss: 0.04299232608592075, acc: 0.9650879566982409, val loss: 0.865425865081652, acc: 0.7719797596457938, test loss: 0.9208220560671285, acc: 0.763430725730443
epoch: 134, train loss: 0.04638455055845606, acc: 0.9644113667117726, val loss: 0.8537787932674015, acc: 0.7722960151802657, test loss: 0.9130838915794629, acc: 0.7662582469368521
epoch: 135, train loss: 0.04363167869716036, acc: 0.9672530446549391, val loss: 0.8447669322998896, acc: 0.7760910815939279, test loss: 0.9256534761428233, acc: 0.7681432610744581
epoch: 136, train loss: 0.05259814082930956, acc: 0.9610960757780784, val loss: 0.9460605418357874, acc: 0.7624920936116382, test loss: 0.9990635206592259, acc: 0.7423814011938423
epoch: 137, train loss: 0.06328481022213082, acc: 0.9539918809201624, val loss: 0.821689584600556, acc: 0.7745098039215687, test loss: 0.8669683213433191, acc: 0.7615457115928369
epoch: 138, train loss: 0.048150682191242224, acc: 0.9650879566982409, val loss: 0.9249135411894073, acc: 0.7628083491461101, test loss: 0.9431976551834308, acc: 0.7618598806157713
epoch: 139, train loss: 0.04161626401991257, acc: 0.9685385656292287, val loss: 0.8660823976141527, acc: 0.7700822264389627, test loss: 0.9184437767724515, acc: 0.765315739868049
epoch: 140, train loss: 0.0424710740739825, acc: 0.9675236806495264, val loss: 0.8743971478403708, acc: 0.7700822264389627, test loss: 0.9318861819047656, acc: 0.7596606974552309
epoch: 141, train loss: 0.04884983196108525, acc: 0.9642083897158322, val loss: 0.8694452042069939, acc: 0.7719797596457938, test loss: 0.9181833409529004, acc: 0.76248821866164
epoch: 142, train loss: 0.06798031885020626, acc: 0.9520297699594046, val loss: 0.7908235056199732, acc: 0.7653383934218849, test loss: 0.8299050374559888, acc: 0.7584040213634936
epoch: 143, train loss: 0.053618958858579036, acc: 0.9596752368064952, val loss: 0.8985415625466643, acc: 0.7697659709044908, test loss: 0.9087109070246676, acc: 0.7646874018221803
epoch: 144, train loss: 0.04936845105378328, acc: 0.9633964817320704, val loss: 0.8519623312748059, acc: 0.7675521821631879, test loss: 0.8922097088217248, acc: 0.7719132893496701
epoch: 145, train loss: 0.04469933225817384, acc: 0.9663734776725305, val loss: 0.8768050369018998, acc: 0.7659709044908286, test loss: 0.8707884414893368, acc: 0.7719132893496701
epoch: 146, train loss: 0.04669994313637846, acc: 0.9637347767253045, val loss: 0.9492495355539726, acc: 0.7536369386464263, test loss: 0.9692838675115155, acc: 0.7470939365378574
epoch: 147, train loss: 0.05905513879572103, acc: 0.9556156968876861, val loss: 0.8501486310771867, acc: 0.7697659709044908, test loss: 0.8603280503683183, acc: 0.7662582469368521
epoch: 148, train loss: 0.054168771785029865, acc: 0.9602841677943167, val loss: 0.8684372567135198, acc: 0.7567994939911449, test loss: 0.8857832525722493, acc: 0.7533773169965441
epoch: 149, train loss: 0.05648908708874686, acc: 0.9611637347767253, val loss: 0.8597537408065072, acc: 0.7716635041113219, test loss: 0.9039983000352328, acc: 0.760603204524034
epoch: 150, train loss: 0.05293256216537565, acc: 0.9613667117726657, val loss: 0.8306268977636329, acc: 0.7685009487666035, test loss: 0.8924518068219519, acc: 0.7615457115928369
epoch: 151, train loss: 0.036183889722388556, acc: 0.9727334235453315, val loss: 0.9155012107513737, acc: 0.765022137887413, test loss: 0.9679051967600476, acc: 0.7571473452717562
epoch: 152, train loss: 0.05978331142413923, acc: 0.956427604871448, val loss: 0.8761824665757239, acc: 0.7514231499051234, test loss: 0.8963890367059112, acc: 0.746779767514923
epoch: 153, train loss: 0.04704447141640564, acc: 0.9656292286874154, val loss: 0.8603277557790467, acc: 0.7615433270082227, test loss: 0.8895598462344789, acc: 0.76248821866164
epoch: 154, train loss: 0.04502428576188901, acc: 0.9679972936400542, val loss: 0.8781052778824306, acc: 0.7703984819734345, test loss: 0.9119368893823795, acc: 0.7628023876845743
epoch: 155, train loss: 0.039047765970754364, acc: 0.9713802435723952, val loss: 0.9118932901041029, acc: 0.7615433270082227, test loss: 0.9522159328409758, acc: 0.764373232799246
epoch: 156, train loss: 0.04066760307719808, acc: 0.96914749661705, val loss: 0.8950037028803093, acc: 0.7662871600253004, test loss: 0.9435952753905339, acc: 0.7590323594093622
epoch: 157, train loss: 0.03633270170716052, acc: 0.9733423545331529, val loss: 0.9273145883464874, acc: 0.7659709044908286, test loss: 0.9516126235401035, acc: 0.7675149230285894
epoch: 158, train loss: 0.041234551852062366, acc: 0.969553450608931, val loss: 0.8782882978462253, acc: 0.7710309930423782, test loss: 0.9601996079803524, acc: 0.76248821866164
epoch: 159, train loss: 0.0642641194618442, acc: 0.9552097428958052, val loss: 0.8568180016065835, acc: 0.7574320050600886, test loss: 0.887286101726852, acc: 0.7508639648130694
epoch: 160, train loss: 0.06154169265649638, acc: 0.9571718538565629, val loss: 0.8219648632047147, acc: 0.7593295382669196, test loss: 0.9018871663240677, acc: 0.7521206409048068
epoch: 161, train loss: 0.04511295866942051, acc: 0.9654939106901218, val loss: 0.861434192615246, acc: 0.767235926628716, test loss: 0.8795569580454441, acc: 0.767200754005655
epoch: 162, train loss: 0.04615423990985699, acc: 0.9673883626522327, val loss: 0.9428575087166677, acc: 0.7400379506641366, test loss: 0.9604458102556732, acc: 0.7474081055607917
epoch: 163, train loss: 0.056208340462715604, acc: 0.9588633288227334, val loss: 0.8774653800600017, acc: 0.7545857052498419, test loss: 0.8820649917643286, acc: 0.7621740496387056
epoch: 164, train loss: 0.057514131369122146, acc: 0.9585926928281462, val loss: 0.8354712729963752, acc: 0.7634408602150538, test loss: 0.8964727175823742, acc: 0.7681432610744581
epoch: 165, train loss: 0.0529110420379329, acc: 0.9621786197564276, val loss: 0.8830767089546971, acc: 0.7523719165085389, test loss: 0.8588912884768847, acc: 0.7599748664781653
epoch: 166, train loss: 0.0412038872867863, acc: 0.9684709066305819, val loss: 0.9640477344553958, acc: 0.7523719165085389, test loss: 0.9678080114897339, acc: 0.746779767514923
epoch: 167, train loss: 0.03613785936400113, acc: 0.9724627875507442, val loss: 0.9523313736176656, acc: 0.7454142947501581, test loss: 0.9740787849761838, acc: 0.7543198240653471
epoch: 168, train loss: 0.04599025411599383, acc: 0.9640730717185385, val loss: 0.8868318869296996, acc: 0.7628083491461101, test loss: 0.8987848707013935, acc: 0.7599748664781653
epoch: 169, train loss: 0.04491713466185759, acc: 0.9664411366711773, val loss: 0.8895546248410036, acc: 0.7640733712839974, test loss: 0.9029886757020414, acc: 0.7637448947533774
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.03114394617389197, acc: 0.9684032476319351, val loss: 0.7588613953943271, acc: 0.7700822264389627, test loss: 0.8052865758917447, acc: 0.7741124725102105
epoch: 171, train loss: 0.027970022262554208, acc: 0.971447902571042, val loss: 0.7267315457848942, acc: 0.7748260594560404, test loss: 0.7888521564632252, acc: 0.7665724159597863
epoch: 172, train loss: 0.02435701280247536, acc: 0.9726657645466847, val loss: 0.7572527295950467, acc: 0.7735610373181531, test loss: 0.8006058566195015, acc: 0.7659440779139177
epoch: 173, train loss: 0.021209763891803395, acc: 0.9753721244925575, val loss: 0.7615594425056645, acc: 0.7786211258697027, test loss: 0.8140147850268523, acc: 0.7734841344643418
epoch: 174, train loss: 0.020823835656651787, acc: 0.977063599458728, val loss: 0.792667785073895, acc: 0.7609108159392789, test loss: 0.8202633280968314, acc: 0.7562048382029531
epoch: 175, train loss: 0.03298441252402749, acc: 0.9668470906630582, val loss: 0.7904056509712841, acc: 0.7666034155597723, test loss: 0.8521807369779874, acc: 0.7577756833176249
epoch: 176, train loss: 0.04532124982171355, acc: 0.9543978349120433, val loss: 0.7039359136897503, acc: 0.7685009487666035, test loss: 0.7660574939990246, acc: 0.7646874018221803
epoch: 177, train loss: 0.03342307389226108, acc: 0.9661028416779431, val loss: 0.7311550811602299, acc: 0.7678684376976597, test loss: 0.787533026238938, acc: 0.7621740496387056
epoch: 178, train loss: 0.026116215104945457, acc: 0.9715832205683356, val loss: 0.7360063185049416, acc: 0.7685009487666035, test loss: 0.772257371140545, acc: 0.7675149230285894
epoch: 179, train loss: 0.024496709224698668, acc: 0.9724627875507442, val loss: 0.7849121717317884, acc: 0.7571157495256167, test loss: 0.80437355505158, acc: 0.7668865849827207
epoch: 180, train loss: 0.022087832814503425, acc: 0.9756427604871448, val loss: 0.7732061872295305, acc: 0.7729285262492094, test loss: 0.8431691324564784, acc: 0.767200754005655
epoch: 181, train loss: 0.039768381198018746, acc: 0.9614343707713126, val loss: 0.7852127187844699, acc: 0.7488931056293485, test loss: 0.8212281332576271, acc: 0.7445805843543827
epoch: 182, train loss: 0.042570248094963285, acc: 0.9585926928281462, val loss: 0.7234197674786871, acc: 0.7621758380771664, test loss: 0.7538445871879274, acc: 0.763430725730443
epoch: 183, train loss: 0.026795800728912442, acc: 0.9715832205683356, val loss: 0.7532474316048667, acc: 0.767235926628716, test loss: 0.7747213255486471, acc: 0.7668865849827207
epoch: 184, train loss: 0.026959477851976398, acc: 0.9722598105548038, val loss: 0.7354308631433708, acc: 0.763124604680582, test loss: 0.7744426910434546, acc: 0.763430725730443
Epoch   184: reducing learning rate of group 0 to 3.7500e-04.
epoch: 185, train loss: 0.021641120369040756, acc: 0.9771989174560216, val loss: 0.7367170351656986, acc: 0.7700822264389627, test loss: 0.7642417778730617, acc: 0.7816525290606346
epoch: 186, train loss: 0.013419162250641879, acc: 0.9836941813261164, val loss: 0.7390414792325661, acc: 0.7700822264389627, test loss: 0.775310116296589, acc: 0.7797675149230285
epoch: 187, train loss: 0.012734907060730521, acc: 0.9847090663058187, val loss: 0.7705209749850346, acc: 0.7710309930423782, test loss: 0.794371579077646, acc: 0.7775683317624882
epoch: 188, train loss: 0.010961850402632247, acc: 0.9872801082543978, val loss: 0.774474897164472, acc: 0.7722960151802657, test loss: 0.8045404883177072, acc: 0.7769399937166196
epoch: 189, train loss: 0.010403750498598903, acc: 0.988700947225981, val loss: 0.7850657598192672, acc: 0.7732447817836812, test loss: 0.8154546858715329, acc: 0.7775683317624882
epoch: 190, train loss: 0.010875648586268515, acc: 0.9874154262516914, val loss: 0.7727214462164457, acc: 0.7697659709044908, test loss: 0.8179757195076327, acc: 0.7759974866478165
epoch: 191, train loss: 0.011252297428770543, acc: 0.9868741542625169, val loss: 0.7893059743800396, acc: 0.7697659709044908, test loss: 0.8159524133060849, acc: 0.7803958529688972
epoch: 192, train loss: 0.010038588418850879, acc: 0.9888362652232747, val loss: 0.7918884867282701, acc: 0.7694497153700189, test loss: 0.8223358650009773, acc: 0.7759974866478165
epoch: 193, train loss: 0.008261818198943373, acc: 0.9904600811907984, val loss: 0.8041962962750767, acc: 0.773877292852625, test loss: 0.8317485910296403, acc: 0.7788250078542256
epoch: 194, train loss: 0.008326864523588804, acc: 0.9910690121786198, val loss: 0.8048624813669772, acc: 0.7697659709044908, test loss: 0.8287043465483387, acc: 0.7759974866478165
epoch: 195, train loss: 0.008804753128343409, acc: 0.9908660351826793, val loss: 0.8151163685404749, acc: 0.775774826059456, test loss: 0.8480371100476355, acc: 0.7794533459000943
epoch: 196, train loss: 0.009302052249312805, acc: 0.9894451962110961, val loss: 0.8091737316953164, acc: 0.7713472485768501, test loss: 0.8454876674483716, acc: 0.7766258246936852
epoch: 197, train loss: 0.010441620858998638, acc: 0.9877537212449256, val loss: 0.8288679486358566, acc: 0.7637571157495257, test loss: 0.8388395705314466, acc: 0.7706566132579328
epoch: 198, train loss: 0.01086116169751981, acc: 0.988700947225981, val loss: 0.815682191474765, acc: 0.7703984819734345, test loss: 0.8402090579730102, acc: 0.7747408105560791
epoch: 199, train loss: 0.011386339200335443, acc: 0.9876860622462788, val loss: 0.7880550071757327, acc: 0.7735610373181531, test loss: 0.8232308628796557, acc: 0.7753691486019478
epoch: 200, train loss: 0.013104705477110708, acc: 0.986468200270636, val loss: 0.8155131349074999, acc: 0.765022137887413, test loss: 0.8342754983916958, acc: 0.7722274583726044
best val acc 0.7786211258697027 at epoch 173.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9937    0.9993    0.9964      5337
           1     0.9926    0.9926    0.9926       810
           2     0.9914    0.9933    0.9924      2100
           3     0.9986    0.9905    0.9946       737
           4     0.9955    0.9897    0.9926       677
           5     0.9917    0.9977    0.9947      1323
           6     0.9886    0.9691    0.9787      1164
           7     0.9929    0.9905    0.9917       421
           8     0.9975    0.9975    0.9975       401
           9     0.9975    1.0000    0.9987       396
          10     0.9889    0.9968    0.9929       627
          11     0.9965    0.9897    0.9931       291
          12     0.9469    0.8889    0.9170       261
          13     0.9582    0.9745    0.9662       235

    accuracy                         0.9917     14780
   macro avg     0.9879    0.9836    0.9857     14780
weighted avg     0.9917    0.9917    0.9917     14780

train confusion matrix:
[[9.99250515e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.62113547e-04
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.87371182e-04 0.00000000e+00]
 [0.00000000e+00 9.92592593e-01 0.00000000e+00 0.00000000e+00
  1.23456790e-03 3.70370370e-03 0.00000000e+00 0.00000000e+00
  1.23456790e-03 0.00000000e+00 1.23456790e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.90476190e-03 0.00000000e+00 9.93333333e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.76190476e-03 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 9.90502035e-01
  0.00000000e+00 1.35685210e-03 2.71370421e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 5.42740841e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 8.86262925e-03 0.00000000e+00 0.00000000e+00
  9.89660266e-01 1.47710487e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.51171580e-03 9.97732426e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 7.55857899e-04
  0.00000000e+00 0.00000000e+00]
 [1.20274914e-02 0.00000000e+00 0.00000000e+00 8.59106529e-04
  0.00000000e+00 5.15463918e-03 9.69072165e-01 0.00000000e+00
  0.00000000e+00 8.59106529e-04 1.71821306e-03 0.00000000e+00
  1.71821306e-03 8.59106529e-03]
 [9.50118765e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.90498812e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.49376559e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.97506234e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.18979266e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.96810207e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.03092784e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.89690722e-01
  0.00000000e+00 0.00000000e+00]
 [2.68199234e-02 0.00000000e+00 6.89655172e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.53256705e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  8.88888889e-01 0.00000000e+00]
 [4.25531915e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.12765957e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.74468085e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8351    0.9038    0.8681      1143
           1     0.8494    0.8150    0.8319       173
           2     0.8116    0.7178    0.7618       450
           3     0.7974    0.7722    0.7846       158
           4     0.8255    0.8483    0.8367       145
           5     0.8409    0.7845    0.8117       283
           6     0.5274    0.6185    0.5693       249
           7     0.7204    0.7444    0.7322        90
           8     0.6316    0.5647    0.5963        85
           9     0.8873    0.7500    0.8129        84
          10     0.8115    0.7388    0.7734       134
          11     0.7442    0.5161    0.6095        62
          12     0.0847    0.0893    0.0870        56
          13     0.7692    0.6000    0.6742        50

    accuracy                         0.7786      3162
   macro avg     0.7240    0.6759    0.6964      3162
weighted avg     0.7820    0.7786    0.7782      3162

validation confusion matrix:
[[9.03762030e-01 3.49956255e-03 2.09973753e-02 0.00000000e+00
  8.74890639e-04 8.74890639e-04 3.41207349e-02 1.13735783e-02
  6.99912511e-03 0.00000000e+00 2.62467192e-03 3.49956255e-03
  8.74890639e-03 2.62467192e-03]
 [4.04624277e-02 8.15028902e-01 0.00000000e+00 5.78034682e-03
  5.20231214e-02 4.62427746e-02 5.78034682e-03 0.00000000e+00
  1.73410405e-02 0.00000000e+00 1.15606936e-02 5.78034682e-03
  0.00000000e+00 0.00000000e+00]
 [1.20000000e-01 4.44444444e-03 7.17777778e-01 1.77777778e-02
  2.22222222e-03 8.88888889e-03 6.88888889e-02 0.00000000e+00
  0.00000000e+00 4.44444444e-03 0.00000000e+00 4.44444444e-03
  4.88888889e-02 2.22222222e-03]
 [1.89873418e-02 6.32911392e-03 1.89873418e-02 7.72151899e-01
  6.32911392e-03 1.26582278e-02 6.32911392e-02 6.32911392e-03
  0.00000000e+00 0.00000000e+00 5.06329114e-02 1.26582278e-02
  2.53164557e-02 6.32911392e-03]
 [6.89655172e-03 5.51724138e-02 6.89655172e-03 0.00000000e+00
  8.48275862e-01 7.58620690e-02 0.00000000e+00 0.00000000e+00
  6.89655172e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.94699647e-02 1.41342756e-02 1.76678445e-02 1.76678445e-02
  4.24028269e-02 7.84452297e-01 1.41342756e-02 1.76678445e-02
  1.76678445e-02 3.53356890e-03 7.06713781e-03 0.00000000e+00
  1.41342756e-02 0.00000000e+00]
 [1.48594378e-01 0.00000000e+00 6.42570281e-02 3.61445783e-02
  4.01606426e-03 3.21285141e-02 6.18473896e-01 1.20481928e-02
  0.00000000e+00 1.60642570e-02 1.20481928e-02 4.01606426e-03
  4.41767068e-02 8.03212851e-03]
 [2.22222222e-01 0.00000000e+00 1.11111111e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 7.44444444e-01
  1.11111111e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.11111111e-02 0.00000000e+00]
 [2.94117647e-01 2.35294118e-02 0.00000000e+00 2.35294118e-02
  1.17647059e-02 2.35294118e-02 3.52941176e-02 1.17647059e-02
  5.64705882e-01 0.00000000e+00 1.17647059e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.19047619e-01 0.00000000e+00 2.38095238e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 5.95238095e-02 0.00000000e+00
  0.00000000e+00 7.50000000e-01 0.00000000e+00 1.19047619e-02
  1.19047619e-02 2.38095238e-02]
 [2.23880597e-02 7.46268657e-03 2.23880597e-02 1.49253731e-02
  0.00000000e+00 2.23880597e-02 1.26865672e-01 1.49253731e-02
  1.49253731e-02 7.46268657e-03 7.38805970e-01 0.00000000e+00
  7.46268657e-03 0.00000000e+00]
 [2.41935484e-01 4.83870968e-02 6.45161290e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.22580645e-02 0.00000000e+00
  9.67741935e-02 0.00000000e+00 0.00000000e+00 5.16129032e-01
  0.00000000e+00 0.00000000e+00]
 [1.42857143e-01 0.00000000e+00 2.32142857e-01 7.14285714e-02
  0.00000000e+00 3.57142857e-02 3.03571429e-01 1.78571429e-02
  3.57142857e-02 0.00000000e+00 7.14285714e-02 0.00000000e+00
  8.92857143e-02 0.00000000e+00]
 [1.40000000e-01 0.00000000e+00 6.00000000e-02 0.00000000e+00
  0.00000000e+00 2.00000000e-02 1.80000000e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 6.00000000e-01]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8223    0.8934    0.8564      1145
           1     0.8926    0.7600    0.8210       175
           2     0.8019    0.7627    0.7818       451
           3     0.8299    0.7673    0.7974       159
           4     0.8041    0.8151    0.8095       146
           5     0.8657    0.8169    0.8406       284
           6     0.4965    0.5640    0.5281       250
           7     0.8250    0.7253    0.7719        91
           8     0.6486    0.5517    0.5963        87
           9     0.8125    0.7558    0.7831        86
          10     0.7672    0.6544    0.7063       136
          11     0.6792    0.5625    0.6154        64
          12     0.1905    0.2105    0.2000        57
          13     0.6667    0.6154    0.6400        52

    accuracy                         0.7735      3183
   macro avg     0.7216    0.6754    0.6963      3183
weighted avg     0.7771    0.7735    0.7735      3183

test confusion matrix:
[[8.93449782e-01 2.62008734e-03 2.18340611e-02 1.74672489e-03
  8.73362445e-04 1.74672489e-03 4.10480349e-02 9.60698690e-03
  1.04803493e-02 2.62008734e-03 8.73362445e-04 3.49344978e-03
  5.24017467e-03 4.36681223e-03]
 [4.57142857e-02 7.60000000e-01 1.71428571e-02 0.00000000e+00
  1.02857143e-01 2.28571429e-02 5.71428571e-03 0.00000000e+00
  1.71428571e-02 0.00000000e+00 1.71428571e-02 5.71428571e-03
  5.71428571e-03 0.00000000e+00]
 [1.15299335e-01 0.00000000e+00 7.62749446e-01 8.86917960e-03
  0.00000000e+00 6.65188470e-03 5.54323725e-02 0.00000000e+00
  2.21729490e-03 4.43458980e-03 4.43458980e-03 2.21729490e-03
  3.54767184e-02 2.21729490e-03]
 [1.25786164e-02 0.00000000e+00 3.77358491e-02 7.67295597e-01
  6.28930818e-03 6.28930818e-03 1.00628931e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 4.40251572e-02 6.28930818e-03
  1.88679245e-02 0.00000000e+00]
 [2.73972603e-02 4.79452055e-02 6.84931507e-03 0.00000000e+00
  8.15068493e-01 7.53424658e-02 6.84931507e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.05479452e-02
  0.00000000e+00 0.00000000e+00]
 [5.28169014e-02 1.05633803e-02 1.05633803e-02 1.40845070e-02
  2.11267606e-02 8.16901408e-01 3.16901408e-02 3.52112676e-03
  1.05633803e-02 3.52112676e-03 1.05633803e-02 7.04225352e-03
  3.52112676e-03 3.52112676e-03]
 [1.52000000e-01 4.00000000e-03 9.20000000e-02 2.40000000e-02
  8.00000000e-03 1.60000000e-02 5.64000000e-01 4.00000000e-03
  4.00000000e-03 2.80000000e-02 2.80000000e-02 8.00000000e-03
  5.60000000e-02 1.20000000e-02]
 [2.41758242e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.09890110e-02 7.25274725e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.09890110e-02
  1.09890110e-02 0.00000000e+00]
 [2.87356322e-01 1.14942529e-02 2.29885057e-02 0.00000000e+00
  1.14942529e-02 5.74712644e-02 2.29885057e-02 0.00000000e+00
  5.51724138e-01 0.00000000e+00 1.14942529e-02 2.29885057e-02
  0.00000000e+00 0.00000000e+00]
 [8.13953488e-02 0.00000000e+00 2.32558140e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 8.13953488e-02 0.00000000e+00
  0.00000000e+00 7.55813953e-01 0.00000000e+00 0.00000000e+00
  1.16279070e-02 4.65116279e-02]
 [8.08823529e-02 7.35294118e-03 2.20588235e-02 3.67647059e-02
  0.00000000e+00 2.20588235e-02 1.02941176e-01 0.00000000e+00
  1.47058824e-02 0.00000000e+00 6.54411765e-01 0.00000000e+00
  5.14705882e-02 7.35294118e-03]
 [2.34375000e-01 0.00000000e+00 4.68750000e-02 0.00000000e+00
  0.00000000e+00 4.68750000e-02 3.12500000e-02 1.56250000e-02
  3.12500000e-02 1.56250000e-02 1.56250000e-02 5.62500000e-01
  0.00000000e+00 0.00000000e+00]
 [2.45614035e-01 0.00000000e+00 2.45614035e-01 7.01754386e-02
  0.00000000e+00 0.00000000e+00 1.40350877e-01 0.00000000e+00
  3.50877193e-02 1.75438596e-02 1.75438596e-02 0.00000000e+00
  2.10526316e-01 1.75438596e-02]
 [1.53846154e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.92307692e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.92307692e-02 0.00000000e+00
  1.92307692e-02 6.15384615e-01]]
---------------------------------------
program finished.
