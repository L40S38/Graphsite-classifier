seed:  666
number of classes (from original clusters): 10
whether to further subcluster data according to chemical reaction: True
positive training pair sampling threshold:  9000
negative training pair sampling threshold:  1500
positive validation pair sampling threshold:  2700
negative validation pair sampling threshold:  450
number of epochs to train: 60
batch size: 256
margin of contrastive loss: 1.7
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['charge', 'hydrophobicity', 'binding_probability', 'distance_to_center', 'sequence_entropy']
number of classes after further clustering:  13
number of pockets in training set:  10525
number of pockets in validation set:  2250
number of pockets in test set:  2269
number of train positive pairs: 117000
number of train negative pairs: 117000
number of validation positive pairs: 35100
number of validation negative pairs: 35100
model architecture:
SiameseNet(
  (embedding_net): EmbeddingNet(
    (set2set): Set2Set(32, 64)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=5, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=5, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=5, out_features=5, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(margin=1.7, normalize=True, mean=True)
epoch: 1, train loss: 0.624924068874783, validation loss: 0.5970209977022264.
epoch: 2, train loss: 0.57716190393562, validation loss: 0.558476518701624.
epoch: 3, train loss: 0.5454339125543578, validation loss: 0.5462872879593461.
epoch: 4, train loss: 0.5196943238413232, validation loss: 0.5250127068544046.
epoch: 5, train loss: 0.494912229293432, validation loss: 0.5156164181266415.
epoch: 6, train loss: 0.473492978837755, validation loss: 0.5137531597865613.
epoch: 7, train loss: 0.4584010297987196, validation loss: 0.5082324038339816.
epoch: 8, train loss: 0.4428399775904468, validation loss: 0.5158938704053221.
epoch: 9, train loss: 0.43521283986018255, validation loss: 0.48833804214781845.
epoch: 10, train loss: 0.4258713034279326, validation loss: 0.4872629764480808.
epoch: 11, train loss: 0.41876457126323996, validation loss: 0.4895199153498027.
epoch: 12, train loss: 0.40952310196966185, validation loss: 0.5054813423048058.
epoch: 13, train loss: 0.40642145658965806, validation loss: 0.48607608762561766.
epoch: 14, train loss: 0.4009978883205316, validation loss: 0.48703849868557053.
epoch: 15, train loss: 0.397683832282694, validation loss: 0.4867749442945518.
epoch: 16, train loss: 0.39425092860164807, validation loss: 0.4942022606178566.
epoch: 17, train loss: 0.3902627924079569, validation loss: 0.48775809654822716.
epoch: 18, train loss: 0.38913908213427945, validation loss: 0.490365927171843.
epoch: 19, train loss: 0.3873192517859304, validation loss: 0.4927234221455718.
epoch: 20, train loss: 0.3825400389972915, validation loss: 0.4994002300382.
epoch: 21, train loss: 0.3790429428622254, validation loss: 0.504094843742175.
epoch: 22, train loss: 0.3821037974887424, validation loss: 0.5154278758991817.
epoch: 23, train loss: 0.3767610774570041, validation loss: 0.5030595707689595.
epoch: 24, train loss: 0.376671207623604, validation loss: 0.4871382633437458.
epoch: 25, train loss: 0.37407493539141795, validation loss: 0.5103421758450674.
epoch: 26, train loss: 0.3738896771944486, validation loss: 0.4978723656417977.
epoch: 27, train loss: 0.3735262281955817, validation loss: 0.4820594535458122.
epoch: 28, train loss: 0.370148560157189, validation loss: 0.49601574183189634.
epoch: 29, train loss: 0.36764330611269697, validation loss: 0.5166607680144133.
epoch: 30, train loss: 0.3668692433609922, validation loss: 0.5100524598037416.
epoch: 31, train loss: 0.366706330633571, validation loss: 0.5003651541250723.
epoch: 32, train loss: 0.3662161771334135, validation loss: 0.5186121166261852.
epoch: 33, train loss: 0.36232723027416786, validation loss: 0.48729186490050747.
epoch: 34, train loss: 0.3665908971607176, validation loss: 0.4894806655927261.
epoch: 35, train loss: 0.3614824041871943, validation loss: 0.48173494713938136.
epoch: 36, train loss: 0.36287872047098274, validation loss: 0.4822701204740084.
epoch: 37, train loss: 0.35961628054757405, validation loss: 0.49797423892550996.
epoch: 38, train loss: 0.36265570651975454, validation loss: 0.499764117922878.
epoch: 39, train loss: 0.3603595330328004, validation loss: 0.48446164264298575.
epoch: 40, train loss: 0.36100159186990854, validation loss: 0.48338550676307784.
epoch: 41, train loss: 0.35750226397392076, validation loss: 0.4855161252796141.
epoch: 42, train loss: 0.3575766653403258, validation loss: 0.47845621810000166.
epoch: 43, train loss: 0.35382303475926064, validation loss: 0.505909047901121.
epoch: 44, train loss: 0.35606250185844224, validation loss: 0.4891977111294738.
epoch: 45, train loss: 0.35723455471462673, validation loss: 0.4837671596168453.
epoch: 46, train loss: 0.3530289730495877, validation loss: 0.4898284754522166.
epoch: 47, train loss: 0.35308999558799287, validation loss: 0.49585599611287784.
epoch: 48, train loss: 0.35403792593214245, validation loss: 0.4783489360972347.
epoch: 49, train loss: 0.3521649653442904, validation loss: 0.48617876916869074.
epoch: 50, train loss: 0.35349647624676045, validation loss: 0.4818868779456853.
epoch: 51, train loss: 0.3517512514163286, validation loss: 0.4833237120364806.
epoch: 52, train loss: 0.35230412351168117, validation loss: 0.4949508738721538.
epoch: 53, train loss: 0.3491042844658224, validation loss: 0.492355133882615.
epoch: 54, train loss: 0.35262941732162084, validation loss: 0.4805786457768193.
epoch: 55, train loss: 0.34880782324636084, validation loss: 0.5070063558942572.
epoch: 56, train loss: 0.34813554796398194, validation loss: 0.484850756435992.
epoch: 57, train loss: 0.3482814363332895, validation loss: 0.48337058694953594.
epoch: 58, train loss: 0.3450910244803143, validation loss: 0.48104828576416714.
epoch: 59, train loss: 0.3471772297753228, validation loss: 0.4707862233436345.
epoch: 60, train loss: 0.347703064388699, validation loss: 0.4840812728751419.
best validation loss 0.4707862233436345 at epoch 59.
