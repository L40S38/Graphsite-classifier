number of classes: 60
number of epochs to train: 50
batch size: 128
number of workers to load data:  36
device:  cuda
number of gpus:  2
number of pockets in training set:  22527
number of pockets in validation set:  4806
number of pockets in test set:  4890
number of train positive pairs: 180000
number of train negative pairs: 177000
number of validation positive pairs: 47172
number of validation negative pairs: 44250
model architecture:
SiameseNet(
  (embedding_net): EmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=5, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=5, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=5, out_features=5, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0002
)
loss function:
ContrastiveLoss(margin=2.0, normalize=True, mean=True)
train loss: 0.8558284523800975, validation loss: 0.8302842717042302.
train loss: 0.7916544768175825, validation loss: 0.7956092226771545.
train loss: 0.7486418246955765, validation loss: 0.7625719893985575.
train loss: 0.7138209442651572, validation loss: 0.7509421552842637.
train loss: 0.688552125989556, validation loss: 0.7508230375212431.
train loss: 0.6684958411144609, validation loss: 0.7258862423256813.
train loss: 0.6524599774144277, validation loss: 0.7156013913042045.
train loss: 0.6404345212663923, validation loss: 0.7159068523261024.
train loss: 0.6305531226086015, validation loss: 0.7087962669710876.
train loss: 0.6230986389138785, validation loss: 0.718435122031003.
train loss: 0.6159716626869864, validation loss: 0.705552441058153.
train loss: 0.609448986032096, validation loss: 0.7107834759752881.
train loss: 0.6039151114669501, validation loss: 0.7132889325203933.
train loss: 0.5980940701888054, validation loss: 0.717757270163561.
train loss: 0.5930984191573968, validation loss: 0.7028542774168144.
train loss: 0.5894779988670883, validation loss: 0.7147748848294068.
train loss: 0.5857484054672284, validation loss: 0.7081645927939094.
train loss: 0.5821146681542491, validation loss: 0.7134021766460475.
train loss: 0.5787438428328485, validation loss: 0.7112735232471108.
train loss: 0.5785714635768858, validation loss: 0.70527348788862.
train loss: 0.5743474969409761, validation loss: 0.7057390296673828.
train loss: 0.5719927973920892, validation loss: 0.7113315078806454.
train loss: 0.5694319249428287, validation loss: 0.7067397757006945.
train loss: 0.5668742886925278, validation loss: 0.7077644982216632.
train loss: 0.5648043674821613, validation loss: 0.6970346786326443.
train loss: 0.5622216204474954, validation loss: 0.7032836291840654.
train loss: 0.5601141407002254, validation loss: 0.7121560452102335.
train loss: 0.5590507919888537, validation loss: 0.7054874067746104.
train loss: 0.5572204187751151, validation loss: 0.7047210798175716.
train loss: 0.5552342556544713, validation loss: 0.707434628428154.
train loss: 0.5550433872447295, validation loss: 0.7074664865872523.
train loss: 0.5518435879982486, validation loss: 0.6986516797000913.
train loss: 0.5515599034093007, validation loss: 0.715144776105646.
train loss: 0.5498240414146616, validation loss: 0.7191065185971481.
train loss: 0.5488394120053417, validation loss: 0.7039203821969917.
train loss: 0.5468480691562514, validation loss: 0.7080948434440698.
train loss: 0.5469828826412767, validation loss: 0.7101361813259089.
train loss: 0.546308443256453, validation loss: 0.7076933652293599.
train loss: 0.5453472751115217, validation loss: 0.7064347683265911.
train loss: 0.5448917816546784, validation loss: 0.7060426125072968.
train loss: 0.5426253826664943, validation loss: 0.7105733346502004.
train loss: 0.5420001806691915, validation loss: 0.7165112815984283.
train loss: 0.5407682706047507, validation loss: 0.710791419234675.
train loss: 0.5405021499185001, validation loss: 0.7122154210804094.
train loss: 0.5393791874583695, validation loss: 0.7047601734753973.
train loss: 0.538648820711451, validation loss: 0.7187941040625233.
train loss: 0.5373387512506224, validation loss: 0.7196511140954259.
train loss: 0.5375661769621178, validation loss: 0.7051858476578416.
train loss: 0.5369753556665586, validation loss: 0.7173116436565022.
train loss: 0.5364535221025055, validation loss: 0.7124422958419755.
best validation loss 0.6970346786326443 at epoch 25.
