seed:  666
number of classes (from original clusters): 14
how to merge clusters:  [[0, 9, 12], [1, 5, 11], 2, [3, 8, 13], 4, 6, 7, 10]
whether to further subcluster data according to chemical reaction: False
positive training pair sampling threshold:  16000
negative training pair sampling threshold:  4600
number of epochs to train: 55
learning rate decay to half at epoch 25.
batch size: 256
similar margin of contrastive loss: 0.0
dissimilar margin of contrastive loss: 2.0
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of classes after merging:  8
number of pockets in training set:  12475
number of pockets in validation set:  2670
number of pockets in test set:  2681
number of train positive pairs: 128000
number of train negative pairs: 128800
model architecture:
ResidualSiameseNet(
  (embedding_net): ResidualEmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=48, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=48, out_features=48, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (rb_2): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_3): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_4): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_5): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_6): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_7): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_8): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (bn_8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(48, 96)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.7593551013625671, train acc: 0.7904624277456648, validation acc: 0.7210960960960962.
epoch: 2, train loss: 0.668662326075949, train acc: 0.7951188182402055, validation acc: 0.7128378378378378.
epoch: 3, train loss: 0.617912870775502, train acc: 0.8006583172768144, validation acc: 0.7177177177177178.
epoch: 4, train loss: 0.5825185317636651, train acc: 0.8049935773924213, validation acc: 0.7132132132132132.
epoch: 5, train loss: 0.5498023793110595, train acc: 0.8128612716763006, validation acc: 0.7357357357357357.
epoch: 6, train loss: 0.5266218621559975, train acc: 0.8156711624919717, validation acc: 0.7147147147147147.
epoch: 7, train loss: 0.5004751961253514, train acc: 0.8086062941554271, validation acc: 0.7075825825825826.
epoch: 8, train loss: 0.48601979882546303, train acc: 0.8199261400128453, validation acc: 0.722972972972973.
epoch: 9, train loss: 0.4684950432821969, train acc: 0.8334938985228002, validation acc: 0.7203453453453453.
epoch: 10, train loss: 0.45464842600243116, train acc: 0.832931920359666, validation acc: 0.7248498498498499.
epoch: 11, train loss: 0.44696058891272616, train acc: 0.8316473988439307, validation acc: 0.7072072072072072.
epoch: 12, train loss: 0.43408817451690956, train acc: 0.8315671162491972, validation acc: 0.728978978978979.
epoch: 13, train loss: 0.4202389711160155, train acc: 0.8257867694283879, validation acc: 0.7218468468468469.
epoch: 14, train loss: 0.41826978600285136, train acc: 0.8379897238278741, validation acc: 0.7301051051051051.
epoch: 15, train loss: 0.40935423851013186, train acc: 0.841441875401413, validation acc: 0.7117117117117117.
epoch: 16, train loss: 0.39414445817656235, train acc: 0.8346178548490687, validation acc: 0.7090840840840841.
epoch: 17, train loss: 0.39532200198307216, train acc: 0.8331727681438664, validation acc: 0.7143393393393394.
epoch: 18, train loss: 0.39445107878925645, train acc: 0.8451348747591522, validation acc: 0.713963963963964.
epoch: 19, train loss: 0.3916296287191991, train acc: 0.8296403339755941, validation acc: 0.698948948948949.
epoch: 20, train loss: 0.3824137854947479, train acc: 0.8236994219653179, validation acc: 0.7045795795795796.
epoch: 21, train loss: 0.39416161813468575, train acc: 0.8579800899165061, validation acc: 0.722972972972973.
epoch: 22, train loss: 0.38312546394324376, train acc: 0.8340558766859345, validation acc: 0.704954954954955.
epoch: 23, train loss: 0.37498705863952636, train acc: 0.8231374438021837, validation acc: 0.698948948948949.
epoch: 24, train loss: 0.3763795495850275, train acc: 0.8528420038535646, validation acc: 0.7274774774774775.
epoch: 25, train loss: 0.31348206826088215, train acc: 0.869781631342325, validation acc: 0.734984984984985.
epoch: 26, train loss: 0.31480750906875943, train acc: 0.8701830443159922, validation acc: 0.7342342342342343.
epoch: 27, train loss: 0.3153976502373954, train acc: 0.8716281310211946, validation acc: 0.7316066066066066.
epoch: 28, train loss: 0.3155399956361527, train acc: 0.861271676300578, validation acc: 0.7207207207207207.
epoch: 29, train loss: 0.30779448011582516, train acc: 0.868577392421323, validation acc: 0.7353603603603603.
epoch: 30, train loss: 0.3073846806469736, train acc: 0.8664097623635196, validation acc: 0.7263513513513513.
epoch: 31, train loss: 0.3137556512630617, train acc: 0.86873795761079, validation acc: 0.7192192192192193.
epoch: 32, train loss: 0.3075896060800998, train acc: 0.8709858702633269, validation acc: 0.7225975975975976.
epoch: 33, train loss: 0.3022855943905602, train acc: 0.876204238921002, validation acc: 0.7353603603603603.
epoch: 34, train loss: 0.3134924256987289, train acc: 0.8660083493898523, validation acc: 0.7120870870870871.
epoch: 35, train loss: 0.30046505859707745, train acc: 0.8744380218368658, validation acc: 0.7240990990990991.
epoch: 36, train loss: 0.299622908084192, train acc: 0.8705041746949261, validation acc: 0.7312312312312312.
epoch: 37, train loss: 0.30164058097055024, train acc: 0.8746788696210661, validation acc: 0.7338588588588588.
epoch: 38, train loss: 0.301323612025965, train acc: 0.8845536287732819, validation acc: 0.7286036036036037.
epoch: 39, train loss: 0.2999677609060412, train acc: 0.8626364804110469, validation acc: 0.7173423423423423.
epoch: 40, train loss: 0.295809091689802, train acc: 0.8801380860629415, validation acc: 0.7338588588588588.
epoch: 41, train loss: 0.30080002138547807, train acc: 0.8721901091843288, validation acc: 0.7361111111111112.
epoch: 42, train loss: 0.3024047551645297, train acc: 0.8778901734104047, validation acc: 0.7364864864864865.
epoch: 43, train loss: 0.29412905800008327, train acc: 0.8770070648683366, validation acc: 0.7327327327327328.
epoch: 44, train loss: 0.30002554180466123, train acc: 0.882466281310212, validation acc: 0.734984984984985.
epoch: 45, train loss: 0.2954228618360383, train acc: 0.8808606294155427, validation acc: 0.7338588588588588.
epoch: 46, train loss: 0.3011191085268775, train acc: 0.865606936416185, validation acc: 0.7319819819819819.
epoch: 47, train loss: 0.297498746943251, train acc: 0.8719492614001284, validation acc: 0.7357357357357357.
epoch: 48, train loss: 0.29362628250478584, train acc: 0.8668914579319204, validation acc: 0.7162162162162162.
epoch: 49, train loss: 0.29594537703790397, train acc: 0.8836705202312138, validation acc: 0.7342342342342343.
epoch: 50, train loss: 0.2938010427736419, train acc: 0.8787732819524727, validation acc: 0.7301051051051051.
epoch: 51, train loss: 0.2976006454396471, train acc: 0.8782113037893384, validation acc: 0.7271021021021021.
epoch: 52, train loss: 0.2919047193735188, train acc: 0.8702633269107257, validation acc: 0.7150900900900901.
epoch: 53, train loss: 0.2971500925854359, train acc: 0.8809409120102761, validation acc: 0.7376126126126126.
epoch: 54, train loss: 0.2903331496782392, train acc: 0.869942196531792, validation acc: 0.7278528528528528.
epoch: 55, train loss: 0.28934428703747805, train acc: 0.8790944123314065, validation acc: 0.7301051051051051.
best validation acc 0.7376126126126126 at epoch 53.
