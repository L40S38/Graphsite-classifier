seed:  666
save trained model at:  ../trained_models/trained_classifier_model_27.pt
save loss at:  ./results/train_classifier_results_27.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  14780
number of pockets in validation set:  3162
number of pockets in test set:  3183
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKMCNMMEmbeddingNet(
    (conv0): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=22, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv6): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn6): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0006
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b516f5d88b0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.073279242812377, acc: 0.38633288227334234, val loss: 1.8521063686698995, acc: 0.4411764705882353, test loss: 1.8606442937003043, acc: 0.44172164624568017
epoch: 2, train loss: 1.7365909674816107, acc: 0.4689445196211096, val loss: 1.6180613442058431, acc: 0.5018975332068312, test loss: 1.6111162926017433, acc: 0.5086396481306943
epoch: 3, train loss: 1.5780686473975485, acc: 0.5188768606224627, val loss: 1.549718528967127, acc: 0.5325743200506009, test loss: 1.5513822245140927, acc: 0.5353440150801131
epoch: 4, train loss: 1.480151973299793, acc: 0.5455345060893099, val loss: 1.7802514458366168, acc: 0.4038583175205566, test loss: 1.7773822260247512, acc: 0.39585296889726673
epoch: 5, train loss: 1.4080360761998632, acc: 0.5705683355886333, val loss: 1.5962125033536039, acc: 0.5088551549652119, test loss: 1.6184021521019105, acc: 0.5136663524976437
epoch: 6, train loss: 1.345467468495298, acc: 0.5930311231393776, val loss: 1.5236906298046848, acc: 0.5120177103099304, test loss: 1.5526073673330985, acc: 0.5114671693371033
epoch: 7, train loss: 1.3180860859131458, acc: 0.5997970230040596, val loss: 1.2768391440896427, acc: 0.5989879822896901, test loss: 1.2650442552611472, acc: 0.6050895381715363
epoch: 8, train loss: 1.2545300999578184, acc: 0.6221244925575101, val loss: 1.5923675302460856, acc: 0.5531309297912713, test loss: 1.586696927366937, acc: 0.5557650015708451
epoch: 9, train loss: 1.2150172431349593, acc: 0.6298376184032476, val loss: 1.260490968512403, acc: 0.6094244149272612, test loss: 1.2289186886485701, acc: 0.623311341501728
epoch: 10, train loss: 1.1688008985596838, acc: 0.6474966170500677, val loss: 1.49627212570258, acc: 0.5955091714104996, test loss: 1.4870122476003098, acc: 0.6003770028275212
epoch: 11, train loss: 1.1660232907864334, acc: 0.6484438430311231, val loss: 1.456866200704653, acc: 0.5698924731182796, test loss: 1.4006640663170942, acc: 0.5680175934652844
epoch: 12, train loss: 1.1343354499388452, acc: 0.6568335588633288, val loss: 1.2071594269044938, acc: 0.6337760910815939, test loss: 1.192869887799914, acc: 0.6336789192585611
epoch: 13, train loss: 1.107220299882721, acc: 0.6648849797023004, val loss: 1.2075230256429885, acc: 0.6344086021505376, test loss: 1.2089202960556946, acc: 0.6368206094879045
epoch: 14, train loss: 1.0812240050835926, acc: 0.6693504736129905, val loss: 1.3152167376349955, acc: 0.6081593927893738, test loss: 1.2885235671985085, acc: 0.6120012566760917
epoch: 15, train loss: 1.0662335369680505, acc: 0.6784844384303113, val loss: 1.23006126203543, acc: 0.6204933586337761, test loss: 1.2127178977429398, acc: 0.6255105246622683
epoch: 16, train loss: 1.0283761151266033, acc: 0.6895805142083897, val loss: 1.138655134728255, acc: 0.6476913345983555, test loss: 1.1161405555564503, acc: 0.6525290606346215
epoch: 17, train loss: 0.988653054450297, acc: 0.6991880920162381, val loss: 1.0797832087577708, acc: 0.6742567994939912, test loss: 1.0665789296931902, acc: 0.6823751178133836
epoch: 18, train loss: 0.9930010115503458, acc: 0.7, val loss: 1.1861604710759277, acc: 0.653067678684377, test loss: 1.182921266091383, acc: 0.6487590323594093
epoch: 19, train loss: 0.9759805915481505, acc: 0.7048714479025711, val loss: 1.2749880139847332, acc: 0.614168247944339, test loss: 1.2687133082121076, acc: 0.626138862708137
epoch: 20, train loss: 0.9448298824978778, acc: 0.712043301759134, val loss: 1.027928805969547, acc: 0.6872232764073372, test loss: 0.9921334221270937, acc: 0.6918001885014138
epoch: 21, train loss: 0.9123353292880749, acc: 0.7256427604871448, val loss: 1.124969101796642, acc: 0.6574952561669829, test loss: 1.1113167711672003, acc: 0.6647816525290606
epoch: 22, train loss: 0.9355140588280313, acc: 0.7146143437077132, val loss: 1.2071557672320854, acc: 0.6151170145477546, test loss: 1.2018159078495554, acc: 0.6185988061577129
epoch: 23, train loss: 0.8971535351828084, acc: 0.7307848443843031, val loss: 1.1177331945852416, acc: 0.6764705882352942, test loss: 1.0753495526171464, acc: 0.6773484134464341
epoch: 24, train loss: 0.8899565727042894, acc: 0.730446549391069, val loss: 1.326882037537374, acc: 0.6287160025300442, test loss: 1.2553671051262685, acc: 0.6346214263273641
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.6866076555884417, acc: 0.7393775372124493, val loss: 0.8310666666386778, acc: 0.7052498418722327, test loss: 0.8163708150667699, acc: 0.7056236255105247
epoch: 26, train loss: 0.6516543940213763, acc: 0.7493234100135318, val loss: 0.8029919168302185, acc: 0.6995572422517394, test loss: 0.7904468428365772, acc: 0.7009110901665095
epoch: 27, train loss: 0.6476704160318646, acc: 0.7515561569688769, val loss: 0.9915019058864528, acc: 0.6476913345983555, test loss: 0.9467461569819329, acc: 0.6522148916116871
epoch: 28, train loss: 0.652246304439111, acc: 0.7451962110960758, val loss: 1.030320845247446, acc: 0.6578115117014548, test loss: 0.9705233785142099, acc: 0.6745208922400251
epoch: 29, train loss: 0.6475127574878068, acc: 0.7495263870094723, val loss: 0.8022700061834288, acc: 0.6894370651486401, test loss: 0.7893505810268113, acc: 0.6961985548224945
epoch: 30, train loss: 0.607501811910224, acc: 0.7613667117726658, val loss: 0.8143385142785398, acc: 0.7017710309930424, test loss: 0.8141814949300504, acc: 0.702167766258247
epoch: 31, train loss: 0.58979212607034, acc: 0.7682002706359946, val loss: 0.7694035957765911, acc: 0.7001897533206831, test loss: 0.7699520832107309, acc: 0.7056236255105247
epoch: 32, train loss: 0.5994585232257198, acc: 0.7662381596752368, val loss: 0.7994577356865706, acc: 0.6973434535104365, test loss: 0.7816282274436172, acc: 0.7046811184417217
epoch: 33, train loss: 0.5932745596714045, acc: 0.7709066305818674, val loss: 0.8112784335313153, acc: 0.7020872865275142, test loss: 0.8017158834132015, acc: 0.7043669494187873
epoch: 34, train loss: 0.6201407372548229, acc: 0.763531799729364, val loss: 0.9133847172994692, acc: 0.6761543327008223, test loss: 0.8920890176367393, acc: 0.6801759346528432
epoch: 35, train loss: 0.5970687607386438, acc: 0.7663058186738836, val loss: 0.7837729951688416, acc: 0.7194813409234662, test loss: 0.7675600789681644, acc: 0.7178762174049639
epoch: 36, train loss: 0.5740822605254363, acc: 0.777807848443843, val loss: 0.8806592474098375, acc: 0.6701454775458571, test loss: 0.8572525496457382, acc: 0.685516808042727
epoch: 37, train loss: 0.563793240332313, acc: 0.7776048714479026, val loss: 0.7801194267888223, acc: 0.7118912080961417, test loss: 0.7752167634008218, acc: 0.7128495130380145
epoch: 38, train loss: 0.5642061675030421, acc: 0.7782814614343708, val loss: 0.7379939590806376, acc: 0.7163187855787476, test loss: 0.7139464306299543, acc: 0.7276154571159283
epoch: 39, train loss: 0.5487960292860684, acc: 0.78085250338295, val loss: 0.9282531356751202, acc: 0.691967109424415, test loss: 0.8953667199353246, acc: 0.6983977379830348
epoch: 40, train loss: 0.5272367320783407, acc: 0.7937077131258458, val loss: 0.7562159386561839, acc: 0.7169512966476913, test loss: 0.7492542161231441, acc: 0.7241595978636507
epoch: 41, train loss: 0.5258942231757715, acc: 0.7943843031123139, val loss: 0.7219166270846584, acc: 0.7254901960784313, test loss: 0.7427950115874995, acc: 0.7181903864278982
epoch: 42, train loss: 0.5245704333740255, acc: 0.7890392422192152, val loss: 0.7626639937087752, acc: 0.7055660974067046, test loss: 0.7627349767975563, acc: 0.7081369776939994
epoch: 43, train loss: 0.5284532682014899, acc: 0.7910690121786198, val loss: 0.8076426304268882, acc: 0.7122074636306135, test loss: 0.7954091073730402, acc: 0.7125353440150801
epoch: 44, train loss: 0.5014367501371123, acc: 0.8043978349120433, val loss: 0.8451433296372513, acc: 0.6774193548387096, test loss: 0.8722187901232327, acc: 0.6792334275840403
epoch: 45, train loss: 0.4940925052991901, acc: 0.8058863328822733, val loss: 0.9009370796292888, acc: 0.6973434535104365, test loss: 0.8719019396964987, acc: 0.7081369776939994
epoch: 46, train loss: 0.4993883306505877, acc: 0.8010825439783491, val loss: 0.7883530596552734, acc: 0.7128399746995573, test loss: 0.7615508989009923, acc: 0.7203895695884386
epoch: 47, train loss: 0.4980797941210467, acc: 0.8083220568335588, val loss: 0.8597934034939886, acc: 0.6856419987349779, test loss: 0.8657365119423538, acc: 0.6814326107445806
epoch: 48, train loss: 0.4862586309041964, acc: 0.8102165087956699, val loss: 0.7758196733331167, acc: 0.711258697027198, test loss: 0.7842457600820375, acc: 0.7065661325793277
epoch: 49, train loss: 0.5034872383484176, acc: 0.8010148849797023, val loss: 0.860817249480265, acc: 0.6881720430107527, test loss: 0.8675609839400742, acc: 0.6858309770656613
epoch: 50, train loss: 0.49548811015643995, acc: 0.8050744248985116, val loss: 0.7923483949609379, acc: 0.7131562302340291, test loss: 0.818675900644152, acc: 0.7084511467169337
epoch: 51, train loss: 0.48857699464880566, acc: 0.8077131258457375, val loss: 0.7614696935186802, acc: 0.7352941176470589, test loss: 0.7624409811473515, acc: 0.7345271756204839
epoch: 52, train loss: 0.4654603485970762, acc: 0.8179972936400541, val loss: 0.7671681009916321, acc: 0.7122074636306135, test loss: 0.7641074903873464, acc: 0.7313854853911405
epoch: 53, train loss: 0.44201631166938193, acc: 0.826319350473613, val loss: 0.8021867008317807, acc: 0.6850094876660342, test loss: 0.8156612168832504, acc: 0.6833176248821866
epoch: 54, train loss: 0.44081644199697834, acc: 0.823274695534506, val loss: 0.820644154041949, acc: 0.7115749525616698, test loss: 0.8342129140764597, acc: 0.7153628652214892
epoch: 55, train loss: 0.4344567392772525, acc: 0.8288903924221922, val loss: 0.8164559433084738, acc: 0.7049335863377609, test loss: 0.8255740102641507, acc: 0.704052780395853
epoch: 56, train loss: 0.4255482889560627, acc: 0.8285520974289581, val loss: 0.8048805585773138, acc: 0.7099936748893105, test loss: 0.823273188956234, acc: 0.7037386113729186
epoch: 57, train loss: 0.4506025102367582, acc: 0.8228687415426251, val loss: 0.75519436465872, acc: 0.7283364958886781, test loss: 0.7598407532185306, acc: 0.7323279924599434
epoch: 58, train loss: 0.44626890491084253, acc: 0.8217861975642761, val loss: 0.7977458571120956, acc: 0.7055660974067046, test loss: 0.7988315653284128, acc: 0.7097078228086711
epoch: 59, train loss: 0.4195510825623679, acc: 0.8324763193504736, val loss: 0.9163336437160196, acc: 0.7058823529411765, test loss: 0.8991096278162749, acc: 0.7097078228086711
epoch: 60, train loss: 0.4098480210252641, acc: 0.8399188092016238, val loss: 0.7493523369100862, acc: 0.7302340290955092, test loss: 0.7176255239018696, acc: 0.7430097392397109
epoch: 61, train loss: 0.4665654903900317, acc: 0.8173207036535859, val loss: 0.8167445132583097, acc: 0.7204301075268817, test loss: 0.7866734887607585, acc: 0.7153628652214892
epoch: 62, train loss: 0.41698114039610784, acc: 0.8340324763193505, val loss: 0.8499898474703553, acc: 0.7071473750790639, test loss: 0.8683375555728166, acc: 0.702167766258247
Epoch    62: reducing learning rate of group 0 to 1.5000e-03.
epoch: 63, train loss: 0.324500306146232, acc: 0.8683355886332882, val loss: 0.7119720312404452, acc: 0.7621758380771664, test loss: 0.7001218923112712, acc: 0.7631165567075087
epoch: 64, train loss: 0.25671617462928625, acc: 0.8941813261163735, val loss: 0.7179414986960877, acc: 0.7628083491461101, test loss: 0.7238067901250592, acc: 0.7684574300973924
epoch: 65, train loss: 0.25364542231669446, acc: 0.895872801082544, val loss: 0.7775351820227318, acc: 0.75426944971537, test loss: 0.7788546731776873, acc: 0.7631165567075087
epoch: 66, train loss: 0.24812595404694626, acc: 0.8948579161028417, val loss: 0.7628194255816794, acc: 0.767235926628716, test loss: 0.7970071190975764, acc: 0.7690857681432611
epoch: 67, train loss: 0.2328501378772062, acc: 0.90148849797023, val loss: 0.7411049749940201, acc: 0.7666034155597723, test loss: 0.771612146497709, acc: 0.76248821866164
epoch: 68, train loss: 0.23036415147845896, acc: 0.9023680649526387, val loss: 0.7980366662512242, acc: 0.7428842504743833, test loss: 0.8273780244047673, acc: 0.7401822180333019
epoch: 69, train loss: 0.22189670452326335, acc: 0.9054803788903925, val loss: 0.7898888440919329, acc: 0.7634408602150538, test loss: 0.8047946291152613, acc: 0.7568331762488218
epoch: 70, train loss: 0.21903812060982028, acc: 0.9045331529093369, val loss: 0.8046730617267432, acc: 0.7586970271979759, test loss: 0.8151832568432001, acc: 0.7631165567075087
epoch: 71, train loss: 0.24512865745811566, acc: 0.8975642760487145, val loss: 0.8054704065944182, acc: 0.7574320050600886, test loss: 0.827588863879152, acc: 0.7558906691800189
epoch: 72, train loss: 0.2175749040061308, acc: 0.9083220568335588, val loss: 0.8154501170014219, acc: 0.7552182163187856, test loss: 0.8252789171244639, acc: 0.7546339930882815
epoch: 73, train loss: 0.21130906600752122, acc: 0.9088633288227335, val loss: 0.8431995479009484, acc: 0.74573055028463, test loss: 0.8631325476655711, acc: 0.746779767514923
epoch: 74, train loss: 0.22385533033430335, acc: 0.906021650879567, val loss: 0.808681581383789, acc: 0.7571157495256167, test loss: 0.8300015992276368, acc: 0.74583726044612
epoch: 75, train loss: 0.21697700989907745, acc: 0.9055480378890393, val loss: 0.8094496475149754, acc: 0.7637571157495257, test loss: 0.8527999438094973, acc: 0.7580898523405593
epoch: 76, train loss: 0.24592268731500525, acc: 0.8962787550744249, val loss: 0.9335288654619346, acc: 0.7197975964579381, test loss: 0.9166709210488394, acc: 0.7175620483820295
epoch: 77, train loss: 0.2493518946740237, acc: 0.8942489851150203, val loss: 0.8239193550775806, acc: 0.7536369386464263, test loss: 0.8212760318722546, acc: 0.7574615142946906
epoch: 78, train loss: 0.20676156241290464, acc: 0.9125169147496617, val loss: 0.7986760306855986, acc: 0.767235926628716, test loss: 0.815041802193884, acc: 0.7662582469368521
epoch: 79, train loss: 0.18437582360875462, acc: 0.9200947225981055, val loss: 0.8665337683202947, acc: 0.7536369386464263, test loss: 0.8899261078069017, acc: 0.7533773169965441
epoch: 80, train loss: 0.22805434289175378, acc: 0.9041271989174561, val loss: 0.7930033234084127, acc: 0.7681846932321316, test loss: 0.8153668499352458, acc: 0.7709707822808671
epoch: 81, train loss: 0.1884921541072035, acc: 0.9201623815967523, val loss: 0.8090271631575022, acc: 0.7707147375079064, test loss: 0.8372045653891795, acc: 0.76248821866164
epoch: 82, train loss: 0.17617106138450045, acc: 0.9243572395128552, val loss: 0.8638099987698385, acc: 0.7574320050600886, test loss: 0.8786799501706098, acc: 0.7524348099277411
epoch: 83, train loss: 0.1850702281249876, acc: 0.9158998646820027, val loss: 0.8817456474340392, acc: 0.7523719165085389, test loss: 0.8949017330718573, acc: 0.7492931196983977
epoch: 84, train loss: 0.212922086073678, acc: 0.908254397834912, val loss: 0.824830196086249, acc: 0.7520556609740671, test loss: 0.8131395703099114, acc: 0.7527489789506755
epoch: 85, train loss: 0.2143861445224011, acc: 0.9085926928281461, val loss: 0.8373748209565445, acc: 0.7450980392156863, test loss: 0.801198293756023, acc: 0.7521206409048068
epoch: 86, train loss: 0.2264346097173807, acc: 0.9017591339648173, val loss: 0.8612648926529229, acc: 0.7552182163187856, test loss: 0.8550469729273416, acc: 0.7524348099277411
epoch: 87, train loss: 0.16928298557643479, acc: 0.9272665764546685, val loss: 1.0426840061330103, acc: 0.7482605945604048, test loss: 1.0670188075523124, acc: 0.7489789506754634
epoch: 88, train loss: 0.1607662686153097, acc: 0.9282814614343707, val loss: 0.8896313187142553, acc: 0.7577482605945604, test loss: 0.8877712815878266, acc: 0.7615457115928369
epoch: 89, train loss: 0.17376688826068007, acc: 0.9242219215155616, val loss: 0.8807062677459427, acc: 0.7647058823529411, test loss: 0.9369198317202688, acc: 0.7536914860194784
epoch: 90, train loss: 0.17660552988758912, acc: 0.9233423545331529, val loss: 0.8701170124184249, acc: 0.7580645161290323, test loss: 0.862094029517358, acc: 0.7656299088909834
epoch: 91, train loss: 0.1663037340478097, acc: 0.9285520974289581, val loss: 0.9208331795752162, acc: 0.7555344718532574, test loss: 0.9450122358062677, acc: 0.7596606974552309
epoch: 92, train loss: 0.15267836327320505, acc: 0.9325439783491204, val loss: 0.9402167091333437, acc: 0.7473118279569892, test loss: 0.9256087792133184, acc: 0.7577756833176249
epoch: 93, train loss: 0.18165818369759598, acc: 0.9218538565629228, val loss: 0.9346107056742601, acc: 0.7185325743200506, test loss: 0.9462502997437626, acc: 0.7194470625196355
epoch: 94, train loss: 0.18757534231158812, acc: 0.9159675236806495, val loss: 0.8488954841751302, acc: 0.7612270714737508, test loss: 0.8326263889141364, acc: 0.764373232799246
epoch: 95, train loss: 0.1459028732308838, acc: 0.9351826792963465, val loss: 0.8948926483808176, acc: 0.7621758380771664, test loss: 0.8536792925757655, acc: 0.765315739868049
epoch: 96, train loss: 0.16816337752648716, acc: 0.9241542625169148, val loss: 0.9655115636069137, acc: 0.7299177735610373, test loss: 0.9412242860641263, acc: 0.7345271756204839
epoch: 97, train loss: 0.1572562785932595, acc: 0.9321380243572395, val loss: 0.9271768662538957, acc: 0.7514231499051234, test loss: 0.913309359528005, acc: 0.7618598806157713
epoch: 98, train loss: 0.15785976539848623, acc: 0.9312584573748308, val loss: 0.9041219247737163, acc: 0.7539531941808981, test loss: 0.9035018882847342, acc: 0.7530631479736098
epoch: 99, train loss: 0.16881851783382715, acc: 0.9251014884979702, val loss: 0.8935537983691066, acc: 0.7545857052498419, test loss: 0.9131600173658969, acc: 0.7470939365378574
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.1069681831682164, acc: 0.9360622462787551, val loss: 0.7815505336312687, acc: 0.7381404174573055, test loss: 0.7977990660695883, acc: 0.7386113729186302
epoch: 101, train loss: 0.08535125979873905, acc: 0.9454668470906631, val loss: 0.7738999904973985, acc: 0.765022137887413, test loss: 0.7931418389671822, acc: 0.7659440779139177
epoch: 102, train loss: 0.07056700040426242, acc: 0.9537212449255751, val loss: 0.8709113136414559, acc: 0.7394054395951929, test loss: 0.8635622039367071, acc: 0.74583726044612
epoch: 103, train loss: 0.09458521221422858, acc: 0.9420162381596753, val loss: 0.8473230341730956, acc: 0.7409867172675522, test loss: 0.8569676115795235, acc: 0.7320138234370092
epoch: 104, train loss: 0.10403222255963272, acc: 0.934573748308525, val loss: 0.7941830013463348, acc: 0.7593295382669196, test loss: 0.8092366716152386, acc: 0.7549481621112158
epoch: 105, train loss: 0.08129979247008028, acc: 0.9470906630581868, val loss: 0.8318938960160431, acc: 0.7450980392156863, test loss: 0.8072793981094911, acc: 0.7483506126295947
epoch: 106, train loss: 0.09442382332275298, acc: 0.9397834912043301, val loss: 0.8888646680143044, acc: 0.7314990512333965, test loss: 0.8610784716475808, acc: 0.7470939365378574
epoch: 107, train loss: 0.1164589251995248, acc: 0.9280108254397835, val loss: 0.7354720910088614, acc: 0.7599620493358634, test loss: 0.7481537065601859, acc: 0.7637448947533774
epoch: 108, train loss: 0.10019206753119081, acc: 0.9378890392422192, val loss: 0.7957776966613731, acc: 0.7488931056293485, test loss: 0.7987948861692897, acc: 0.7565190072258875
epoch: 109, train loss: 0.10572947818427995, acc: 0.9341001353179973, val loss: 0.7557925303324652, acc: 0.7609108159392789, test loss: 0.7721559295780135, acc: 0.7536914860194784
epoch: 110, train loss: 0.08439032913706138, acc: 0.9451962110960758, val loss: 0.8488842535441168, acc: 0.7454142947501581, test loss: 0.8786974959953229, acc: 0.742067232170908
epoch: 111, train loss: 0.10810531220916805, acc: 0.9339648173207037, val loss: 0.7656726059771881, acc: 0.7583807716635041, test loss: 0.7878862583071714, acc: 0.7470939365378574
epoch: 112, train loss: 0.10086063858580363, acc: 0.937212449255751, val loss: 0.793984286548366, acc: 0.7302340290955092, test loss: 0.8296774555743209, acc: 0.7247879359095193
epoch: 113, train loss: 0.10869904014512553, acc: 0.9340324763193505, val loss: 0.8485535246748324, acc: 0.7229601518026565, test loss: 0.8597690656879508, acc: 0.7238454288407163
Epoch   113: reducing learning rate of group 0 to 7.5000e-04.
epoch: 114, train loss: 0.06510002502619575, acc: 0.9527740189445196, val loss: 0.7612125109751566, acc: 0.7732447817836812, test loss: 0.7531533831513381, acc: 0.7753691486019478
epoch: 115, train loss: 0.04027383717858259, acc: 0.9713125845737484, val loss: 0.8441753517141227, acc: 0.765022137887413, test loss: 0.8515838235944387, acc: 0.7650015708451147
epoch: 116, train loss: 0.034055865618509915, acc: 0.9750338294993234, val loss: 0.8652666424890623, acc: 0.7703984819734345, test loss: 0.8561103343214586, acc: 0.7800816839459629
epoch: 117, train loss: 0.03055757734929596, acc: 0.9778755074424899, val loss: 0.8593106040616793, acc: 0.7719797596457938, test loss: 0.8517592639395773, acc: 0.7778825007854225
epoch: 118, train loss: 0.023903531033712084, acc: 0.9831529093369418, val loss: 0.9009845656431754, acc: 0.7675521821631879, test loss: 0.9192754294263317, acc: 0.7687715991203268
epoch: 119, train loss: 0.031246652776761048, acc: 0.9780784844384303, val loss: 0.89191393291249, acc: 0.7647058823529411, test loss: 0.9197017834915368, acc: 0.7662582469368521
epoch: 120, train loss: 0.04232816668038278, acc: 0.9687415426251691, val loss: 0.8960836515028517, acc: 0.7612270714737508, test loss: 0.9014729607828076, acc: 0.7612315425699026
epoch: 121, train loss: 0.041497732452917165, acc: 0.9726657645466847, val loss: 0.8412614940918374, acc: 0.7675521821631879, test loss: 0.8606404688910347, acc: 0.7700282752120641
epoch: 122, train loss: 0.03221117892127398, acc: 0.9769959404600812, val loss: 0.881306688217028, acc: 0.7624920936116382, test loss: 0.902780546843912, acc: 0.7640590637763116
epoch: 123, train loss: 0.0328863557231523, acc: 0.9760487144790257, val loss: 0.8806085812751437, acc: 0.765022137887413, test loss: 0.8735109632433941, acc: 0.7678290920515237
epoch: 124, train loss: 0.029153373247675387, acc: 0.9790933694181326, val loss: 0.9176992622076297, acc: 0.7612270714737508, test loss: 0.9405564256632736, acc: 0.7596606974552309
epoch: 125, train loss: 0.04302684005465817, acc: 0.9704330175913396, val loss: 0.8735075687623189, acc: 0.7628083491461101, test loss: 0.888972681571656, acc: 0.7618598806157713
epoch: 126, train loss: 0.038662584181446503, acc: 0.972192151556157, val loss: 0.8537069137604609, acc: 0.7713472485768501, test loss: 0.8576320807139078, acc: 0.7785108388312912
epoch: 127, train loss: 0.0320477264443052, acc: 0.977807848443843, val loss: 0.8766689734847993, acc: 0.7726122707147375, test loss: 0.8929783207344777, acc: 0.7681432610744581
epoch: 128, train loss: 0.026169691135015476, acc: 0.9820027063599459, val loss: 0.8990381460714612, acc: 0.7700822264389627, test loss: 0.8976852895327421, acc: 0.7659440779139177
epoch: 129, train loss: 0.02607930563390255, acc: 0.9822056833558863, val loss: 0.9018163797148114, acc: 0.7760910815939279, test loss: 0.9491090032989186, acc: 0.7687715991203268
epoch: 130, train loss: 0.03791817669001823, acc: 0.9739512855209743, val loss: 0.9306314403237157, acc: 0.7450980392156863, test loss: 0.9087413838926142, acc: 0.7587181903864278
epoch: 131, train loss: 0.038117336823588616, acc: 0.9739512855209743, val loss: 0.9415755964094593, acc: 0.756483238456673, test loss: 0.907379196231709, acc: 0.760603204524034
epoch: 132, train loss: 0.03989663786772785, acc: 0.9715155615696888, val loss: 0.9182728890148165, acc: 0.7504743833017078, test loss: 0.920707767428447, acc: 0.7587181903864278
epoch: 133, train loss: 0.040926509105546224, acc: 0.972192151556157, val loss: 0.8519172327643629, acc: 0.7751423149905123, test loss: 0.8744907730448894, acc: 0.7703424442349984
epoch: 134, train loss: 0.029996485127002526, acc: 0.978958051420839, val loss: 0.8980648057661352, acc: 0.7681846932321316, test loss: 0.9295425835248999, acc: 0.7750549795790135
epoch: 135, train loss: 0.030230122494620947, acc: 0.9801082543978349, val loss: 0.912819280268496, acc: 0.7488931056293485, test loss: 0.920552729661458, acc: 0.7637448947533774
epoch: 136, train loss: 0.04146637080567454, acc: 0.9719215155615697, val loss: 0.8562416087819533, acc: 0.7697659709044908, test loss: 0.8737268492818215, acc: 0.764373232799246
epoch: 137, train loss: 0.0580909720121685, acc: 0.9612313937753721, val loss: 0.8552998203329464, acc: 0.7520556609740671, test loss: 0.8722863243766974, acc: 0.7480364436066603
epoch: 138, train loss: 0.05219567051517786, acc: 0.9663058186738837, val loss: 0.8512642508741424, acc: 0.7583807716635041, test loss: 0.8583899877597954, acc: 0.7533773169965441
epoch: 139, train loss: 0.0540116170937944, acc: 0.964276048714479, val loss: 0.8703090056522521, acc: 0.7536369386464263, test loss: 0.878299276371104, acc: 0.7555765001570846
epoch: 140, train loss: 0.048632941027052186, acc: 0.9677266576454668, val loss: 0.8894155194380855, acc: 0.7482605945604048, test loss: 0.895958731094322, acc: 0.7496072887213321
epoch: 141, train loss: 0.049939703420107034, acc: 0.9644790257104194, val loss: 0.8019824845364092, acc: 0.7647058823529411, test loss: 0.8176720191800965, acc: 0.7656299088909834
epoch: 142, train loss: 0.03436570533517249, acc: 0.9760487144790257, val loss: 0.8301446906077116, acc: 0.7666034155597723, test loss: 0.8324469021195853, acc: 0.7719132893496701
epoch: 143, train loss: 0.031776276217705506, acc: 0.9774018944519621, val loss: 0.8803385895011849, acc: 0.7647058823529411, test loss: 0.8975710089836636, acc: 0.7700282752120641
