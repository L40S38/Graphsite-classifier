seed:  14
save trained model at:  ../trained_models/trained_classifier_model_114.pt
save loss at:  ./results/train_classifier_results_114.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['6fcwA00', '5dm3C01', '5yijA00', '1v84A02', '6h1bC00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['3nx8A00', '2bruB00', '5wfnA00', '4yhjA00', '4bc2C00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b1575a71d60>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0372598783862905, acc: 0.38907304368414825; test loss: 1.80795188875408, acc: 0.4438666981800993
epoch: 2, train loss: 1.7347808719401079, acc: 0.4650763584704629; test loss: 1.6183502681428337, acc: 0.49822736941621365
epoch: 3, train loss: 1.6306120548584735, acc: 0.4991121108085711; test loss: 1.5765017801580967, acc: 0.5121720633419995
epoch: 4, train loss: 1.5700984213977958, acc: 0.5217828814963893; test loss: 1.5586972634467187, acc: 0.5150082722760577
epoch: 5, train loss: 1.5818103519405602, acc: 0.5263407126790577; test loss: 1.6286937755998554, acc: 0.5041361380288348
epoch: 6, train loss: 1.5040135096837546, acc: 0.5438617260565881; test loss: 1.4078794964321735, acc: 0.5677144883006382
epoch: 7, train loss: 1.4699418858210547, acc: 0.556173789511069; test loss: 1.4291688679690846, acc: 0.5594422122429685
epoch: 8, train loss: 1.4295945081313404, acc: 0.564342370072215; test loss: 1.4161998135799552, acc: 0.5712597494682108
epoch: 9, train loss: 1.405783065002995, acc: 0.5722149875695514; test loss: 1.3452566837593896, acc: 0.584731741904987
epoch: 10, train loss: 1.3790671567853645, acc: 0.5784302119095537; test loss: 1.3580863714725293, acc: 0.5800047270148901
epoch: 11, train loss: 1.3745255639627303, acc: 0.57985083461584; test loss: 1.3862013709204215, acc: 0.5696052942566769
epoch: 12, train loss: 1.3445904076388704, acc: 0.5914525867171777; test loss: 1.3326720720316834, acc: 0.5818955329709289
epoch: 13, train loss: 1.3260412836718365, acc: 0.5944714099680359; test loss: 1.2867271642655633, acc: 0.6010399432758213
epoch: 14, train loss: 1.3175622409965295, acc: 0.5977861962827039; test loss: 1.394119100349785, acc: 0.5691325927676673
epoch: 15, train loss: 1.3354783840570081, acc: 0.5964247661891796; test loss: 1.3280080666471052, acc: 0.5866225478610257
epoch: 16, train loss: 1.2958316836284296, acc: 0.6021072570143247; test loss: 1.2672058872172411, acc: 0.6043488536988891
epoch: 17, train loss: 1.268362808103348, acc: 0.6133538534390908; test loss: 1.257171651539975, acc: 0.6100212715670055
epoch: 18, train loss: 1.2608670178455503, acc: 0.6148928613709009; test loss: 1.2688837729850522, acc: 0.6062396596549279
epoch: 19, train loss: 1.2339863580007362, acc: 0.6255475316680478; test loss: 1.2821047265488497, acc: 0.6036398014653747
epoch: 20, train loss: 1.2265925033425273, acc: 0.6247188350893809; test loss: 1.296004008243789, acc: 0.5986764358307729
epoch: 21, train loss: 1.2097609247613181, acc: 0.6315851781697644; test loss: 1.1973621673376615, acc: 0.6242023162372962
epoch: 22, train loss: 1.209985052422968, acc: 0.6344264235823369; test loss: 1.4006658583717464, acc: 0.5714961002127157
epoch: 23, train loss: 1.1920092444850579, acc: 0.6376820172842429; test loss: 1.296839969993849, acc: 0.5986764358307729
epoch: 24, train loss: 1.1896746913060445, acc: 0.6420622706286255; test loss: 1.1717414108187107, acc: 0.63365634601749
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9455170606852735, acc: 0.6455546347815793; test loss: 0.9522809277356192, acc: 0.6379106594185772
epoch: 26, train loss: 0.9247212604515592, acc: 0.6502308511897715; test loss: 0.9550548531264735, acc: 0.6372016071850626
epoch: 27, train loss: 0.913577281385819, acc: 0.653131289215106; test loss: 1.0955456344547692, acc: 0.5804774285038998
epoch: 28, train loss: 0.9096789418331501, acc: 0.6565052681425358; test loss: 0.9135587112302336, acc: 0.6582368234459939
epoch: 29, train loss: 0.8951425618132967, acc: 0.6602344027465372; test loss: 1.1229573306505405, acc: 0.5873316000945403
epoch: 30, train loss: 0.8963154426295051, acc: 0.6608263288741565; test loss: 0.9904238652518048, acc: 0.6369652564405578
epoch: 31, train loss: 0.8962733047847086, acc: 0.6617734106783474; test loss: 1.2680675626730868, acc: 0.5488064287402505
epoch: 32, train loss: 0.8740057399657629, acc: 0.6632532259973956; test loss: 1.1527884393627552, acc: 0.5660600330891042
epoch: 33, train loss: 0.8734577364211423, acc: 0.6643778856398721; test loss: 1.0314658844451254, acc: 0.6114393760340345
epoch: 34, train loss: 0.8449401185542744, acc: 0.6725464662010181; test loss: 1.1328457904126847, acc: 0.5722051524462302
epoch: 35, train loss: 0.8565994104267056, acc: 0.6728424292648277; test loss: 0.9534611417395769, acc: 0.6374379579295675
epoch: 36, train loss: 0.8496609278031181, acc: 0.6731975849413994; test loss: 1.0089014847772788, acc: 0.6112030252895296
epoch: 37, train loss: 0.8253176972486993, acc: 0.6781697644134012; test loss: 0.9664131695709733, acc: 0.6362562042070432
epoch: 38, train loss: 0.8311318841095688, acc: 0.6775778382857819; test loss: 0.9398960392685315, acc: 0.6312928385724415
epoch: 39, train loss: 0.8264049425821721, acc: 0.6819580916301645; test loss: 0.836838303207365, acc: 0.676199480028362
epoch: 40, train loss: 0.811922689746989, acc: 0.6839706404640701; test loss: 0.9241381599896908, acc: 0.6327109430394706
epoch: 41, train loss: 0.8062020795746387, acc: 0.6883508938084527; test loss: 1.0372489760936898, acc: 0.6112030252895296
epoch: 42, train loss: 0.8093764964710316, acc: 0.689771516514739; test loss: 1.0495097980914008, acc: 0.6220751595367525
epoch: 43, train loss: 0.7923230596854304, acc: 0.6900674795785486; test loss: 0.9668420066621618, acc: 0.6369652564405578
epoch: 44, train loss: 0.7921255136419644, acc: 0.6959867408547413; test loss: 0.7981780072922009, acc: 0.6929803828882061
epoch: 45, train loss: 0.7764451326499172, acc: 0.6982952527524565; test loss: 0.8909712448606985, acc: 0.6669818009926731
epoch: 46, train loss: 0.7782231230022809, acc: 0.6969338226589321; test loss: 0.8423398311356229, acc: 0.6731269203497992
epoch: 47, train loss: 0.758051556525914, acc: 0.7050432106073162; test loss: 0.9823997878374655, acc: 0.6405105176081305
epoch: 48, train loss: 0.7529399229941939, acc: 0.7083579969219841; test loss: 0.7918215010414673, acc: 0.7005436067123612
epoch: 49, train loss: 0.753153565890467, acc: 0.7039185509648396; test loss: 0.851823968319317, acc: 0.6740723233278185
epoch: 50, train loss: 0.7467486862813395, acc: 0.7075293003433172; test loss: 0.8645635678452032, acc: 0.6783266367289057
epoch: 51, train loss: 0.7414414420118837, acc: 0.7162306144193205; test loss: 0.8644183287240743, acc: 0.6714724651382652
epoch: 52, train loss: 0.7407283480102188, acc: 0.7112584349473186; test loss: 0.810596331048874, acc: 0.6894351217206334
epoch: 53, train loss: 0.7301291179075543, acc: 0.711731975849414; test loss: 0.8106844973490892, acc: 0.6832900023635075
epoch: 54, train loss: 0.736914537045269, acc: 0.7086539599857937; test loss: 0.9372747488208933, acc: 0.6542188607894115
epoch: 55, train loss: 0.7293226274355101, acc: 0.7148691843257962; test loss: 0.9103395501594119, acc: 0.6563460174899551
epoch: 56, train loss: 0.7151842765502144, acc: 0.7130342133301764; test loss: 0.8011899145464896, acc: 0.6875443157645946
epoch: 57, train loss: 0.728682556215286, acc: 0.7160530365810347; test loss: 0.8518719357030946, acc: 0.6832900023635075
epoch: 58, train loss: 0.7293974384974202, acc: 0.7158754587427489; test loss: 0.8033839582241729, acc: 0.6951075395887497
epoch: 59, train loss: 0.7028860762032642, acc: 0.7226234165976086; test loss: 1.0022429263047985, acc: 0.6348380997400142
Epoch    59: reducing learning rate of group 0 to 1.5000e-03.
epoch: 60, train loss: 0.6405783762547396, acc: 0.7441695276429502; test loss: 0.7711216901893228, acc: 0.7090522335145356
epoch: 61, train loss: 0.6122531428116376, acc: 0.7509766781105718; test loss: 0.752168851213178, acc: 0.706925076813992
epoch: 62, train loss: 0.5962971209195488, acc: 0.7609210370545756; test loss: 0.8258338718373734, acc: 0.690144173954148
epoch: 63, train loss: 0.5972749712101112, acc: 0.7593820291227655; test loss: 0.7298729403366634, acc: 0.7289056960529425
epoch: 64, train loss: 0.590731728256165, acc: 0.763051971114005; test loss: 0.7468602530529695, acc: 0.7161427558496809
epoch: 65, train loss: 0.5888465149693056, acc: 0.7635847046288623; test loss: 0.7697005986998308, acc: 0.7123611439376034
epoch: 66, train loss: 0.5864645985508722, acc: 0.7642950159820054; test loss: 0.7355049416630636, acc: 0.7211061214842827
epoch: 67, train loss: 0.5800757926710262, acc: 0.7660116017521014; test loss: 0.7273766212332646, acc: 0.7203970692507682
epoch: 68, train loss: 0.5714947938199324, acc: 0.7674322244583875; test loss: 0.7503134389488276, acc: 0.7142519498936422
epoch: 69, train loss: 0.5718714536203073, acc: 0.7717532851900083; test loss: 0.7386934638896808, acc: 0.723705979673836
epoch: 70, train loss: 0.5633802544623279, acc: 0.7732922931218184; test loss: 0.805453134298719, acc: 0.6953438903332545
epoch: 71, train loss: 0.5502283209962394, acc: 0.7735290635728661; test loss: 0.7556663818940911, acc: 0.7255967856298747
epoch: 72, train loss: 0.542876514525361, acc: 0.7744761453770569; test loss: 0.7497535541361315, acc: 0.7208697707397779
epoch: 73, train loss: 0.5541006855545798, acc: 0.7734698709601042; test loss: 0.7361697956789359, acc: 0.7329236587095249
epoch: 74, train loss: 0.5579686231044139, acc: 0.7725819817686753; test loss: 0.7113280497496642, acc: 0.7338690616875443
epoch: 75, train loss: 0.5441056314171793, acc: 0.7779093169172487; test loss: 0.7261871875135422, acc: 0.7255967856298747
epoch: 76, train loss: 0.5329197251586444, acc: 0.7822303776488694; test loss: 0.8355175769236093, acc: 0.6832900023635075
epoch: 77, train loss: 0.5238639007937602, acc: 0.788268024150586; test loss: 0.7568365348706824, acc: 0.7213424722287876
epoch: 78, train loss: 0.5195992925865094, acc: 0.7870841718953474; test loss: 0.76930664228621, acc: 0.720633419995273
epoch: 79, train loss: 0.5238070093175569, acc: 0.7860778974783947; test loss: 0.7652237744400108, acc: 0.7161427558496809
epoch: 80, train loss: 0.52020067468372, acc: 0.7877352906357287; test loss: 0.7265942441011202, acc: 0.7312692034979911
epoch: 81, train loss: 0.5214088884658244, acc: 0.7864330531549663; test loss: 0.7684066950415528, acc: 0.723705979673836
epoch: 82, train loss: 0.5157331816681859, acc: 0.7857227418018231; test loss: 0.7470232344383722, acc: 0.7229969274403214
epoch: 83, train loss: 0.49962175714838386, acc: 0.7951935598437315; test loss: 0.7634426231672792, acc: 0.7293783975419522
epoch: 84, train loss: 0.4967072106358905, acc: 0.7975020717414467; test loss: 0.7364177010183891, acc: 0.7329236587095249
epoch: 85, train loss: 0.5124864307778735, acc: 0.7914052326269682; test loss: 0.7546106307846354, acc: 0.7260694871188844
epoch: 86, train loss: 0.5108595300076402, acc: 0.7917603883035397; test loss: 0.8463946295472021, acc: 0.6922713306546916
epoch: 87, train loss: 0.4901567063092254, acc: 0.7970285308393512; test loss: 0.7633767875760645, acc: 0.7381233750886316
epoch: 88, train loss: 0.4839702641821421, acc: 0.8012312063454481; test loss: 0.7798600754707424, acc: 0.725124084140865
epoch: 89, train loss: 0.4829143369439171, acc: 0.7993962353498283; test loss: 0.7495351384869988, acc: 0.7253604348853699
epoch: 90, train loss: 0.48593401037420314, acc: 0.7992186575115425; test loss: 0.7446958788234104, acc: 0.7367052706216024
epoch: 91, train loss: 0.4853617242769305, acc: 0.7999881614774477; test loss: 0.7509526337497596, acc: 0.7369416213661073
epoch: 92, train loss: 0.4723207991448072, acc: 0.8052563040132591; test loss: 0.7449299557162244, acc: 0.734341763176554
epoch: 93, train loss: 0.4726416227823745, acc: 0.8045459926601161; test loss: 0.7681650377790631, acc: 0.7329236587095249
epoch: 94, train loss: 0.4686434009138107, acc: 0.8091038238427845; test loss: 0.7290761545740722, acc: 0.7333963601985346
epoch: 95, train loss: 0.46398486333710776, acc: 0.807801586362022; test loss: 0.7667899625404482, acc: 0.7324509572205152
epoch: 96, train loss: 0.4567962584021925, acc: 0.8114123357404995; test loss: 0.785648390407738, acc: 0.7185062632947293
epoch: 97, train loss: 0.4591612239391744, acc: 0.8111163726766899; test loss: 0.8820101215156982, acc: 0.705270621602458
epoch: 98, train loss: 0.4547227310248066, acc: 0.8115307209660234; test loss: 0.7666976694180536, acc: 0.7284329945639328
epoch: 99, train loss: 0.44584132065727283, acc: 0.8117674914170712; test loss: 0.8121708022649338, acc: 0.7293783975419522
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3447769277474295, acc: 0.8212975020717415; test loss: 0.6395973798034603, acc: 0.7289056960529425
epoch: 101, train loss: 0.3421052940727, acc: 0.8173907896294542; test loss: 0.6374299299685957, acc: 0.7367052706216024
epoch: 102, train loss: 0.35149872121309067, acc: 0.8129513436723097; test loss: 0.6115181836302019, acc: 0.7289056960529425
epoch: 103, train loss: 0.3477330123148995, acc: 0.8137800402509767; test loss: 0.609412868967915, acc: 0.737414322855117
epoch: 104, train loss: 0.33878118438354904, acc: 0.8173907896294542; test loss: 0.6956315390524417, acc: 0.7088158827700307
epoch: 105, train loss: 0.34884482483842943, acc: 0.8105244465490706; test loss: 0.6606104011598757, acc: 0.7203970692507682
epoch: 106, train loss: 0.33816284562576954, acc: 0.8150230851189771; test loss: 0.5929635942080882, acc: 0.7485228078468447
epoch: 107, train loss: 0.3316970064193522, acc: 0.81756836746774; test loss: 0.7348693807872861, acc: 0.7111793902150791
epoch: 108, train loss: 0.3347265751168222, acc: 0.8144903516041198; test loss: 0.6146272487094909, acc: 0.7378870243441267
epoch: 109, train loss: 0.3394990091960614, acc: 0.8148455072806914; test loss: 0.7032290804445307, acc: 0.7147246513826518
epoch: 110, train loss: 0.33266249275057924, acc: 0.8139576180892625; test loss: 0.6512704605583997, acc: 0.7324509572205152
epoch: 111, train loss: 0.3247567834523326, acc: 0.820883153782408; test loss: 0.6752304597488111, acc: 0.7255967856298747
epoch: 112, train loss: 0.33583480594242, acc: 0.8147863146679294; test loss: 0.665354812886241, acc: 0.7232332781848263
epoch: 113, train loss: 0.34664087702963525, acc: 0.8083343198768793; test loss: 0.6523567638451313, acc: 0.7300874497754668
epoch: 114, train loss: 0.3263251697352416, acc: 0.8166212856635492; test loss: 0.673296275135474, acc: 0.7118884424485937
epoch: 115, train loss: 0.3208275892028332, acc: 0.8188114123357405; test loss: 0.6754468458816251, acc: 0.718978964783739
epoch: 116, train loss: 0.3353366120667151, acc: 0.8125961879957382; test loss: 0.630564551408678, acc: 0.7326873079650201
epoch: 117, train loss: 0.3273034626045805, acc: 0.8173315970166923; test loss: 0.7322083640059126, acc: 0.7040888678799339
Epoch   117: reducing learning rate of group 0 to 7.5000e-04.
epoch: 118, train loss: 0.2731292954567439, acc: 0.8456256659168936; test loss: 0.6263137712345651, acc: 0.752777121247932
epoch: 119, train loss: 0.24716228807665433, acc: 0.8529655498993726; test loss: 0.6494225254861268, acc: 0.752540770503427
epoch: 120, train loss: 0.24894293494601072, acc: 0.851781697644134; test loss: 0.6348087751845665, acc: 0.7577404868825337
epoch: 121, train loss: 0.23248548744147354, acc: 0.8630874866816621; test loss: 0.6671487651686103, acc: 0.7506499645473883
epoch: 122, train loss: 0.23946568895913276, acc: 0.8559843731502309; test loss: 0.6788115858101895, acc: 0.735759867643583
epoch: 123, train loss: 0.23949888937075603, acc: 0.8592991594648988; test loss: 0.63674584662934, acc: 0.7577404868825337
epoch: 124, train loss: 0.237299791104034, acc: 0.8587072333372795; test loss: 0.6507488211061734, acc: 0.7485228078468447
epoch: 125, train loss: 0.22442418943710887, acc: 0.8636202201965195; test loss: 0.6473157231730509, acc: 0.7553769794374853
epoch: 126, train loss: 0.21973064641310672, acc: 0.8642713389369007; test loss: 0.6653959734787532, acc: 0.7546679272039707
epoch: 127, train loss: 0.23263412699539082, acc: 0.8606605895584231; test loss: 0.6475473252686167, acc: 0.752777121247932
epoch: 128, train loss: 0.22639876134482814, acc: 0.8613117082988043; test loss: 0.6588576009047564, acc: 0.7565587331600094
epoch: 129, train loss: 0.2284900606352436, acc: 0.8626139457795667; test loss: 0.6787382185924028, acc: 0.7423776884897187
epoch: 130, train loss: 0.22675756509450534, acc: 0.8621995974902332; test loss: 0.6976440706255306, acc: 0.7331600094540298
epoch: 131, train loss: 0.22031597506086883, acc: 0.8666982360601397; test loss: 0.7032222933477331, acc: 0.7489955093358545
epoch: 132, train loss: 0.2296143628174389, acc: 0.8572274180182313; test loss: 0.6871709670898791, acc: 0.7430867407232333
epoch: 133, train loss: 0.22472978916318653, acc: 0.8617260565881378; test loss: 0.6643645315411468, acc: 0.7463956511463011
epoch: 134, train loss: 0.22009115294195342, acc: 0.8645673020007103; test loss: 0.7083806804493792, acc: 0.7381233750886316
epoch: 135, train loss: 0.22362034975762649, acc: 0.8619036344264236; test loss: 0.7392328352377054, acc: 0.7369416213661073
epoch: 136, train loss: 0.227856330943133, acc: 0.8594175446904226; test loss: 0.7078451401965891, acc: 0.7461593004017962
epoch: 137, train loss: 0.2145423227542954, acc: 0.8681188587664259; test loss: 0.724798588120495, acc: 0.7385960765776413
epoch: 138, train loss: 0.2192559463851929, acc: 0.8666982360601397; test loss: 0.6828168289680003, acc: 0.7532498227369416
epoch: 139, train loss: 0.20730990013371167, acc: 0.8664022729963301; test loss: 0.6936282967878723, acc: 0.7428503899787284
epoch: 140, train loss: 0.21014349864152138, acc: 0.8682964366047118; test loss: 0.7007941948947712, acc: 0.7487591585913496
epoch: 141, train loss: 0.22336745287901524, acc: 0.8608381673967089; test loss: 0.694450220439598, acc: 0.7478137556133302
epoch: 142, train loss: 0.22193109076609058, acc: 0.8652776133538534; test loss: 0.6766247046639017, acc: 0.7534861734814464
epoch: 143, train loss: 0.20170204654535318, acc: 0.8727358825618563; test loss: 0.7195602100640993, acc: 0.7466320018908059
epoch: 144, train loss: 0.19291792003250663, acc: 0.8758730910382384; test loss: 0.7419858427370054, acc: 0.7508863152918932
epoch: 145, train loss: 0.19768052702967875, acc: 0.8736237717532852; test loss: 0.7083481797313217, acc: 0.7362325691325927
epoch: 146, train loss: 0.20323507285434225, acc: 0.872143956434237; test loss: 0.701789442401382, acc: 0.748050106357835
epoch: 147, train loss: 0.1992242325953341, acc: 0.8741565052681425; test loss: 0.7091766986067034, acc: 0.7544315764594659
epoch: 148, train loss: 0.20181043699914722, acc: 0.8719071859831893; test loss: 0.7107479464103358, acc: 0.7454502481682818
epoch: 149, train loss: 0.19717914605544307, acc: 0.8761098614892862; test loss: 0.727086729035257, acc: 0.7426140392342235
epoch: 150, train loss: 0.19606062084192974, acc: 0.8742156978809045; test loss: 0.7089850654052071, acc: 0.7508863152918932
epoch: 151, train loss: 0.19452919824858367, acc: 0.8735053865277613; test loss: 0.7094918485229896, acc: 0.7473410541243205
epoch: 152, train loss: 0.20338114420450606, acc: 0.8694802888599503; test loss: 0.7222692184881013, acc: 0.7485228078468447
epoch: 153, train loss: 0.1835937898152253, acc: 0.8817923523144312; test loss: 0.7475210341176534, acc: 0.7414322855116994
epoch: 154, train loss: 0.18887666928015956, acc: 0.8770569432934769; test loss: 0.721128686413757, acc: 0.7397778303001654
epoch: 155, train loss: 0.20171738165898537, acc: 0.8742748904936664; test loss: 0.72466567860966, acc: 0.7416686362562042
epoch: 156, train loss: 0.1954179163446677, acc: 0.8730318456256659; test loss: 0.7228943680905473, acc: 0.7433230914677381
epoch: 157, train loss: 0.19879541555191338, acc: 0.8736829643660471; test loss: 0.7082353696505304, acc: 0.7501772630583786
epoch: 158, train loss: 0.18269432612787523, acc: 0.8815555818633835; test loss: 0.7119823687467077, acc: 0.7452138974237769
epoch: 159, train loss: 0.19545683568506478, acc: 0.8759914762637623; test loss: 0.6933583242339294, acc: 0.7421413377452138
epoch: 160, train loss: 0.19787040323361257, acc: 0.8742156978809045; test loss: 0.6996981558915826, acc: 0.7402505317891751
epoch: 161, train loss: 0.18918114829496674, acc: 0.8766425950041434; test loss: 0.7163149952127759, acc: 0.74048688253368
epoch: 162, train loss: 0.178432680856085, acc: 0.8797798034805256; test loss: 0.756280636567648, acc: 0.7390687780666509
epoch: 163, train loss: 0.17298366762702108, acc: 0.8826802415058601; test loss: 0.7586309067945451, acc: 0.7411959347671945
epoch: 164, train loss: 0.1958547626477918, acc: 0.8753403575233811; test loss: 0.7854483923825268, acc: 0.7241786811628457
epoch: 165, train loss: 0.18196861256089059, acc: 0.880253344382621; test loss: 0.7271659616543817, acc: 0.7518317182699126
epoch: 166, train loss: 0.16800048214007343, acc: 0.8897241624245295; test loss: 0.7155654204424265, acc: 0.7497045615693689
epoch: 167, train loss: 0.17500969647260423, acc: 0.8845152125014798; test loss: 0.7769557575445822, acc: 0.7348144646655637
epoch: 168, train loss: 0.16557038839405014, acc: 0.888421924943767; test loss: 0.7742948896128132, acc: 0.738832427322146
Epoch   168: reducing learning rate of group 0 to 3.7500e-04.
epoch: 169, train loss: 0.14324337841350396, acc: 0.901503492364153; test loss: 0.7406728063681198, acc: 0.7544315764594659
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.09302265433689624, acc: 0.9094944950870132; test loss: 0.6810308067095925, acc: 0.754195225714961
epoch: 171, train loss: 0.09180744144568517, acc: 0.9083698354445365; test loss: 0.6769168764331667, acc: 0.757267785393524
epoch: 172, train loss: 0.08607962567809489, acc: 0.9184917722268261; test loss: 0.6789380627528231, acc: 0.7563223824155046
epoch: 173, train loss: 0.08818839968831438, acc: 0.9125725109506334; test loss: 0.6968312704712469, acc: 0.7492318600803592
epoch: 174, train loss: 0.08694593826948545, acc: 0.9136379779803481; test loss: 0.6819867467914059, acc: 0.7553769794374853
epoch: 175, train loss: 0.08498343909663167, acc: 0.9160056824908251; test loss: 0.6940484991650636, acc: 0.7549042779484756
epoch: 176, train loss: 0.08294668297972554, acc: 0.9167751864567302; test loss: 0.699452866056348, acc: 0.752777121247932
epoch: 177, train loss: 0.08441837923518579, acc: 0.9139339410441577; test loss: 0.7173698342908671, acc: 0.7506499645473883
epoch: 178, train loss: 0.08501314903076151, acc: 0.9142890967207292; test loss: 0.7149468184757165, acc: 0.7468683526353108
epoch: 179, train loss: 0.08539819057686408, acc: 0.9151769859121581; test loss: 0.7135584308756804, acc: 0.7501772630583786
epoch: 180, train loss: 0.08426376822570718, acc: 0.9142890967207292; test loss: 0.6935019349409485, acc: 0.7478137556133302
epoch: 181, train loss: 0.08325392283641249, acc: 0.9147034450100627; test loss: 0.7119906898425055, acc: 0.7468683526353108
epoch: 182, train loss: 0.08333927391195517, acc: 0.9147034450100627; test loss: 0.6862212830867871, acc: 0.754195225714961
epoch: 183, train loss: 0.08404653128031458, acc: 0.9149994080738724; test loss: 0.7173678343021624, acc: 0.7523044197589223
epoch: 184, train loss: 0.08893084497748571, acc: 0.911211080857109; test loss: 0.7292546078094324, acc: 0.7350508154100686
epoch: 185, train loss: 0.08444493501669305, acc: 0.9150586006866344; test loss: 0.7107532541680297, acc: 0.7414322855116994
epoch: 186, train loss: 0.07685671215037898, acc: 0.9203267432224458; test loss: 0.7228521734116856, acc: 0.7456865989127865
epoch: 187, train loss: 0.07910275365440637, acc: 0.9155913342014916; test loss: 0.7177335605979727, acc: 0.7478137556133302
epoch: 188, train loss: 0.08292849884063404, acc: 0.9142890967207292; test loss: 0.7445882250022392, acc: 0.748050106357835
epoch: 189, train loss: 0.0846411871711813, acc: 0.9168343790694922; test loss: 0.7161690537119779, acc: 0.7497045615693689
epoch: 190, train loss: 0.08335090038994296, acc: 0.913756363205872; test loss: 0.728631592760467, acc: 0.7426140392342235
epoch: 191, train loss: 0.09279941763837789, acc: 0.9062389013851071; test loss: 0.7220066030153398, acc: 0.7423776884897187
epoch: 192, train loss: 0.08754947556642946, acc: 0.9130460518527288; test loss: 0.7057760435197686, acc: 0.7463956511463011
epoch: 193, train loss: 0.08022999931076116, acc: 0.9139339410441577; test loss: 0.7261486074879953, acc: 0.7499409123138738
epoch: 194, train loss: 0.07722185425812272, acc: 0.9165384160056825; test loss: 0.7261874787042789, acc: 0.751122666036398
epoch: 195, train loss: 0.08252089068768108, acc: 0.9156505268142536; test loss: 0.7326873177981664, acc: 0.7397778303001654
epoch: 196, train loss: 0.08546458701858355, acc: 0.9121581626612999; test loss: 0.708285467248807, acc: 0.7497045615693689
epoch: 197, train loss: 0.07975568373615789, acc: 0.9160056824908251; test loss: 0.7129979505236902, acc: 0.7492318600803592
epoch: 198, train loss: 0.07966790805143323, acc: 0.9160056824908251; test loss: 0.7170527889722348, acc: 0.7445048451902624
epoch: 199, train loss: 0.08132594763946854, acc: 0.9151769859121581; test loss: 0.7471430359143677, acc: 0.7397778303001654
epoch: 200, train loss: 0.0870729088642138, acc: 0.9116846217592045; test loss: 0.7526085055260421, acc: 0.7359962183880879
best test acc 0.7577404868825337 at epoch 120.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9202    0.9559    0.9377      6100
           1     0.9359    0.8823    0.9083       926
           2     0.8444    0.9271    0.8838      2400
           3     0.8885    0.9170    0.9025       843
           4     0.8847    0.9612    0.9214       774
           5     0.9231    0.9372    0.9301      1512
           6     0.7435    0.7737    0.7583      1330
           7     0.9322    0.8004    0.8613       481
           8     0.8126    0.8144    0.8135       458
           9     0.9190    0.9535    0.9359       452
          10     0.9277    0.8047    0.8618       717
          11     0.8881    0.7147    0.7920       333
          12     0.8889    0.0268    0.0519       299
          13     0.8771    0.5836    0.7009       269

    accuracy                         0.8882     16894
   macro avg     0.8847    0.7895    0.8042     16894
weighted avg     0.8892    0.8882    0.8800     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8260    0.8780    0.8512      1525
           1     0.7974    0.7974    0.7974       232
           2     0.7206    0.7854    0.7516       601
           3     0.7617    0.7725    0.7671       211
           4     0.8077    0.8660    0.8358       194
           5     0.8374    0.8175    0.8273       378
           6     0.4589    0.5195    0.4873       333
           7     0.8118    0.5702    0.6699       121
           8     0.6574    0.6174    0.6368       115
           9     0.7642    0.7105    0.7364       114
          10     0.7278    0.6389    0.6805       180
          11     0.6667    0.4048    0.5037        84
          12     0.0000    0.0000    0.0000        75
          13     0.6750    0.3971    0.5000        68

    accuracy                         0.7577      4231
   macro avg     0.6795    0.6268    0.6461      4231
weighted avg     0.7465    0.7577    0.7494      4231

---------------------------------------
program finished.
