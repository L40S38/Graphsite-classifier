seed:  13
save trained model at:  ../trained_models/trained_classifier_model_113.pt
save loss at:  ./results/train_classifier_results_113.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['1mkjA00', '2zbuB00', '2o2zA00', '1zp9B00', '5zbzA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['4tz0A00', '5ko6A01', '5ck4B00', '5thaA00', '4mvdE00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b918f1e7d60>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.006910689994543, acc: 0.3962353498283414; test loss: 1.7009039359069045, acc: 0.4698652800756322
epoch: 2, train loss: 1.7045613026300013, acc: 0.4800520894992305; test loss: 1.614309899267703, acc: 0.5072086977073977
epoch: 3, train loss: 1.6193091377670614, acc: 0.5058008760506689; test loss: 1.5046255505132888, acc: 0.5417159064051051
epoch: 4, train loss: 1.5533650242055932, acc: 0.5194151769859121; test loss: 1.571085756480398, acc: 0.5199716379106594
epoch: 5, train loss: 1.5094786970463252, acc: 0.5385935835207766; test loss: 1.4331097562271944, acc: 0.5684235405341527
epoch: 6, train loss: 1.4612291128632302, acc: 0.5512608026518291; test loss: 1.4036652714443725, acc: 0.5736232569132593
epoch: 7, train loss: 1.431116489146438, acc: 0.5642831774594531; test loss: 1.4156904831419486, acc: 0.5651146301110849
epoch: 8, train loss: 1.4235889864233213, acc: 0.5696697052207884; test loss: 1.384967824635678, acc: 0.5745686598912787
epoch: 9, train loss: 1.3907212753387348, acc: 0.5742867290162188; test loss: 1.4007548516705592, acc: 0.5731505554242496
epoch: 10, train loss: 1.3513185280349052, acc: 0.5867171776962236; test loss: 1.3918917720071589, acc: 0.5934767194516662
epoch: 11, train loss: 1.3388693784275916, acc: 0.5951225287084172; test loss: 1.3398512980245412, acc: 0.586386197116521
epoch: 12, train loss: 1.322879694362526, acc: 0.5942346395169883; test loss: 1.3645350052318064, acc: 0.5677144883006382
epoch: 13, train loss: 1.3007056652137736, acc: 0.6023440274653723; test loss: 1.2921684471830017, acc: 0.6100212715670055
epoch: 14, train loss: 1.2870264910827434, acc: 0.6079673256777555; test loss: 1.2657151454392688, acc: 0.6164027416686363
epoch: 15, train loss: 1.2763581885746191, acc: 0.615662365336806; test loss: 1.3213894376797588, acc: 0.5868588986055306
epoch: 16, train loss: 1.2550961397040508, acc: 0.6173197584941399; test loss: 1.2663960234827534, acc: 0.6102576223115103
epoch: 17, train loss: 1.2242388393634225, acc: 0.6280336214040487; test loss: 1.2511569097852966, acc: 0.6126211297565587
epoch: 18, train loss: 1.2143495886568856, acc: 0.6348999644844323; test loss: 1.2294346632559625, acc: 0.6256204207043252
epoch: 19, train loss: 1.208136016184781, acc: 0.6330649934888126; test loss: 1.2659768820992319, acc: 0.6074214133774521
epoch: 20, train loss: 1.1867770554681523, acc: 0.6438380490114833; test loss: 1.2126330883050467, acc: 0.6310564878279367
epoch: 21, train loss: 1.1914115793763977, acc: 0.6421806558541494; test loss: 1.2648545835238487, acc: 0.6114393760340345
epoch: 22, train loss: 1.1640562513699626, acc: 0.6492245767728188; test loss: 1.2168716284941905, acc: 0.6246750177263058
epoch: 23, train loss: 1.167802851722553, acc: 0.6474487983899609; test loss: 1.2066252661551518, acc: 0.6308201370834318
epoch: 24, train loss: 1.1520069420965688, acc: 0.6529537113768201; test loss: 1.4290132034024965, acc: 0.5681871897896479
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9288689819852234, acc: 0.6542559488575825; test loss: 0.9751483465418751, acc: 0.6249113684708106
epoch: 26, train loss: 0.9096338498150651, acc: 0.6578075056232983; test loss: 0.9673731424543092, acc: 0.6334199952729851
epoch: 27, train loss: 0.904442096442404, acc: 0.662069373742157; test loss: 0.9797196726910585, acc: 0.6225478610257622
epoch: 28, train loss: 0.8976055542590337, acc: 0.6554989937255831; test loss: 1.0223725025210362, acc: 0.6178208461356653
epoch: 29, train loss: 0.8691760503612102, acc: 0.6700011838522553; test loss: 1.0069661843694995, acc: 0.6251477192153155
epoch: 30, train loss: 0.8694854784438042, acc: 0.6695276429501599; test loss: 0.9204675228255941, acc: 0.6480737414322855
epoch: 31, train loss: 0.8700545832957546, acc: 0.6689949094353025; test loss: 1.3928251028117264, acc: 0.5081541006854171
epoch: 32, train loss: 0.8631733895702955, acc: 0.6736711258434948; test loss: 1.0027680981950933, acc: 0.6256204207043252
epoch: 33, train loss: 0.8404486352795889, acc: 0.6752101337753048; test loss: 0.9360269724237978, acc: 0.6464192862207516
epoch: 34, train loss: 0.8343551241322722, acc: 0.6795311945069256; test loss: 0.930062997259448, acc: 0.6542188607894115
epoch: 35, train loss: 0.8302083559906721, acc: 0.6826092103705458; test loss: 0.9112787325594673, acc: 0.6542188607894115
epoch: 36, train loss: 0.8337553587413701, acc: 0.6792352314431159; test loss: 1.0009270534253916, acc: 0.6130938312455684
epoch: 37, train loss: 0.8138554055577583, acc: 0.6861015745234995; test loss: 0.9586410017561275, acc: 0.624438666981801
epoch: 38, train loss: 0.8091679117630767, acc: 0.6887652420977862; test loss: 0.9500119728953019, acc: 0.6303474355944221
epoch: 39, train loss: 0.800920330082381, acc: 0.6937374215697881; test loss: 0.9351924166727844, acc: 0.6492554951548097
epoch: 40, train loss: 0.7977843884746041, acc: 0.6901266721913105; test loss: 0.9052871633325449, acc: 0.6518553533443631
epoch: 41, train loss: 0.8030400500010213, acc: 0.6900082869657866; test loss: 1.158081790454786, acc: 0.5601512644764831
epoch: 42, train loss: 0.7937371665392456, acc: 0.6907777909316917; test loss: 0.8496999841467817, acc: 0.676199480028362
epoch: 43, train loss: 0.7715430444862146, acc: 0.7021427725819818; test loss: 0.9446772896237645, acc: 0.6516190025998582
epoch: 44, train loss: 0.7916410349312091, acc: 0.6942701550846454; test loss: 0.9269928112291496, acc: 0.6523280548333728
epoch: 45, train loss: 0.776091182635524, acc: 0.6971705931099799; test loss: 0.96672412093101, acc: 0.6315291893169463
epoch: 46, train loss: 0.7668461357967182, acc: 0.6985912158162662; test loss: 0.889296565712893, acc: 0.6613093831245569
epoch: 47, train loss: 0.7530577116511031, acc: 0.7005445720374097; test loss: 0.8933685516753227, acc: 0.6639092413141101
epoch: 48, train loss: 0.756382084004312, acc: 0.7032082396116964; test loss: 0.8208273808969107, acc: 0.6877806665090995
epoch: 49, train loss: 0.7569074886881333, acc: 0.7050432106073162; test loss: 1.013995792927175, acc: 0.6156936894351217
epoch: 50, train loss: 0.7511067859757027, acc: 0.7082396116964603; test loss: 0.8661888081873599, acc: 0.6643819428031198
epoch: 51, train loss: 0.737349002583148, acc: 0.7114952053983663; test loss: 0.9198289455687907, acc: 0.650437248877334
epoch: 52, train loss: 0.7345679609101242, acc: 0.7103705457558896; test loss: 0.8214372201778454, acc: 0.6894351217206334
epoch: 53, train loss: 0.7161661213751416, acc: 0.7196045933467503; test loss: 0.8460483151500993, acc: 0.6726542188607895
epoch: 54, train loss: 0.7244058014077238, acc: 0.7131525985557002; test loss: 0.9253477332327728, acc: 0.6360198534625384
epoch: 55, train loss: 0.7250715777819519, acc: 0.7172960814490351; test loss: 1.6504459173170507, acc: 0.47293783975419523
epoch: 56, train loss: 0.7220585296827758, acc: 0.7146324138747484; test loss: 0.8205690882048745, acc: 0.6797447411959348
epoch: 57, train loss: 0.714713102108461, acc: 0.7183615484787499; test loss: 0.8774756536616751, acc: 0.662491136847081
epoch: 58, train loss: 0.7038392940187956, acc: 0.7195454007339884; test loss: 0.8732341059948698, acc: 0.6757267785393524
epoch: 59, train loss: 0.700321865749314, acc: 0.7249319284953237; test loss: 0.9112089191929045, acc: 0.6658000472701489
epoch: 60, train loss: 0.6856127474820867, acc: 0.7289570261631348; test loss: 0.8473619717115828, acc: 0.6735996218388088
epoch: 61, train loss: 0.6749952569281094, acc: 0.7364744879838996; test loss: 0.8554951299366672, acc: 0.6731269203497992
epoch: 62, train loss: 0.6759770433126828, acc: 0.7300224931928495; test loss: 0.9566545399670566, acc: 0.6286929803828882
epoch: 63, train loss: 0.6808987372836319, acc: 0.7279507517461821; test loss: 0.816416109800057, acc: 0.6865989127865753
Epoch    63: reducing learning rate of group 0 to 1.5000e-03.
epoch: 64, train loss: 0.6153906238944175, acc: 0.7536403456848585; test loss: 0.7411247177683019, acc: 0.7168518080831955
epoch: 65, train loss: 0.585855926735748, acc: 0.764650171658577; test loss: 0.7541574515002581, acc: 0.708343181281021
epoch: 66, train loss: 0.5758276788020142, acc: 0.7684976914881023; test loss: 0.7403063572374423, acc: 0.7192153155282439
epoch: 67, train loss: 0.5664185503396506, acc: 0.7729963300580087; test loss: 0.7377554720047095, acc: 0.7166154573386906
epoch: 68, train loss: 0.565884760791654, acc: 0.770391855096484; test loss: 0.7422615261231154, acc: 0.7170881588277003
epoch: 69, train loss: 0.5686126024437695, acc: 0.7690304250029596; test loss: 0.7477529466476206, acc: 0.7102339872370598
epoch: 70, train loss: 0.5600442798007547, acc: 0.7732922931218184; test loss: 0.7746943934424159, acc: 0.7081068305365162
epoch: 71, train loss: 0.5470363401838826, acc: 0.7783828578193441; test loss: 0.8037551664849653, acc: 0.6960529425667691
epoch: 72, train loss: 0.5450638103614943, acc: 0.7790931691724873; test loss: 0.7510591607036333, acc: 0.7232332781848263
epoch: 73, train loss: 0.5357800346091469, acc: 0.7827039185509649; test loss: 0.7699804128526486, acc: 0.7140155991491374
epoch: 74, train loss: 0.5407675904212682, acc: 0.7802178288149639; test loss: 0.7736966910866017, acc: 0.6984164500118175
epoch: 75, train loss: 0.5471991549741168, acc: 0.7782052799810584; test loss: 0.7540940985953489, acc: 0.7104703379815647
epoch: 76, train loss: 0.520398568666742, acc: 0.789096720729253; test loss: 0.7501634248067685, acc: 0.7187426140392342
epoch: 77, train loss: 0.5282409130023953, acc: 0.782526340712679; test loss: 0.8119184181949991, acc: 0.7047979201134483
epoch: 78, train loss: 0.5314432346173062, acc: 0.7801586362022019; test loss: 0.7837388254006515, acc: 0.703852517135429
epoch: 79, train loss: 0.5093609044237702, acc: 0.7893334911803007; test loss: 0.790904469310857, acc: 0.705270621602458
epoch: 80, train loss: 0.5197918500733175, acc: 0.7873209423463952; test loss: 0.7561357730219929, acc: 0.7213424722287876
epoch: 81, train loss: 0.5028088620968707, acc: 0.7944832484905884; test loss: 0.7684830728926239, acc: 0.7118884424485937
epoch: 82, train loss: 0.5087703543611429, acc: 0.786669823606014; test loss: 0.7365513990123175, acc: 0.7239423304183408
epoch: 83, train loss: 0.503007114180534, acc: 0.7941280928140169; test loss: 0.7405972171973686, acc: 0.720633419995273
epoch: 84, train loss: 0.5196609930751083, acc: 0.7897478394696342; test loss: 0.7672913297077523, acc: 0.7156700543606712
epoch: 85, train loss: 0.4937525510759739, acc: 0.7969693382265893; test loss: 0.8578433976954417, acc: 0.6903805246986529
epoch: 86, train loss: 0.4863417306933669, acc: 0.8000473540902096; test loss: 0.7359095747322654, acc: 0.7279602930749232
epoch: 87, train loss: 0.4820500725018734, acc: 0.8005800876050669; test loss: 0.7362875099921447, acc: 0.722051524462302
epoch: 88, train loss: 0.4713899762133416, acc: 0.8069136971705931; test loss: 0.745124419342398, acc: 0.7241786811628457
epoch: 89, train loss: 0.4816698034804319, acc: 0.8004617023795431; test loss: 0.8294214462450678, acc: 0.6934530843772158
epoch: 90, train loss: 0.48151997245457434, acc: 0.7998697762519238; test loss: 0.780639446984789, acc: 0.6955802410777594
epoch: 91, train loss: 0.47907448025925053, acc: 0.8007576654433527; test loss: 0.7629907429852234, acc: 0.7246513826518554
epoch: 92, train loss: 0.4703043272393094, acc: 0.8016455546347816; test loss: 0.7655782193558291, acc: 0.7291420467974474
epoch: 93, train loss: 0.4672380485371808, acc: 0.8064401562684977; test loss: 0.8020718621745568, acc: 0.7107066887260695
epoch: 94, train loss: 0.45695058962106844, acc: 0.8133064993488812; test loss: 0.7524956113986162, acc: 0.7272512408414087
epoch: 95, train loss: 0.45800369435673677, acc: 0.8120042618681188; test loss: 0.7611640757968933, acc: 0.7142519498936422
epoch: 96, train loss: 0.4557278645388598, acc: 0.8115899135787854; test loss: 0.7467028091658771, acc: 0.7286693453084377
epoch: 97, train loss: 0.4496051586703096, acc: 0.8133064993488812; test loss: 0.8676671075696952, acc: 0.6960529425667691
epoch: 98, train loss: 0.4377779489270862, acc: 0.8161477447614538; test loss: 0.766905801343005, acc: 0.7263058378633893
epoch: 99, train loss: 0.44143079780284733, acc: 0.814312773765834; test loss: 0.746965696050382, acc: 0.7286693453084377
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3384118210363349, acc: 0.8197584941399313; test loss: 0.621904963411301, acc: 0.7369416213661073
epoch: 101, train loss: 0.33000978967191424, acc: 0.8154966260210725; test loss: 0.6421374741019833, acc: 0.7270148900969038
epoch: 102, train loss: 0.3297085007422208, acc: 0.8196401089144075; test loss: 0.6592465090543151, acc: 0.7140155991491374
epoch: 103, train loss: 0.33159133148388115, acc: 0.8156742038593584; test loss: 0.6601396609186647, acc: 0.7102339872370598
epoch: 104, train loss: 0.32047296144404297, acc: 0.8215934651355511; test loss: 0.7433949066825691, acc: 0.6825809501299929
epoch: 105, train loss: 0.3384971156217113, acc: 0.8182786788208831; test loss: 0.6347918283626617, acc: 0.7107066887260695
epoch: 106, train loss: 0.34215986067139215, acc: 0.814312773765834; test loss: 0.6566298644724706, acc: 0.7024344126683999
epoch: 107, train loss: 0.3252414343052818, acc: 0.8186338344974547; test loss: 0.6389171059490575, acc: 0.735759867643583
epoch: 108, train loss: 0.32149852142117213, acc: 0.8217710429738369; test loss: 0.6612788141548591, acc: 0.7133065469156228
epoch: 109, train loss: 0.3346130742474365, acc: 0.8157925890848822; test loss: 0.6923559824926186, acc: 0.7000709052233515
epoch: 110, train loss: 0.32725837607101194, acc: 0.8198768793654552; test loss: 0.6337796484316757, acc: 0.725124084140865
epoch: 111, train loss: 0.3173311551964691, acc: 0.8237835918077424; test loss: 0.6608193166372597, acc: 0.718978964783739
epoch: 112, train loss: 0.3319874621713403, acc: 0.8152598555700249; test loss: 0.6326817810267766, acc: 0.7170881588277003
epoch: 113, train loss: 0.31728510952876476, acc: 0.824079554871552; test loss: 0.6812559061088733, acc: 0.7095249350035453
epoch: 114, train loss: 0.3094899295116439, acc: 0.8239019770332663; test loss: 0.6886410419328419, acc: 0.7107066887260695
Epoch   114: reducing learning rate of group 0 to 7.5000e-04.
epoch: 115, train loss: 0.26571029354104264, acc: 0.8491772226826092; test loss: 0.6417953388765895, acc: 0.7430867407232333
epoch: 116, train loss: 0.23938203060134453, acc: 0.8584704628862317; test loss: 0.6364261612095357, acc: 0.7348144646655637
epoch: 117, train loss: 0.24121721643688582, acc: 0.8565762992778502; test loss: 0.6465866846329381, acc: 0.7407232332781848
epoch: 118, train loss: 0.23240801305852446, acc: 0.860719782171185; test loss: 0.6889827787890217, acc: 0.7371779721106122
epoch: 119, train loss: 0.2242282259685832, acc: 0.863975375873091; test loss: 0.6776637558225931, acc: 0.7244150319073505
epoch: 120, train loss: 0.2374656050109547, acc: 0.8545045578311826; test loss: 0.68636809672286, acc: 0.722051524462302
epoch: 121, train loss: 0.23758551339200223, acc: 0.8578785367586125; test loss: 0.6547762043113772, acc: 0.7284329945639328
epoch: 122, train loss: 0.2199681311037832, acc: 0.8621995974902332; test loss: 0.6656144074135936, acc: 0.7400141810446703
epoch: 123, train loss: 0.23029138317888018, acc: 0.8607789747839469; test loss: 0.6822907027835345, acc: 0.7341054124320492
epoch: 124, train loss: 0.22813723099518207, acc: 0.859121581626613; test loss: 0.685835819553185, acc: 0.7293783975419522
epoch: 125, train loss: 0.22676437870382457, acc: 0.8589440037883272; test loss: 0.684417059871779, acc: 0.7329236587095249
epoch: 126, train loss: 0.2243101624016707, acc: 0.859891085592518; test loss: 0.6857406030842892, acc: 0.7322146064760104
epoch: 127, train loss: 0.2204244196859495, acc: 0.8665206582218539; test loss: 0.7560054898064797, acc: 0.7208697707397779
epoch: 128, train loss: 0.21873257554316924, acc: 0.8630282940689002; test loss: 0.7107192416812083, acc: 0.7255967856298747
epoch: 129, train loss: 0.2126211824943656, acc: 0.8660471173197585; test loss: 0.6843068257935699, acc: 0.7303238005199716
epoch: 130, train loss: 0.2143427854997245, acc: 0.8651592281283296; test loss: 0.7548580608083914, acc: 0.7310328527534862
epoch: 131, train loss: 0.21699170577740604, acc: 0.8634426423582336; test loss: 0.7365842973952089, acc: 0.7185062632947293
epoch: 132, train loss: 0.21350818123777365, acc: 0.8646856872262342; test loss: 0.6930404592986029, acc: 0.7315055542424959
epoch: 133, train loss: 0.20897291221427566, acc: 0.8684148218302356; test loss: 0.7211832790559585, acc: 0.7385960765776413
epoch: 134, train loss: 0.21171584085372688, acc: 0.8682964366047118; test loss: 0.7183134304493828, acc: 0.7284329945639328
epoch: 135, train loss: 0.23358985519527725, acc: 0.8552148691843258; test loss: 0.7265578962279617, acc: 0.7324509572205152
epoch: 136, train loss: 0.1993768278315771, acc: 0.8731502308511898; test loss: 0.7078130271693432, acc: 0.7322146064760104
epoch: 137, train loss: 0.19448514210729045, acc: 0.8769385580679531; test loss: 0.758737230267093, acc: 0.7317419049870008
epoch: 138, train loss: 0.20531579262196728, acc: 0.8675861252515686; test loss: 0.7433359156937442, acc: 0.7196880170172536
epoch: 139, train loss: 0.20380079791583125, acc: 0.8685332070557594; test loss: 0.7446422774694231, acc: 0.7232332781848263
epoch: 140, train loss: 0.20483670810686705, acc: 0.86977625192376; test loss: 0.7015052311416496, acc: 0.7289056960529425
epoch: 141, train loss: 0.2086764606898023, acc: 0.8610157452349947; test loss: 0.7129075947565211, acc: 0.734341763176554
epoch: 142, train loss: 0.1926830094464223, acc: 0.8739789274298567; test loss: 0.7411622247693669, acc: 0.7310328527534862
epoch: 143, train loss: 0.18591504969250539, acc: 0.8739789274298567; test loss: 0.7323104076367466, acc: 0.7312692034979911
epoch: 144, train loss: 0.1907269167902908, acc: 0.8740973126553806; test loss: 0.7776900644534532, acc: 0.7255967856298747
epoch: 145, train loss: 0.19962256341181212, acc: 0.8719071859831893; test loss: 0.7973791566815847, acc: 0.7248877333963601
epoch: 146, train loss: 0.1937676571160502, acc: 0.8711376820172843; test loss: 0.7301983502536089, acc: 0.7315055542424959
epoch: 147, train loss: 0.18693289832489102, acc: 0.8770569432934769; test loss: 0.7489331427884987, acc: 0.7270148900969038
epoch: 148, train loss: 0.1935853657072446, acc: 0.871374452468332; test loss: 0.7538389036543288, acc: 0.7213424722287876
epoch: 149, train loss: 0.189335829559707, acc: 0.8739197348170948; test loss: 0.7547963827503288, acc: 0.7317419049870008
epoch: 150, train loss: 0.17768053235571046, acc: 0.8842784420504322; test loss: 0.7630373892336696, acc: 0.7229969274403214
epoch: 151, train loss: 0.1828156766350825, acc: 0.8810228483485261; test loss: 0.741966158243196, acc: 0.7402505317891751
epoch: 152, train loss: 0.1757246323932737, acc: 0.8826802415058601; test loss: 0.759053727578001, acc: 0.7234696289293311
epoch: 153, train loss: 0.17801674800898826, acc: 0.8803717296081449; test loss: 0.818263176343481, acc: 0.7241786811628457
epoch: 154, train loss: 0.1831851518953624, acc: 0.8788919142890967; test loss: 0.770900606855493, acc: 0.7336327109430395
epoch: 155, train loss: 0.17941241357592388, acc: 0.8789511069018586; test loss: 0.7462144244565267, acc: 0.7359962183880879
epoch: 156, train loss: 0.20316223091023256, acc: 0.8715520303066178; test loss: 0.7392298922249116, acc: 0.7274875915859135
epoch: 157, train loss: 0.18567164843817976, acc: 0.878655143838049; test loss: 0.7917239095718972, acc: 0.7260694871188844
epoch: 158, train loss: 0.1634476714385703, acc: 0.8887178880075767; test loss: 0.7591451110357484, acc: 0.7409595840226897
epoch: 159, train loss: 0.1666964641919401, acc: 0.8871196874630046; test loss: 0.8195136523196049, acc: 0.71756086031671
epoch: 160, train loss: 0.17272985032871516, acc: 0.8817331597016692; test loss: 0.8035452457726411, acc: 0.7296147482864571
epoch: 161, train loss: 0.17107886024109636, acc: 0.8806085000591927; test loss: 0.765556483425392, acc: 0.7378870243441267
epoch: 162, train loss: 0.17509315434476816, acc: 0.8822067006037646; test loss: 0.7882658299811482, acc: 0.725124084140865
epoch: 163, train loss: 0.17196884201721593, acc: 0.8826802415058601; test loss: 0.7534554221618263, acc: 0.7270148900969038
epoch: 164, train loss: 0.16844147806994883, acc: 0.8874156505268143; test loss: 0.776266648735005, acc: 0.723705979673836
epoch: 165, train loss: 0.17650416375399058, acc: 0.8789511069018586; test loss: 0.7546621663326407, acc: 0.7173245095722052
Epoch   165: reducing learning rate of group 0 to 3.7500e-04.
epoch: 166, train loss: 0.1443904650850636, acc: 0.8992541730791997; test loss: 0.7647980759538846, acc: 0.738832427322146
epoch: 167, train loss: 0.1213291938543122, acc: 0.9110926956315851; test loss: 0.7919165462614711, acc: 0.7409595840226897
epoch: 168, train loss: 0.1170076456816645, acc: 0.9113294660826329; test loss: 0.7980802030434876, acc: 0.737414322855117
epoch: 169, train loss: 0.11822310631283926, acc: 0.910441576891204; test loss: 0.8245107401237226, acc: 0.7378870243441267
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.08766020728689518, acc: 0.9113886586953948; test loss: 0.7553158621786897, acc: 0.7331600094540298
epoch: 171, train loss: 0.08840488693435983, acc: 0.9120989700485379; test loss: 0.7132281823745436, acc: 0.74048688253368
epoch: 172, train loss: 0.08272370589870355, acc: 0.9113294660826329; test loss: 0.7497041761325736, acc: 0.7362325691325927
epoch: 173, train loss: 0.08204303182440141, acc: 0.9143482893334912; test loss: 0.7465002523813088, acc: 0.7324509572205152
epoch: 174, train loss: 0.07903842463854771, acc: 0.9167751864567302; test loss: 0.7759650686733325, acc: 0.723705979673836
epoch: 175, train loss: 0.08137795375665574, acc: 0.9120989700485379; test loss: 0.7255130145488352, acc: 0.7307965020089813
epoch: 176, train loss: 0.07988727370761507, acc: 0.9130460518527288; test loss: 0.7496376572644786, acc: 0.738832427322146
epoch: 177, train loss: 0.08110538243848756, acc: 0.9145850597845389; test loss: 0.7403410440696043, acc: 0.7345781139210589
epoch: 178, train loss: 0.07399229545993426, acc: 0.9183733870013022; test loss: 0.7540124825229884, acc: 0.7312692034979911
epoch: 179, train loss: 0.07960949375002965, acc: 0.9141115188824435; test loss: 0.7604446424093406, acc: 0.7281966438194281
epoch: 180, train loss: 0.07899047530560405, acc: 0.9151769859121581; test loss: 0.7438105693299617, acc: 0.7293783975419522
epoch: 181, train loss: 0.08071967688706326, acc: 0.9122765478868238; test loss: 0.7744327275022156, acc: 0.7336327109430395
epoch: 182, train loss: 0.0764481010608092, acc: 0.9134604001420623; test loss: 0.7547784883963022, acc: 0.7310328527534862
epoch: 183, train loss: 0.07839739457437861, acc: 0.9139931336569196; test loss: 0.757600539571933, acc: 0.7317419049870008
epoch: 184, train loss: 0.08388840347419085, acc: 0.912039777435776; test loss: 0.7453947957033695, acc: 0.7369416213661073
epoch: 185, train loss: 0.07944011366288309, acc: 0.9132828223037764; test loss: 0.749091792461636, acc: 0.7329236587095249
epoch: 186, train loss: 0.0770212268064089, acc: 0.9139339410441577; test loss: 0.777892072666793, acc: 0.735759867643583
epoch: 187, train loss: 0.07474738842499769, acc: 0.9163016455546348; test loss: 0.7834731843335723, acc: 0.7296147482864571
epoch: 188, train loss: 0.0796624246591772, acc: 0.9141115188824435; test loss: 0.7636824996441539, acc: 0.7371779721106122
epoch: 189, train loss: 0.076486204861642, acc: 0.9142299041079673; test loss: 0.7603376421683845, acc: 0.7281966438194281
epoch: 190, train loss: 0.08244198400624704, acc: 0.9111518882443471; test loss: 0.7600171192688064, acc: 0.7322146064760104
epoch: 191, train loss: 0.08378585842734076, acc: 0.9095536876997751; test loss: 0.7665454208132393, acc: 0.7355235168990782
epoch: 192, train loss: 0.08033671833745006, acc: 0.9124541257251095; test loss: 0.7833015570486164, acc: 0.7348144646655637
epoch: 193, train loss: 0.08098693952587098, acc: 0.9116846217592045; test loss: 0.7821267929180833, acc: 0.7255967856298747
epoch: 194, train loss: 0.07374369844263286, acc: 0.9181958091630165; test loss: 0.8442211822256415, acc: 0.7118884424485937
epoch: 195, train loss: 0.0790410514396578, acc: 0.9122765478868238; test loss: 0.7685813212389079, acc: 0.7281966438194281
epoch: 196, train loss: 0.07249268640956893, acc: 0.9173671125843494; test loss: 0.7861224711293505, acc: 0.7317419049870008
epoch: 197, train loss: 0.07561731429880182, acc: 0.9151177932993962; test loss: 0.7752465651238734, acc: 0.7239423304183408
epoch: 198, train loss: 0.07711604116364346, acc: 0.9117438143719664; test loss: 0.7695400856877297, acc: 0.7307965020089813
epoch: 199, train loss: 0.08114060081874067, acc: 0.9107375399550136; test loss: 0.8059244991560349, acc: 0.7211061214842827
epoch: 200, train loss: 0.09001964026281675, acc: 0.9049366639043447; test loss: 0.7807088933974906, acc: 0.7234696289293311
best test acc 0.7430867407232333 at epoch 115.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9219    0.9439    0.9328      6100
           1     0.9729    0.8521    0.9085       926
           2     0.8301    0.9263    0.8755      2400
           3     0.8432    0.8932    0.8675       843
           4     0.8757    0.9561    0.9141       774
           5     0.9217    0.9187    0.9202      1512
           6     0.6997    0.7639    0.7304      1330
           7     0.8290    0.8565    0.8425       481
           8     0.8819    0.7009    0.7810       458
           9     0.9180    0.9159    0.9169       452
          10     0.9055    0.8424    0.8728       717
          11     0.9139    0.7327    0.8133       333
          12     0.5000    0.0201    0.0386       299
          13     0.8333    0.6320    0.7188       269

    accuracy                         0.8784     16894
   macro avg     0.8462    0.7825    0.7952     16894
weighted avg     0.8745    0.8784    0.8709     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8081    0.8531    0.8300      1525
           1     0.8700    0.7500    0.8056       232
           2     0.7143    0.7571    0.7351       601
           3     0.6883    0.7536    0.7195       211
           4     0.7960    0.8247    0.8101       194
           5     0.7878    0.7857    0.7868       378
           6     0.4832    0.6036    0.5367       333
           7     0.6560    0.6777    0.6667       121
           8     0.6133    0.4000    0.4842       115
           9     0.7723    0.6842    0.7256       114
          10     0.7800    0.6500    0.7091       180
          11     0.7222    0.4643    0.5652        84
          12     0.0000    0.0000    0.0000        75
          13     0.8140    0.5147    0.6306        68

    accuracy                         0.7431      4231
   macro avg     0.6790    0.6228    0.6432      4231
weighted avg     0.7365    0.7431    0.7366      4231

---------------------------------------
program finished.
