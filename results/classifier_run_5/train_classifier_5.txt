seed:  666
save trained model at:  ../trained_models/trained_classifier_model_5.pt
save loss at:  ./results/train_classifier_results_5.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  100
learning rate decay at epoch:  50
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(80, 160)
  )
  (fc1): Linear(in_features=160, out_features=80, bias=True)
  (fc2): Linear(in_features=80, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [30, 70]
loss function:
FocalLoss(gamma=0, alpha=1, reduction=mean)
begin training...
epoch: 1, train loss: 2.274929466092379, acc: 0.33548387096774196, val loss: 2.4349263922068034, acc: 0.20896717373899118, test loss: 2.449391572877433, acc: 0.20837751855779427
epoch: 2, train loss: 2.0021481446528346, acc: 0.3967456465886383, val loss: 2.0066104903677986, acc: 0.4032559380838004, test loss: 2.0199776269724614, acc: 0.4005832449628844
epoch: 3, train loss: 1.894179120391837, acc: 0.420211247502141, val loss: 1.923356979154796, acc: 0.41793434747798236, test loss: 1.9226699904444868, acc: 0.41569459172852596
epoch: 4, train loss: 1.799272022770025, acc: 0.4595489580359692, val loss: 1.7664289193882572, acc: 0.459033893781692, test loss: 1.7567392073152932, acc: 0.46633085896076354
epoch: 5, train loss: 1.7348771833944687, acc: 0.47696260348272906, val loss: 1.680653390802954, acc: 0.48572191086202293, test loss: 1.6641450112849505, acc: 0.4899257688229056
epoch: 6, train loss: 1.6896615635964178, acc: 0.4915786468741079, val loss: 1.625810466895652, acc: 0.49719775820656525, test loss: 1.6333043153060962, acc: 0.5047720042417816
epoch: 7, train loss: 1.6338186521990246, acc: 0.5042534970025693, val loss: 1.943024637541963, acc: 0.43901788097144384, test loss: 1.9385676510387109, acc: 0.45227995758218453
epoch: 8, train loss: 1.5945644725706454, acc: 0.5181273194404796, val loss: 1.5660282686165818, acc: 0.5196156925540433, test loss: 1.5646324180469533, acc: 0.5174973488865323
epoch: 9, train loss: 1.5644882248089784, acc: 0.5242934627462176, val loss: 1.5907344758940087, acc: 0.5065385641846811, test loss: 1.5727367916896007, acc: 0.5119300106044539
epoch: 10, train loss: 1.534586891010425, acc: 0.5309734513274337, val loss: 1.5775303328422219, acc: 0.515879370162797, test loss: 1.5640169216491764, acc: 0.5185577942735949
epoch: 11, train loss: 1.518385425969733, acc: 0.537482158150157, val loss: 1.4774196002844144, acc: 0.5428342674139311, test loss: 1.4696153302460688, acc: 0.542948038176034
epoch: 12, train loss: 1.507278275816501, acc: 0.5390236939765914, val loss: 1.4916401857498776, acc: 0.5425673872431278, test loss: 1.4856049276232592, acc: 0.5424178154825027
epoch: 13, train loss: 1.4921428297983725, acc: 0.5453611190408222, val loss: 1.5113756768634106, acc: 0.5281558580197492, test loss: 1.5148026164490749, acc: 0.5270413573700954
epoch: 14, train loss: 1.4689014922201107, acc: 0.5510705109905795, val loss: 1.5297066033411446, acc: 0.5265545769949292, test loss: 1.518566558874298, acc: 0.5265111346765642
epoch: 15, train loss: 1.449206743395536, acc: 0.5551241792749072, val loss: 1.4883335400874818, acc: 0.5372297838270617, test loss: 1.474543915573638, acc: 0.545068928950159
epoch: 16, train loss: 1.4407566686647129, acc: 0.5611761347416501, val loss: 1.8057618547764847, acc: 0.44782492660795303, test loss: 1.808437995415098, acc: 0.4490986214209968
epoch: 17, train loss: 1.422370425667247, acc: 0.5614045104196403, val loss: 1.633085294758634, acc: 0.5057379236722711, test loss: 1.6318617473857147, acc: 0.5063626723223754
epoch: 18, train loss: 1.4013422520678622, acc: 0.5691692834713103, val loss: 1.4903050749213467, acc: 0.5428342674139311, test loss: 1.4719690159042214, acc: 0.5479851537645811
epoch: 19, train loss: 1.3787693631604778, acc: 0.5783614045104196, val loss: 1.435976160727471, acc: 0.5588470776621297, test loss: 1.4072326304171776, acc: 0.5771474019088016
epoch: 20, train loss: 1.3773028974767212, acc: 0.5792749072223808, val loss: 1.5606938406789275, acc: 0.5292233787029623, test loss: 1.5520917120313593, acc: 0.5347295864262991
epoch: 21, train loss: 1.35688948706154, acc: 0.5820154153582644, val loss: 1.397881014047383, acc: 0.5753936482519348, test loss: 1.393141127593444, acc: 0.5715800636267232
epoch: 22, train loss: 1.3468382339698057, acc: 0.588752497858978, val loss: 1.567266773025863, acc: 0.5148118494795837, test loss: 1.5741816765430243, acc: 0.5246553552492047
epoch: 23, train loss: 1.370002607655396, acc: 0.5821296031972595, val loss: 1.3655929443897932, acc: 0.5783293301307713, test loss: 1.3594676090576236, acc: 0.5816542948038176
epoch: 24, train loss: 1.3176519499415844, acc: 0.5952041107622038, val loss: 1.3823354103611665, acc: 0.5785962103015746, test loss: 1.3624834298834694, acc: 0.5819194061505832
epoch: 25, train loss: 1.301775097050668, acc: 0.599600342563517, val loss: 1.4083193727324796, acc: 0.5684547638110489, test loss: 1.390958891440803, acc: 0.573170731707317
epoch: 26, train loss: 1.2955216629315267, acc: 0.6039965743648301, val loss: 1.3249058286953457, acc: 0.5932746196957566, test loss: 1.318839877685838, acc: 0.5949098621420997
epoch: 27, train loss: 1.2709450915511118, acc: 0.6124464744504711, val loss: 1.324292586649581, acc: 0.5948759007205765, test loss: 1.3245944536882646, acc: 0.5935843054082715
epoch: 28, train loss: 1.265569117033172, acc: 0.6125035683699686, val loss: 1.4994881806578482, acc: 0.5727248465439018, test loss: 1.4770306509347635, acc: 0.5845705196182397
epoch: 29, train loss: 1.2707373760851186, acc: 0.6110762203825293, val loss: 1.5919740420199726, acc: 0.5070723245262877, test loss: 1.5999976865560102, acc: 0.5119300106044539
epoch 30, gamma increased to 1.
epoch: 30, train loss: 1.03129879311429, acc: 0.6168998001712818, val loss: 1.2488335478232644, acc: 0.5580464371497198, test loss: 1.2379346530991167, acc: 0.5729056203605515
epoch: 31, train loss: 1.0223289273923852, acc: 0.6166714244932915, val loss: 1.094033065591776, acc: 0.5866026154256738, test loss: 1.087133302921202, acc: 0.5874867444326617
epoch: 32, train loss: 0.9850669443454193, acc: 0.6314016557236655, val loss: 1.1087242220763305, acc: 0.5954096610621831, test loss: 1.0977115800499537, acc: 0.5972958642629904
epoch: 33, train loss: 0.984422401765262, acc: 0.6269483300028547, val loss: 1.1778279125833626, acc: 0.5668534827862289, test loss: 1.179745261009273, acc: 0.5617709437963945
epoch: 34, train loss: 0.9991826321375632, acc: 0.6266628604053668, val loss: 1.2045904591079073, acc: 0.5708566853482786, test loss: 1.189833007207731, acc: 0.573170731707317
epoch: 35, train loss: 0.9844985088858303, acc: 0.6288324293462746, val loss: 1.1969492881495951, acc: 0.563384040565786, test loss: 1.2043656976705897, acc: 0.5511664899257688
epoch: 36, train loss: 0.977394797224812, acc: 0.6328290037111047, val loss: 1.321945508893789, acc: 0.5201494528956498, test loss: 1.3197342598552035, acc: 0.5193531283138918
epoch: 37, train loss: 0.955957584601622, acc: 0.6413359977162432, val loss: 1.2222971778759677, acc: 0.563384040565786, test loss: 1.2087506964755337, acc: 0.5718451749734889
epoch: 38, train loss: 0.9559436040710049, acc: 0.6384813017413645, val loss: 1.0501171135730607, acc: 0.5999466239658393, test loss: 1.0611083183915593, acc: 0.5980911983032874
epoch: 39, train loss: 0.9439203219348419, acc: 0.6428204396231801, val loss: 1.0259700043538045, acc: 0.6159594342140379, test loss: 1.0181045772414952, acc: 0.6296394485683987
epoch: 40, train loss: 0.9281688516255008, acc: 0.6443619754496146, val loss: 1.0185315730510027, acc: 0.6204963971176941, test loss: 0.9982619788714963, acc: 0.6314952279957582
epoch: 41, train loss: 0.9344607207219056, acc: 0.644304881530117, val loss: 1.031873414126402, acc: 0.6140912730184147, test loss: 1.0465887581429831, acc: 0.6097560975609756
epoch: 42, train loss: 0.9182214787473959, acc: 0.6504139309163575, val loss: 1.2592691311939321, acc: 0.5340272217774219, test loss: 1.2653612832628656, acc: 0.5490455991516436
epoch: 43, train loss: 0.9027755972081989, acc: 0.6532115329717385, val loss: 1.177940337410411, acc: 0.5692554043234588, test loss: 1.194334056192845, acc: 0.560710498409332
epoch: 44, train loss: 0.9005649258366932, acc: 0.658235797887525, val loss: 1.0347293620749667, acc: 0.6106218307979717, test loss: 1.0333840366638605, acc: 0.623541887592789
epoch: 45, train loss: 0.9059122309492821, acc: 0.6600628033114473, val loss: 1.3359213755994217, acc: 0.5302908993861756, test loss: 1.3243734348616696, acc: 0.5432131495227995
epoch: 46, train loss: 0.9065602533823417, acc: 0.6582928918070226, val loss: 1.0082949146385793, acc: 0.6234320789965305, test loss: 1.005723429636384, acc: 0.6238069989395546
epoch: 47, train loss: 0.8946890071702283, acc: 0.6582928918070226, val loss: 1.0173411193070871, acc: 0.6285028022417934, test loss: 1.0076886568443646, acc: 0.6322905620360552
epoch: 48, train loss: 0.8812583927763825, acc: 0.6665144162146731, val loss: 1.0263158239171701, acc: 0.6327728849746463, test loss: 1.0221070049929089, acc: 0.6299045599151644
epoch: 49, train loss: 0.864197953421832, acc: 0.6641735655152726, val loss: 1.034683672497996, acc: 0.6082199092607419, test loss: 1.0426675615988252, acc: 0.6147932131495228
epoch: 50, train loss: 0.7891970299156672, acc: 0.6904367684841565, val loss: 0.9510510681087188, acc: 0.652255137443288, test loss: 0.9414858443865973, acc: 0.6601272534464475
epoch: 51, train loss: 0.751640306940223, acc: 0.7025977733371396, val loss: 0.9427191985903787, acc: 0.6498532159060582, test loss: 0.9263875173485797, acc: 0.6617179215270413
epoch: 52, train loss: 0.7483551939385229, acc: 0.7067085355409649, val loss: 0.893779011576978, acc: 0.6690685882038965, test loss: 0.8757526475071022, acc: 0.676829268292683
epoch: 53, train loss: 0.7326541618248615, acc: 0.7099628889523266, val loss: 0.9403085110503767, acc: 0.6637309847878302, test loss: 0.9282678500971384, acc: 0.6755037115588547
epoch: 54, train loss: 0.7191598067309494, acc: 0.7163574079360548, val loss: 0.9334607241406325, acc: 0.6605284227381906, test loss: 0.928773103564314, acc: 0.6678154825026511
epoch: 55, train loss: 0.724291726350444, acc: 0.7152726234656009, val loss: 0.9370442785070138, acc: 0.6706698692287163, test loss: 0.9305625371437437, acc: 0.6638388123011665
epoch: 56, train loss: 0.7073188747333317, acc: 0.7196117613474164, val loss: 0.9955181933607392, acc: 0.6447824926607953, test loss: 1.0057408164945665, acc: 0.6495227995758218
epoch: 57, train loss: 0.7117291614698268, acc: 0.7156722809020839, val loss: 0.951036376892041, acc: 0.6525220176140912, test loss: 0.9461476775014641, acc: 0.6667550371155886
epoch: 58, train loss: 0.6979852729343259, acc: 0.7205823579788753, val loss: 0.9620462284744788, acc: 0.6562583400053376, test loss: 0.9385794612281775, acc: 0.6646341463414634
epoch: 59, train loss: 0.688403293972794, acc: 0.7264630316871253, val loss: 1.0872863560063952, acc: 0.614358153189218, test loss: 1.1018624798743368, acc: 0.6084305408271474
epoch: 60, train loss: 0.6918762031773517, acc: 0.7202968883813874, val loss: 0.9023800445360154, acc: 0.6741393114491593, test loss: 0.8988803957578106, acc: 0.6744432661717922
epoch: 61, train loss: 0.6872566370905517, acc: 0.7234370539537539, val loss: 1.0831686461611305, acc: 0.6277021617293835, test loss: 1.091735562742199, acc: 0.6291092258748674
epoch: 62, train loss: 0.6696447727886432, acc: 0.7313160148444191, val loss: 1.0983999456314522, acc: 0.6261008807045636, test loss: 1.0836276310246165, acc: 0.6272534464475079
epoch: 63, train loss: 0.6608319762601806, acc: 0.7348558378532686, val loss: 0.904873853369907, acc: 0.671203629570323, test loss: 0.9025318415268607, acc: 0.6818663838812301
epoch: 64, train loss: 0.6896677509771222, acc: 0.7224093634027976, val loss: 0.8801779816365096, acc: 0.6786762743528156, test loss: 0.8780412057045283, acc: 0.689289501590668
epoch: 65, train loss: 0.6467897823528668, acc: 0.7394804453325721, val loss: 0.9111288651995001, acc: 0.6674673071790765, test loss: 0.9002611960737728, acc: 0.679745493107105
epoch: 66, train loss: 0.6303854168312161, acc: 0.7462746217527834, val loss: 0.953990703587345, acc: 0.6586602615425674, test loss: 0.9650051148799924, acc: 0.6601272534464475
epoch: 67, train loss: 0.6233877390886831, acc: 0.7476448758207251, val loss: 0.9174392057922257, acc: 0.6672004270082733, test loss: 0.9176825559278308, acc: 0.672322375397667
epoch: 68, train loss: 0.6250500963461116, acc: 0.7473594062232373, val loss: 0.9369173256721629, acc: 0.6733386709367494, test loss: 0.9305926668808291, acc: 0.6792152704135737
epoch: 69, train loss: 0.621436908358749, acc: 0.7466742791892663, val loss: 0.9087583302878876, acc: 0.6802775553776355, test loss: 0.8887620573964125, acc: 0.6898197242841994
epoch 70, gamma increased to 2.
epoch: 70, train loss: 0.5014309145112463, acc: 0.7485012846131887, val loss: 0.8086487379015558, acc: 0.6661329063250601, test loss: 0.8133655390835516, acc: 0.6810710498409331
epoch: 71, train loss: 0.4799942831652116, acc: 0.7538681130459606, val loss: 0.8058715822984927, acc: 0.6677341873498799, test loss: 0.7973271151994723, acc: 0.6694061505832449
epoch: 72, train loss: 0.4808180385339679, acc: 0.7528975164145019, val loss: 0.8133652946425337, acc: 0.6666666666666666, test loss: 0.8088223173155638, acc: 0.6842523860021209
epoch: 73, train loss: 0.4807038992022297, acc: 0.7551241792749073, val loss: 0.8350594678114724, acc: 0.6720042700827329, test loss: 0.8411749756854513, acc: 0.6784199363732768
epoch: 74, train loss: 0.465405944017829, acc: 0.7578075934912932, val loss: 0.8103114338852674, acc: 0.6800106752068321, test loss: 0.8272305438778939, acc: 0.6879639448568399
epoch: 75, train loss: 0.46483933336048033, acc: 0.7547245218384242, val loss: 0.8481881906739992, acc: 0.6538564184681078, test loss: 0.8296785728802484, acc: 0.6641039236479321
epoch: 76, train loss: 0.4793930340163748, acc: 0.7519269197830432, val loss: 0.8316548144241508, acc: 0.6594609020549773, test loss: 0.8227397988482219, acc: 0.675238600212089
epoch: 77, train loss: 0.45022393562029406, acc: 0.7600913502711961, val loss: 0.849948208332698, acc: 0.6637309847878302, test loss: 0.8325948452266884, acc: 0.6672852598091198
epoch: 78, train loss: 0.45516850856280894, acc: 0.7575792178133028, val loss: 0.805192628987223, acc: 0.685081398452095, test loss: 0.7940690689349857, acc: 0.6898197242841994
epoch: 79, train loss: 0.44960522660520325, acc: 0.7619183556951185, val loss: 0.8999398649931909, acc: 0.6535895382973045, test loss: 0.9291018155321463, acc: 0.6426299045599152
epoch: 80, train loss: 0.48191776720415075, acc: 0.7474165001427348, val loss: 0.8060217077394215, acc: 0.6709367493995196, test loss: 0.8116420255865298, acc: 0.6678154825026511
epoch: 81, train loss: 0.4548078024179773, acc: 0.7572366542963175, val loss: 0.7822049157219249, acc: 0.671203629570323, test loss: 0.7756918465218893, acc: 0.6800106044538706
epoch: 82, train loss: 0.4492694707707, acc: 0.7604910077076791, val loss: 0.9559488119619638, acc: 0.6354416866826795, test loss: 0.9389842110246515, acc: 0.6447507953340403
epoch: 83, train loss: 0.42770079951472806, acc: 0.769340565229803, val loss: 0.8472396419434538, acc: 0.6688017080330931, test loss: 0.8444273914284569, acc: 0.6853128313891834
epoch: 84, train loss: 0.4517665107625231, acc: 0.7577504995717956, val loss: 0.8450886807124838, acc: 0.6666666666666666, test loss: 0.8311097174408832, acc: 0.6810710498409331
epoch: 85, train loss: 0.4392819572645291, acc: 0.7603197259491864, val loss: 0.8443617096639551, acc: 0.6658660261542567, test loss: 0.8247355357966013, acc: 0.6755037115588547
epoch: 86, train loss: 0.42811200691909745, acc: 0.7660862118184414, val loss: 0.8698179674110382, acc: 0.6642647451294369, test loss: 0.8621285619058133, acc: 0.6715270413573701
epoch: 87, train loss: 0.4321381945253405, acc: 0.7644875820725093, val loss: 0.7930319328959669, acc: 0.6762743528155858, test loss: 0.77868992020697, acc: 0.6993637327677624
epoch: 88, train loss: 0.4265583212617939, acc: 0.7686554381958322, val loss: 0.9884100203390659, acc: 0.6335735254870564, test loss: 0.9815151011324383, acc: 0.6399787910922587
epoch: 89, train loss: 0.41569072643314603, acc: 0.7699685983442763, val loss: 0.8441801904709396, acc: 0.6733386709367494, test loss: 0.8325025739497875, acc: 0.6829268292682927
epoch: 90, train loss: 0.43180627669908306, acc: 0.7700256922637739, val loss: 0.8029881137703335, acc: 0.6816119562316519, test loss: 0.8145764025751885, acc: 0.6884941675503712
epoch: 91, train loss: 0.4167040253890231, acc: 0.7707108192977448, val loss: 0.9959398365860658, acc: 0.6413130504403523, test loss: 0.9540443235844343, acc: 0.6529692470837752
epoch: 92, train loss: 0.42839626477780834, acc: 0.764658863831002, val loss: 0.8320742721776502, acc: 0.6786762743528156, test loss: 0.818910297381663, acc: 0.6879639448568399
epoch: 93, train loss: 0.42370287562791464, acc: 0.7657436483014559, val loss: 0.7895234225430678, acc: 0.6810781958900454, test loss: 0.781970311576463, acc: 0.6869034994697774
epoch: 94, train loss: 0.417868239240513, acc: 0.7679703111618612, val loss: 0.8459950654640941, acc: 0.6631972244462236, test loss: 0.8408240571633881, acc: 0.6725874867444327
epoch: 95, train loss: 0.4129202418656748, acc: 0.7700256922637739, val loss: 0.8683311369758242, acc: 0.678142514011209, test loss: 0.8891355432609486, acc: 0.6853128313891834
epoch: 96, train loss: 0.3872664673070447, acc: 0.7784755923494148, val loss: 0.8325200964681047, acc: 0.6760074726447824, test loss: 0.8219719566444325, acc: 0.6879639448568399
epoch: 97, train loss: 0.40699298949095986, acc: 0.7751641450185556, val loss: 1.056179148692082, acc: 0.6202295169468909, test loss: 1.051953851481384, acc: 0.6296394485683987
epoch: 98, train loss: 0.3845552863817435, acc: 0.7784755923494148, val loss: 1.0330595270232705, acc: 0.6530557779556979, test loss: 1.0027495282578696, acc: 0.6561505832449629
epoch: 99, train loss: 0.4088223599465343, acc: 0.7725378247216671, val loss: 0.8815015008807787, acc: 0.6599946623965839, test loss: 0.8526697138587086, acc: 0.6702014846235419
epoch: 100, train loss: 0.3907024551690254, acc: 0.780588067370825, val loss: 0.8704874900490054, acc: 0.6672004270082733, test loss: 0.8570831542556132, acc: 0.6694061505832449
best val loss 0.7822049157219249 at epoch 81.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9271    0.9121    0.9195      5337
           1     0.6782    0.7446    0.7098      2502
           2     0.9010    0.8543    0.8771       810
           3     0.7773    0.7647    0.7710      1840
           4     0.8746    0.8141    0.8433       737
           5     0.8495    0.9675    0.9047       677
           6     0.7521    0.8692    0.8065      1323
           7     0.4503    0.7144    0.5524       907
           8     0.8564    0.7363    0.7918       421
           9     0.6798    0.9052    0.7765       401
          10     0.8412    0.9495    0.8921       396
          11     0.9607    0.9578    0.9593       332
          12     0.5851    0.5593    0.5719       295
          13     0.8488    0.7526    0.7978       291
          14     0.0000    0.0000    0.0000       261
          15     0.9192    0.1842    0.3069       494
          16     1.0000    0.0078    0.0155       256
          17     0.6432    0.5447    0.5899       235

    accuracy                         0.7910     17515
   macro avg     0.7525    0.6799    0.6714     17515
weighted avg     0.7990    0.7910    0.7780     17515

train confusion matrix:
[[4868  115    4   36    1    0    3   90   48   96   16    2    4   15
     0    0    0   39]
 [  87 1863    9  111   20    3  242  118    0   24    1    2   14    3
     0    1    0    4]
 [   0    2  692    0    1   80    5    3    0   19    0    0    8    0
     0    0    0    0]
 [  64  269    3 1407   10    0    3   70    0    3    0    2    1    0
     0    6    0    2]
 [   1   47    4   12  600    0    1   42    0    0    3    4   22    0
     0    0    0    1]
 [   0    0   14    0    0  655    8    0    0    0    0    0    0    0
     0    0    0    0]
 [   5  100   11    1    9   22 1150    6    1   11    0    0    6    1
     0    0    0    0]
 [  31   81    4    7   24    0    6  648    1    0   24    0   55    6
     0    0    0   20]
 [  88    8    1    1    0    0    0    2  310    0    1    1    0    9
     0    0    0    0]
 [  14   10    8    0    0    1    1    1    0  363    0    0    2    1
     0    0    0    0]
 [   3    0    0    0    1    0    0   13    0    0  376    0    1    2
     0    0    0    0]
 [   0    2    3    4    0    1    0    0    0    3    0  318    0    0
     0    1    0    0]
 [   3    7    8    0   12    0    1   95    0    0    1    0  165    1
     0    0    0    2]
 [  29    7    4    0    2    0    4    1    2   14    4    1    4  219
     0    0    0    0]
 [  28  142    0   37    1    0    0   47    0    0    4    0    0    1
     0    0    0    1]
 [   7   86    1  192    2    9  104    1    0    0    0    1    0    0
     0   91    0    0]
 [   9    5    0    2    3    0    1  215    0    1   16    0    0    0
     0    0    2    2]
 [  14    3    2    0    0    0    0   87    0    0    1    0    0    0
     0    0    0  128]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8467    0.8215    0.8339      1143
           1     0.5479    0.5970    0.5714       536
           2     0.7368    0.7283    0.7326       173
           3     0.6701    0.6599    0.6650       394
           4     0.7895    0.6646    0.7216       158
           5     0.7744    0.8759    0.8220       145
           6     0.6586    0.7703    0.7101       283
           7     0.2986    0.5464    0.3862       194
           8     0.7000    0.5444    0.6125        90
           9     0.4948    0.5647    0.5275        85
          10     0.7184    0.8810    0.7914        84
          11     0.9355    0.8169    0.8722        71
          12     0.3279    0.3175    0.3226        63
          13     0.5441    0.5968    0.5692        62
          14     0.0000    0.0000    0.0000        56
          15     0.7647    0.1238    0.2131       105
          16     0.0000    0.0000    0.0000        55
          17     0.4412    0.3000    0.3571        50

    accuracy                         0.6712      3747
   macro avg     0.5694    0.5449    0.5394      3747
weighted avg     0.6733    0.6712    0.6623      3747

validation confusion matrix:
[[939  40   7  27   2   2   5  49  17  22   5   2   6  10   0   0   0  10]
 [ 34 320  13  39   7   1  43  56   2   6   0   0   9   2   0   1   0   3]
 [  4   4 126   2   1  23   3   2   0   4   0   0   3   1   0   0   0   0]
 [ 21  64   1 260   1   1   3  27   0   1   5   0   2   3   0   3   0   2]
 [  2  24   3   4 105   0   3   9   0   0   2   0   4   2   0   0   0   0]
 [  0   2   6   1   0 127   8   0   0   0   0   0   0   1   0   0   0   0]
 [  7  28   7   2   4   7 218   3   1   3   0   0   1   2   0   0   0   0]
 [ 18  18   2  10   5   0   6 106   0   2  10   0  12   1   0   0   0   4]
 [ 26   3   1   1   1   0   0   1  49   1   1   0   2   4   0   0   0   0]
 [ 12  10   4   0   0   0   5   1   0  48   0   1   0   4   0   0   0   0]
 [  6   0   0   0   0   0   0   4   0   0  74   0   0   0   0   0   0   0]
 [  0   5   0   4   0   1   0   1   0   1   0  58   0   1   0   0   0   0]
 [  4   8   0   1   2   0   3  25   0   0   0   0  20   0   0   0   0   0]
 [ 10   2   0   0   1   0   1   1   0   7   3   0   0  37   0   0   0   0]
 [  8  21   0   6   0   0   1  17   0   1   1   0   1   0   0   0   0   0]
 [  6  19   0  29   1   2  31   2   0   1   0   1   0   0   0  13   0   0]
 [  5  16   1   1   3   0   0  25   1   0   2   0   1   0   0   0   0   0]
 [  7   0   0   1   0   0   1  26   0   0   0   0   0   0   0   0   0  15]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8487    0.8183    0.8333      1145
           1     0.5421    0.6350    0.5849       537
           2     0.8024    0.7657    0.7836       175
           3     0.7025    0.6456    0.6728       395
           4     0.8030    0.6667    0.7285       159
           5     0.7711    0.8767    0.8205       146
           6     0.6740    0.7570    0.7131       284
           7     0.3105    0.5590    0.3993       195
           8     0.8171    0.7363    0.7746        91
           9     0.4746    0.6437    0.5463        87
          10     0.6733    0.7907    0.7273        86
          11     0.8939    0.8194    0.8551        72
          12     0.3387    0.3281    0.3333        64
          13     0.6102    0.5625    0.5854        64
          14     0.0000    0.0000    0.0000        57
          15     0.6875    0.1028    0.1789       107
          16     0.0000    0.0000    0.0000        56
          17     0.5946    0.4231    0.4944        52

    accuracy                         0.6800      3772
   macro avg     0.5858    0.5628    0.5573      3772
weighted avg     0.6830    0.6800    0.6715      3772

test confusion matrix:
[[937  53   9  10   0   1   2  48  11  36   8   1   8  14   0   0   0   7]
 [ 38 341   3  37   5   3  55  31   0   8   1   2   9   2   0   1   0   1]
 [  1   3 134   1   0  21   3   3   0   5   1   0   2   1   0   0   0   0]
 [ 25  64   1 255   2   0   0  35   0   2   1   2   1   0   0   4   0   3]
 [  1  22   1   3 106   0   1  15   0   0   0   1   5   3   0   0   0   1]
 [  0   5   3   1   0 128   8   0   0   1   0   0   0   0   0   0   0   0]
 [ 11  25   6   3   2  10 215   2   1   6   0   0   1   2   0   0   0   0]
 [ 16  29   2   6   6   0   7 109   1   1  10   0   6   0   0   0   0   2]
 [ 16   1   1   1   0   0   0   2  67   0   1   1   0   1   0   0   0   0]
 [ 12   8   5   0   0   0   1   3   0  56   0   0   2   0   0   0   0   0]
 [  5   2   0   0   0   0   0  10   0   0  68   0   0   0   0   0   0   1]
 [  6   1   0   3   1   0   0   0   0   2   0  59   0   0   0   0   0   0]
 [  3   6   1   0   6   0   1  25   0   0   1   0  21   0   0   0   0   0]
 [ 15   6   0   1   0   0   2   0   0   1   1   0   2  36   0   0   0   0]
 [  6  28   0   9   2   1   0   6   1   0   2   0   2   0   0   0   0   0]
 [  4  30   0  29   2   2  24   3   0   0   1   0   1   0   0  11   0   0]
 [  3   4   0   4   0   0   0  37   1   0   6   0   1   0   0   0   0   0]
 [  5   1   1   0   0   0   0  22   0   0   0   0   1   0   0   0   0  22]]
---------------------------------------
program finished.
