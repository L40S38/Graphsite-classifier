seed:  666
save trained model at:  ../trained_models/trained_classifier_model_9.pt
save loss at:  ./results/train_classifier_results_9.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  100
learning rate decay at epoch:  60
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(80, 160)
  )
  (fc1): Linear(in_features=160, out_features=80, bias=True)
  (fc2): Linear(in_features=80, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [20, 60, 100]
loss function:
FocalLoss(gamma=0, alpha=1, reduction=mean)
begin training...
epoch: 1, train loss: 2.284890593333548, acc: 0.33976591493006, val loss: 2.278889147924174, acc: 0.3514811849479584, test loss: 2.2882470098558185, acc: 0.3597560975609756
epoch: 2, train loss: 1.9855150254086498, acc: 0.402683414216386, val loss: 1.881449380937754, acc: 0.4278089137977048, test loss: 1.8606389486650647, acc: 0.4337221633085896
epoch: 3, train loss: 1.860209146050974, acc: 0.43608335712246643, val loss: 1.7438696907589142, acc: 0.46810781958900455, test loss: 1.7247766437105763, acc: 0.47481442205726404
epoch: 4, train loss: 1.775638832577698, acc: 0.4616614330573794, val loss: 1.710662857767802, acc: 0.48305310915398986, test loss: 1.6868711349693615, acc: 0.485949098621421
epoch: 5, train loss: 1.712293257000036, acc: 0.4817013988010277, val loss: 1.6340873599020616, acc: 0.49479583666933546, test loss: 1.6095289847757253, acc: 0.5042417815482503
epoch: 6, train loss: 1.6581867645442265, acc: 0.4954610333999429, val loss: 1.6377481211653893, acc: 0.4955964771817454, test loss: 1.6190218799061133, acc: 0.5076882290562036
epoch: 7, train loss: 1.6140227067821067, acc: 0.5068798172994576, val loss: 1.6334586248481817, acc: 0.4809180677875634, test loss: 1.6325224340910118, acc: 0.48382820784729585
epoch: 8, train loss: 1.5808335943627692, acc: 0.5178989437624893, val loss: 1.8442301410403226, acc: 0.47050974112623434, test loss: 1.826798129056443, acc: 0.47348886532343587
epoch: 9, train loss: 1.5580920037629773, acc: 0.5249214958606908, val loss: 1.5871067327723873, acc: 0.5188150520416333, test loss: 1.5588601217037295, acc: 0.5161717921527041
epoch: 10, train loss: 1.5109929470617092, acc: 0.5465029974307736, val loss: 1.6333545998379106, acc: 0.4875900720576461, test loss: 1.6212523013890612, acc: 0.4949628844114528
epoch: 11, train loss: 1.4967008297694535, acc: 0.5430773622609192, val loss: 1.7250053608993354, acc: 0.45930077395249536, test loss: 1.6933728738402511, acc: 0.4809119830328738
epoch: 12, train loss: 1.461695841377611, acc: 0.5554096488723951, val loss: 1.6201218319855468, acc: 0.496130237523352, test loss: 1.614484881040021, acc: 0.5023860021208908
epoch: 13, train loss: 1.43429576035673, acc: 0.5655723665429632, val loss: 1.4557404247067087, acc: 0.5390979450226848, test loss: 1.4492458349068116, acc: 0.5442735949098622
epoch: 14, train loss: 1.3999865367485393, acc: 0.5746502997430774, val loss: 1.5457235316069629, acc: 0.5132105684547638, test loss: 1.5243985326773035, acc: 0.5267762460233298
epoch: 15, train loss: 1.3851296417855414, acc: 0.5787610619469027, val loss: 1.3672573614794954, acc: 0.5860688550840673, test loss: 1.3587486465814131, acc: 0.5858960763520679
epoch: 16, train loss: 1.3465917272224721, acc: 0.591036254638881, val loss: 1.39576995350502, acc: 0.5724579663730984, test loss: 1.3598592228247277, acc: 0.5840402969247084
epoch: 17, train loss: 1.3255706271341179, acc: 0.5990864972880389, val loss: 1.5193915150787216, acc: 0.5348278622898318, test loss: 1.5041360089027995, acc: 0.5357900318133616
epoch: 18, train loss: 1.3241711010158386, acc: 0.5998287182415073, val loss: 1.5906811382791537, acc: 0.5020016012810248, test loss: 1.5707035739753052, acc: 0.5060975609756098
epoch: 19, train loss: 1.305826403323562, acc: 0.6067941764202113, val loss: 1.3627459975538618, acc: 0.5804643714971978, test loss: 1.3440749177497873, acc: 0.5943796394485684
epoch 20, gamma increased to 1.
epoch: 20, train loss: 1.0574992191372958, acc: 0.6168427062517842, val loss: 1.0748177398068763, acc: 0.6044835868694955, test loss: 1.0590553291276303, acc: 0.6073700954400848
epoch: 21, train loss: 1.0249893761586366, acc: 0.6215815015700827, val loss: 1.1077291527715785, acc: 0.5975447024286096, test loss: 1.1059901236476473, acc: 0.6055143160127253
epoch: 22, train loss: 1.0212336228535648, acc: 0.6263202968883814, val loss: 1.0789278691248287, acc: 0.5922070990125433, test loss: 1.0720802721264253, acc: 0.6065747613997879
epoch: 23, train loss: 1.0059063024089365, acc: 0.6264915786468741, val loss: 1.012587033242011, acc: 0.622898318654924, test loss: 1.0073096135261834, acc: 0.6288441145281018
epoch: 24, train loss: 0.9778632151682377, acc: 0.6373965172709106, val loss: 1.1383422897884552, acc: 0.5898051774753136, test loss: 1.1590531547400758, acc: 0.5927889713679746
epoch: 25, train loss: 0.9782376898039487, acc: 0.6359120753639738, val loss: 1.0699242948054122, acc: 0.5994128636242327, test loss: 1.0651498917936646, acc: 0.6126723223753977
epoch: 26, train loss: 0.9595028977920899, acc: 0.6438481301741364, val loss: 1.3448705712667872, acc: 0.5588470776621297, test loss: 1.4181920898049154, acc: 0.5601802757158006
epoch: 27, train loss: 0.9436953157620399, acc: 0.6483585498144447, val loss: 1.167815022446933, acc: 0.5700560448358687, test loss: 1.1733581382674605, acc: 0.5758218451749735
epoch: 28, train loss: 0.9438036478828302, acc: 0.6513845275478162, val loss: 1.1090545569670052, acc: 0.5975447024286096, test loss: 1.1922852373578263, acc: 0.5851007423117709
epoch: 29, train loss: 0.9212962138118521, acc: 0.6572652012560662, val loss: 1.2147704542621727, acc: 0.5775286896183613, test loss: 1.2162959836067855, acc: 0.5911983032873807
epoch: 30, train loss: 0.9117249184188385, acc: 0.6577219526120468, val loss: 1.0420509030540879, acc: 0.6114224713103816, test loss: 1.0638827539324633, acc: 0.6089607635206787
epoch: 31, train loss: 0.895807218347452, acc: 0.664116471595775, val loss: 1.0048436795293982, acc: 0.6244995996797438, test loss: 1.0253797194358527, acc: 0.6248674443266172
epoch: 32, train loss: 0.8926746311577054, acc: 0.666000570939195, val loss: 1.0135948759796207, acc: 0.6234320789965305, test loss: 1.0127096474486723, acc: 0.6306998939554613
epoch: 33, train loss: 0.8717160053298094, acc: 0.6687981729945761, val loss: 1.25279642742158, acc: 0.5217507339204697, test loss: 1.2585926870020425, acc: 0.5395015906680806
epoch: 34, train loss: 0.8750448813616736, acc: 0.670682272337996, val loss: 1.0288996524673353, acc: 0.6159594342140379, test loss: 1.06988630719554, acc: 0.6253976670201484
epoch: 35, train loss: 0.8798046419391148, acc: 0.6699971453040251, val loss: 1.261501096743789, acc: 0.5519081932212436, test loss: 1.2513035173001497, acc: 0.5527571580063627
epoch: 36, train loss: 0.9002581061140796, acc: 0.6616043391378819, val loss: 1.045304354712713, acc: 0.6242327195089404, test loss: 1.0591333655764923, acc: 0.6261930010604454
epoch: 37, train loss: 0.8502987438530504, acc: 0.6797602055381102, val loss: 1.0078936688321605, acc: 0.633306645316253, test loss: 1.0101089389048603, acc: 0.6450159066808059
epoch: 38, train loss: 0.8523481937738409, acc: 0.6799885812161005, val loss: 1.1233291222567492, acc: 0.5844675740592474, test loss: 1.1360593845836684, acc: 0.5858960763520679
epoch: 39, train loss: 0.840646446394369, acc: 0.6838138738224379, val loss: 1.0200679365844767, acc: 0.6263677608753669, test loss: 1.0290330857006895, acc: 0.6291092258748674
epoch: 40, train loss: 0.824233188814277, acc: 0.6876391664287753, val loss: 1.1106982199961832, acc: 0.6002135041366427, test loss: 1.0981355072458374, acc: 0.6047189819724285
epoch: 41, train loss: 0.8018293593190514, acc: 0.6897516414501855, val loss: 1.2277077439310458, acc: 0.5575126768081131, test loss: 1.256730692510514, acc: 0.5548780487804879
epoch: 42, train loss: 0.790596053813343, acc: 0.6980873536968313, val loss: 1.1782748363226103, acc: 0.5887376567921003, test loss: 1.1834811969627632, acc: 0.5877518557794273
epoch: 43, train loss: 0.7985999888942271, acc: 0.6947188124464745, val loss: 0.9584206933494183, acc: 0.6551908193221244, test loss: 0.9782868235639651, acc: 0.651643690349947
epoch: 44, train loss: 0.792152719384699, acc: 0.6980302597773337, val loss: 0.9473855386010672, acc: 0.6543901788097144, test loss: 0.9490379629812716, acc: 0.6550901378579003
epoch: 45, train loss: 0.7771306495955084, acc: 0.7036254638880959, val loss: 0.9846277625076543, acc: 0.6463837736856152, test loss: 1.0149782778475975, acc: 0.6529692470837752
epoch: 46, train loss: 0.7751602567606847, acc: 0.7027690550956324, val loss: 1.0294412391994487, acc: 0.633306645316253, test loss: 1.0149234729253096, acc: 0.6428950159066809
epoch: 47, train loss: 0.7825944546798349, acc: 0.7020839280616614, val loss: 0.9611869472359097, acc: 0.6485188150520417, test loss: 0.957744505600105, acc: 0.6548250265111347
epoch: 48, train loss: 0.7862934971583968, acc: 0.7003140165572367, val loss: 0.9737467277135917, acc: 0.6466506538564185, test loss: 0.9895616073244323, acc: 0.6529692470837752
epoch: 49, train loss: 0.7542515471701006, acc: 0.713788181558664, val loss: 1.488076363304694, acc: 0.5284227381905524, test loss: 1.5646740459814303, acc: 0.5198833510074231
epoch: 50, train loss: 0.7333751223205057, acc: 0.7192691978304311, val loss: 0.9291441943889177, acc: 0.6503869762476648, test loss: 0.9397828672496031, acc: 0.6553552492046659
epoch: 51, train loss: 0.7319807612280012, acc: 0.7177276620039966, val loss: 1.046844690171184, acc: 0.6469175340272217, test loss: 1.0727940364202537, acc: 0.6423647932131495
epoch: 52, train loss: 0.8210403454055318, acc: 0.6898658292891807, val loss: 0.9577272624246654, acc: 0.6453162530024019, test loss: 0.9970953335564816, acc: 0.6362672322375398
epoch: 53, train loss: 0.750574362842893, acc: 0.7149871538681131, val loss: 0.9437730472692973, acc: 0.644515612489992, test loss: 0.9626860808213721, acc: 0.6508483563096501
epoch: 54, train loss: 0.7228525779943551, acc: 0.7210962032543534, val loss: 0.9178892994105163, acc: 0.6615959434214038, test loss: 0.9364664865576703, acc: 0.6577412513255567
epoch: 55, train loss: 0.7143157391625746, acc: 0.7236083357122467, val loss: 1.0459425054234062, acc: 0.6295703229250067, test loss: 1.0169496255613713, acc: 0.6389183457051962
epoch: 56, train loss: 0.7180830247940556, acc: 0.721153297173851, val loss: 0.9972181343096939, acc: 0.6375767280491059, test loss: 1.0227496340823452, acc: 0.6328207847295865
epoch: 57, train loss: 0.7160037448873801, acc: 0.7268626891236083, val loss: 0.9751889401510425, acc: 0.6549239391513211, test loss: 0.9870659675476786, acc: 0.6508483563096501
epoch: 58, train loss: 0.7057407255176813, acc: 0.7275478161575792, val loss: 0.8862616245925411, acc: 0.6744061916199626, test loss: 0.8999961254326182, acc: 0.6744432661717922
epoch: 59, train loss: 0.6980357140566396, acc: 0.7274907222380816, val loss: 0.9038106841598157, acc: 0.6752068321323725, test loss: 0.9055961424826564, acc: 0.6739130434782609
epoch 60, gamma increased to 2.
epoch: 60, train loss: 0.49878340471379595, acc: 0.761575792178133, val loss: 0.7118530086367678, acc: 0.7042967707499332, test loss: 0.7230373672817064, acc: 0.7020148462354189
epoch: 61, train loss: 0.4550902880007221, acc: 0.7728803882386526, val loss: 0.7185462141851441, acc: 0.7029623698959168, test loss: 0.7330073407700054, acc: 0.7081124072110286
epoch: 62, train loss: 0.4359832229510124, acc: 0.7794461889808736, val loss: 0.7595797827124818, acc: 0.6904190018681612, test loss: 0.7786663132280206, acc: 0.6980381760339343
epoch: 63, train loss: 0.43496169399575374, acc: 0.7811019126463031, val loss: 0.7430312375179888, acc: 0.6861489191353083, test loss: 0.7633684901392219, acc: 0.6800106044538706
epoch: 64, train loss: 0.4300657046817896, acc: 0.7784184984299172, val loss: 0.8034027652610675, acc: 0.6866826794769149, test loss: 0.8214646890079987, acc: 0.6786850477200425
epoch: 65, train loss: 0.4296642577107076, acc: 0.7822437910362546, val loss: 0.7272820440266398, acc: 0.7045636509207366, test loss: 0.7455651939610535, acc: 0.7036055143160127
epoch: 66, train loss: 0.4290255877721864, acc: 0.7814444761632886, val loss: 0.752595136793639, acc: 0.6989591673338671, test loss: 0.7432991319344886, acc: 0.7044008483563097
epoch: 67, train loss: 0.4144657556904475, acc: 0.7796745646588639, val loss: 0.7567424223777419, acc: 0.6877502001601281, test loss: 0.7359810577091205, acc: 0.7009544008483564
epoch: 68, train loss: 0.4019012959963793, acc: 0.7912075363973737, val loss: 0.722529746419053, acc: 0.7024286095543101, test loss: 0.7475073088517498, acc: 0.7110286320254506
epoch: 69, train loss: 0.43721020851446973, acc: 0.7737938909506138, val loss: 0.8567211414802415, acc: 0.6677341873498799, test loss: 0.8693393484026605, acc: 0.676033934252386
epoch: 70, train loss: 0.40133129292102054, acc: 0.7889237796174707, val loss: 0.8172595984527897, acc: 0.6832132372564719, test loss: 0.7978170342814506, acc: 0.693001060445387
epoch: 71, train loss: 0.40401770832570594, acc: 0.7881815586640023, val loss: 0.7592630357083111, acc: 0.6984254069922605, test loss: 0.7822740227142043, acc: 0.6956521739130435
epoch: 72, train loss: 0.41744352794190387, acc: 0.7803596916928347, val loss: 0.8179703715135805, acc: 0.677341873498799, test loss: 0.8534072072483196, acc: 0.6866383881230117
epoch: 73, train loss: 0.41843762170951976, acc: 0.7775049957179561, val loss: 0.7672682493758068, acc: 0.6941553242594075, test loss: 0.8059414116979783, acc: 0.6842523860021209
epoch: 74, train loss: 0.40428456550082786, acc: 0.7885241221809878, val loss: 0.7836318002690116, acc: 0.6861489191353083, test loss: 0.8027913250574251, acc: 0.6879639448568399
epoch: 75, train loss: 0.4001764349756396, acc: 0.7858407079646018, val loss: 0.7968669800176791, acc: 0.6893514811849479, test loss: 0.8145595949502664, acc: 0.6898197242841994
epoch: 76, train loss: 0.3970515820168782, acc: 0.7905224093634028, val loss: 0.7663064306501964, acc: 0.6976247664798505, test loss: 0.7860360698002141, acc: 0.693796394485684
epoch: 77, train loss: 0.4337949590022109, acc: 0.7741364544675992, val loss: 0.8103424890027989, acc: 0.678142514011209, test loss: 0.8408602320629618, acc: 0.679745493107105
epoch: 78, train loss: 0.3772346007783107, acc: 0.7966885526691407, val loss: 0.8207109151345938, acc: 0.6802775553776355, test loss: 0.8476230481270136, acc: 0.6839872746553552
epoch: 79, train loss: 0.3899439908319496, acc: 0.7893234370539538, val loss: 0.7721154815138325, acc: 0.6984254069922605, test loss: 0.8029486889045286, acc: 0.694591728525981
epoch: 80, train loss: 0.3703054918116105, acc: 0.7978875249785898, val loss: 0.7663970682059157, acc: 0.7042967707499332, test loss: 0.7827794881792361, acc: 0.704135737009544
epoch: 81, train loss: 0.35114797781339346, acc: 0.8023408506994005, val loss: 0.8067560411943956, acc: 0.692020282892981, test loss: 0.8264154987143054, acc: 0.693001060445387
epoch: 82, train loss: 0.3748922053470361, acc: 0.7958892377961747, val loss: 0.7672312187136794, acc: 0.6952228449426208, test loss: 0.7599573546224535, acc: 0.7104984093319194
epoch: 83, train loss: 0.3613136972405725, acc: 0.8006280331144733, val loss: 0.8023033569041716, acc: 0.6936215639178009, test loss: 0.8435002465516107, acc: 0.6956521739130435
epoch: 84, train loss: 0.34294739671853347, acc: 0.8029688838138738, val loss: 0.7899372404595837, acc: 0.688550840672538, test loss: 0.851183359595144, acc: 0.693796394485684
epoch: 85, train loss: 0.34877891432820407, acc: 0.8031401655723666, val loss: 0.8071276592451125, acc: 0.7013610888710968, test loss: 0.8685540202313744, acc: 0.6943266171792153
epoch: 86, train loss: 0.3362659139239104, acc: 0.8046246074793034, val loss: 0.8282944878355993, acc: 0.6832132372564719, test loss: 0.8426045260272628, acc: 0.6932661717921527
epoch: 87, train loss: 0.36436518884167957, acc: 0.7956608621181844, val loss: 0.760771523643119, acc: 0.6965572457966374, test loss: 0.8070108545420658, acc: 0.6980381760339343
epoch: 88, train loss: 0.35864820803807795, acc: 0.7974878675421068, val loss: 0.842927041866953, acc: 0.6890846010141447, test loss: 0.8481648638291425, acc: 0.6948568398727466
epoch: 89, train loss: 0.36268590241202686, acc: 0.8000570939194975, val loss: 0.8807082732963918, acc: 0.6874833199893248, test loss: 0.8816187705366634, acc: 0.6866383881230117
epoch: 90, train loss: 0.33296273873936544, acc: 0.8088495575221238, val loss: 0.8835568326869137, acc: 0.6770749933279957, test loss: 0.9208173253756187, acc: 0.6728525980911984
epoch: 91, train loss: 0.34053617576820455, acc: 0.805366828432772, val loss: 0.8434203161632089, acc: 0.6882839605017347, test loss: 0.86961686699666, acc: 0.6916755037115588
epoch: 92, train loss: 0.3265763126183945, acc: 0.8079360548101627, val loss: 0.8253724512686117, acc: 0.6914865225513744, test loss: 0.8673341552374345, acc: 0.6903499469777307
epoch: 93, train loss: 0.3441506575370835, acc: 0.8063945189837283, val loss: 0.8324555254377045, acc: 0.6872164398185214, test loss: 0.9009378462050146, acc: 0.6922057264050901
epoch: 94, train loss: 0.34642051566235854, acc: 0.8046246074793034, val loss: 0.9076697006841516, acc: 0.6941553242594075, test loss: 0.9389540395706355, acc: 0.6985683987274656
epoch: 95, train loss: 0.3276672137862371, acc: 0.8103910933485584, val loss: 0.8714460323803069, acc: 0.6853482786228983, test loss: 0.8958714008331299, acc: 0.6784199363732768
epoch: 96, train loss: 0.32779381933499496, acc: 0.8122751926919783, val loss: 0.8472076475954323, acc: 0.6842807579396851, test loss: 0.881508974735709, acc: 0.6882290562036055
epoch: 97, train loss: 0.32581021399828763, acc: 0.8091921210391093, val loss: 0.8793957342234872, acc: 0.6904190018681612, test loss: 0.9321184648815167, acc: 0.6876988335100742
epoch: 98, train loss: 0.3210090168775438, acc: 0.8122751926919783, val loss: 0.851938037577075, acc: 0.685882038964505, test loss: 0.8594882196232219, acc: 0.6935312831389183
epoch: 99, train loss: 0.30635476244234405, acc: 0.8190693691121895, val loss: 0.8608611631558869, acc: 0.688550840672538, test loss: 0.8850921530233081, acc: 0.6983032873806999
epoch 100, gamma increased to 3.
epoch: 100, train loss: 0.27848064363292035, acc: 0.8019411932629175, val loss: 0.8683548251374231, acc: 0.6493194555644516, test loss: 0.9012760486481425, acc: 0.6481972428419936
best val loss 0.7118530086367678 at epoch 60.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.8617    0.9470    0.9023      5337
           1     0.6659    0.6930    0.6792      2502
           2     0.9176    0.8383    0.8761       810
           3     0.7269    0.8565    0.7864      1840
           4     0.8479    0.8019    0.8243       737
           5     0.8757    0.8848    0.8802       677
           6     0.7426    0.8503    0.7928      1323
           7     0.5106    0.5579    0.5332       907
           8     0.8299    0.7648    0.7960       421
           9     0.8311    0.7731    0.8010       401
          10     0.8865    0.9268    0.9062       396
          11     0.9901    0.9006    0.9432       332
          12     0.7017    0.4305    0.5336       295
          13     0.7982    0.6117    0.6926       291
          14     0.0000    0.0000    0.0000       261
          15     0.9507    0.2733    0.4245       494
          16     1.0000    0.0547    0.1037       256
          17     0.7103    0.6468    0.6771       235

    accuracy                         0.7861     17515
   macro avg     0.7693    0.6562    0.6751     17515
weighted avg     0.7824    0.7861    0.7702     17515

train confusion matrix:
[[5054   89    1   44    2    0    2   28   55   21    5    0    0   20
     0    0    0   16]
 [ 203 1734    3  208   39    2  229   56    2   10    0    0    7    4
     0    3    0    2]
 [  11   15  679    0    2   56   25    0    1    9    0    0   10    2
     0    0    0    0]
 [  90  131    0 1576    1    1    4   28    0    1    1    1    0    0
     0    3    0    3]
 [   3   70    0   18  591    0    4   34    1    0    0    0   15    1
     0    0    0    0]
 [   2    1   46    1    0  599   27    0    0    0    0    0    0    1
     0    0    0    0]
 [  21  133    3    0    4   19 1125    1    0   10    0    0    4    3
     0    0    0    0]
 [ 115  141    0   43   21    0   16  506    3    1   23    0    9    5
     0    0    0   24]
 [  84    6    0    2    1    0    0    3  322    0    0    0    0    3
     0    0    0    0]
 [  65   21    0    1    0    0    1    1    1  310    0    1    0    0
     0    0    0    0]
 [   8    3    0    0    1    0    0   11    0    0  367    0    3    2
     0    0    0    1]
 [   8    6    1   10    1    0    0    1    0    5    0  299    0    1
     0    0    0    0]
 [  14   27    4    1   22    0    1   87    1    0    3    0  127    3
     0    0    0    5]
 [  82   10    0    2    8    0    3    1    2    3    0    0    2  178
     0    0    0    0]
 [  44  127    0   66    1    0    0   14    0    1    4    0    3    0
     0    1    0    0]
 [  19   75    3  175    3    6   75    0    0    2    0    1    0    0
     0  135    0    0]
 [  24   11    0   15    0    0    3  166    0    0   11    0    1    0
     0    0   14   11]
 [  18    4    0    6    0    1    0   54    0    0    0    0    0    0
     0    0    0  152]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.7953    0.8871    0.8387      1143
           1     0.5682    0.5672    0.5677       536
           2     0.8446    0.7225    0.7788       173
           3     0.6310    0.8071    0.7082       394
           4     0.7986    0.7025    0.7475       158
           5     0.8162    0.7655    0.7900       145
           6     0.6788    0.7915    0.7308       283
           7     0.3850    0.4227    0.4029       194
           8     0.7561    0.6889    0.7209        90
           9     0.6575    0.5647    0.6076        85
          10     0.8022    0.8690    0.8343        84
          11     0.9483    0.7746    0.8527        71
          12     0.5758    0.3016    0.3958        63
          13     0.7500    0.6774    0.7119        62
          14     0.0000    0.0000    0.0000        56
          15     0.8214    0.2190    0.3459       105
          16     1.0000    0.0545    0.1034        55
          17     0.5814    0.5000    0.5376        50

    accuracy                         0.7043      3747
   macro avg     0.6895    0.5731    0.5931      3747
weighted avg     0.7023    0.7043    0.6884      3747

validation confusion matrix:
[[1014   40    1   29    1    1    1   13   12   10    6    1    2    8
     0    0    0    4]
 [  75  304    6   65    9    0   45   21    1    4    0    0    1    1
     0    0    0    4]
 [   7    8  125    1    1   18    8    1    0    2    0    1    1    0
     0    0    0    0]
 [  28   28    0  318    1    0    3    9    0    0    1    0    1    0
     0    4    0    1]
 [   3   23    0    7  111    1    2    7    0    0    0    0    2    2
     0    0    0    0]
 [   1    2   13    1    0  111   16    0    0    0    0    0    0    1
     0    0    0    0]
 [   9   30    2    2    2    4  224    2    0    4    0    1    2    1
     0    0    0    0]
 [  33   29    1   15    9    0    6   82    2    0    9    0    2    0
     0    0    0    6]
 [  18    1    0    3    0    0    0    4   62    0    0    0    1    1
     0    0    0    0]
 [  24    9    0    0    1    0    1    1    0   48    0    0    0    0
     0    0    0    1]
 [   3    1    0    0    0    0    0    7    0    0   73    0    0    0
     0    0    0    0]
 [   6    4    0    1    0    0    0    0    1    4    0   55    0    0
     0    0    0    0]
 [   6   14    0    2    2    0    0   18    0    0    0    0   19    0
     0    0    0    2]
 [  16    2    0    0    0    0    1    1    0    0    0    0    0   42
     0    0    0    0]
 [  15   14    0   17    0    0    0    7    1    0    1    0    0    0
     0    1    0    0]
 [   7   14    0   36    0    1   22    1    0    1    0    0    0    0
     0   23    0    0]
 [   3   12    0    7    1    0    1   23    2    0    1    0    2    0
     0    0    3    0]
 [   7    0    0    0    1    0    0   16    1    0    0    0    0    0
     0    0    0   25]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.7810    0.8969    0.8350      1145
           1     0.5565    0.5959    0.5755       537
           2     0.8497    0.7429    0.7927       175
           3     0.6476    0.7722    0.7044       395
           4     0.7986    0.6981    0.7450       159
           5     0.8169    0.7945    0.8056       146
           6     0.6975    0.7711    0.7324       284
           7     0.4338    0.4872    0.4589       195
           8     0.7200    0.7912    0.7539        91
           9     0.6290    0.4483    0.5235        87
          10     0.8046    0.8140    0.8092        86
          11     0.9643    0.7500    0.8437        72
          12     0.5455    0.2812    0.3711        64
          13     0.7368    0.4375    0.5490        64
          14     0.0000    0.0000    0.0000        57
          15     0.7778    0.1963    0.3134       107
          16     1.0000    0.0357    0.0690        56
          17     0.5385    0.4038    0.4615        52

    accuracy                         0.7020      3772
   macro avg     0.6832    0.5509    0.5747      3772
weighted avg     0.6985    0.7020    0.6841      3772

test confusion matrix:
[[1027   41    0   20    2    0    0   17   19    7    3    0    0    5
     0    0    0    4]
 [  67  320    1   64    7    3   48   17    1    5    0    1    0    1
     0    0    0    2]
 [  10    5  130    1    1   14    7    0    0    2    0    0    3    1
     0    1    0    0]
 [  36   32    2  305    0    0    1    9    0    0    1    0    1    1
     0    5    0    2]
 [   5   19    0    6  111    0    1   11    0    0    0    0    5    1
     0    0    0    0]
 [   2    1   13    2    0  116   12    0    0    0    0    0    0    0
     0    0    0    0]
 [  13   31    2    1    2    8  219    1    1    5    0    0    0    0
     0    0    0    1]
 [  30   29    0   13    3    0    8   95    1    0    7    0    3    0
     0    0    0    6]
 [  13    1    0    1    0    0    0    2   72    1    0    0    1    0
     0    0    0    0]
 [  32   10    1    1    1    0    1    0    2   39    0    0    0    0
     0    0    0    0]
 [   6    0    0    2    0    0    0    8    0    0   70    0    0    0
     0    0    0    0]
 [   8    4    1    3    0    0    0    0    0    2    0   54    0    0
     0    0    0    0]
 [   3   12    1    1    3    0    2   22    0    0    1    0   18    0
     0    0    0    1]
 [  22    4    0    2    5    0    1    0    0    1    1    0    0   28
     0    0    0    0]
 [  12   29    0    6    1    0    1    4    1    0    1    0    1    1
     0    0    0    0]
 [   9   26    0   33    2    1   13    1    0    0    0    1    0    0
     0   21    0    0]
 [   8   10    0    7    0    0    0   21    3    0    3    0    0    0
     0    0    2    2]
 [  12    1    2    3    1    0    0   11    0    0    0    0    1    0
     0    0    0   21]]
---------------------------------------
program finished.
