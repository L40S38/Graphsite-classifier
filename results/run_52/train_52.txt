seed:  666
number of classes (from original clusters): 14
how to merge clusters:  [[0, 9, 12], [1, 5, 11], 2, [3, 8, 13], 4, 6, 7, 10]
whether to further subcluster data according to chemical reaction: False
positive training pair sampling threshold:  24000
negative training pair sampling threshold:  8000
number of epochs to train: 35
learning rate decay to half at epoch 18.
batch size: 256
similar margin of contrastive loss: 0.0
dissimilar margin of contrastive loss: 2.0
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of classes after merging:  8
number of pockets in training set:  12475
number of pockets in validation set:  2670
number of pockets in test set:  2681
number of train positive pairs: 192000
number of train negative pairs: 224000
model architecture:
ResidualSiameseNet(
  (embedding_net): ResidualEmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=48, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=48, out_features=48, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (rb_2): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_3): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_4): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_5): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_6): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_7): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_8): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (bn_8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(48, 96)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.7246021546217112, train acc: 0.7934328837508028, validation acc: 0.7064564564564565.
epoch: 2, train loss: 0.6233142632887914, train acc: 0.7966441875401413, validation acc: 0.7045795795795796.
epoch: 3, train loss: 0.5667976861917056, train acc: 0.8204881181759794, validation acc: 0.7120870870870871.
epoch: 4, train loss: 0.524423254599938, train acc: 0.8148683365446371, validation acc: 0.6981981981981982.
epoch: 5, train loss: 0.49596162876716027, train acc: 0.8280346820809249, validation acc: 0.7210960960960962.
epoch: 6, train loss: 0.47081113901505106, train acc: 0.8338150289017341, validation acc: 0.7188438438438438.
epoch: 7, train loss: 0.45469402425105754, train acc: 0.8288375080282595, validation acc: 0.7158408408408409.
epoch: 8, train loss: 0.440233058324227, train acc: 0.8120584457289659, validation acc: 0.7135885885885885.
epoch: 9, train loss: 0.42973615257556624, train acc: 0.8318079640333975, validation acc: 0.7124624624624625.
epoch: 10, train loss: 0.4212350829564608, train acc: 0.8348587026332691, validation acc: 0.7162162162162162.
epoch: 11, train loss: 0.40990632886153, train acc: 0.8366249197174053, validation acc: 0.7233483483483484.
epoch: 12, train loss: 0.40201437306404114, train acc: 0.8026653821451509, validation acc: 0.6861861861861862.
epoch: 13, train loss: 0.4025708116384653, train acc: 0.8432883750802826, validation acc: 0.7105855855855856.
epoch: 14, train loss: 0.3918433022865882, train acc: 0.8446531791907514, validation acc: 0.7252252252252253.
epoch: 15, train loss: 0.4029594541329604, train acc: 0.8600674373795761, validation acc: 0.7286036036036037.
epoch: 16, train loss: 0.38296478928052463, train acc: 0.8553307642903019, validation acc: 0.7319819819819819.
epoch: 17, train loss: 0.3816988304211543, train acc: 0.8346981374438022, validation acc: 0.6981981981981982.
epoch: 18, train loss: 0.3248020692972037, train acc: 0.8694605009633911, validation acc: 0.7327327327327328.
epoch: 19, train loss: 0.3221697585857832, train acc: 0.8730732177263969, validation acc: 0.7293543543543544.
epoch: 20, train loss: 0.3217416371015402, train acc: 0.8807803468208093, validation acc: 0.7473723723723724.
epoch: 21, train loss: 0.3168544419545394, train acc: 0.8591843288375081, validation acc: 0.7237237237237237.
epoch: 22, train loss: 0.3181605132634823, train acc: 0.8650449582530507, validation acc: 0.7353603603603603.
epoch: 23, train loss: 0.309738770035597, train acc: 0.8752408477842004, validation acc: 0.725975975975976.
epoch: 24, train loss: 0.4058816221310542, train acc: 0.7758509955041747, validation acc: 0.6921921921921922.
epoch: 25, train loss: 0.42893120580453137, train acc: 0.8659280667951188, validation acc: 0.7312312312312312.
epoch: 26, train loss: 0.3448404034651243, train acc: 0.8870423892100193, validation acc: 0.7515015015015015.
epoch: 27, train loss: 0.31043341008516456, train acc: 0.873795761078998, validation acc: 0.7282282282282282.
epoch: 28, train loss: 0.3098501539505445, train acc: 0.8688182402055235, validation acc: 0.7154654654654654.
epoch: 29, train loss: 0.310035258430701, train acc: 0.8776493256262042, validation acc: 0.7466216216216216.
epoch: 30, train loss: 0.3056401261091232, train acc: 0.8743577392421323, validation acc: 0.7406156156156156.
epoch: 31, train loss: 0.3088295644338314, train acc: 0.8730732177263969, validation acc: 0.7361111111111112.
epoch: 32, train loss: 0.30320308361603665, train acc: 0.8867212588310854, validation acc: 0.7537537537537538.
epoch: 33, train loss: 0.3113275427909998, train acc: 0.8842324983943481, validation acc: 0.7432432432432432.
epoch: 34, train loss: 0.30733241260968724, train acc: 0.8874438021836866, validation acc: 0.7338588588588588.
epoch: 35, train loss: 0.2970039598758404, train acc: 0.8678548490687219, validation acc: 0.7323573573573574.
best validation acc 0.7537537537537538 at epoch 32.
