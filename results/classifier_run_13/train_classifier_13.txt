seed:  666
save trained model at:  ../trained_models/trained_classifier_model_13.pt
save loss at:  ./results/train_classifier_results_13.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=96, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=96, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=96, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b2f9f143280>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([0.0880, 0.1878, 0.5801, 0.2554, 0.6376, 0.6941, 0.3552, 0.5181, 1.1161,
        1.1718, 1.1866, 1.4153, 1.5928, 1.6147, 1.8003, 0.9512, 1.8355, 1.9995],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 1.3193843674802657, acc: 0.07496431630031401, val loss: 1.2327261916599115, acc: 0.14171337069655723, test loss: 1.238360877001652, acc: 0.14846235418875928
epoch: 2, train loss: 1.2000585021749415, acc: 0.164658863831002, val loss: 1.1550535395968142, acc: 0.19001868161195623, test loss: 1.1494885246928146, acc: 0.2009544008483563
epoch: 3, train loss: 1.1482132115705743, acc: 0.19508992292320868, val loss: 1.2352076962626009, acc: 0.17267147050974113, test loss: 1.233577595788626, acc: 0.17073170731707318
epoch: 4, train loss: 1.112964461178498, acc: 0.21792749072223808, val loss: 1.1367252478129966, acc: 0.19829196690685882, test loss: 1.134476808070631, acc: 0.20068928950159068
epoch: 5, train loss: 1.0838018956267421, acc: 0.24516129032258063, val loss: 1.0673306909662454, acc: 0.3154523618895116, test loss: 1.0739599528267174, acc: 0.3067338282078473
epoch: 6, train loss: 1.0426602636580122, acc: 0.2673708250071367, val loss: 1.0908683241606714, acc: 0.2556712036295703, test loss: 1.1021169109283804, acc: 0.25053022269353126
epoch: 7, train loss: 1.0235730543090316, acc: 0.2951184698829575, val loss: 1.0113321159373991, acc: 0.32425940752602084, test loss: 1.027712065746271, acc: 0.32873806998939553
epoch: 8, train loss: 1.0117738327025823, acc: 0.290493862403654, val loss: 1.0059269984499561, acc: 0.31118227915665864, test loss: 1.0216957306684897, acc: 0.3266171792152704
epoch: 9, train loss: 0.9883163765162153, acc: 0.3017984584641736, val loss: 1.0368807465355014, acc: 0.27061649319455566, test loss: 1.0343444223494949, acc: 0.2709437963944857
epoch: 10, train loss: 0.9742500201926856, acc: 0.316871253211533, val loss: 1.3192141239949153, acc: 0.17160394982652788, test loss: 1.3250237790549673, acc: 0.18080593849416754
epoch: 11, train loss: 0.9543114217288011, acc: 0.31881244647445045, val loss: 1.008813629476172, acc: 0.27328529490258874, test loss: 1.0263542178326928, acc: 0.2767762460233298
epoch: 12, train loss: 0.9365778487204416, acc: 0.33257208107336567, val loss: 0.9208532127978994, acc: 0.34320789965305576, test loss: 0.9330599012202954, acc: 0.345440084835631
epoch: 13, train loss: 0.9240770488679664, acc: 0.33182986011989724, val loss: 0.9143347651728764, acc: 0.37389911929543634, test loss: 0.9139001452910179, acc: 0.3671792152704136
epoch: 14, train loss: 0.9074749398020516, acc: 0.3463888095917785, val loss: 0.9743544286112483, acc: 0.3341339738457433, test loss: 0.9784705350659432, acc: 0.3364262990455992
epoch: 15, train loss: 0.9057100589943041, acc: 0.34216385954895806, val loss: 0.911339547686937, acc: 0.3319989324793168, test loss: 0.9288187618599464, acc: 0.3266171792152704
epoch: 16, train loss: 0.8851927231455952, acc: 0.3493576934056523, val loss: 0.9265818861219958, acc: 0.2981051507872965, test loss: 0.9414429007254123, acc: 0.29931071049840935
epoch: 17, train loss: 0.8580274635847724, acc: 0.3609477590636597, val loss: 0.8600825461063635, acc: 0.38323992527355216, test loss: 0.8977716811009202, acc: 0.38494167550371156
epoch: 18, train loss: 0.8552421825530356, acc: 0.36494433342848986, val loss: 0.8587617644444128, acc: 0.32425940752602084, test loss: 0.8838821698853219, acc: 0.3207847295864263
epoch: 19, train loss: 0.8597013166452386, acc: 0.36134741650014274, val loss: 0.858293782942512, acc: 0.35628502802241796, test loss: 0.8703753791457143, acc: 0.3544538706256628
epoch: 20, train loss: 0.8419540554906109, acc: 0.36842706251784185, val loss: 0.8497163606765336, acc: 0.39444889244729114, test loss: 0.8780288999684921, acc: 0.3862672322375398
epoch: 21, train loss: 0.8337337495734138, acc: 0.36797031116186124, val loss: 1.0824792129948262, acc: 0.24259407526020815, test loss: 1.0963335130778502, acc: 0.2441675503711559
epoch: 22, train loss: 0.8219270954191974, acc: 0.3748215815015701, val loss: 0.8456355332373809, acc: 0.3528155858019749, test loss: 0.8671932728647048, acc: 0.344644750795334
epoch: 23, train loss: 0.8024974205818579, acc: 0.3721381672851841, val loss: 0.8710093875551084, acc: 0.3493461435815319, test loss: 0.8893958096286778, acc: 0.34994697773064687
epoch: 24, train loss: 0.809032711504937, acc: 0.36979731658578363, val loss: 0.86278404131997, acc: 0.4104617026954897, test loss: 0.8714254625528767, acc: 0.4074761399787911
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.6691196876017326, acc: 0.40776477305166997, val loss: 0.7738350324704547, acc: 0.32746196957566054, test loss: 0.8036158623397034, acc: 0.32847295864262993
epoch: 26, train loss: 0.6674476504053621, acc: 0.3956608621181844, val loss: 0.7408855947011433, acc: 0.4096610621830798, test loss: 0.755623412663206, acc: 0.40827147401908803
epoch: 27, train loss: 0.6595356317441735, acc: 0.4037111047673423, val loss: 0.6595875029567722, acc: 0.3861756071523886, test loss: 0.6825187972342854, acc: 0.37910922587486745
epoch: 28, train loss: 0.6501570594300551, acc: 0.40656580074222093, val loss: 0.6525871969395585, acc: 0.45636509207365894, test loss: 0.6635701095059214, acc: 0.4493637327677625
epoch: 29, train loss: 0.6641089114467654, acc: 0.3948615472452184, val loss: 0.7558105698833155, acc: 0.34560982119028555, test loss: 0.7961811485877093, acc: 0.3321845174973489
epoch: 30, train loss: 0.6392223811816597, acc: 0.4155866400228376, val loss: 0.7571118037669474, acc: 0.3255938083800374, test loss: 0.7728012930424064, acc: 0.3337751855779427
epoch: 31, train loss: 0.6284354776271642, acc: 0.4105623751070511, val loss: 0.6952241708477052, acc: 0.40005337603416063, test loss: 0.7176240130168635, acc: 0.3796394485683987
epoch: 32, train loss: 0.6244796482794563, acc: 0.42380816443048813, val loss: 0.6983570607783224, acc: 0.38564184681078195, test loss: 0.7085886724680378, acc: 0.3944856839872747
epoch: 33, train loss: 0.6146879272327538, acc: 0.4222095346845561, val loss: 0.9756543197916895, acc: 0.2634107285828663, test loss: 0.9914884870656601, acc: 0.2526511134676564
epoch: 34, train loss: 0.6309928504222126, acc: 0.40982015415358264, val loss: 0.708247598279531, acc: 0.3893781692020283, test loss: 0.7372465338459196, acc: 0.3897136797454931
epoch: 35, train loss: 0.6141482270686176, acc: 0.41712817584927203, val loss: 0.6341207624277115, acc: 0.43901788097144384, test loss: 0.665038623840154, acc: 0.43133616118769885
epoch: 36, train loss: 0.6012635552743623, acc: 0.4324864401941193, val loss: 0.6609881178105326, acc: 0.43688283960501734, test loss: 0.6570651328955577, acc: 0.44406150583244963
epoch: 37, train loss: 0.6001364722409794, acc: 0.4273479874393377, val loss: 0.8671437269342656, acc: 0.3474779823859087, test loss: 0.8928108655256026, acc: 0.34597030752916225
epoch: 38, train loss: 0.5990162888221185, acc: 0.443505566657151, val loss: 0.7077100983867335, acc: 0.3789698425406992, test loss: 0.7410231154897938, acc: 0.36320254506892896
epoch: 39, train loss: 0.5782739388408438, acc: 0.4398515558093063, val loss: 0.797951399436912, acc: 0.3405390979450227, test loss: 0.8006623964420938, acc: 0.33510074231177095
Epoch    39: reducing learning rate of group 0 to 1.5000e-03.
epoch: 40, train loss: 0.5389336995659234, acc: 0.4708535540964887, val loss: 0.6177342790013668, acc: 0.44382172404590337, test loss: 0.63142116072322, acc: 0.4376988335100742
epoch: 41, train loss: 0.5181898509744846, acc: 0.48027405081358837, val loss: 0.5749612837624225, acc: 0.4694422204430211, test loss: 0.5844695472009361, acc: 0.47136797454931073
epoch: 42, train loss: 0.5016001774554861, acc: 0.4930059948615472, val loss: 0.6077377136974675, acc: 0.47157726180944753, test loss: 0.6147668002645386, acc: 0.46898197242841994
epoch: 43, train loss: 0.5047041609904578, acc: 0.4834142163859549, val loss: 0.7398705837533797, acc: 0.41419802508673603, test loss: 0.7231296168152374, acc: 0.4133085896076352
epoch: 44, train loss: 0.49472189761725893, acc: 0.4926063374250642, val loss: 0.6006811084446985, acc: 0.48251934881238323, test loss: 0.6071444028002839, acc: 0.4899257688229056
epoch: 45, train loss: 0.4897057666102716, acc: 0.4918641164715958, val loss: 0.5654298027960943, acc: 0.4945289564985322, test loss: 0.5846291222981911, acc: 0.493372216330859
epoch: 46, train loss: 0.4855754375900162, acc: 0.4946046246074793, val loss: 0.5795711594198047, acc: 0.477982385908727, test loss: 0.5975180483825133, acc: 0.48064687168610815
epoch: 47, train loss: 0.4800084676498894, acc: 0.49643163003140167, val loss: 0.5870247379060679, acc: 0.4867894315452362, test loss: 0.5942292274119113, acc: 0.4809119830328738
epoch: 48, train loss: 0.47834146998965194, acc: 0.497459320582358, val loss: 0.5989141374516239, acc: 0.466773418734988, test loss: 0.5921606977064309, acc: 0.4642099681866384
epoch: 49, train loss: 0.4673745223906188, acc: 0.5042534970025693, val loss: 0.5993427157497483, acc: 0.455564451561249, test loss: 0.6026033221221552, acc: 0.45572640509013784
epoch: 50, train loss: 0.47336643133679357, acc: 0.5011704253497002, val loss: 0.6421735635400614, acc: 0.47691486522551374, test loss: 0.6477382246282422, acc: 0.47799575821845175
epoch: 51, train loss: 0.45947579017816664, acc: 0.5020268341421639, val loss: 0.5829435988747663, acc: 0.4552975713904457, test loss: 0.6054429568010575, acc: 0.45811240721102864
epoch: 52, train loss: 0.46762748967740253, acc: 0.49797316585783613, val loss: 0.62197923921158, acc: 0.4795836669335468, test loss: 0.6605697598462393, acc: 0.48038176033934255
epoch: 53, train loss: 0.44757232185843465, acc: 0.5127033970882101, val loss: 0.6001948967977686, acc: 0.48545503069121965, test loss: 0.6165395877773411, acc: 0.48541887592788974
epoch: 54, train loss: 0.4412069077734058, acc: 0.5115615186982586, val loss: 0.5876798585220309, acc: 0.47371230317587404, test loss: 0.5981337849686785, acc: 0.4832979851537646
epoch: 55, train loss: 0.44721811028027786, acc: 0.505224093634028, val loss: 0.6113400516555823, acc: 0.4870563117160395, test loss: 0.60797007364534, acc: 0.4886002120890774
epoch: 56, train loss: 0.438975129384774, acc: 0.5228090208392806, val loss: 0.7615025819127389, acc: 0.389111289031225, test loss: 0.7584624947824002, acc: 0.3889183457051962
epoch: 57, train loss: 0.4411976099694895, acc: 0.514187838995147, val loss: 0.5689521084666602, acc: 0.5092073658927142, test loss: 0.6003307044695695, acc: 0.5095440084835631
epoch: 58, train loss: 0.42492535508192847, acc: 0.5216100485298316, val loss: 0.5959987000907488, acc: 0.492660795302909, test loss: 0.6304187117300509, acc: 0.4954931071049841
epoch: 59, train loss: 0.42602761805891276, acc: 0.5254924350556666, val loss: 0.6253761676969546, acc: 0.45956765412329864, test loss: 0.6428922441326801, acc: 0.4660657476139979
epoch: 60, train loss: 0.41766994387260203, acc: 0.5208678275763631, val loss: 0.5622178759293649, acc: 0.49399519615692555, test loss: 0.5979434586784873, acc: 0.4960233297985154
epoch: 61, train loss: 0.41412267200510533, acc: 0.5240079931487297, val loss: 0.6278528996649061, acc: 0.4403522818254604, test loss: 0.6538822585173655, acc: 0.4392895015906681
epoch: 62, train loss: 0.39865632836661474, acc: 0.537482158150157, val loss: 0.5707445441165351, acc: 0.4734454230050707, test loss: 0.6109119273192809, acc: 0.47348886532343587
epoch: 63, train loss: 0.39944154691294603, acc: 0.5302883242934627, val loss: 0.6676435560934506, acc: 0.45076060848678945, test loss: 0.700783449417713, acc: 0.4533404029692471
epoch: 64, train loss: 0.4080963592341448, acc: 0.5312589209249214, val loss: 0.6516203778503353, acc: 0.46597277822257804, test loss: 0.6980600200301137, acc: 0.46898197242841994
epoch: 65, train loss: 0.4064813450068159, acc: 0.527719097916072, val loss: 0.6221599912274384, acc: 0.4755804643714972, test loss: 0.6708123585325789, acc: 0.4801166489925769
epoch: 66, train loss: 0.4100204101365667, acc: 0.5173280045675136, val loss: 0.6784929564898257, acc: 0.41953562850280224, test loss: 0.7178375217846317, acc: 0.41542948038176036
epoch: 67, train loss: 0.40152048327192524, acc: 0.5293748215815016, val loss: 0.5488002411613535, acc: 0.536696023485455, test loss: 0.5887976853238184, acc: 0.54135737009544
epoch: 68, train loss: 0.3850746954998356, acc: 0.5464459035112761, val loss: 0.6722506902871337, acc: 0.41099546303709633, test loss: 0.703817051263528, acc: 0.40641569459172855
epoch: 69, train loss: 0.3702359764483395, acc: 0.5445047102483586, val loss: 0.6551305985558914, acc: 0.4739791833466773, test loss: 0.6794647730547196, acc: 0.48064687168610815
epoch: 70, train loss: 0.38496344556067963, acc: 0.5431344561804168, val loss: 0.6097668154512623, acc: 0.4859887910328263, test loss: 0.6805228597918093, acc: 0.485949098621421
epoch: 71, train loss: 0.3861533452298619, acc: 0.546046246074793, val loss: 0.6364423447938038, acc: 0.45102748865759273, test loss: 0.6739762858393842, acc: 0.4432661717921527
epoch: 72, train loss: 0.4229029352873215, acc: 0.5141307450756495, val loss: 0.614266475932516, acc: 0.4787830264211369, test loss: 0.6226327613960015, acc: 0.485949098621421
epoch: 73, train loss: 0.40075756023177206, acc: 0.5318298601198972, val loss: 0.7292454245887758, acc: 0.47157726180944753, test loss: 0.7624475189888591, acc: 0.47481442205726404
epoch: 74, train loss: 0.4021111992693206, acc: 0.5265772195261205, val loss: 0.6646469838147739, acc: 0.4270082732852949, test loss: 0.67266295343037, acc: 0.4286850477200424
epoch: 75, train loss: 0.3650985905979825, acc: 0.5455324007993149, val loss: 0.7036951172342293, acc: 0.4347477982385909, test loss: 0.7447741137835279, acc: 0.4469777306468717
epoch: 76, train loss: 0.3736494421431449, acc: 0.5480445332572081, val loss: 0.8389954272352411, acc: 0.3592207099012543, test loss: 0.852709952240152, acc: 0.3669141039236479
epoch: 77, train loss: 0.3858835186629577, acc: 0.5374250642306594, val loss: 0.5814987054849772, acc: 0.5105417667467307, test loss: 0.6251029409507679, acc: 0.5121951219512195
epoch: 78, train loss: 0.36055347244977065, acc: 0.5554667427918927, val loss: 0.5690272061195506, acc: 0.5001334400854016, test loss: 0.6323598943105558, acc: 0.4949628844114528
epoch: 79, train loss: 0.352257357915061, acc: 0.5570082786183271, val loss: 0.6115469417795997, acc: 0.4985321590605818, test loss: 0.6532746837854638, acc: 0.5074231177094379
epoch: 80, train loss: 0.3495796153043768, acc: 0.5573508421353126, val loss: 0.6204873330757908, acc: 0.4755804643714972, test loss: 0.6449864576881789, acc: 0.48541887592788974
epoch: 81, train loss: 0.33218306118918595, acc: 0.5662003996574365, val loss: 0.5992653735135122, acc: 0.5196156925540433, test loss: 0.627605812167818, acc: 0.5315482502651113
epoch: 82, train loss: 0.3459130318597695, acc: 0.562375107051099, val loss: 0.5929075250760504, acc: 0.5089404857219109, test loss: 0.6355579630061905, acc: 0.5066277836691411
epoch: 83, train loss: 0.37248025470517754, acc: 0.5459891521552954, val loss: 0.6052468645117713, acc: 0.48331998932479314, test loss: 0.6597821813625849, acc: 0.4878048780487805
epoch: 84, train loss: 0.3457252467866288, acc: 0.5624322009705967, val loss: 0.5811953194656148, acc: 0.5252201761409128, test loss: 0.6381351179181581, acc: 0.5251855779427359
epoch: 85, train loss: 0.3523611535149372, acc: 0.5655723665429632, val loss: 0.6152428731238775, acc: 0.44008540165465704, test loss: 0.64939099430153, acc: 0.4353128313891835
epoch: 86, train loss: 0.3501396068123249, acc: 0.5638595489580359, val loss: 0.5857670141087234, acc: 0.5305577795569789, test loss: 0.6279880492330735, acc: 0.5307529162248145
epoch: 87, train loss: 0.3242061646791856, acc: 0.5745932058235798, val loss: 0.5896477559931856, acc: 0.5382973045102749, test loss: 0.6573435605393994, acc: 0.5400318133616119
epoch: 88, train loss: 0.3274424961080832, acc: 0.5682557807593491, val loss: 0.621013075757542, acc: 0.5097411262343208, test loss: 0.6554993455508608, acc: 0.5053022269353128
epoch: 89, train loss: 0.31535651575589974, acc: 0.5787610619469027, val loss: 0.6308289218146164, acc: 0.5196156925540433, test loss: 0.6545255029947356, acc: 0.5230646871686108
epoch: 90, train loss: 0.32695494868773717, acc: 0.5748786754210676, val loss: 0.7887293715333188, acc: 0.4582332532692821, test loss: 0.8169466943528467, acc: 0.46977730646871685
epoch: 91, train loss: 0.3292382450621706, acc: 0.5631173280045675, val loss: 0.646250501899424, acc: 0.5100080064051241, test loss: 0.6958886338190461, acc: 0.5074231177094379
epoch: 92, train loss: 0.32214132099535475, acc: 0.5737938909506137, val loss: 0.6069707201104307, acc: 0.5260208166533227, test loss: 0.6333586959798414, acc: 0.528897136797455
epoch: 93, train loss: 0.31000900181776586, acc: 0.5787039680274051, val loss: 0.6075652429572035, acc: 0.5382973045102749, test loss: 0.6615432489461889, acc: 0.5389713679745494
epoch: 94, train loss: 0.32726569679883966, acc: 0.5719097916071938, val loss: 0.6042171762184935, acc: 0.5065385641846811, test loss: 0.6530432240960454, acc: 0.49787910922587486
epoch: 95, train loss: 0.3262231841500473, acc: 0.5765343990864973, val loss: 0.6990726518414642, acc: 0.4865225513744329, test loss: 0.7468978893466112, acc: 0.48886532343584305
epoch: 96, train loss: 0.34418741620304993, acc: 0.5626034827290893, val loss: 0.6377875344862072, acc: 0.5038697624766479, test loss: 0.6532869184763029, acc: 0.5053022269353128
epoch: 97, train loss: 0.3306547176010977, acc: 0.5716814159292035, val loss: 0.6273467670544517, acc: 0.5273552175073392, test loss: 0.6605838361499419, acc: 0.5395015906680806
epoch: 98, train loss: 0.3025377154537449, acc: 0.5884670282614901, val loss: 0.5932248145827046, acc: 0.5468374699759808, test loss: 0.6444452837947064, acc: 0.556998939554613
epoch: 99, train loss: 0.2991177944687139, acc: 0.6075934912931773, val loss: 0.6134320943235301, acc: 0.5201494528956498, test loss: 0.650212146065268, acc: 0.5095440084835631
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.24000709236025505, acc: 0.5978875249785898, val loss: 0.5196223483583212, acc: 0.5511075527088337, test loss: 0.579223372650551, acc: 0.5567338282078473
epoch: 101, train loss: 0.2342989982688832, acc: 0.6075363973736797, val loss: 0.550366398936308, acc: 0.49693087803576197, test loss: 0.5743154248149626, acc: 0.5074231177094379
epoch: 102, train loss: 0.2297522480542953, acc: 0.6049100770767913, val loss: 0.5908471972967168, acc: 0.49079263410728585, test loss: 0.6291437707801891, acc: 0.48462354188759277
epoch: 103, train loss: 0.26978078248873116, acc: 0.5803596916928347, val loss: 0.5454309061746264, acc: 0.4985321590605818, test loss: 0.5830754275539394, acc: 0.500795334040297
epoch: 104, train loss: 0.25720647256390694, acc: 0.5760205538110191, val loss: 0.5251926536524426, acc: 0.5361622631438484, test loss: 0.5598714632295222, acc: 0.5294273594909862
epoch: 105, train loss: 0.24344880413811718, acc: 0.5945760776477306, val loss: 0.5430011042983557, acc: 0.5273552175073392, test loss: 0.5867922796550763, acc: 0.5227995758218452
epoch: 106, train loss: 0.2303116779445819, acc: 0.6054239223522695, val loss: 0.5632791215017505, acc: 0.5439017880971444, test loss: 0.6058810851986057, acc: 0.5355249204665959
epoch: 107, train loss: 0.23567695739675582, acc: 0.5956608621181844, val loss: 0.5784015468447756, acc: 0.5228182546036829, test loss: 0.6242921177933856, acc: 0.5180275715800636
epoch: 108, train loss: 0.24036903267940588, acc: 0.5905224093634028, val loss: 0.5859077558983222, acc: 0.5014678409394182, test loss: 0.6057567720322189, acc: 0.5031813361611877
epoch: 109, train loss: 0.24766970521853102, acc: 0.5873822437910362, val loss: 0.5551341332719903, acc: 0.5209500934080598, test loss: 0.6152049155149328, acc: 0.5320784729586426
epoch: 110, train loss: 0.22338797503834004, acc: 0.6057093919497574, val loss: 0.5988689784148485, acc: 0.5406992260475046, test loss: 0.6418810479840696, acc: 0.5434782608695652
epoch: 111, train loss: 0.23833051139955141, acc: 0.5866971167570654, val loss: 0.6550313249918376, acc: 0.4654390178809714, test loss: 0.7030505008940227, acc: 0.45042417815482505
Epoch   111: reducing learning rate of group 0 to 7.5000e-04.
epoch: 112, train loss: 0.21529697952034335, acc: 0.6127319440479589, val loss: 0.5278439612143002, acc: 0.5807312516680011, test loss: 0.5724916655337191, acc: 0.588016967126193
epoch: 113, train loss: 0.2240473460026411, acc: 0.6176420211247502, val loss: 0.5379322623137572, acc: 0.5385641846810783, test loss: 0.561484601818744, acc: 0.5352598091198303
epoch: 114, train loss: 0.19786002384044088, acc: 0.6265486725663717, val loss: 0.5205300735542862, acc: 0.5748598879103283, test loss: 0.5689337647985173, acc: 0.5726405090137858
epoch: 115, train loss: 0.17766892762651043, acc: 0.6502997430773623, val loss: 0.5371397396135495, acc: 0.5687216439818521, test loss: 0.579150999085395, acc: 0.5710498409331919
epoch: 116, train loss: 0.16982240731158735, acc: 0.6610904938624037, val loss: 0.5454549715618849, acc: 0.578062449959968, test loss: 0.5903226537896619, acc: 0.5689289501590669
epoch: 117, train loss: 0.16544954122780325, acc: 0.6660576648586926, val loss: 0.5693140376686319, acc: 0.5868694955964772, test loss: 0.6123880935499043, acc: 0.5877518557794273
epoch: 118, train loss: 0.16113103099492765, acc: 0.666799885812161, val loss: 0.5595902382993431, acc: 0.5740592473979184, test loss: 0.6136192231264246, acc: 0.574761399787911
epoch: 119, train loss: 0.15860681563930581, acc: 0.6794176420211248, val loss: 0.5770692851405351, acc: 0.5769949292767548, test loss: 0.6432308475640015, acc: 0.5768822905620361
epoch: 120, train loss: 0.16251660076390326, acc: 0.6684556094775906, val loss: 0.6030647969099564, acc: 0.5820656525220176, test loss: 0.6561784038867323, acc: 0.577677624602333
epoch: 121, train loss: 0.15918768800278238, acc: 0.6735369683128747, val loss: 0.6073606867519162, acc: 0.581531892180411, test loss: 0.6769917597826314, acc: 0.5723753976670202
epoch: 122, train loss: 0.1573869191712051, acc: 0.6697687696260348, val loss: 0.5857084693353845, acc: 0.5724579663730984, test loss: 0.6463071822108798, acc: 0.5683987274655355
epoch: 123, train loss: 0.16333757742979918, acc: 0.6620039965743648, val loss: 0.5864868444030941, acc: 0.5700560448358687, test loss: 0.6463336304920476, acc: 0.5721102863202545
epoch: 124, train loss: 0.16136764989149696, acc: 0.6656580074222095, val loss: 0.6121337091824135, acc: 0.5809981318388043, test loss: 0.6595588895953473, acc: 0.5877518557794273
epoch: 125, train loss: 0.15203860918777246, acc: 0.6689123608335712, val loss: 0.604646456728371, acc: 0.5855350947424607, test loss: 0.6417063620032334, acc: 0.5904029692470838
epoch: 126, train loss: 0.15224439112792312, acc: 0.6779902940336854, val loss: 0.6216504671174684, acc: 0.5804643714971978, test loss: 0.6400203816079133, acc: 0.5829798515376459
epoch: 127, train loss: 0.16088022008100103, acc: 0.6610333999429061, val loss: 0.6035332534037687, acc: 0.5799306111555912, test loss: 0.6495742714417702, acc: 0.5856309650053022
epoch: 128, train loss: 0.16197869198137851, acc: 0.6691407365115615, val loss: 0.6021905932262289, acc: 0.5721910862022952, test loss: 0.6494962294306508, acc: 0.5707847295864263
epoch: 129, train loss: 0.15865268567579663, acc: 0.6671995432486441, val loss: 0.6818593223476079, acc: 0.5455030691219642, test loss: 0.7333549224432807, acc: 0.5371155885471898
epoch: 130, train loss: 0.14554326043889892, acc: 0.6826719954324865, val loss: 0.6158351688217029, acc: 0.5911395783293302, test loss: 0.6628503994117486, acc: 0.5943796394485684
epoch: 131, train loss: 0.14810100418480676, acc: 0.680331144733086, val loss: 0.6005547462159995, acc: 0.5954096610621831, test loss: 0.68350218664677, acc: 0.5991516436903499
epoch: 132, train loss: 0.15048311489107538, acc: 0.6821581501570083, val loss: 0.6802422104436683, acc: 0.5593808380037363, test loss: 0.7131103956054655, acc: 0.5601802757158006
epoch: 133, train loss: 0.15171044129211494, acc: 0.6719383385669426, val loss: 0.6728480861191435, acc: 0.5716573258606885, test loss: 0.7183138917132121, acc: 0.5673382820784729
epoch: 134, train loss: 0.16786823115738808, acc: 0.6683414216385954, val loss: 0.6117475450850182, acc: 0.5743261275687216, test loss: 0.6346118872390698, acc: 0.5845705196182397
epoch: 135, train loss: 0.1559001366023741, acc: 0.6697687696260348, val loss: 0.6825787026563389, acc: 0.5564451561248999, test loss: 0.7129786740178647, acc: 0.563626723223754
epoch: 136, train loss: 0.14919006952533645, acc: 0.6842706251784185, val loss: 0.6779233421170937, acc: 0.562583400053376, test loss: 0.708228527930736, acc: 0.5649522799575822
epoch: 137, train loss: 0.14963273021312498, acc: 0.6833000285469597, val loss: 0.6784009378625133, acc: 0.552175073392047, test loss: 0.7209586237546368, acc: 0.5543478260869565
epoch: 138, train loss: 0.16769204043402797, acc: 0.6588638310019983, val loss: 0.6293603591929126, acc: 0.577261809447558, test loss: 0.6873890365042338, acc: 0.5808589607635207
epoch: 139, train loss: 0.1553017917825874, acc: 0.6770196974022267, val loss: 0.6892390877907201, acc: 0.5214838537496664, test loss: 0.7961406348745239, acc: 0.5238600212089077
epoch: 140, train loss: 0.19088018642828325, acc: 0.6418498429917214, val loss: 0.6732369393134137, acc: 0.5068054443554844, test loss: 0.6755335141340723, acc: 0.5193531283138918
epoch: 141, train loss: 0.18844785380203521, acc: 0.6369968598344277, val loss: 0.5807676370665014, acc: 0.5721910862022952, test loss: 0.6226032457827004, acc: 0.5707847295864263
epoch: 142, train loss: 0.15424963149162077, acc: 0.6767913217242364, val loss: 0.6332666703405653, acc: 0.5596477181745396, test loss: 0.6745085607530709, acc: 0.5689289501590669
epoch: 143, train loss: 0.13956360973241225, acc: 0.6895232657721952, val loss: 0.6126658229914417, acc: 0.5986122231118228, test loss: 0.6472826325122271, acc: 0.6084305408271474
epoch: 144, train loss: 0.1347515507936988, acc: 0.7007136739937196, val loss: 0.6336471947282354, acc: 0.5863357352548706, test loss: 0.701505517403369, acc: 0.588016967126193
epoch: 145, train loss: 0.12537558682469208, acc: 0.7083642592063946, val loss: 0.659438555037654, acc: 0.5863357352548706, test loss: 0.7140547635067306, acc: 0.5790031813361611
epoch: 146, train loss: 0.1354442129662947, acc: 0.6927205252640594, val loss: 0.6524018604404804, acc: 0.5988791032826261, test loss: 0.7055882335594073, acc: 0.6004772004241782
epoch: 147, train loss: 0.1498193891532347, acc: 0.6800456751355981, val loss: 0.7544661058182076, acc: 0.5358953829730451, test loss: 0.789063022346537, acc: 0.5405620360551432
epoch: 148, train loss: 0.21492037330308777, acc: 0.6194690265486725, val loss: 0.6276747556187039, acc: 0.5457699492927676, test loss: 0.6446218351544151, acc: 0.5538176033934252
epoch: 149, train loss: 0.16147117662346436, acc: 0.6654296317442192, val loss: 0.5961022549449269, acc: 0.5697891646650654, test loss: 0.6392556010475846, acc: 0.5723753976670202
epoch: 150, train loss: 0.13871742558120287, acc: 0.6895232657721952, val loss: 0.6365138883109661, acc: 0.5911395783293302, test loss: 0.6630958509596888, acc: 0.5858960763520679
epoch: 151, train loss: 0.12890804486294458, acc: 0.7029974307736226, val loss: 0.6385988458048544, acc: 0.5967440619161997, test loss: 0.7073884389559765, acc: 0.5935843054082715
epoch: 152, train loss: 0.13048931609944484, acc: 0.702940336854125, val loss: 0.654590934252529, acc: 0.5793968508139845, test loss: 0.7189523321699869, acc: 0.5965005302226936
epoch: 153, train loss: 0.15326937562545437, acc: 0.6807308021695689, val loss: 0.7211322615806471, acc: 0.5284227381905524, test loss: 0.7488072591015036, acc: 0.5395015906680806
epoch: 154, train loss: 0.16976860535253704, acc: 0.6568655438195832, val loss: 0.6136358389384847, acc: 0.5692554043234588, test loss: 0.663726942157442, acc: 0.5755567338282078
epoch: 155, train loss: 0.14964017193554946, acc: 0.6786754210676563, val loss: 0.6183612888769878, acc: 0.5834000533760342, test loss: 0.6620952213303535, acc: 0.5795334040296924
epoch: 156, train loss: 0.1322581691637129, acc: 0.6982015415358265, val loss: 0.6787334300372834, acc: 0.5724579663730984, test loss: 0.7200489183245888, acc: 0.5874867444326617
epoch: 157, train loss: 0.12348799401611864, acc: 0.7091635740793606, val loss: 0.6483347757612828, acc: 0.6015479049906592, test loss: 0.7017744778322642, acc: 0.6007423117709438
epoch: 158, train loss: 0.12058073587666707, acc: 0.7168141592920354, val loss: 0.7091324748056298, acc: 0.5860688550840673, test loss: 0.7412440976560558, acc: 0.5832449628844114
epoch: 159, train loss: 0.12444366840100649, acc: 0.7132743362831858, val loss: 0.6480848798759467, acc: 0.5828662930344275, test loss: 0.7013173409256678, acc: 0.584305408271474
epoch: 160, train loss: 0.12221967982813661, acc: 0.7175563802455038, val loss: 0.7318562342066239, acc: 0.5713904456898853, test loss: 0.7885205561131208, acc: 0.5721102863202545
epoch: 161, train loss: 0.14554853824642838, acc: 0.6851841278903796, val loss: 0.6731856247250608, acc: 0.5567120362957032, test loss: 0.7079878819203705, acc: 0.560710498409332
epoch: 162, train loss: 0.17603743559474302, acc: 0.6509848701113331, val loss: 0.6592298311203487, acc: 0.5593808380037363, test loss: 0.6847437836332513, acc: 0.5697242841993637
epoch: 163, train loss: 0.13608139094338564, acc: 0.6957465029974308, val loss: 0.6223131831499745, acc: 0.5972778222578062, test loss: 0.6701266957745193, acc: 0.6081654294803818
epoch: 164, train loss: 0.12544700056867467, acc: 0.7075078504139309, val loss: 0.6573923393221006, acc: 0.5930077395249533, test loss: 0.7075996181492082, acc: 0.5909331919406151
epoch: 165, train loss: 0.15476206806330145, acc: 0.6756494433342849, val loss: 0.6642505726179251, acc: 0.5615158793701628, test loss: 0.7068581065342763, acc: 0.5668080593849417
epoch: 166, train loss: 0.16765958251797947, acc: 0.6687410790750785, val loss: 0.686368386841533, acc: 0.5543101147584735, test loss: 0.7471137934246852, acc: 0.5455991516436903
epoch: 167, train loss: 0.18665625879399067, acc: 0.6438481301741364, val loss: 0.6123683087374644, acc: 0.5644515612489992, test loss: 0.6782401136477199, acc: 0.5683987274655355
epoch: 168, train loss: 0.16033036391496183, acc: 0.6647445047102484, val loss: 0.5996181668998529, acc: 0.5903389378169202, test loss: 0.6470616875119578, acc: 0.5965005302226936
Epoch   168: reducing learning rate of group 0 to 3.7500e-04.
epoch: 169, train loss: 0.12208323555565412, acc: 0.7164715957750499, val loss: 0.6089218614067686, acc: 0.6020816653322658, test loss: 0.6687331452475879, acc: 0.6097560975609756
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.08556715948366282, acc: 0.7376534399086497, val loss: 0.5786265796236842, acc: 0.6090205497731519, test loss: 0.640804068028358, acc: 0.6126723223753977
epoch: 171, train loss: 0.08390361121529992, acc: 0.734399086497288, val loss: 0.5941381937413334, acc: 0.6132906325060048, test loss: 0.654798416671672, acc: 0.6118769883351007
epoch: 172, train loss: 0.07822690896313428, acc: 0.7434199257779046, val loss: 0.5924376179003098, acc: 0.625567120362957, test loss: 0.6502065127626583, acc: 0.6171792152704135
epoch: 173, train loss: 0.07375037196812002, acc: 0.7556951184698829, val loss: 0.6099489929518892, acc: 0.625567120362957, test loss: 0.6805337307182627, acc: 0.6139978791092259
epoch: 174, train loss: 0.07306542462883865, acc: 0.75341136168998, val loss: 0.617177906372339, acc: 0.6111555911395783, test loss: 0.689092282146563, acc: 0.6079003181336161
epoch: 175, train loss: 0.06908505726462087, acc: 0.7632315158435626, val loss: 0.6150595760040038, acc: 0.6202295169468909, test loss: 0.6809470886137428, acc: 0.620625662778367
epoch: 176, train loss: 0.06881828280413045, acc: 0.7658007422209535, val loss: 0.6314627162163691, acc: 0.6247664798505471, test loss: 0.6975776269352448, acc: 0.6216861081654295
epoch: 177, train loss: 0.0694160341365726, acc: 0.7643163003140165, val loss: 0.6331459078624594, acc: 0.6159594342140379, test loss: 0.7029941719132036, acc: 0.6137327677624602
epoch: 178, train loss: 0.06896240570835818, acc: 0.7627176705680845, val loss: 0.6278511302007304, acc: 0.6274352815585802, test loss: 0.6925077612807111, acc: 0.63016967126193
epoch: 179, train loss: 0.07151965549310854, acc: 0.7587210962032543, val loss: 0.6215065380144285, acc: 0.6212970376301041, test loss: 0.7068208482586567, acc: 0.6179745493107105
epoch: 180, train loss: 0.06622851730550047, acc: 0.7682557807593491, val loss: 0.6289133130947303, acc: 0.63063784360822, test loss: 0.7019523838544828, acc: 0.630965005302227
epoch: 181, train loss: 0.0632097494077281, acc: 0.7707108192977448, val loss: 0.64716360045332, acc: 0.6311716039498265, test loss: 0.7165819063924909, acc: 0.6243372216330859
epoch: 182, train loss: 0.06488977744005558, acc: 0.7692263773908079, val loss: 0.6413478243023293, acc: 0.6218307979717107, test loss: 0.6965256040700546, acc: 0.6171792152704135
epoch: 183, train loss: 0.06592994467627822, acc: 0.7713388524122181, val loss: 0.6432235265115372, acc: 0.6175607152388578, test loss: 0.7080069134369382, acc: 0.61983032873807
epoch: 184, train loss: 0.0665072685828081, acc: 0.7659720239794462, val loss: 0.6616121250882542, acc: 0.617827595409661, test loss: 0.7391191428944011, acc: 0.616118769883351
epoch: 185, train loss: 0.06503211670421445, acc: 0.7686554381958322, val loss: 0.6340777011181088, acc: 0.6175607152388578, test loss: 0.7036292343099195, acc: 0.6126723223753977
epoch: 186, train loss: 0.06605619925738675, acc: 0.7715672280902084, val loss: 0.689324624957738, acc: 0.6124899919935949, test loss: 0.7549365191292788, acc: 0.6113467656415694
epoch: 187, train loss: 0.06522824910200564, acc: 0.7700827861832715, val loss: 0.6496350373272454, acc: 0.6084867894315452, test loss: 0.7268451617858822, acc: 0.6187698833510075
epoch: 188, train loss: 0.06634683617814215, acc: 0.7662003996574365, val loss: 0.671774545796305, acc: 0.6172938350680545, test loss: 0.769106448518383, acc: 0.6063096500530223
epoch: 189, train loss: 0.06496773341847698, acc: 0.7690550956323151, val loss: 0.6656245127149286, acc: 0.6258340005337604, test loss: 0.7416726448123806, acc: 0.615323435843054
epoch: 190, train loss: 0.06570454583299389, acc: 0.7676277476448758, val loss: 0.6722699725408379, acc: 0.6164931945556446, test loss: 0.7300684216925094, acc: 0.6185047720042418
epoch: 191, train loss: 0.07031961206773196, acc: 0.7591207536397374, val loss: 0.6718281040771948, acc: 0.618628235922071, test loss: 0.7318794818344198, acc: 0.6195652173913043
epoch: 192, train loss: 0.06421477280444429, acc: 0.7710533828147302, val loss: 0.6479086924910005, acc: 0.6218307979717107, test loss: 0.6997429017423952, acc: 0.6200954400848356
epoch: 193, train loss: 0.0676670405261863, acc: 0.7703111618612618, val loss: 0.6795376207404115, acc: 0.5938083800373632, test loss: 0.7371159249626313, acc: 0.5949098621420997
epoch: 194, train loss: 0.0875768098828114, acc: 0.7256066228946617, val loss: 0.6382090397634473, acc: 0.6135575126768081, test loss: 0.6958973534160302, acc: 0.6052492046659597
epoch: 195, train loss: 0.0725289548452398, acc: 0.7548958035969169, val loss: 0.6678180908374161, acc: 0.6047504670402989, test loss: 0.7237203447335852, acc: 0.5975609756097561
epoch: 196, train loss: 0.0686454844447568, acc: 0.7575221238938054, val loss: 0.671621329312901, acc: 0.6271684013877769, test loss: 0.7424599469529736, acc: 0.6187698833510075
epoch: 197, train loss: 0.06107125281947291, acc: 0.7763631173280046, val loss: 0.6576338443623755, acc: 0.6261008807045636, test loss: 0.730693118064047, acc: 0.623541887592789
epoch: 198, train loss: 0.06161253968120268, acc: 0.7705966314587497, val loss: 0.6825267205341422, acc: 0.6130237523352015, test loss: 0.7424949391143573, acc: 0.6118769883351007
epoch: 199, train loss: 0.07590296115665615, acc: 0.7497573508421354, val loss: 0.6587857222092575, acc: 0.6063517480651187, test loss: 0.7109106150816253, acc: 0.6018027571580064
epoch: 200, train loss: 0.08511768150952351, acc: 0.735883528404225, val loss: 0.6858118048724665, acc: 0.5895382973045102, test loss: 0.7551292821938766, acc: 0.5821845174973489
best val acc 0.6311716039498265 at epoch 181.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9581    0.7499    0.8413      5337
           1     0.7936    0.5424    0.6443      2502
           2     0.9671    0.9074    0.9363       810
           3     0.7110    0.7473    0.7287      1840
           4     0.8426    0.9661    0.9001       737
           5     0.8929    0.9970    0.9421       677
           6     0.7902    0.8768    0.8312      1323
           7     0.6492    0.8490    0.7358       907
           8     0.6200    1.0000    0.7655       421
           9     0.7030    0.9975    0.8247       401
          10     0.9039    0.9975    0.9484       396
          11     0.9881    0.9970    0.9925       332
          12     0.9049    1.0000    0.9501       295
          13     0.8106    1.0000    0.8954       291
          14     0.4754    1.0000    0.6444       261
          15     0.5476    0.9555    0.6962       494
          16     0.8388    0.9961    0.9107       256
          17     0.9073    1.0000    0.9514       235

    accuracy                         0.8074     17515
   macro avg     0.7947    0.9211    0.8411     17515
weighted avg     0.8338    0.8074    0.8072     17515

train confusion matrix:
[[7.49859472e-01 3.20404722e-02 3.37268128e-03 4.44069702e-02
  1.31159828e-03 3.74742365e-04 4.30953719e-03 4.59059397e-02
  4.77796515e-02 2.94172756e-02 4.68427956e-03 5.62113547e-04
  9.36855912e-04 1.21791269e-02 1.03054150e-02 4.30953719e-03
  5.05902192e-03 3.18531010e-03]
 [3.55715428e-02 5.42366107e-01 3.99680256e-04 1.23501199e-01
  3.87689848e-02 7.99360512e-04 1.04316547e-01 5.03597122e-02
  3.99680256e-04 3.99680256e-03 7.99360512e-04 3.99680256e-04
  3.59712230e-03 7.99360512e-04 5.27577938e-02 3.99680256e-02
  0.00000000e+00 1.19904077e-03]
 [0.00000000e+00 0.00000000e+00 9.07407407e-01 0.00000000e+00
  1.23456790e-03 7.90123457e-02 3.70370370e-03 0.00000000e+00
  1.23456790e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  7.40740741e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.88043478e-02 5.70652174e-02 5.43478261e-04 7.47282609e-01
  1.08695652e-03 0.00000000e+00 5.43478261e-04 1.35869565e-02
  0.00000000e+00 1.08695652e-03 1.63043478e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 5.10869565e-02 9.51086957e-02
  1.63043478e-03 5.43478261e-04]
 [0.00000000e+00 8.14111262e-03 0.00000000e+00 0.00000000e+00
  9.66078697e-01 0.00000000e+00 0.00000000e+00 2.03527815e-02
  0.00000000e+00 0.00000000e+00 1.35685210e-03 0.00000000e+00
  1.35685210e-03 0.00000000e+00 2.71370421e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 1.47710487e-03 0.00000000e+00
  0.00000000e+00 9.97045790e-01 1.47710487e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.04686319e-03 2.49433107e-02 3.02343159e-03 1.51171580e-03
  2.26757370e-03 9.82615268e-03 8.76795163e-01 3.77928949e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.51171580e-03 7.55857899e-04 0.00000000e+00 6.95389267e-02
  0.00000000e+00 0.00000000e+00]
 [2.31532525e-02 4.18963616e-02 0.00000000e+00 3.30760750e-03
  2.53583241e-02 0.00000000e+00 6.61521499e-03 8.48952591e-01
  1.10253583e-03 0.00000000e+00 1.21278942e-02 0.00000000e+00
  8.82028666e-03 0.00000000e+00 4.41014333e-03 0.00000000e+00
  2.09481808e-02 3.30760750e-03]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.49376559e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.97506234e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.52525253e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.97474747e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.01204819e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.96987952e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.61943320e-02
  0.00000000e+00 0.00000000e+00 2.63157895e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.02429150e-03 9.55465587e-01
  0.00000000e+00 0.00000000e+00]
 [3.90625000e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.96093750e-01 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.00000000e+00]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8707    0.6719    0.7585      1143
           1     0.5810    0.4216    0.4886       536
           2     0.7949    0.7168    0.7538       173
           3     0.5381    0.6091    0.5714       394
           4     0.7273    0.7595    0.7430       158
           5     0.7515    0.8759    0.8089       145
           6     0.6422    0.7102    0.6745       283
           7     0.3447    0.5206    0.4148       194
           8     0.5474    0.8333    0.6608        90
           9     0.4811    0.6000    0.5340        85
          10     0.7204    0.7976    0.7571        84
          11     0.9500    0.8028    0.8702        71
          12     0.5405    0.6349    0.5839        63
          13     0.6316    0.7742    0.6957        62
          14     0.1157    0.2500    0.1582        56
          15     0.3171    0.4952    0.3866       105
          16     0.4038    0.3818    0.3925        55
          17     0.6471    0.6600    0.6535        50

    accuracy                         0.6312      3747
   macro avg     0.5892    0.6398    0.6059      3747
weighted avg     0.6710    0.6312    0.6417      3747

validation confusion matrix:
[[0.67191601 0.03499563 0.00787402 0.06124234 0.00524934 0.00262467
  0.0096238  0.05511811 0.04461942 0.03587052 0.01049869 0.
  0.00437445 0.01662292 0.01574803 0.00874891 0.00699913 0.00787402]
 [0.05783582 0.42164179 0.00932836 0.13059701 0.02798507 0.00746269
  0.10261194 0.07649254 0.00746269 0.00932836 0.00373134 0.
  0.0130597  0.00373134 0.07276119 0.03731343 0.0130597  0.00559701]
 [0.04624277 0.01156069 0.71676301 0.01734104 0.         0.12138728
  0.04046243 0.00578035 0.         0.01156069 0.         0.
  0.         0.00578035 0.         0.01734104 0.00578035 0.        ]
 [0.05329949 0.06345178 0.00253807 0.60913706 0.0177665  0.00253807
  0.00761421 0.06091371 0.         0.         0.00507614 0.00507614
  0.00761421 0.         0.05076142 0.11167513 0.00253807 0.        ]
 [0.         0.09493671 0.00632911 0.03164557 0.75949367 0.
  0.01265823 0.03797468 0.00632911 0.         0.         0.
  0.00632911 0.01898734 0.02531646 0.         0.         0.        ]
 [0.0137931  0.         0.04827586 0.00689655 0.         0.87586207
  0.02758621 0.00689655 0.         0.         0.         0.
  0.         0.         0.         0.02068966 0.         0.        ]
 [0.00353357 0.07773852 0.01413428 0.00353357 0.01060071 0.02473498
  0.71024735 0.01060071 0.00706714 0.01060071 0.         0.00353357
  0.01766784 0.         0.00706714 0.09893993 0.         0.        ]
 [0.04639175 0.11340206 0.00515464 0.07216495 0.04123711 0.
  0.01546392 0.52061856 0.01030928 0.         0.03608247 0.
  0.03092784 0.         0.05670103 0.00515464 0.03608247 0.01030928]
 [0.04444444 0.01111111 0.         0.03333333 0.         0.
  0.         0.03333333 0.83333333 0.         0.         0.
  0.02222222 0.01111111 0.01111111 0.         0.         0.        ]
 [0.14117647 0.05882353 0.02352941 0.01176471 0.         0.04705882
  0.05882353 0.         0.         0.6        0.         0.
  0.         0.02352941 0.01176471 0.01176471 0.01176471 0.        ]
 [0.03571429 0.         0.         0.         0.         0.
  0.         0.10714286 0.         0.         0.79761905 0.
  0.02380952 0.         0.         0.         0.02380952 0.01190476]
 [0.02816901 0.02816901 0.         0.04225352 0.         0.
  0.01408451 0.         0.         0.04225352 0.         0.8028169
  0.         0.         0.01408451 0.02816901 0.         0.        ]
 [0.         0.0952381  0.         0.01587302 0.         0.
  0.03174603 0.14285714 0.03174603 0.         0.         0.
  0.63492063 0.         0.03174603 0.         0.01587302 0.        ]
 [0.09677419 0.         0.01612903 0.03225806 0.03225806 0.
  0.03225806 0.01612903 0.         0.         0.         0.
  0.         0.77419355 0.         0.         0.         0.        ]
 [0.125      0.23214286 0.         0.19642857 0.         0.
  0.         0.125      0.         0.         0.01785714 0.
  0.01785714 0.         0.25       0.         0.         0.03571429]
 [0.04761905 0.04761905 0.         0.14285714 0.         0.00952381
  0.16190476 0.02857143 0.         0.00952381 0.         0.
  0.         0.         0.04761905 0.4952381  0.00952381 0.        ]
 [0.01818182 0.09090909 0.01818182 0.07272727 0.07272727 0.01818182
  0.         0.21818182 0.         0.         0.01818182 0.
  0.01818182 0.         0.05454545 0.         0.38181818 0.01818182]
 [0.04       0.         0.         0.04       0.         0.
  0.         0.18       0.         0.         0.02       0.
  0.02       0.         0.         0.         0.04       0.66      ]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8424    0.6769    0.7506      1145
           1     0.5635    0.4376    0.4927       537
           2     0.8408    0.7543    0.7952       175
           3     0.5639    0.5696    0.5668       395
           4     0.6784    0.7296    0.7030       159
           5     0.7310    0.8562    0.7886       146
           6     0.6455    0.6796    0.6621       284
           7     0.3481    0.4821    0.4043       195
           8     0.4873    0.8462    0.6185        91
           9     0.4775    0.6092    0.5354        87
          10     0.7553    0.8256    0.7889        86
          11     0.9104    0.8472    0.8777        72
          12     0.5738    0.5469    0.5600        64
          13     0.5493    0.6094    0.5778        64
          14     0.1354    0.2281    0.1699        57
          15     0.2827    0.5047    0.3624       107
          16     0.3788    0.4464    0.4098        56
          17     0.6038    0.6154    0.6095        52

    accuracy                         0.6243      3772
   macro avg     0.5760    0.6258    0.5930      3772
weighted avg     0.6579    0.6243    0.6338      3772

test confusion matrix:
[[0.6768559  0.0419214  0.00436681 0.04541485 0.00524017 0.00436681
  0.00786026 0.04978166 0.05851528 0.03406114 0.01048035 0.00262009
  0.00524017 0.01746725 0.01310044 0.00524017 0.00873362 0.00873362]
 [0.0689013  0.43761639 0.00372439 0.09869646 0.03910615 0.00558659
  0.10986965 0.06703911 0.00744879 0.00931099 0.         0.00372439
  0.00744879 0.00744879 0.06703911 0.05586592 0.00372439 0.00744879]
 [0.04       0.02285714 0.75428571 0.01142857 0.01142857 0.12571429
  0.         0.         0.00571429 0.00571429 0.         0.
  0.01142857 0.         0.00571429 0.00571429 0.         0.        ]
 [0.0556962  0.07848101 0.00759494 0.56962025 0.01518987 0.
  0.         0.0556962  0.00253165 0.00506329 0.00506329 0.
  0.         0.00253165 0.04050633 0.1443038  0.01518987 0.00253165]
 [0.         0.09433962 0.         0.03773585 0.72955975 0.
  0.02515723 0.06918239 0.00628931 0.         0.         0.
  0.01257862 0.00628931 0.01257862 0.00628931 0.         0.        ]
 [0.00684932 0.         0.03424658 0.         0.         0.85616438
  0.06849315 0.         0.         0.         0.         0.00684932
  0.         0.         0.         0.02739726 0.         0.        ]
 [0.03873239 0.04577465 0.01056338 0.00704225 0.00352113 0.04225352
  0.67957746 0.00704225 0.         0.02112676 0.00352113 0.
  0.01408451 0.00352113 0.00352113 0.11619718 0.         0.00352113]
 [0.05128205 0.0974359  0.         0.09230769 0.03589744 0.01025641
  0.03589744 0.48205128 0.01538462 0.         0.02051282 0.
  0.02051282 0.01025641 0.04615385 0.01025641 0.07179487 0.        ]
 [0.08791209 0.02197802 0.         0.         0.         0.
  0.         0.01098901 0.84615385 0.         0.         0.
  0.         0.01098901 0.         0.         0.01098901 0.01098901]
 [0.1954023  0.06896552 0.01149425 0.01149425 0.         0.
  0.04597701 0.02298851 0.01149425 0.6091954  0.         0.
  0.01149425 0.01149425 0.         0.         0.         0.        ]
 [0.04651163 0.         0.         0.         0.02325581 0.
  0.         0.06976744 0.         0.         0.8255814  0.
  0.01162791 0.         0.         0.         0.02325581 0.        ]
 [0.04166667 0.04166667 0.         0.         0.01388889 0.
  0.         0.         0.         0.04166667 0.         0.84722222
  0.         0.         0.         0.         0.01388889 0.        ]
 [0.03125    0.109375   0.015625   0.046875   0.046875   0.
  0.03125    0.140625   0.         0.         0.015625   0.
  0.546875   0.         0.         0.015625   0.         0.        ]
 [0.125      0.125      0.015625   0.03125    0.015625   0.
  0.03125    0.015625   0.         0.03125    0.         0.
  0.         0.609375   0.         0.         0.         0.        ]
 [0.07017544 0.19298246 0.         0.19298246 0.05263158 0.
  0.05263158 0.10526316 0.05263158 0.         0.         0.
  0.         0.         0.22807018 0.01754386 0.01754386 0.01754386]
 [0.06542056 0.11214953 0.01869159 0.14953271 0.00934579 0.01869159
  0.05607477 0.03738318 0.         0.         0.         0.
  0.         0.         0.00934579 0.5046729  0.         0.01869159]
 [0.03571429 0.05357143 0.         0.10714286 0.01785714 0.
  0.         0.21428571 0.         0.         0.03571429 0.
  0.01785714 0.01785714 0.01785714 0.01785714 0.44642857 0.01785714]
 [0.03846154 0.         0.03846154 0.03846154 0.         0.
  0.         0.13461538 0.         0.         0.01923077 0.
  0.01923077 0.         0.01923077 0.         0.07692308 0.61538462]]
---------------------------------------
program finished.
