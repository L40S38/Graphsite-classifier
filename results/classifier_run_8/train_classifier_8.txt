seed:  666
save trained model at:  ../trained_models/trained_classifier_model_8.pt
save loss at:  ./results/train_classifier_results_8.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  100
learning rate decay at epoch:  60
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn5): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(80, 160)
  )
  (fc1): Linear(in_features=160, out_features=80, bias=True)
  (fc2): Linear(in_features=80, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [20, 60, 100]
loss function:
FocalLoss(gamma=0, alpha=1, reduction=mean)
begin training...
epoch: 1, train loss: 2.328148196902644, acc: 0.31824150727947476, val loss: 2.0540597296282868, acc: 0.3952495329597011, test loss: 2.0706066900700297, acc: 0.39501590668080594
epoch: 2, train loss: 2.0532341960359224, acc: 0.3898943762489295, val loss: 1.8901763326300283, acc: 0.43341339738457435, test loss: 1.8852449891928778, acc: 0.4361081654294804
epoch: 3, train loss: 1.9431955292603986, acc: 0.41650014273479874, val loss: 1.8456736611212798, acc: 0.4384841206298372, test loss: 1.8413379149371214, acc: 0.45068928950159065
epoch: 4, train loss: 1.8560923320853027, acc: 0.44818726805595205, val loss: 1.884815393144492, acc: 0.433146517213771, test loss: 1.8702871068790634, acc: 0.4329268292682927
epoch: 5, train loss: 1.7625761813856076, acc: 0.47622038252926063, val loss: 1.7104292979137339, acc: 0.48385374966639977, test loss: 1.7088494063182196, acc: 0.4875397667020148
epoch: 6, train loss: 1.6979824816401332, acc: 0.4909506137596346, val loss: 1.6379954790731797, acc: 0.4897251134240726, test loss: 1.6251895546533903, acc: 0.5021208907741251
epoch: 7, train loss: 1.6523472661057304, acc: 0.5051669997145304, val loss: 1.6576861502298967, acc: 0.5041366426474513, test loss: 1.6476718069766259, acc: 0.5050371155885471
epoch: 8, train loss: 1.610284381779881, acc: 0.5147016842706251, val loss: 1.538527054248379, acc: 0.5257539364825193, test loss: 1.5255685770372571, acc: 0.5299575821845175
epoch: 9, train loss: 1.5645936520889832, acc: 0.5305737938909506, val loss: 1.59910226269089, acc: 0.5196156925540433, test loss: 1.600262306907144, acc: 0.5174973488865323
epoch: 10, train loss: 1.5425788521868755, acc: 0.5382243791036254, val loss: 1.4973989801149798, acc: 0.5412329863891113, test loss: 1.497303841349176, acc: 0.546659597030753
epoch: 11, train loss: 1.523361823547237, acc: 0.5429060805024265, val loss: 1.4918555637534727, acc: 0.5500400320256205, test loss: 1.474367504787344, acc: 0.5540827147401909
epoch: 12, train loss: 1.4945712089470513, acc: 0.5482729089351984, val loss: 1.5525173387560554, acc: 0.5244195356285029, test loss: 1.5443900870366667, acc: 0.5352598091198303
epoch: 13, train loss: 1.483500073931403, acc: 0.5539252069654582, val loss: 1.460271070377013, acc: 0.5575126768081131, test loss: 1.4525712777802193, acc: 0.5665429480381761
epoch: 14, train loss: 1.4456216357824907, acc: 0.5630602340850699, val loss: 1.502309909400095, acc: 0.5380304243394716, test loss: 1.4865024709246444, acc: 0.5532873806998939
epoch: 15, train loss: 1.4410487461185373, acc: 0.5688267199543249, val loss: 1.415135689280591, acc: 0.5665866026154257, test loss: 1.4165609593356023, acc: 0.5588547189819725
epoch: 16, train loss: 1.4156955093238954, acc: 0.571338852412218, val loss: 1.4241474944812824, acc: 0.5569789164665065, test loss: 1.4209814210711709, acc: 0.5638918345705196
epoch: 17, train loss: 1.4046172455518953, acc: 0.5812731944047959, val loss: 1.4166461181284302, acc: 0.5673872431278356, test loss: 1.419981731716168, acc: 0.5715800636267232
epoch: 18, train loss: 1.3847630715867025, acc: 0.5841278903796746, val loss: 1.4633997184039753, acc: 0.547104350146784, test loss: 1.4574947397630515, acc: 0.5559384941675504
epoch: 19, train loss: 1.3732211925491347, acc: 0.5817299457607765, val loss: 1.3499853871872116, acc: 0.5866026154256738, test loss: 1.3576307347318906, acc: 0.5949098621420997
epoch 20, gamma increased to 1.
epoch: 20, train loss: 1.1214177638436942, acc: 0.5954895803596917, val loss: 1.0993723002057856, acc: 0.588470776621297, test loss: 1.1142146605069918, acc: 0.5965005302226936
epoch: 21, train loss: 1.109952990252598, acc: 0.5962888952326577, val loss: 1.1141602986648556, acc: 0.5863357352548706, test loss: 1.1151330744095092, acc: 0.5925238600212089
epoch: 22, train loss: 1.0882323523300088, acc: 0.5984013702540679, val loss: 1.1069154216984223, acc: 0.5916733386709367, test loss: 1.1230672290696824, acc: 0.5909331919406151
epoch: 23, train loss: 1.0812941264330302, acc: 0.5996574364830145, val loss: 1.110122462097791, acc: 0.5817987723512144, test loss: 1.1249910209995566, acc: 0.5893425238600212
epoch: 24, train loss: 1.0551075146125448, acc: 0.611247502141022, val loss: 1.346119508636389, acc: 0.5118761676007473, test loss: 1.3666734690377973, acc: 0.5119300106044539
epoch: 25, train loss: 1.041842130718999, acc: 0.6102769055095633, val loss: 1.1158287203849586, acc: 0.5732586068855084, test loss: 1.1484550069523767, acc: 0.5670731707317073
epoch: 26, train loss: 1.1557558777466386, acc: 0.5782472166714245, val loss: 1.2940663024524641, acc: 0.5484387510008006, test loss: 1.2980201059788434, acc: 0.5442735949098622
epoch: 27, train loss: 1.1040084472289264, acc: 0.5946902654867257, val loss: 1.2478484030815962, acc: 0.5340272217774219, test loss: 1.2753980051042673, acc: 0.5368504772004242
epoch: 28, train loss: 1.0405077056547183, acc: 0.6139880102769055, val loss: 1.1120751558827628, acc: 0.5879370162796904, test loss: 1.1072613123492243, acc: 0.5930540827147401
epoch: 29, train loss: 1.035103545374711, acc: 0.6162146731373109, val loss: 1.0136969907143163, acc: 0.6124899919935949, test loss: 1.0336935590964487, acc: 0.6134676564156946
epoch: 30, train loss: 1.0099573144666338, acc: 0.6266057664858693, val loss: 1.032345421956258, acc: 0.6036829463570856, test loss: 1.0537735538543345, acc: 0.6132025450689289
epoch: 31, train loss: 1.009320436958718, acc: 0.6208963745361119, val loss: 1.0873062509455806, acc: 0.596210301574593, test loss: 1.094431304223717, acc: 0.5999469777306469
epoch: 32, train loss: 0.9967134013095652, acc: 0.6312303739651727, val loss: 1.1480394514968053, acc: 0.585001334400854, test loss: 1.1773509000796403, acc: 0.5795334040296924
epoch: 33, train loss: 0.9951757486500197, acc: 0.6282614901512988, val loss: 1.0586179299198026, acc: 0.6092874299439551, test loss: 1.0792554496833906, acc: 0.6155885471898197
epoch: 34, train loss: 0.9662777004124878, acc: 0.635341136168998, val loss: 1.0194950119030772, acc: 0.618628235922071, test loss: 1.032433647618946, acc: 0.6139978791092259
epoch: 35, train loss: 0.9786831086919949, acc: 0.6309449043676848, val loss: 1.0931448463697513, acc: 0.5882038964504938, test loss: 1.1100026084661232, acc: 0.5975609756097561
epoch: 36, train loss: 0.9399798410945438, acc: 0.6451612903225806, val loss: 1.2164209310869105, acc: 0.5671203629570323, test loss: 1.2353214871213394, acc: 0.5649522799575822
epoch: 37, train loss: 0.9481669206058438, acc: 0.6422495004282044, val loss: 1.0794077527850732, acc: 0.6018147851614625, test loss: 1.1213095461197142, acc: 0.6092258748674443
epoch: 38, train loss: 0.9656888748147711, acc: 0.6396802740508136, val loss: 1.0940715075493623, acc: 0.5767280491059514, test loss: 1.1001915575211525, acc: 0.5909331919406151
epoch: 39, train loss: 0.9509113247177446, acc: 0.638024550385384, val loss: 1.028357251154317, acc: 0.6159594342140379, test loss: 1.0449577490320134, acc: 0.6142629904559915
epoch: 40, train loss: 0.9350789348840237, acc: 0.6479588923779618, val loss: 1.029000150257026, acc: 0.6052842273819056, test loss: 1.0396188900806997, acc: 0.6134676564156946
epoch: 41, train loss: 0.9310694892441175, acc: 0.644990008564088, val loss: 1.2199310592756227, acc: 0.5612489991993594, test loss: 1.2697171864049432, acc: 0.5503711558854719
epoch: 42, train loss: 0.9172073846914208, acc: 0.6524693120182701, val loss: 1.0061934461331477, acc: 0.622097678142514, test loss: 1.0162221222389034, acc: 0.6304347826086957
epoch: 43, train loss: 0.9066998384082586, acc: 0.6585783614045104, val loss: 1.0786406784525928, acc: 0.6034160661862824, test loss: 1.093612583672128, acc: 0.6047189819724285
epoch: 44, train loss: 0.9072083401537064, acc: 0.657836140451042, val loss: 1.0206631868401623, acc: 0.6191619962636776, test loss: 1.027861659119769, acc: 0.6248674443266172
epoch: 45, train loss: 0.8885027854911538, acc: 0.6625749357693406, val loss: 0.9640624007130102, acc: 0.6466506538564185, test loss: 1.0000235121677434, acc: 0.6389183457051962
epoch: 46, train loss: 0.8701057296700386, acc: 0.6674279189266343, val loss: 0.9798952110423894, acc: 0.6349079263410728, test loss: 1.0044644662709403, acc: 0.63016967126193
epoch: 47, train loss: 0.875364595428726, acc: 0.6674279189266343, val loss: 0.9944722716700786, acc: 0.6247664798505471, test loss: 1.0090900228486208, acc: 0.6275185577942736
epoch: 48, train loss: 0.8750074024170492, acc: 0.6634313445618042, val loss: 1.0107641942284094, acc: 0.6231651988257273, test loss: 1.0394495056896675, acc: 0.6134676564156946
epoch: 49, train loss: 0.8808345156467339, acc: 0.6677133885241222, val loss: 1.1592341094199008, acc: 0.5871363757672805, test loss: 1.1656859616333202, acc: 0.5805938494167551
epoch: 50, train loss: 0.8747378625360652, acc: 0.669083642592064, val loss: 0.9496184381001912, acc: 0.6439818521483853, test loss: 0.9830676288139529, acc: 0.6481972428419936
epoch: 51, train loss: 0.8753075775812941, acc: 0.6698258635455324, val loss: 1.2605138775632323, acc: 0.5380304243394716, test loss: 1.277617847426446, acc: 0.5463944856839873
epoch: 52, train loss: 0.9373948851279656, acc: 0.6413359977162432, val loss: 0.9420307309970495, acc: 0.6485188150520417, test loss: 0.9550288641313733, acc: 0.644220572640509
epoch: 53, train loss: 0.8543280201063066, acc: 0.6759920068512704, val loss: 0.996237851919923, acc: 0.6389111289031225, test loss: 1.0032917998122763, acc: 0.6386532343584306
epoch: 54, train loss: 0.8525263409562837, acc: 0.6727376534399087, val loss: 0.9610853514608969, acc: 0.6498532159060582, test loss: 0.967306925914194, acc: 0.6373276776246023
epoch: 55, train loss: 0.8337781142242425, acc: 0.681130459606052, val loss: 0.9476079027717006, acc: 0.6487856952228449, test loss: 0.9694439017254374, acc: 0.6508483563096501
epoch: 56, train loss: 0.8443759936759583, acc: 0.676848415643734, val loss: 0.9780677822007158, acc: 0.6285028022417934, test loss: 0.9958740999943884, acc: 0.6216861081654295
epoch: 57, train loss: 0.9038283264368151, acc: 0.6582928918070226, val loss: 1.0461728301148814, acc: 0.6202295169468909, test loss: 1.0509608418918743, acc: 0.6219512195121951
epoch: 58, train loss: 0.8500192267296487, acc: 0.6734798743933771, val loss: 1.026627911164788, acc: 0.6242327195089404, test loss: 1.0280771270662956, acc: 0.6259278897136797
epoch: 59, train loss: 0.8252754149830345, acc: 0.6782757636311733, val loss: 0.9850420537617227, acc: 0.6373098478783027, test loss: 1.0076981010517918, acc: 0.6344114528101803
epoch 60, gamma increased to 2.
epoch: 60, train loss: 0.6295574188810943, acc: 0.7020839280616614, val loss: 0.7516077940451931, acc: 0.6754737123031759, test loss: 0.7665560382042053, acc: 0.6699363732767762
epoch: 61, train loss: 0.5869962996936954, acc: 0.7164715957750499, val loss: 0.806599828571455, acc: 0.6530557779556979, test loss: 0.8310141260019669, acc: 0.6521739130434783
epoch: 62, train loss: 0.5776237544896362, acc: 0.7173850984870112, val loss: 0.756781271816668, acc: 0.6736055511075527, test loss: 0.7746207046609921, acc: 0.6694061505832449
epoch: 63, train loss: 0.5530395864385419, acc: 0.7286896945475307, val loss: 0.7667972660395569, acc: 0.6658660261542567, test loss: 0.7912682062242089, acc: 0.6694061505832449
epoch: 64, train loss: 0.5669536602503907, acc: 0.7221238938053097, val loss: 0.7852483447914541, acc: 0.6583933813717641, test loss: 0.7946824440900493, acc: 0.6683457051961824
epoch: 65, train loss: 0.5490120628000564, acc: 0.7261775620896375, val loss: 0.8024137345892542, acc: 0.6655991459834534, test loss: 0.8208460954441624, acc: 0.6601272534464475
epoch: 66, train loss: 0.5435321191940994, acc: 0.7316585783614045, val loss: 0.7595616652547502, acc: 0.6776087536696024, test loss: 0.7890147496635057, acc: 0.6678154825026511
epoch: 67, train loss: 0.5837092313295495, acc: 0.7204110762203826, val loss: 0.8441887596686236, acc: 0.6349079263410728, test loss: 0.8743555004750937, acc: 0.6413043478260869
epoch: 68, train loss: 0.5683333749264743, acc: 0.7162432200970597, val loss: 0.7746207369210595, acc: 0.6672004270082733, test loss: 0.8084828168438196, acc: 0.6585365853658537
epoch: 69, train loss: 0.5268224896378698, acc: 0.7326862689123609, val loss: 0.7698445512925843, acc: 0.6698692287163064, test loss: 0.803592706914924, acc: 0.6614528101802757
epoch: 70, train loss: 0.5362463065940177, acc: 0.7351413074507565, val loss: 0.7786135475318019, acc: 0.6690685882038965, test loss: 0.7985192303440098, acc: 0.6625132555673383
epoch: 71, train loss: 0.5279398671974294, acc: 0.7367970311161861, val loss: 0.7677572132397691, acc: 0.6637309847878302, test loss: 0.7986808761180016, acc: 0.66118769883351
epoch: 72, train loss: 0.5149363494892648, acc: 0.7390236939765915, val loss: 0.7904875264474478, acc: 0.6728049105951428, test loss: 0.8306106540077185, acc: 0.6511134676564156
epoch: 73, train loss: 0.5174916306496211, acc: 0.7381672851841279, val loss: 0.8445475882837414, acc: 0.6437149719775821, test loss: 0.8481752859825546, acc: 0.6450159066808059
epoch: 74, train loss: 0.5244674114714069, acc: 0.7349129317727662, val loss: 0.792608735082243, acc: 0.674673071790766, test loss: 0.8248308979188145, acc: 0.6678154825026511
epoch: 75, train loss: 0.5160292156587285, acc: 0.7380530973451327, val loss: 0.7635890841961289, acc: 0.6837469975980784, test loss: 0.7813753388972197, acc: 0.6686108165429481
epoch: 76, train loss: 0.5022458949294595, acc: 0.741364544675992, val loss: 0.7790876851897575, acc: 0.6621297037630104, test loss: 0.7925665721155047, acc: 0.6702014846235419
epoch: 77, train loss: 0.505269891985001, acc: 0.7433628318584071, val loss: 0.7576456437404868, acc: 0.6749399519615693, test loss: 0.7995810359558396, acc: 0.665694591728526
epoch: 78, train loss: 0.49429841330144664, acc: 0.7435912075363974, val loss: 0.7883672403086653, acc: 0.6776087536696024, test loss: 0.8125281387017615, acc: 0.6741781548250265
epoch: 79, train loss: 0.47426526354205223, acc: 0.7496431630031402, val loss: 0.7933543424209277, acc: 0.6818788364024553, test loss: 0.8062112264390462, acc: 0.6784199363732768
epoch: 80, train loss: 0.47904989307279966, acc: 0.7473023123037397, val loss: 0.8072462825734423, acc: 0.6677341873498799, test loss: 0.8257380199331241, acc: 0.6614528101802757
epoch: 81, train loss: 0.4969549686712025, acc: 0.7443334284898658, val loss: 0.786815194374789, acc: 0.6802775553776355, test loss: 0.8048422361861357, acc: 0.6694061505832449
epoch: 82, train loss: 0.46568745245022874, acc: 0.7504995717956038, val loss: 0.8285602465737238, acc: 0.6680010675206832, test loss: 0.8625147471625125, acc: 0.6651643690349947
epoch: 83, train loss: 0.47748762905648734, acc: 0.7479874393377105, val loss: 0.7746341855550786, acc: 0.6789431545236189, test loss: 0.8180438559482611, acc: 0.6699363732767762
epoch: 84, train loss: 0.46311460588613373, acc: 0.7500999143591207, val loss: 0.791976505704075, acc: 0.6813450760608487, test loss: 0.8456100095241219, acc: 0.6728525980911984
epoch: 85, train loss: 0.45204069278528103, acc: 0.7591207536397374, val loss: 0.8112381912722154, acc: 0.6701361088871097, test loss: 0.843349029757438, acc: 0.6590668080593849
epoch: 86, train loss: 0.47684584363950444, acc: 0.7467884670282615, val loss: 0.8735951497963915, acc: 0.6621297037630104, test loss: 0.9493962317736758, acc: 0.6489925768822906
epoch: 87, train loss: 0.4697030060688768, acc: 0.7532400799314873, val loss: 0.8202045372017931, acc: 0.6599946623965839, test loss: 0.8653333529687762, acc: 0.6564156945917285
epoch: 88, train loss: 0.4844614005279378, acc: 0.7420496717099629, val loss: 0.8479791649634405, acc: 0.6551908193221244, test loss: 0.8701230981569907, acc: 0.6500530222693531
epoch: 89, train loss: 0.4486047445111298, acc: 0.7599200685127034, val loss: 0.8311373563267117, acc: 0.6685348278622898, test loss: 0.8455449799085599, acc: 0.6694061505832449
epoch: 90, train loss: 0.4466415971509055, acc: 0.760376819868684, val loss: 0.7943559537291495, acc: 0.6768081131571925, test loss: 0.8184411270368137, acc: 0.6741781548250265
epoch: 91, train loss: 0.4425595969225453, acc: 0.7602055381101913, val loss: 0.8034364028647641, acc: 0.682412596744062, test loss: 0.8382731619214961, acc: 0.6712619300106044
epoch: 92, train loss: 0.42151799843953125, acc: 0.7656865543819583, val loss: 0.8282704570308062, acc: 0.685882038964505, test loss: 0.8605714597732366, acc: 0.6776246023329798
epoch: 93, train loss: 0.44340909352276686, acc: 0.7579217813302883, val loss: 0.8585387328035646, acc: 0.6728049105951428, test loss: 0.8936362079446415, acc: 0.6590668080593849
epoch: 94, train loss: 0.4264091803079464, acc: 0.76306023408507, val loss: 0.8355935826313028, acc: 0.6802775553776355, test loss: 0.8950327605735965, acc: 0.6691410392364793
epoch: 95, train loss: 0.4402563038618946, acc: 0.7598058806737082, val loss: 0.8153412490009403, acc: 0.6797437950360288, test loss: 0.8424681586905729, acc: 0.6739130434782609
epoch: 96, train loss: 0.42452226035124635, acc: 0.7655152726234656, val loss: 0.8505522942651199, acc: 0.6698692287163064, test loss: 0.8654671526915954, acc: 0.6585365853658537
epoch: 97, train loss: 0.43387261785815656, acc: 0.7630031401655724, val loss: 0.8313572937118107, acc: 0.6698692287163064, test loss: 0.8716937026694369, acc: 0.6720572640509014
epoch: 98, train loss: 0.428275596363831, acc: 0.7636882671995433, val loss: 0.9053660960460873, acc: 0.6493194555644516, test loss: 0.9433939327996457, acc: 0.6405090137857901
epoch: 99, train loss: 0.427729604678463, acc: 0.7602626320296888, val loss: 0.8879725737097043, acc: 0.6639978649586336, test loss: 0.8924844295323717, acc: 0.6534994697773064
epoch 100, gamma increased to 3.
epoch: 100, train loss: 0.35469026118049, acc: 0.7531258920924921, val loss: 0.7021838082037896, acc: 0.6752068321323725, test loss: 0.7429466236181249, acc: 0.6654294803817603
best val loss 0.7021838082037896 at epoch 100.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.8965    0.9052    0.9008      5337
           1     0.6412    0.7170    0.6770      2502
           2     0.9627    0.7642    0.8520       810
           3     0.7238    0.8103    0.7646      1840
           4     0.9179    0.7734    0.8395       737
           5     0.8070    0.9572    0.8757       677
           6     0.6829    0.9214    0.7844      1323
           7     0.5597    0.6152    0.5861       907
           8     0.8338    0.7268    0.7766       421
           9     0.7523    0.8180    0.7838       401
          10     0.7162    0.9495    0.8165       396
          11     0.9692    0.9488    0.9589       332
          12     0.7362    0.5864    0.6528       295
          13     0.7167    0.7216    0.7192       291
          14     0.0000    0.0000    0.0000       261
          15     0.8224    0.1781    0.2928       494
          16     0.0000    0.0000    0.0000       256
          17     0.8923    0.4936    0.6356       235

    accuracy                         0.7789     17515
   macro avg     0.7017    0.6604    0.6620     17515
weighted avg     0.7670    0.7789    0.7627     17515

train confusion matrix:
[[4831  165    4   27    1    0   25   14   60   87   50    1    0   65
     0    1    0    6]
 [ 123 1794    1  209    7    0  316   37    1    3    3    1    4    1
     0    1    0    1]
 [   3   10  619    3    0  138   23    1    0    6    0    0    6    1
     0    0    0    0]
 [  88  201    0 1491    2    0    3   34    0    2    4    2    0    0
     0   12    0    1]
 [   1   63    6   22  570    0   14   21    0    0    7    0   31    2
     0    0    0    0]
 [   0    0    3    0    0  648   26    0    0    0    0    0    0    0
     0    0    0    0]
 [   4   69    3    0    1   14 1219    1    0    2    1    0    2    7
     0    0    0    0]
 [  76  166    1   12   19    0   19  558    0    0   32    0   14    3
     0    1    1    5]
 [  95    9    0    0    0    0    5    2  306    0    0    1    0    2
     0    1    0    0]
 [  44   18    0    0    0    0    7    0    0  328    0    2    0    1
     0    1    0    0]
 [  12    3    0    0    1    0    0    4    0    0  376    0    0    0
     0    0    0    0]
 [   1    7    1    5    0    0    0    0    0    3    0  315    0    0
     0    0    0    0]
 [   1   21    4    0   19    1   12   43    0    0   21    0  173    0
     0    0    0    0]
 [  28   21    0    1    0    0   18    0    0    4    1    3    3  210
     0    2    0    0]
 [  37  160    0   44    0    0    2   11    0    0    5    0    2    0
     0    0    0    0]
 [   2   68    0  241    0    1   94    0    0    0    0    0    0    0
     0   88    0    0]
 [  15   20    0    4    0    1    2  196    0    1   15    0    0    1
     0    0    0    1]
 [  28    3    1    1    1    0    0   75    0    0   10    0    0    0
     0    0    0  116]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8267    0.8303    0.8285      1143
           1     0.5217    0.6045    0.5601       536
           2     0.8750    0.6474    0.7442       173
           3     0.6112    0.7183    0.6604       394
           4     0.8031    0.6456    0.7158       158
           5     0.7500    0.8690    0.8051       145
           6     0.5960    0.8445    0.6988       283
           7     0.3676    0.3866    0.3769       194
           8     0.6883    0.5889    0.6347        90
           9     0.5926    0.5647    0.5783        85
          10     0.5923    0.9167    0.7196        84
          11     0.9500    0.8028    0.8702        71
          12     0.6154    0.3810    0.4706        63
          13     0.5000    0.5000    0.5000        62
          14     0.0000    0.0000    0.0000        56
          15     0.7273    0.0762    0.1379       105
          16     0.0000    0.0000    0.0000        55
          17     0.8148    0.4400    0.5714        50

    accuracy                         0.6752      3747
   macro avg     0.6018    0.5453    0.5485      3747
weighted avg     0.6695    0.6752    0.6589      3747

validation confusion matrix:
[[949  51   3  29   0   2  11  22  22  18  20   0   0  16   0   0   0   0]
 [ 42 324   6  64   2   2  61  22   1   2   5   0   2   2   0   0   0   1]
 [  6   5 112   4   0  30  10   1   0   2   0   0   1   2   0   0   0   0]
 [ 23  59   1 283   2   0   3  17   0   0   4   0   0   1   0   1   0   0]
 [  0  25   2   8 102   0   7   7   0   0   1   0   4   2   0   0   0   0]
 [  1   4   0   0   0 126  14   0   0   0   0   0   0   0   0   0   0   0]
 [  4  23   1   1   3   7 239   0   0   3   1   0   0   0   0   1   0   0]
 [ 33  34   0  11   6   0  11  75   1   1  14   0   6   0   0   0   0   2]
 [ 21   4   0   3   0   0   4   2  53   0   0   0   0   3   0   0   0   0]
 [ 15  10   3   0   0   0   6   1   0  48   0   0   0   2   0   0   0   0]
 [  4   2   0   0   0   0   0   1   0   0  77   0   0   0   0   0   0   0]
 [  2   5   0   1   0   0   0   0   0   5   0  57   0   0   0   1   0   0]
 [  3  15   0   1   6   0   3   8   0   0   3   0  24   0   0   0   0   0]
 [ 12   7   0   1   0   0   6   1   0   2   0   1   1  31   0   0   0   0]
 [ 11  24   0  12   0   0   0   6   0   0   2   0   0   1   0   0   0   0]
 [  6  20   0  42   0   1  25   1   0   0   0   2   0   0   0   8   0   0]
 [  4   9   0   2   4   0   1  28   0   0   2   0   1   2   0   0   0   2]
 [ 12   0   0   1   2   0   0  12   0   0   1   0   0   0   0   0   0  22]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8240    0.8140    0.8190      1145
           1     0.4962    0.6052    0.5453       537
           2     0.9350    0.6571    0.7718       175
           3     0.5978    0.6734    0.6333       395
           4     0.8115    0.6226    0.7046       159
           5     0.7410    0.8425    0.7885       146
           6     0.6111    0.8521    0.7118       284
           7     0.3961    0.4205    0.4080       195
           8     0.6897    0.6593    0.6742        91
           9     0.5357    0.5172    0.5263        87
          10     0.5274    0.8953    0.6638        86
          11     0.9492    0.7778    0.8550        72
          12     0.6429    0.4219    0.5094        64
          13     0.4576    0.4219    0.4390        64
          14     0.0000    0.0000    0.0000        57
          15     0.6400    0.1495    0.2424       107
          16     0.0000    0.0000    0.0000        56
          17     0.7200    0.3462    0.4675        52

    accuracy                         0.6654      3772
   macro avg     0.5875    0.5376    0.5422      3772
weighted avg     0.6620    0.6654    0.6525      3772

test confusion matrix:
[[932  65   2  21   2   3   6  23  24  18  27   0   0  19   0   1   0   2]
 [ 42 325   1  69   6   1  69   8   2   3   2   2   3   3   0   0   0   1]
 [  3   9 115   4   0  27   7   0   0   5   0   0   0   4   0   1   0   0]
 [ 35  63   1 266   1   0   3  18   0   0   5   1   0   1   0   1   0   0]
 [  3  23   0   8  99   1   6   9   0   1   4   0   4   1   0   0   0   0]
 [  0   4   0   1   0 123  16   0   0   0   0   0   0   0   0   2   0   0]
 [  5  24   0   2   2   6 242   0   0   3   0   0   0   0   0   0   0   0]
 [ 24  38   0   8   6   2  13  82   0   0  13   0   5   1   0   1   0   2]
 [ 17   6   0   0   0   0   3   2  60   1   1   0   0   1   0   0   0   0]
 [ 22   7   1   1   1   1   5   1   0  45   0   0   1   2   0   0   0   0]
 [  4   0   0   0   0   0   1   3   0   0  77   0   0   0   0   1   0   0]
 [  4   5   0   3   0   0   0   0   0   4   0  56   0   0   0   0   0   0]
 [  3  11   1   0   3   0   2  10   0   0   6   0  27   0   0   1   0   0]
 [  7  13   2   1   1   1   3   1   0   4   2   0   1  27   0   1   0   0]
 [ 10  27   0  10   0   0   2   4   1   0   2   0   1   0   0   0   0   0]
 [  5  27   0  41   0   0  17   1   0   0   0   0   0   0   0  16   0   0]
 [  7   6   0   9   1   0   0  29   0   0   2   0   0   0   0   0   0   2]
 [  8   2   0   1   0   1   1  16   0   0   5   0   0   0   0   0   0  18]]
---------------------------------------
program finished.
