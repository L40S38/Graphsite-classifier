seed:  6
save trained model at:  ../trained_models/trained_classifier_model_46.pt
save loss at:  ./results/train_classifier_results_46.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['2y3zA03', '2z02B00', '5nwlH00', '3n3xA02', '1rgcA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['6mn8A02', '5eomI01', '3u4oB00', '6azrC00', '1rffC01']
model architecture:
DeepDruG(
  (embedding_net): JKMCNMMEmbeddingNet(
    (conv0): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=33, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b9e5672fb20>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0065518459709804, acc: 0.3974783946963419; test loss: 1.8261118109303427, acc: 0.43228551169936186
epoch: 2, train loss: 1.70902511705742, acc: 0.48022966733751626; test loss: 1.7853460041971192, acc: 0.44221224296856537
epoch: 3, train loss: 1.613124855936131, acc: 0.5062744169527643; test loss: 1.5704229100605354, acc: 0.5192625856771449
epoch: 4, train loss: 1.5181396752260727, acc: 0.5327927074701078; test loss: 1.5536319260562292, acc: 0.5228078468447176
epoch: 5, train loss: 1.458219601555634, acc: 0.5593109979874512; test loss: 1.5609430620445481, acc: 0.5402978019380761
epoch: 6, train loss: 1.404157572516862, acc: 0.5726293358588848; test loss: 1.319003591467601, acc: 0.5882770030725597
epoch: 7, train loss: 1.370532457597357, acc: 0.5842310879602226; test loss: 1.5842269847534476, acc: 0.499172772394233
epoch: 8, train loss: 1.2793341316922227, acc: 0.6128211199242335; test loss: 1.3166504639932322, acc: 0.6052942566769085
epoch: 9, train loss: 1.2200290655347459, acc: 0.6316443707825263; test loss: 1.3125811072281703, acc: 0.6156936894351217
epoch: 10, train loss: 1.1948736586118571, acc: 0.6452586717177696; test loss: 1.194431425070712, acc: 0.6322382415504609
epoch: 11, train loss: 1.146108887979402, acc: 0.6570380016573931; test loss: 1.164318728238464, acc: 0.6424013235641692
epoch: 12, train loss: 1.0986831443216922, acc: 0.6707114952053984; test loss: 1.1991909866607997, acc: 0.6303474355944221
epoch: 13, train loss: 1.091549350324292, acc: 0.6746774002604475; test loss: 1.11209067490275, acc: 0.6598912786575277
epoch: 14, train loss: 1.0677231474714166, acc: 0.6799455427962591; test loss: 1.1443957586543156, acc: 0.6409832190971402
epoch: 15, train loss: 1.0486090834414281, acc: 0.6860423819107375; test loss: 1.0732228468563842, acc: 0.6709997636492555
epoch: 16, train loss: 0.9974531791358922, acc: 0.6975257487865515; test loss: 1.127710495793039, acc: 0.6634365398251004
epoch: 17, train loss: 0.9958590575050317, acc: 0.7001894163608382; test loss: 1.224137708442596, acc: 0.632001890805956
epoch: 18, train loss: 1.0174708257585754, acc: 0.6945069255356932; test loss: 1.0624853211693437, acc: 0.6584731741904987
epoch: 19, train loss: 0.9896754550132241, acc: 0.7028530839351249; test loss: 1.2306898125968795, acc: 0.63365634601749
epoch: 20, train loss: 0.9728065255098376, acc: 0.7064046407008405; test loss: 1.053800790086195, acc: 0.67170881588277
epoch: 21, train loss: 0.974584316267436, acc: 0.7071149520539837; test loss: 1.1415556118973489, acc: 0.6487827936658
epoch: 22, train loss: 0.9397752691799894, acc: 0.7187167041553214; test loss: 1.1570353784112608, acc: 0.635074450484519
epoch: 23, train loss: 0.9193364106594097, acc: 0.7235704984017994; test loss: 0.9648735407142035, acc: 0.7057433230914677
epoch: 24, train loss: 0.8844393828472076, acc: 0.73126553806085; test loss: 1.0477035082251545, acc: 0.6837627038525171
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.6893177858552835, acc: 0.7335740499585651; test loss: 0.7860028540882426, acc: 0.6922713306546916
epoch: 26, train loss: 0.679401854071883, acc: 0.7383094589795194; test loss: 0.7874910088189525, acc: 0.7050342708579532
epoch: 27, train loss: 0.671741131651228, acc: 0.738605422043329; test loss: 0.7851744698903376, acc: 0.7029071141574096
epoch: 28, train loss: 0.6659610463949975, acc: 0.7439919498046644; test loss: 0.7598555294285708, acc: 0.7159064051051761
epoch: 29, train loss: 0.6527991894858821, acc: 0.7455901503492364; test loss: 0.7513441807326062, acc: 0.7071614275584968
epoch: 30, train loss: 0.6539834348904504, acc: 0.7498520184680951; test loss: 0.8101263450578831, acc: 0.6943984873552351
epoch: 31, train loss: 0.6540894621897834, acc: 0.7482538179235232; test loss: 0.7797552587517721, acc: 0.705270621602458
epoch: 32, train loss: 0.633618178587207, acc: 0.7520421451402864; test loss: 0.8885583019346752, acc: 0.6813991964074687
epoch: 33, train loss: 0.6427021574815999, acc: 0.7509174854978099; test loss: 0.7412670761438387, acc: 0.7182699125502245
epoch: 34, train loss: 0.6301603486682729, acc: 0.7564815910974311; test loss: 0.795111358377055, acc: 0.696289293311274
epoch: 35, train loss: 0.61123637865461, acc: 0.7618089262460045; test loss: 0.7861646634978454, acc: 0.696289293311274
epoch: 36, train loss: 0.6140739281981658, acc: 0.7610986148928613; test loss: 0.7607004999184659, acc: 0.7142519498936422
epoch: 37, train loss: 0.5961876546008581, acc: 0.7677873801349592; test loss: 0.7650837467173768, acc: 0.7097612857480501
epoch: 38, train loss: 0.5896556023184161, acc: 0.7691488102284835; test loss: 0.8250397377559632, acc: 0.6988891515008272
epoch: 39, train loss: 0.6032641911853823, acc: 0.7675506096839114; test loss: 0.7539054026025543, acc: 0.7130701961711179
epoch: 40, train loss: 0.5806830160234248, acc: 0.7745945306025808; test loss: 0.827428955582123, acc: 0.6861262112975656
epoch: 41, train loss: 0.5724533188973883, acc: 0.7788563987214395; test loss: 0.8388673137249154, acc: 0.6967619948002837
epoch: 42, train loss: 0.576245137138509, acc: 0.7763111163726767; test loss: 0.7933330545749994, acc: 0.7019617111793902
epoch: 43, train loss: 0.5512673464966791, acc: 0.7845980821593466; test loss: 0.7547430003740296, acc: 0.71756086031671
epoch: 44, train loss: 0.5638168368083254, acc: 0.7792115543980112; test loss: 0.7597050006382292, acc: 0.7050342708579532
Epoch    44: reducing learning rate of group 0 to 1.5000e-03.
epoch: 45, train loss: 0.4727059750390699, acc: 0.8131881141233575; test loss: 0.6234981261987941, acc: 0.7634129047506499
epoch: 46, train loss: 0.4232982188671372, acc: 0.8263880667692672; test loss: 0.695033423107754, acc: 0.7442684944457575
epoch: 47, train loss: 0.4203774058051091, acc: 0.8307683201136499; test loss: 0.7055482423156183, acc: 0.7416686362562042
epoch: 48, train loss: 0.42605090019575054, acc: 0.8266840298330769; test loss: 0.6214404367606033, acc: 0.7593949420940675
epoch: 49, train loss: 0.3936731980738025, acc: 0.8397064046407008; test loss: 0.6750514579985728, acc: 0.767903568896242
epoch: 50, train loss: 0.39412048870435873, acc: 0.8407718716704156; test loss: 0.691332560232474, acc: 0.7523044197589223
epoch: 51, train loss: 0.3937384590137861, acc: 0.839588019415177; test loss: 0.6351899312084038, acc: 0.7723942330418341
epoch: 52, train loss: 0.39686216244094114, acc: 0.8411270273469871; test loss: 0.7273453549272595, acc: 0.7459229496572914
epoch: 53, train loss: 0.41029144931689115, acc: 0.8344382621048894; test loss: 0.6736740346947789, acc: 0.7508863152918932
epoch: 54, train loss: 0.37206781888496415, acc: 0.8479341778146088; test loss: 0.6604750457595522, acc: 0.7693216733632711
epoch: 55, train loss: 0.3965076281782493, acc: 0.8392920563513674; test loss: 0.6770752840531236, acc: 0.7596312928385724
epoch: 56, train loss: 0.37528127999902405, acc: 0.8487628743932757; test loss: 0.6406785743621415, acc: 0.7690853226187663
epoch: 57, train loss: 0.35627192091247495, acc: 0.8539718243163253; test loss: 0.6447045168282715, acc: 0.7721578822973293
epoch: 58, train loss: 0.33982867167957637, acc: 0.8633242571327099; test loss: 0.6164781449957396, acc: 0.7733396360198534
epoch: 59, train loss: 0.32749268733299286, acc: 0.8646856872262342; test loss: 0.662033313190196, acc: 0.7771212479319309
epoch: 60, train loss: 0.3231843068456599, acc: 0.8662246951580442; test loss: 0.6811483628753792, acc: 0.7617584495391161
epoch: 61, train loss: 0.32450462090442683, acc: 0.8645673020007103; test loss: 0.7017461448323217, acc: 0.7705034270857953
epoch: 62, train loss: 0.31247439758938783, acc: 0.8709601041789985; test loss: 0.7042332003229872, acc: 0.7686126211297566
epoch: 63, train loss: 0.32791865009929605, acc: 0.8649816502900438; test loss: 0.6635214156559021, acc: 0.7650673599621839
epoch: 64, train loss: 0.31856428441160795, acc: 0.8674677400260448; test loss: 0.6751258793634547, acc: 0.7761758449539116
epoch: 65, train loss: 0.31273948720246075, acc: 0.8665206582218539; test loss: 0.6700993187741393, acc: 0.7662491136847082
epoch: 66, train loss: 0.29266675321663765, acc: 0.8773529063572866; test loss: 0.7780490511558603, acc: 0.7515953675254077
epoch: 67, train loss: 0.2927108165412538, acc: 0.8760506688765242; test loss: 0.7838050640663394, acc: 0.7494682108248641
epoch: 68, train loss: 0.3112097998102614, acc: 0.8682964366047118; test loss: 0.6325978434021043, acc: 0.784684471756086
epoch: 69, train loss: 0.2789587653892004, acc: 0.8793062625784303; test loss: 0.6717290793206782, acc: 0.7742850389978728
epoch: 70, train loss: 0.29211650075214746, acc: 0.8775304841955724; test loss: 0.7278696610498755, acc: 0.7570314346490191
epoch: 71, train loss: 0.2927780473699397, acc: 0.8764058245530958; test loss: 0.631033648612848, acc: 0.7927203970692508
epoch: 72, train loss: 0.2852846453976487, acc: 0.8797798034805256; test loss: 0.7630209721619569, acc: 0.7645946584731742
epoch: 73, train loss: 0.2772417425089677, acc: 0.8837457085355748; test loss: 0.6339992527926793, acc: 0.7835027180335618
epoch: 74, train loss: 0.2683426068882498, acc: 0.8866461465609092; test loss: 0.7527637350956505, acc: 0.7508863152918932
epoch: 75, train loss: 0.2769468355470964, acc: 0.8831537824079555; test loss: 0.7471276458852033, acc: 0.7575041361380288
epoch: 76, train loss: 0.26483839085056354, acc: 0.8857582573694803; test loss: 0.6223990421322214, acc: 0.7865752777121248
epoch: 77, train loss: 0.24364083217166085, acc: 0.8968272759559607; test loss: 0.7046123950708507, acc: 0.7693216733632711
epoch: 78, train loss: 0.2609470719569701, acc: 0.888421924943767; test loss: 0.7420860351160383, acc: 0.7463956511463011
epoch: 79, train loss: 0.2559869166336159, acc: 0.8914999408073873; test loss: 0.685225713143138, acc: 0.7783030016544552
epoch: 80, train loss: 0.2372804291328481, acc: 0.8988990174026281; test loss: 0.715906322213725, acc: 0.7714488300638147
epoch: 81, train loss: 0.2556979863392724, acc: 0.8914407481946253; test loss: 0.7003576827370452, acc: 0.7823209643110376
epoch: 82, train loss: 0.24588224376653572, acc: 0.8960577719900557; test loss: 0.6704322976498028, acc: 0.7742850389978728
epoch: 83, train loss: 0.21852163550620024, acc: 0.905587782644726; test loss: 0.6983609064001193, acc: 0.784684471756086
epoch: 84, train loss: 0.22431792549209453, acc: 0.9026873446193915; test loss: 0.814169620177166, acc: 0.7608130465610967
epoch: 85, train loss: 0.22785480903128386, acc: 0.8986030543388185; test loss: 0.7584697003906654, acc: 0.770976128574805
epoch: 86, train loss: 0.22036547262040315, acc: 0.9036936190363443; test loss: 0.816362910280834, acc: 0.7494682108248641
epoch: 87, train loss: 0.25697033699834304, acc: 0.8870013022374807; test loss: 0.7719196388552978, acc: 0.7504136138028835
epoch: 88, train loss: 0.22452031206143344, acc: 0.9020362258790103; test loss: 0.7452586506365941, acc: 0.7712124793193098
epoch: 89, train loss: 0.24842983183316597, acc: 0.8946963418965314; test loss: 0.7437161174797612, acc: 0.7532498227369416
epoch: 90, train loss: 0.2809596819836416, acc: 0.8814963892506216; test loss: 0.7353198733405283, acc: 0.7655400614511936
epoch: 91, train loss: 0.19753458458064532, acc: 0.9115662365336806; test loss: 0.7552432950240053, acc: 0.7773575986764358
epoch: 92, train loss: 0.19581282166167663, acc: 0.9142890967207292; test loss: 0.7024623564416087, acc: 0.7820846135665327
epoch: 93, train loss: 0.1973052528785279, acc: 0.9144074819462531; test loss: 0.7931042908776936, acc: 0.7809028598440085
epoch: 94, train loss: 0.19641197747036565, acc: 0.9150586006866344; test loss: 0.7269332456239372, acc: 0.7820846135665327
epoch: 95, train loss: 0.1781611046036616, acc: 0.9209186693500652; test loss: 0.7599912365963205, acc: 0.7653037107066887
Epoch    95: reducing learning rate of group 0 to 7.5000e-04.
epoch: 96, train loss: 0.14351928732452188, acc: 0.9344737776725465; test loss: 0.7136427469215222, acc: 0.8007563223824155
epoch: 97, train loss: 0.10457860513052122, acc: 0.952764295015982; test loss: 0.7333208232364371, acc: 0.8005199716379107
epoch: 98, train loss: 0.09124744995745791, acc: 0.9571445483603647; test loss: 0.7778923225808555, acc: 0.8024107775939494
epoch: 99, train loss: 0.08067872937020287, acc: 0.962590268734462; test loss: 0.7940527210560174, acc: 0.8002836208934058
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.05937479548240055, acc: 0.962590268734462; test loss: 0.6960308228618737, acc: 0.7976837627038526
epoch: 101, train loss: 0.05847960343421021, acc: 0.9609920681898899; test loss: 0.6526516839365282, acc: 0.7993382179153864
epoch: 102, train loss: 0.06568860958252162, acc: 0.9580324375517936; test loss: 0.632350601814115, acc: 0.8024107775939494
epoch: 103, train loss: 0.06042402805774836, acc: 0.9609328755771279; test loss: 0.641922582325657, acc: 0.8002836208934058
epoch: 104, train loss: 0.07520039420699712, acc: 0.9562566591689358; test loss: 0.7519268295325954, acc: 0.7596312928385724
epoch: 105, train loss: 0.1161745760064584, acc: 0.9285545163963538; test loss: 0.5910577552590746, acc: 0.7922476955802411
epoch: 106, train loss: 0.06134795743808445, acc: 0.9589795193559844; test loss: 0.6441349010607278, acc: 0.7894114866461829
epoch: 107, train loss: 0.054983380378418115, acc: 0.9632413874748431; test loss: 0.6874639809878266, acc: 0.7861025762231151
epoch: 108, train loss: 0.05062762701318746, acc: 0.9643068545045579; test loss: 0.6674715384499341, acc: 0.795320255258804
epoch: 109, train loss: 0.05036902663750988, acc: 0.9676808334319877; test loss: 0.6654917422098292, acc: 0.7979201134483573
epoch: 110, train loss: 0.04842430569484655, acc: 0.9655498993725583; test loss: 0.6867329473051442, acc: 0.7967383597258332
epoch: 111, train loss: 0.054867754506333785, acc: 0.9644252397300817; test loss: 0.6875384982892792, acc: 0.7913022926022217
epoch: 112, train loss: 0.057751680235563124, acc: 0.9622943056706523; test loss: 0.7058156969642504, acc: 0.7809028598440085
epoch: 113, train loss: 0.05297975524500537, acc: 0.9651947436959868; test loss: 0.691632239482161, acc: 0.7872843299456393
epoch: 114, train loss: 0.05497099223755176, acc: 0.9629454244110335; test loss: 0.6939626986522309, acc: 0.7967383597258332
epoch: 115, train loss: 0.05641289447634139, acc: 0.9614064164792234; test loss: 0.6943802073108364, acc: 0.793902150791775
epoch: 116, train loss: 0.05756721214012125, acc: 0.9598082159346514; test loss: 0.695730271533262, acc: 0.7955566060033089
epoch: 117, train loss: 0.06630331256060935, acc: 0.9564342370072215; test loss: 0.6975651228013712, acc: 0.7868116284566297
epoch: 118, train loss: 0.0629831974993193, acc: 0.958505978453889; test loss: 0.685493575374276, acc: 0.787757031434649
epoch: 119, train loss: 0.06819895530108293, acc: 0.9555463478157926; test loss: 0.6950815867881802, acc: 0.7889387851571732
epoch: 120, train loss: 0.05145389186991267, acc: 0.9644844323428436; test loss: 0.6797037499082255, acc: 0.7913022926022217
epoch: 121, train loss: 0.061422198004001866, acc: 0.9604001420622706; test loss: 0.6226831425788751, acc: 0.790829591113212
epoch: 122, train loss: 0.05325998344930641, acc: 0.9634189653131289; test loss: 0.6570055543657906, acc: 0.7988655164263767
epoch: 123, train loss: 0.0491748268647569, acc: 0.9676216408192257; test loss: 0.6697230909316428, acc: 0.7962656582368235
epoch: 124, train loss: 0.04435240000846246, acc: 0.9691606487510359; test loss: 0.6956257242537706, acc: 0.7887024344126684
epoch: 125, train loss: 0.055181960228090104, acc: 0.9627086539599858; test loss: 0.6519419680360755, acc: 0.796974710470338
epoch: 126, train loss: 0.055920098974067696, acc: 0.9628862317982716; test loss: 0.671448968257557, acc: 0.8021744268494446
epoch: 127, train loss: 0.06242646096573929, acc: 0.9587427489049367; test loss: 0.660936561410688, acc: 0.7894114866461829
epoch: 128, train loss: 0.061217803060408665, acc: 0.9595714454836036; test loss: 0.6612740314139229, acc: 0.7924840463247459
epoch: 129, train loss: 0.05493850800163212, acc: 0.9628862317982716; test loss: 0.6392624421312408, acc: 0.7981564641928622
epoch: 130, train loss: 0.05081411502700565, acc: 0.9669705220788446; test loss: 0.6885900001271175, acc: 0.7820846135665327
epoch: 131, train loss: 0.09601055127996883, acc: 0.9412217355274062; test loss: 0.6620155127159503, acc: 0.7681399196407469
epoch: 132, train loss: 0.07565741211959169, acc: 0.9516988279862673; test loss: 0.669302342040013, acc: 0.793902150791775
epoch: 133, train loss: 0.07038452510838962, acc: 0.9530010654670297; test loss: 0.6524178537458033, acc: 0.7849208225005909
epoch: 134, train loss: 0.07366136908248937, acc: 0.9542441103350302; test loss: 0.62994803592855, acc: 0.7896478373906878
epoch: 135, train loss: 0.05319165444764298, acc: 0.9654907067597964; test loss: 0.7010226630341835, acc: 0.7853935239896006
epoch: 136, train loss: 0.046881030279601894, acc: 0.9698117674914171; test loss: 0.6993682324477781, acc: 0.7853935239896006
epoch: 137, train loss: 0.061531566179573456, acc: 0.9601633716112229; test loss: 0.6662597533124357, acc: 0.8019380761049397
epoch: 138, train loss: 0.04977804533868227, acc: 0.9669705220788446; test loss: 0.7823087030819645, acc: 0.7662491136847082
epoch: 139, train loss: 0.05803306747505308, acc: 0.9617023795430331; test loss: 0.6823950585843372, acc: 0.7875206806901441
epoch: 140, train loss: 0.04752130603929998, acc: 0.9667929442405587; test loss: 0.6660946949918455, acc: 0.7995745686598913
epoch: 141, train loss: 0.044335438634278974, acc: 0.9705220788445602; test loss: 0.664322466045597, acc: 0.8038288820609785
epoch: 142, train loss: 0.05019426301855568, acc: 0.9652539363087487; test loss: 0.6808194167730629, acc: 0.7882297329236587
epoch: 143, train loss: 0.064193870914597, acc: 0.9586835562921747; test loss: 0.6980518212529171, acc: 0.7901205388796975
epoch: 144, train loss: 0.06936058393946706, acc: 0.9550728069136972; test loss: 0.6318816887912556, acc: 0.80146537461593
epoch: 145, train loss: 0.04950015532516355, acc: 0.9654315141470344; test loss: 0.6693841320942604, acc: 0.7986291656818719
epoch: 146, train loss: 0.052187795647046994, acc: 0.9658458624363679; test loss: 0.6673883748826505, acc: 0.7950839045142992
epoch: 147, train loss: 0.05599822509009358, acc: 0.9633005800876051; test loss: 0.6885440242231329, acc: 0.7794847553769795
epoch: 148, train loss: 0.07408815943568194, acc: 0.9509293240203622; test loss: 0.6470749098988071, acc: 0.7995745686598913
epoch: 149, train loss: 0.047208249236410635, acc: 0.970699656682846; test loss: 0.683648764509536, acc: 0.7936658000472702
epoch: 150, train loss: 0.044559019120597294, acc: 0.9693382265893217; test loss: 0.6962515357481168, acc: 0.7931930985582605
epoch: 151, train loss: 0.06594743940997325, acc: 0.9589203267432225; test loss: 0.6590362341796724, acc: 0.7806665090995036
epoch: 152, train loss: 0.078275724748888, acc: 0.9492127382502664; test loss: 0.6460634067362292, acc: 0.7849208225005909
Epoch   152: reducing learning rate of group 0 to 3.7500e-04.
epoch: 153, train loss: 0.0375325598208117, acc: 0.9757902213803717; test loss: 0.6324565215923351, acc: 0.8033561805719688
epoch: 154, train loss: 0.020486357402441466, acc: 0.986977625192376; test loss: 0.6474219865760632, acc: 0.810683053651619
epoch: 155, train loss: 0.01527530571234795, acc: 0.9908251450219012; test loss: 0.6749465759694787, acc: 0.8111557551406287
epoch: 156, train loss: 0.01393292832040677, acc: 0.9910027228601871; test loss: 0.6837789631994361, acc: 0.812101158118648
epoch: 157, train loss: 0.012284463984847449, acc: 0.9929560790813307; test loss: 0.702435449036941, acc: 0.8111557551406287
epoch: 158, train loss: 0.010476676409437191, acc: 0.9942583165620931; test loss: 0.7157675258038709, acc: 0.8090285984400851
epoch: 159, train loss: 0.009050900322629125, acc: 0.9941399313365692; test loss: 0.726407145304309, acc: 0.812101158118648
epoch: 160, train loss: 0.008754304382957564, acc: 0.9953829762045697; test loss: 0.7566204332510819, acc: 0.8087922476955802
epoch: 161, train loss: 0.009583699344623733, acc: 0.994317509174855; test loss: 0.7588869317207345, acc: 0.8083195462065705
epoch: 162, train loss: 0.010352091049850234, acc: 0.9932520421451403; test loss: 0.7407406466814734, acc: 0.810683053651619
epoch: 163, train loss: 0.009745958332085703, acc: 0.9940215461110453; test loss: 0.7471021701471937, acc: 0.8043015835499882
epoch: 164, train loss: 0.01609412056351032, acc: 0.9898780632177104; test loss: 0.7357764362978952, acc: 0.8061923895060269
epoch: 165, train loss: 0.01609237726464227, acc: 0.9886942109624719; test loss: 0.7437326679610665, acc: 0.8028834790829591
epoch: 166, train loss: 0.01947985421718437, acc: 0.9881022848348526; test loss: 0.7365415451572286, acc: 0.7967383597258332
epoch: 167, train loss: 0.014199854148064716, acc: 0.9905883745708536; test loss: 0.7932377669637126, acc: 0.7946112030252895
epoch: 168, train loss: 0.026935706273201118, acc: 0.9820646383331361; test loss: 0.735981135074198, acc: 0.7965020089813283
epoch: 169, train loss: 0.019269623070114254, acc: 0.9869184325796141; test loss: 0.7554532710160806, acc: 0.7974474119593477
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.015353581894904012, acc: 0.9847874985201847; test loss: 0.6976333516226173, acc: 0.7901205388796975
epoch: 171, train loss: 0.018066622511547347, acc: 0.9820054457203741; test loss: 0.629991147174984, acc: 0.7976837627038526
epoch: 172, train loss: 0.011349288485851657, acc: 0.9889309814135195; test loss: 0.6379436832656535, acc: 0.8026471283384543
epoch: 173, train loss: 0.009390239778516246, acc: 0.9911803006984728; test loss: 0.6290429143076162, acc: 0.8009926731269204
epoch: 174, train loss: 0.015158904258980315, acc: 0.987806321771043; test loss: 0.6225030230901056, acc: 0.7974474119593477
epoch: 175, train loss: 0.012656783391810303, acc: 0.9886942109624719; test loss: 0.6204695261784181, acc: 0.7955566060033089
epoch: 176, train loss: 0.01008036376131566, acc: 0.9896412927666627; test loss: 0.6444526750464947, acc: 0.8005199716379107
epoch: 177, train loss: 0.011132886188067924, acc: 0.9889309814135195; test loss: 0.6759123150774334, acc: 0.7872843299456393
epoch: 178, train loss: 0.01657600934132431, acc: 0.9857345803243756; test loss: 0.6263740550708162, acc: 0.7962656582368235
epoch: 179, train loss: 0.014730331184642616, acc: 0.9875695513199952; test loss: 0.602017153382893, acc: 0.7998109194043961
epoch: 180, train loss: 0.011221556313747367, acc: 0.9889309814135195; test loss: 0.6422356631112871, acc: 0.7946112030252895
epoch: 181, train loss: 0.010024778906318144, acc: 0.9909435302474251; test loss: 0.6391867019977447, acc: 0.8035925313164737
epoch: 182, train loss: 0.008705599941233884, acc: 0.992660116017521; test loss: 0.6178100462135652, acc: 0.7988655164263767
epoch: 183, train loss: 0.009477686303721419, acc: 0.9908251450219012; test loss: 0.6459694070698832, acc: 0.8005199716379107
epoch: 184, train loss: 0.009256474961072111, acc: 0.9911803006984728; test loss: 0.6241750451470909, acc: 0.8024107775939494
epoch: 185, train loss: 0.011649124959610095, acc: 0.9892269444773293; test loss: 0.6368971169065342, acc: 0.8097376506735996
epoch: 186, train loss: 0.012048596855726195, acc: 0.9887534035752338; test loss: 0.6349270226571667, acc: 0.8007563223824155
epoch: 187, train loss: 0.019444574569864106, acc: 0.9838404167159939; test loss: 0.6478967862161895, acc: 0.796974710470338
epoch: 188, train loss: 0.026403699404177115, acc: 0.9760269918314194; test loss: 0.6083755255083586, acc: 0.7941385015362799
epoch: 189, train loss: 0.019714008750156057, acc: 0.9818278678820883; test loss: 0.6135549794920396, acc: 0.7965020089813283
epoch: 190, train loss: 0.024433684789241454, acc: 0.9792825855333255; test loss: 0.5982089943706609, acc: 0.795320255258804
epoch: 191, train loss: 0.015680358438208263, acc: 0.9849058837457085; test loss: 0.6250280733509735, acc: 0.8002836208934058
epoch: 192, train loss: 0.010680340746016535, acc: 0.990233218894282; test loss: 0.6199015113259849, acc: 0.8087922476955802
epoch: 193, train loss: 0.013258061570092124, acc: 0.9881614774476145; test loss: 0.644481139666531, acc: 0.7986291656818719
epoch: 194, train loss: 0.01671329640514458, acc: 0.9854978098733278; test loss: 0.6713987362635555, acc: 0.7882297329236587
epoch: 195, train loss: 0.043658236705619684, acc: 0.964957973244939; test loss: 0.6467653986861649, acc: 0.7771212479319309
epoch: 196, train loss: 0.032983716116867304, acc: 0.9724754350657038; test loss: 0.5734301254670972, acc: 0.8002836208934058
epoch: 197, train loss: 0.018088710962591743, acc: 0.9834260684266604; test loss: 0.6213690286211245, acc: 0.7955566060033089
epoch: 198, train loss: 0.01755799723369466, acc: 0.9830709127500888; test loss: 0.5969818966858101, acc: 0.803119829827464
epoch: 199, train loss: 0.012058439997071251, acc: 0.9884574405114241; test loss: 0.6081960728985005, acc: 0.804537934294493
epoch: 200, train loss: 0.03132408774081131, acc: 0.976382147507991; test loss: 0.5782485372545139, acc: 0.7780666509099504
best test acc 0.812101158118648 at epoch 156.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9995    1.0000    0.9998      6100
           1     0.9989    0.9870    0.9929       926
           2     0.9983    1.0000    0.9992      2400
           3     0.9976    0.9988    0.9982       843
           4     0.9847    1.0000    0.9923       774
           5     0.9960    0.9987    0.9974      1512
           6     0.9977    0.9917    0.9947      1330
           7     0.9979    1.0000    0.9990       481
           8     0.9978    0.9956    0.9967       458
           9     0.9912    0.9956    0.9934       452
          10     1.0000    0.9958    0.9979       717
          11     0.9970    0.9970    0.9970       333
          12     1.0000    0.9900    0.9950       299
          13     1.0000    0.9963    0.9981       269

    accuracy                         0.9978     16894
   macro avg     0.9969    0.9962    0.9965     16894
weighted avg     0.9978    0.9978    0.9977     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8782    0.9128    0.8952      1525
           1     0.8685    0.7974    0.8315       232
           2     0.8406    0.7987    0.8191       601
           3     0.8073    0.8341    0.8205       211
           4     0.8200    0.8454    0.8325       194
           5     0.8333    0.8466    0.8399       378
           6     0.6145    0.6126    0.6135       333
           7     0.8235    0.6942    0.7534       121
           8     0.6694    0.7043    0.6864       115
           9     0.8174    0.8246    0.8210       114
          10     0.8580    0.7722    0.8129       180
          11     0.7361    0.6310    0.6795        84
          12     0.2353    0.3200    0.2712        75
          13     0.7407    0.5882    0.6557        68

    accuracy                         0.8121      4231
   macro avg     0.7531    0.7273    0.7380      4231
weighted avg     0.8152    0.8121    0.8128      4231

---------------------------------------
program finished.
