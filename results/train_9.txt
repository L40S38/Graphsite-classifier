number of classes: 60
number of epochs to train: 50
batch size: 256
number of workers to load data:  36
device:  cuda
number of gpus:  2
number of pockets in training set:  22527
number of pockets in validation set:  4806
number of pockets in test set:  4890
number of train positive pairs: 180000
number of train negative pairs: 177000
number of validation positive pairs: 47172
number of validation negative pairs: 44250
model architecture:
SiameseNet(
  (embedding_net): EmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=5, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=5, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=5, out_features=5, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0002
)
loss function:
ContrastiveLoss(margin=2.0, normalize=False, mean=True)
train loss: 2671.3203991979603, validation loss: 109.03607696742168.
train loss: 59.422974744780724, validation loss: 32.6734360558044.
train loss: 22.75974436288187, validation loss: 13.88173285430049.
train loss: 11.465400773590686, validation loss: 6.871875832121576.
train loss: 6.235674533534117, validation loss: 3.8970931870307646.
train loss: 3.4169596127635624, validation loss: 3.219977396683187.
train loss: 2.518272418195794, validation loss: 1.697606112204833.
train loss: 1.7645448112594648, validation loss: 1.2616915281280379.
train loss: 1.3934785313966895, validation loss: 1.1543362513948985.
train loss: 1.2083893860461665, validation loss: 1.9773760460510608.
train loss: 1.0792107202011665, validation loss: 1.079138182124986.
train loss: 0.9803388557327227, validation loss: 1.1080742879581604.
train loss: 0.908399094311797, validation loss: 0.8802366684616565.
train loss: 0.8578870997068261, validation loss: 0.8555491900324589.
train loss: 0.8700855108522901, validation loss: 0.8449523749529568.
train loss: 0.8316876642991181, validation loss: 0.8398041189080689.
train loss: 0.7867301631895434, validation loss: 0.8191784883342964.
train loss: 0.7644577638855835, validation loss: 0.7910540508709141.
train loss: 0.7572857740813611, validation loss: 0.7797328089998018.
train loss: 0.7293983614197632, validation loss: 0.7781278952343019.
train loss: 0.7114321024491339, validation loss: 0.7691763669049632.
train loss: 0.7401670062495213, validation loss: 0.7413202771334978.
train loss: 0.6909385216549999, validation loss: 0.7440650619316898.
train loss: 0.6757825778138404, validation loss: 0.7355783655341417.
train loss: 0.6647549027728767, validation loss: 0.7344466693415262.
train loss: 0.6547316700483905, validation loss: 0.7315402063386984.
train loss: 0.6503399469712201, validation loss: 0.7297768817008753.
train loss: 0.637893918504902, validation loss: 0.8235673279568345.
train loss: 0.6322933117521911, validation loss: 0.7245448633794702.
train loss: 0.6246369313899877, validation loss: 0.7169149946711105.
train loss: 0.6193694021187577, validation loss: 0.7187252595046978.
train loss: 0.6132620971850654, validation loss: 0.7232582613195666.
train loss: 0.6064689463126559, validation loss: 0.7148893653038609.
train loss: 0.6041799191173052, validation loss: 0.7217206429831193.
train loss: 0.5987420719691685, validation loss: 0.7043689082611919.
train loss: 0.5941312530047419, validation loss: 0.7132560159332269.
train loss: 0.5928763072764506, validation loss: 0.7183776075688583.
train loss: 0.5906997992037392, validation loss: 0.7313020587715933.
train loss: 0.5841594929414637, validation loss: 0.7151261260573072.
train loss: 0.5801336113424862, validation loss: 0.7078456921640147.
train loss: 0.5784410682699593, validation loss: 0.7159351319585125.
train loss: 0.5748269429353773, validation loss: 0.7144886332379609.
train loss: 0.5731431244067451, validation loss: 0.7146576719531703.
train loss: 0.5788740078581481, validation loss: 0.7181843264028716.
train loss: 0.5700660099916431, validation loss: 0.7111425698331595.
train loss: 0.5772297242322222, validation loss: 0.7185185443220318.
train loss: 0.5644568831072468, validation loss: 0.7141979221117755.
train loss: 0.5639109872342492, validation loss: 0.7178146877373522.
train loss: 0.5601088698144052, validation loss: 0.7422422862329875.
train loss: 0.5649797544506083, validation loss: 0.7314221140400081.
best validation loss 0.7043689082611919 at epoch 35.
