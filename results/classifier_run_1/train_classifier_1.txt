seed:  666
number of classes: 60
number of epochs to train: 60
batch size: 64
number of workers to load data:  36
device:  cuda
number of gpus:  2
number of pockets in training set:  22527
number of pockets in validation set:  4806
number of pockets in test set:  4890
model architecture:
MoNet(
  (conv1): GINMolecularConv(nn=Sequential(
    (0): Linear(in_features=5, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
  ))(edge_transformer=Sequential(
    (0): Linear(in_features=1, out_features=5, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=5, out_features=5, bias=True)
    (3): ELU(alpha=1.0)
  ))
  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): GINMolecularConv(nn=Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
  ))(edge_transformer=Sequential(
    (0): Linear(in_features=1, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ELU(alpha=1.0)
  ))
  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): GINMolecularConv(nn=Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
  ))(edge_transformer=Sequential(
    (0): Linear(in_features=1, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ELU(alpha=1.0)
  ))
  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv4): GINMolecularConv(nn=Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
  ))(edge_transformer=Sequential(
    (0): Linear(in_features=1, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ELU(alpha=1.0)
  ))
  (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=32, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=60, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0002
)
[0.05595423 0.22641421 0.277008   0.29484426 0.3044457  0.31691593
 0.33142759 0.35390612 0.36722828 0.38224272 0.4460765  0.44875296
 0.47137916 0.48149459 0.51819048 0.53296076 0.55676546 0.55954234
 0.56660727 0.58583937 0.67583277 0.73566059 0.74296847 0.76059823
 0.77105319 0.85314251 0.85968    0.86298646 0.87647062 0.95479353
 0.96298918 0.96714    0.97554991 1.03878    1.1218824  1.14477795
 1.1686275  1.21943739 1.26054202 1.26766372 1.31986164 1.31986164
 1.39364273 1.4201043  1.43831076 1.46651294 1.54742399 1.58011605
 1.63778452 1.63778452 1.6870412  1.72597292 1.75294124 1.78076571
 1.79501183 1.82419902 1.82419902 1.82419902 1.82419902 1.90149559]
loss function:
NLLLoss()
begin training...
epoch: 1, train loss: 7.259344965490705, acc: 0.11683757269054912, val loss: 4.048065565945057, acc: 0.03620474406991261
epoch: 2, train loss: 3.984059627625418, acc: 0.06849558307808408, val loss: 4.025068744638786, acc: 0.04681647940074907
epoch: 3, train loss: 3.9764722492912137, acc: 0.09109069117059529, val loss: 3.996103236340901, acc: 0.17436537661256762
epoch: 4, train loss: 3.832976201140791, acc: 0.1712611532827274, val loss: 3.972221187388356, acc: 0.1764461090303787
epoch: 5, train loss: 3.7385255738708505, acc: 0.18710880277000932, val loss: 3.9524951350227573, acc: 0.17852684144818975
epoch: 6, train loss: 3.826849434121437, acc: 0.19181426732365606, val loss: 3.9284481855417064, acc: 0.17852684144818975
epoch: 7, train loss: 3.8509536827960513, acc: 0.19452212900075466, val loss: 3.897716286477078, acc: 0.17894298793175198
epoch: 8, train loss: 3.817182295055815, acc: 0.19838416122874772, val loss: 3.863976338680615, acc: 0.17873491468997088
epoch: 9, train loss: 3.7579759685919214, acc: 0.18311359701691304, val loss: 3.8236050631570357, acc: 0.17873491468997088
epoch: 10, train loss: 3.80545491150281, acc: 0.17805300306299107, val loss: 3.7868379316278364, acc: 0.17873491468997088
epoch: 11, train loss: 3.7668112871326938, acc: 0.1827140764416034, val loss: 3.7851578833111317, acc: 0.17873491468997088
epoch: 12, train loss: 3.7549834285694934, acc: 0.1869756292449061, val loss: 3.7577797331712763, acc: 0.17873491468997088
epoch: 13, train loss: 3.6917611390872174, acc: 0.18435654991787634, val loss: 4.220158293006126, acc: 0.0988347898460258
epoch: 14, train loss: 3.448017672914139, acc: 0.19030496737248634, val loss: 3.8521295754650557, acc: 0.15189346650020807
epoch: 15, train loss: 3.766468454276208, acc: 0.1742353620100324, val loss: 4.361451763338811, acc: 0.14294631710362046
epoch: 16, train loss: 3.754569415103772, acc: 0.17747591778754385, val loss: 3.827832200355546, acc: 0.16853932584269662
epoch: 17, train loss: 3.463166671020031, acc: 0.19172548497358724, val loss: 4.066226849097586, acc: 0.11568872243029546
epoch: 18, train loss: 3.58770090659343, acc: 0.1630931770763972, val loss: 10.61742299842279, acc: 0.006658343736995423
epoch: 19, train loss: 3.4839391461562585, acc: 0.17752030896257823, val loss: 3.910066796896907, acc: 0.17415730337078653
epoch: 20, train loss: 3.631449276922482, acc: 0.1769876148621654, val loss: 3.735161539022991, acc: 0.17873491468997088
epoch: 21, train loss: 3.7543856945326564, acc: 0.17800861188795666, val loss: 3.721818711228833, acc: 0.17873491468997088
epoch: 22, train loss: 3.737994664919891, acc: 0.17800861188795666, val loss: 3.721100616693199, acc: 0.17873491468997088
epoch: 23, train loss: 3.156031764659085, acc: 0.210858081413415, val loss: 10.198824064561936, acc: 0.005409904286308781
epoch: 24, train loss: 2.977402931725121, acc: 0.2062857903848715, val loss: 3.7216537773633966, acc: 0.17873491468997088
epoch: 25, train loss: 2.915190003728636, acc: 0.20939317263727972, val loss: 6.8654778448185025, acc: 0.005826050769870994
epoch: 26, train loss: 3.036418551097312, acc: 0.2052647933590802, val loss: 3.7428868129458768, acc: 0.17873491468997088
epoch: 27, train loss: 3.399937250105409, acc: 0.19554312602654592, val loss: 3.7275801935247515, acc: 0.17873491468997088
epoch: 28, train loss: 3.4831406957020365, acc: 0.1951436054512363, val loss: 3.7124505191855364, acc: 0.17873491468997088
epoch: 29, train loss: 3.542841100669465, acc: 0.1798730412394016, val loss: 3.7090706618884877, acc: 0.17873491468997088
epoch: 30, train loss: 3.4439895588775795, acc: 0.19190304967372487, val loss: 3.7698378003343067, acc: 0.17873491468997088
epoch: 31, train loss: 3.4896930619732287, acc: 0.20704044036045635, val loss: 3.6951743255691434, acc: 0.17873491468997088
epoch: 32, train loss: 3.4907740681520782, acc: 0.19474408487592668, val loss: 3.6918648211002547, acc: 0.17873491468997088
epoch: 33, train loss: 3.5971209100005352, acc: 0.18626537044435565, val loss: 4.016449994957552, acc: 0.177070328755722
epoch: 34, train loss: 3.0844064533575537, acc: 0.20269010520708483, val loss: 3.6908018748759233, acc: 0.17873491468997088
epoch: 35, train loss: 3.583369954075705, acc: 0.19771829360323168, val loss: 3.697932122699231, acc: 0.17873491468997088
epoch: 36, train loss: 3.4599137436816, acc: 0.19181426732365606, val loss: 3.689687049641889, acc: 0.17873491468997088
epoch: 37, train loss: 3.243574773539778, acc: 0.1855995028188396, val loss: 3.7448425574745183, acc: 0.17831876820640866
epoch: 38, train loss: 3.342068820457545, acc: 0.21250055488968794, val loss: 3.7051207393196983, acc: 0.17873491468997088
epoch: 39, train loss: 3.6363708460998265, acc: 0.18706441159497492, val loss: 3.6946692641357854, acc: 0.17873491468997088
epoch: 40, train loss: 3.4958203385544824, acc: 0.20748435211080038, val loss: 3.7018616083409457, acc: 0.17873491468997088
epoch: 41, train loss: 3.5498763317379605, acc: 0.18484485284325475, val loss: 4.577719167129129, acc: 0.02517686225551394
epoch: 42, train loss: 3.377583861091713, acc: 0.20326719048253208, val loss: 4.013269440511639, acc: 0.1560549313358302
epoch: 43, train loss: 3.6092534025663614, acc: 0.1879078439206286, val loss: 4.4433523377328425, acc: 0.03911776945484811
epoch: 44, train loss: 3.6982840074967696, acc: 0.17929595596395437, val loss: 3.750820941544055, acc: 0.17811069496462756
epoch: 45, train loss: 3.607885014353587, acc: 0.18519998224353, val loss: 3.7089968773408875, acc: 0.17873491468997088
epoch: 46, train loss: 3.5451765959826007, acc: 0.1967860789275092, val loss: 3.7163549023174616, acc: 0.17790262172284643
epoch: 47, train loss: 3.608774804241326, acc: 0.17796422071292228, val loss: 3.6949587390167835, acc: 0.17873491468997088
epoch: 48, train loss: 3.5678953444555024, acc: 0.19447773782572025, val loss: 3.693989283632746, acc: 0.17873491468997088
epoch: 49, train loss: 3.513001121862722, acc: 0.1850668087184268, val loss: 3.6897633729554493, acc: 0.17873491468997088
epoch: 50, train loss: 3.508667797605615, acc: 0.20313401695742886, val loss: 4.013538095289302, acc: 0.17873491468997088
epoch: 51, train loss: 3.4191905508643803, acc: 0.19838416122874772, val loss: 3.694548594569644, acc: 0.17873491468997088
epoch: 52, train loss: 3.4207123330817715, acc: 0.18746393217028454, val loss: 3.687148631933675, acc: 0.17873491468997088
epoch: 53, train loss: 3.3990786190213305, acc: 0.20313401695742886, val loss: 3.6855028311609974, acc: 0.17873491468997088
epoch: 54, train loss: 3.4294374188184262, acc: 0.19949394060460782, val loss: 3.687416480076695, acc: 0.17873491468997088
epoch: 55, train loss: 3.4660433043270236, acc: 0.19066009677276158, val loss: 3.6903518058834006, acc: 0.17873491468997088
epoch: 56, train loss: 3.5361661618266127, acc: 0.18795223509566297, val loss: 3.6766090986384383, acc: 0.17873491468997088
epoch: 57, train loss: 3.466683643077065, acc: 0.18577706751897724, val loss: 3.686967501479588, acc: 0.17873491468997088
epoch: 58, train loss: 3.61317400018517, acc: 0.1965641230523372, val loss: 3.6714922948226896, acc: 0.17873491468997088
epoch: 59, train loss: 3.6524300339240265, acc: 0.18369068229236027, val loss: 3.6884474718615357, acc: 0.17873491468997088
epoch: 60, train loss: 3.5645033928485104, acc: 0.19119279087317442, val loss: 4.62253326264412, acc: 0.043279234290470245
best val loss 3.6714922948226896 at epoch 58.
