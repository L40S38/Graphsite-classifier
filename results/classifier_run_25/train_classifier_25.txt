seed:  666
save trained model at:  ../trained_models/trained_classifier_model_25.pt
save loss at:  ./results/train_classifier_results_25.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  14780
number of pockets in validation set:  3162
number of pockets in test set:  3183
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0006
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b7cfabdcb50>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.051210297815532, acc: 0.38660351826792966, val loss: 1.92677294054339, acc: 0.407337128399747, test loss: 1.9238319084624094, acc: 0.4194156456173421
epoch: 2, train loss: 1.7413543095608042, acc: 0.47320703653585927, val loss: 1.6375161244249434, acc: 0.4845034788108792, test loss: 1.6202218855397341, acc: 0.4938737040527804
epoch: 3, train loss: 1.653238356323139, acc: 0.4953315290933694, val loss: 1.6299215361409969, acc: 0.48323845667299176, test loss: 1.6227538708504963, acc: 0.49670122525918947
epoch: 4, train loss: 1.5933226602164592, acc: 0.5059539918809202, val loss: 1.5189627381082882, acc: 0.5370018975332068, test loss: 1.5146024350733491, acc: 0.5422557335846685
epoch: 5, train loss: 1.5170607256469932, acc: 0.5395128552097429, val loss: 1.4665072829869636, acc: 0.5360531309297912, test loss: 1.4647980734045836, acc: 0.55105246622683
epoch: 6, train loss: 1.4738760890108968, acc: 0.556021650879567, val loss: 1.4373060690308583, acc: 0.562302340290955, test loss: 1.430305181641718, acc: 0.5683317624882187
epoch: 7, train loss: 1.4171368865747414, acc: 0.5713125845737483, val loss: 1.4374259502958604, acc: 0.5572422517394054, test loss: 1.42594230208725, acc: 0.5526233113415018
epoch: 8, train loss: 1.3769840485026936, acc: 0.5853856562922869, val loss: 1.4794074715427927, acc: 0.5569259962049335, test loss: 1.4650422998692605, acc: 0.5601633678919259
epoch: 9, train loss: 1.3624530702224442, acc: 0.5884303112313938, val loss: 1.398754149279513, acc: 0.568943706514864, test loss: 1.393850011492989, acc: 0.5780710021991832
epoch: 10, train loss: 1.3613726035829166, acc: 0.5853856562922869, val loss: 1.297965439417029, acc: 0.5958254269449715, test loss: 1.29129046012574, acc: 0.5934652843229657
epoch: 11, train loss: 1.2913382871547796, acc: 0.6113667117726658, val loss: 1.4312981155233848, acc: 0.5686274509803921, test loss: 1.4291884733602156, acc: 0.5670750863964813
epoch: 12, train loss: 1.2762984921064364, acc: 0.6152232746955345, val loss: 1.2350680132895464, acc: 0.620809614168248, test loss: 1.252649811427997, acc: 0.6157712849513038
epoch: 13, train loss: 1.2489952221613614, acc: 0.6223274695534506, val loss: 1.3468118360870478, acc: 0.5698924731182796, test loss: 1.3471639729355103, acc: 0.5774426641533145
epoch: 14, train loss: 1.2066880235491328, acc: 0.6320027063599458, val loss: 1.2410605354900228, acc: 0.620809614168248, test loss: 1.239024496438029, acc: 0.6248821866163996
epoch: 15, train loss: 1.1691477047413064, acc: 0.6467523680649526, val loss: 1.2448562628705617, acc: 0.6094244149272612, test loss: 1.2391695714094262, acc: 0.6176562990889098
epoch: 16, train loss: 1.1676410565356925, acc: 0.6439106901217861, val loss: 1.2421637294716206, acc: 0.6094244149272612, test loss: 1.232102461442459, acc: 0.6163996229971724
epoch: 17, train loss: 1.123957509068578, acc: 0.6592692828146144, val loss: 1.1651850114661935, acc: 0.6464263124604681, test loss: 1.15762353617554, acc: 0.6468740182218033
epoch: 18, train loss: 1.1015299640263208, acc: 0.6707036535859269, val loss: 1.3952560168440624, acc: 0.5600885515496521, test loss: 1.394779669201078, acc: 0.5733584668551681
epoch: 19, train loss: 1.114228734531325, acc: 0.6675236806495264, val loss: 1.341194933234401, acc: 0.5866540164452878, test loss: 1.3378391149768356, acc: 0.6003770028275212
epoch: 20, train loss: 1.0634802142080015, acc: 0.6803788903924222, val loss: 1.3347401483235368, acc: 0.5847564832384566, test loss: 1.3450554793187368, acc: 0.589066918001885
epoch: 21, train loss: 1.0598616591487104, acc: 0.6817997293640055, val loss: 1.087803806254262, acc: 0.6752055660974067, test loss: 1.0709829958886647, acc: 0.6694941878730757
epoch: 22, train loss: 1.0200035069404985, acc: 0.6897834912043301, val loss: 1.240771976020953, acc: 0.6378874130297281, test loss: 1.2156307811149816, acc: 0.6377631165567075
epoch: 23, train loss: 1.009580877395057, acc: 0.693978349120433, val loss: 1.0950851564087647, acc: 0.6672991777356104, test loss: 1.083413951995423, acc: 0.6742067232170907
epoch: 24, train loss: 1.0132558690678928, acc: 0.6938430311231394, val loss: 1.2240222013124855, acc: 0.6265022137887413, test loss: 1.2298587097374103, acc: 0.6182846371347785
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.7857145911301908, acc: 0.6990527740189445, val loss: 0.8569059387028481, acc: 0.6672991777356104, test loss: 0.8657357285991241, acc: 0.666038328620798
epoch: 26, train loss: 0.7654248267936449, acc: 0.7064952638700948, val loss: 0.824305680037902, acc: 0.6834282099936749, test loss: 0.826914934178699, acc: 0.685516808042727
epoch: 27, train loss: 0.7450948102871039, acc: 0.7142083897158322, val loss: 0.7987230284013452, acc: 0.6907020872865275, test loss: 0.8018391635707816, acc: 0.6927426955702167
epoch: 28, train loss: 0.7391331815751867, acc: 0.7147496617050068, val loss: 0.8210088969935804, acc: 0.6771030993042378, test loss: 0.8211173404510239, acc: 0.6852026390197926
epoch: 29, train loss: 0.7395359886837909, acc: 0.715764546684709, val loss: 0.8321075801378862, acc: 0.6865907653383935, test loss: 0.8219749519295412, acc: 0.6826892868363179
epoch: 30, train loss: 0.7273370062708048, acc: 0.7172530446549391, val loss: 0.8437913643416538, acc: 0.6710942441492727, test loss: 0.8249600011524149, acc: 0.6823751178133836
epoch: 31, train loss: 0.7304749897271596, acc: 0.7150879566982409, val loss: 0.7905866558381381, acc: 0.6903858317520557, test loss: 0.8083432925934692, acc: 0.6905435124096764
epoch: 32, train loss: 0.7109519347447665, acc: 0.7257104194857916, val loss: 0.7947604804005824, acc: 0.691967109424415, test loss: 0.805953162816797, acc: 0.6867734841344644
epoch: 33, train loss: 0.7239092244669614, acc: 0.7209066305818674, val loss: 0.8879154886647466, acc: 0.6587602783048704, test loss: 0.8932804655362104, acc: 0.6663524976437323
epoch: 34, train loss: 0.7158481821962235, acc: 0.7207036535859269, val loss: 0.9484152602365241, acc: 0.6555977229601518, test loss: 0.931665853700509, acc: 0.6584982720703738
epoch: 35, train loss: 0.7040480107673934, acc: 0.7262516914749662, val loss: 0.8181116433176793, acc: 0.6714104996837444, test loss: 0.8198560827227085, acc: 0.6742067232170907
epoch: 36, train loss: 0.7040974986085389, acc: 0.7219891745602165, val loss: 0.8451534171107448, acc: 0.6733080328905756, test loss: 0.8533573110186751, acc: 0.6770342444234998
epoch: 37, train loss: 0.7031485349948093, acc: 0.7254397834912043, val loss: 0.8429532100851215, acc: 0.6644528779253637, test loss: 0.8252528884520967, acc: 0.6830034558592523
epoch: 38, train loss: 0.6848130506494855, acc: 0.7351826792963464, val loss: 0.762115805756812, acc: 0.6951296647691335, test loss: 0.7727734135487826, acc: 0.7034244423499842
epoch: 39, train loss: 0.6641014848413583, acc: 0.7356562922868741, val loss: 0.7877057924216341, acc: 0.6840607210626186, test loss: 0.7955984449072001, acc: 0.6867734841344644
epoch: 40, train loss: 0.6641206613896825, acc: 0.738700947225981, val loss: 0.7879705373280264, acc: 0.6859582542694497, test loss: 0.7767983945780043, acc: 0.6914860194784794
epoch: 41, train loss: 0.6699750416978931, acc: 0.7407307171853856, val loss: 0.8023902852651366, acc: 0.6733080328905756, test loss: 0.819372895675525, acc: 0.6823751178133836
epoch: 42, train loss: 0.6727136034281553, acc: 0.7379566982408661, val loss: 0.8031645284130331, acc: 0.6888045540796964, test loss: 0.7967262025378314, acc: 0.6936852026390198
epoch: 43, train loss: 0.6536287956211984, acc: 0.7418809201623816, val loss: 0.8746335295315862, acc: 0.6815306767868438, test loss: 0.8614564109739926, acc: 0.6820609487904492
epoch: 44, train loss: 0.6488459332063492, acc: 0.7503382949932341, val loss: 0.7897788412129102, acc: 0.6995572422517394, test loss: 0.7992043225524788, acc: 0.704052780395853
epoch: 45, train loss: 0.634531233081637, acc: 0.7496617050067659, val loss: 0.7634881857751971, acc: 0.698292220113852, test loss: 0.7810852482676169, acc: 0.6952560477536914
epoch: 46, train loss: 0.6235203068375749, acc: 0.7529093369418133, val loss: 0.8933093139146567, acc: 0.6571790006325111, test loss: 0.8852350086376347, acc: 0.6672950047125353
epoch: 47, train loss: 0.6310682417575335, acc: 0.7508795669824087, val loss: 0.7481225607592723, acc: 0.6995572422517394, test loss: 0.7633876046256528, acc: 0.6987119070059692
epoch: 48, train loss: 0.6057792948127922, acc: 0.7604871447902571, val loss: 0.7575699720557622, acc: 0.7074636306135358, test loss: 0.7440170506205606, acc: 0.7153628652214892
epoch: 49, train loss: 0.597391280689162, acc: 0.7608254397834912, val loss: 0.9427715596483147, acc: 0.6287160025300442, test loss: 0.9332407991203766, acc: 0.6427898209236569
epoch: 50, train loss: 0.5939106199834924, acc: 0.7631935047361299, val loss: 0.7160962906764475, acc: 0.7118912080961417, test loss: 0.7276393964984976, acc: 0.7134778510838832
epoch: 51, train loss: 0.5762759723424589, acc: 0.7677943166441137, val loss: 0.8207091885529916, acc: 0.7036685641998734, test loss: 0.8245249486666149, acc: 0.704052780395853
epoch: 52, train loss: 0.5921982935058893, acc: 0.7648173207036536, val loss: 0.7952198353392802, acc: 0.6821631878557874, test loss: 0.8069411541282325, acc: 0.6748350612629594
epoch: 53, train loss: 0.5754332844072169, acc: 0.7723274695534507, val loss: 0.682686442291337, acc: 0.7235926628716003, test loss: 0.699284611128802, acc: 0.7266729500471254
epoch: 54, train loss: 0.5785613864942558, acc: 0.7686738836265223, val loss: 0.7950183975779505, acc: 0.7039848197343453, test loss: 0.774404621333998, acc: 0.7046811184417217
epoch: 55, train loss: 0.5583265729298934, acc: 0.7784844384303112, val loss: 0.8219638339784008, acc: 0.683111954459203, test loss: 0.8095419908745873, acc: 0.6927426955702167
epoch: 56, train loss: 0.5803093167861195, acc: 0.7723274695534507, val loss: 1.05748889538248, acc: 0.6283997469955724, test loss: 1.072219954996112, acc: 0.6239396795475967
epoch: 57, train loss: 0.5841669923720405, acc: 0.7694857916102842, val loss: 0.7892618364808229, acc: 0.704617330803289, test loss: 0.8050789512480875, acc: 0.6933710336160854
epoch: 58, train loss: 0.5565384073412305, acc: 0.7784167794316644, val loss: 1.2202117001233712, acc: 0.5844402277039848, test loss: 1.2448300185010301, acc: 0.5821551994973295
epoch: 59, train loss: 0.5771054835700214, acc: 0.7727334235453315, val loss: 0.7040209726469944, acc: 0.7239089184060721, test loss: 0.7118526427130859, acc: 0.7357838517122212
epoch: 60, train loss: 0.5794571966699076, acc: 0.772192151556157, val loss: 0.7384742645430158, acc: 0.717583807716635, test loss: 0.7301564101262022, acc: 0.7194470625196355
epoch: 61, train loss: 0.5662209043806073, acc: 0.7723951285520975, val loss: 0.7962726984198979, acc: 0.7008222643896268, test loss: 0.7705136778367379, acc: 0.7075086396481307
epoch: 62, train loss: 0.5402802480735056, acc: 0.784235453315291, val loss: 0.7502869193422727, acc: 0.7213788741302973, test loss: 0.7513401449157351, acc: 0.7213320766572416
epoch: 63, train loss: 0.5427943437283352, acc: 0.7831529093369418, val loss: 0.7307768947787107, acc: 0.722011385199241, test loss: 0.7218941755351727, acc: 0.7156770342444235
epoch: 64, train loss: 0.5309175157659916, acc: 0.7867388362652232, val loss: 0.8198116293140486, acc: 0.6783681214421252, test loss: 0.8450547744746183, acc: 0.6779767514923029
epoch: 65, train loss: 0.5193222784705672, acc: 0.7881596752368065, val loss: 0.8085776323913246, acc: 0.7030360531309298, test loss: 0.8132019781519548, acc: 0.702167766258247
epoch: 66, train loss: 0.5330954901259065, acc: 0.7882273342354533, val loss: 0.8569718159730491, acc: 0.6843769765970904, test loss: 0.8429962396996222, acc: 0.6896010053408734
epoch: 67, train loss: 0.5333479967427028, acc: 0.7863328822733423, val loss: 1.055821223246908, acc: 0.6331435800126503, test loss: 1.0579245911429522, acc: 0.6317939051209551
epoch: 68, train loss: 0.5299883646635951, acc: 0.7836265223274695, val loss: 0.8036820826690307, acc: 0.6954459203036053, test loss: 0.8133797664294781, acc: 0.6886584982720704
epoch: 69, train loss: 0.5123813077142017, acc: 0.793640054127199, val loss: 0.7585424061895246, acc: 0.7201138519924098, test loss: 0.7920091605358741, acc: 0.7065661325793277
epoch: 70, train loss: 0.5003667288929909, acc: 0.7962787550744249, val loss: 0.7876544389296453, acc: 0.6976597090449083, test loss: 0.7576835892539784, acc: 0.7012252591894439
Epoch    70: reducing learning rate of group 0 to 1.5000e-03.
epoch: 71, train loss: 0.48110622247436535, acc: 0.8022327469553451, val loss: 0.662763462422544, acc: 0.7520556609740671, test loss: 0.643637306166639, acc: 0.7540056550424128
epoch: 72, train loss: 0.4137927983062032, acc: 0.830446549391069, val loss: 0.6518973728134088, acc: 0.7501581277672359, test loss: 0.6567620617617536, acc: 0.7524348099277411
epoch: 73, train loss: 0.3901888249933478, acc: 0.8368741542625169, val loss: 0.6651083818347601, acc: 0.7590132827324478, test loss: 0.6565683117236426, acc: 0.7565190072258875
epoch: 74, train loss: 0.38079403614642976, acc: 0.8439106901217862, val loss: 0.7677149656526202, acc: 0.7381404174573055, test loss: 0.746562293816991, acc: 0.74583726044612
epoch: 75, train loss: 0.38031103905386143, acc: 0.840595399188092, val loss: 0.6856634072455178, acc: 0.7507906388361796, test loss: 0.7116719622526609, acc: 0.7360980207351555
epoch: 76, train loss: 0.3771980199907404, acc: 0.8419485791610284, val loss: 0.6981555699246508, acc: 0.7561669829222012, test loss: 0.7255784427074753, acc: 0.7489789506754634
epoch: 77, train loss: 0.3673420193956734, acc: 0.847361299052774, val loss: 0.7145341958221494, acc: 0.743516761543327, test loss: 0.7164839496037289, acc: 0.743952246308514
epoch: 78, train loss: 0.3684198685444095, acc: 0.8462787550744248, val loss: 0.7065748905999837, acc: 0.7580645161290323, test loss: 0.7014578516813632, acc: 0.7621740496387056
epoch: 79, train loss: 0.36736989722200275, acc: 0.8464140730717186, val loss: 0.667279816669124, acc: 0.7536369386464263, test loss: 0.6729603599460853, acc: 0.7483506126295947
epoch: 80, train loss: 0.3647681015350822, acc: 0.8491204330175913, val loss: 0.682400025875036, acc: 0.7586970271979759, test loss: 0.6789593499971942, acc: 0.7587181903864278
epoch: 81, train loss: 0.3682069705368218, acc: 0.8476995940460081, val loss: 0.7023009095743892, acc: 0.7413029728020241, test loss: 0.6880431793187723, acc: 0.746779767514923
epoch: 82, train loss: 0.356175979387131, acc: 0.8501353179972937, val loss: 0.7271181644328707, acc: 0.7330803289057558, test loss: 0.7490381558586358, acc: 0.7298146402764687
epoch: 83, train loss: 0.36058149723303334, acc: 0.8504059539918809, val loss: 0.8181544757206028, acc: 0.713472485768501, test loss: 0.7958625652038785, acc: 0.7181903864278982
epoch: 84, train loss: 0.34964067271339716, acc: 0.8569012178619756, val loss: 0.6860634790501362, acc: 0.7609108159392789, test loss: 0.6814990340258915, acc: 0.7571473452717562
epoch: 85, train loss: 0.3389193777302444, acc: 0.8560893098782139, val loss: 0.7393556423838722, acc: 0.7422517394054396, test loss: 0.7141854926321142, acc: 0.7483506126295947
epoch: 86, train loss: 0.33743525195831536, acc: 0.8583897158322057, val loss: 0.7151070149317337, acc: 0.7539531941808981, test loss: 0.6926816436729827, acc: 0.7514923028589381
epoch: 87, train loss: 0.33116948944951263, acc: 0.8574424898511502, val loss: 0.6972029266441871, acc: 0.7694497153700189, test loss: 0.6788974442723185, acc: 0.7615457115928369
epoch: 88, train loss: 0.3335002181817133, acc: 0.8585250338294993, val loss: 0.7398731402397759, acc: 0.7501581277672359, test loss: 0.7217603950728195, acc: 0.7555765001570846
epoch: 89, train loss: 0.3411729525447053, acc: 0.8551420838971583, val loss: 0.7005170014445952, acc: 0.7536369386464263, test loss: 0.7065600353406354, acc: 0.7596606974552309
epoch: 90, train loss: 0.3174950981091743, acc: 0.8635994587280108, val loss: 0.7315904729204944, acc: 0.7507906388361796, test loss: 0.7063809444872119, acc: 0.7502356267672008
epoch: 91, train loss: 0.32129080376218555, acc: 0.8599458728010826, val loss: 0.7511572380898347, acc: 0.7454142947501581, test loss: 0.7649699460025707, acc: 0.7477222745837261
epoch: 92, train loss: 0.33611672627909744, acc: 0.8563599458728011, val loss: 0.731221347575999, acc: 0.7596457938013915, test loss: 0.7376732537104504, acc: 0.7596606974552309
epoch: 93, train loss: 0.30453971344498726, acc: 0.8696887686062247, val loss: 0.8925716467555153, acc: 0.7179000632511069, test loss: 0.9189498932002814, acc: 0.7159912032673579
epoch: 94, train loss: 0.3550669467739549, acc: 0.8490527740189445, val loss: 0.7387041020137007, acc: 0.7593295382669196, test loss: 0.7299395519871401, acc: 0.7558906691800189
epoch: 95, train loss: 0.31651257673764904, acc: 0.8656968876860622, val loss: 0.7107754741417766, acc: 0.7558507273877293, test loss: 0.7327397908574187, acc: 0.7593465284322966
epoch: 96, train loss: 0.3080234047443844, acc: 0.8681326116373478, val loss: 0.7155897217713754, acc: 0.7590132827324478, test loss: 0.6850375793581671, acc: 0.7530631479736098
epoch: 97, train loss: 0.2898957504471197, acc: 0.8754397834912043, val loss: 0.7964963840578719, acc: 0.7492093611638204, test loss: 0.7826785292971482, acc: 0.7442664153314483
epoch: 98, train loss: 0.29001498522390695, acc: 0.8754397834912043, val loss: 0.7899946697447437, acc: 0.7492093611638204, test loss: 0.7602675158086903, acc: 0.7571473452717562
epoch: 99, train loss: 0.3040494090407401, acc: 0.8700270635994587, val loss: 0.6843583430013349, acc: 0.7555344718532574, test loss: 0.6937809184479181, acc: 0.7483506126295947
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.23528152555025642, acc: 0.8670500676589986, val loss: 0.5788298195332835, acc: 0.7662871600253004, test loss: 0.5803419257573575, acc: 0.7640590637763116
epoch: 101, train loss: 0.21582622607925425, acc: 0.8771989174560216, val loss: 0.6096387634090349, acc: 0.7643896268184693, test loss: 0.6068795762869535, acc: 0.763430725730443
epoch: 102, train loss: 0.19956313330120581, acc: 0.8817997293640054, val loss: 0.5999076360392465, acc: 0.7567994939911449, test loss: 0.5941640099676158, acc: 0.7533773169965441
epoch: 103, train loss: 0.19424292263707224, acc: 0.8857916102841678, val loss: 0.6326505387606008, acc: 0.75426944971537, test loss: 0.6291539206131801, acc: 0.7552623311341502
epoch: 104, train loss: 0.1996484027623485, acc: 0.8831529093369418, val loss: 0.6419628936983527, acc: 0.7634408602150538, test loss: 0.6425983941746027, acc: 0.7593465284322966
epoch: 105, train loss: 0.22686041307223503, acc: 0.8700947225981055, val loss: 0.6888344118219626, acc: 0.7460468058191019, test loss: 0.6839698149354212, acc: 0.7401822180333019
epoch: 106, train loss: 0.21855279641157882, acc: 0.8722598105548038, val loss: 0.5648646236295417, acc: 0.7675521821631879, test loss: 0.5827917348203739, acc: 0.7687715991203268
epoch: 107, train loss: 0.18835041049933401, acc: 0.8878890392422192, val loss: 0.6831125070745624, acc: 0.7432005060088551, test loss: 0.6540763606225175, acc: 0.7477222745837261
epoch: 108, train loss: 0.20461010439712074, acc: 0.8754397834912043, val loss: 0.6245976451979944, acc: 0.7501581277672359, test loss: 0.6119051147773136, acc: 0.7470939365378574
epoch: 109, train loss: 0.22525185734154568, acc: 0.86914749661705, val loss: 0.6107855977927635, acc: 0.7523719165085389, test loss: 0.6290680955424834, acc: 0.7511781338360037
epoch: 110, train loss: 0.21041676765043132, acc: 0.8745602165087957, val loss: 0.6485930035040395, acc: 0.7586970271979759, test loss: 0.647616448808383, acc: 0.7461514294690543
epoch: 111, train loss: 0.2064241037276827, acc: 0.8769959404600812, val loss: 0.7212179280171886, acc: 0.7254901960784313, test loss: 0.7068290674869946, acc: 0.7329563305058121
epoch: 112, train loss: 0.20756310110405107, acc: 0.8744248985115021, val loss: 0.5921342053947232, acc: 0.7586970271979759, test loss: 0.5863039832874697, acc: 0.7571473452717562
epoch: 113, train loss: 0.1987822685778867, acc: 0.8794993234100136, val loss: 0.6125814116204712, acc: 0.7555344718532574, test loss: 0.6337933124628525, acc: 0.7536914860194784
epoch: 114, train loss: 0.22017867035246347, acc: 0.868403247631935, val loss: 0.5844148312242328, acc: 0.7719797596457938, test loss: 0.6039075547705797, acc: 0.7637448947533774
epoch: 115, train loss: 0.18861708457963552, acc: 0.886468200270636, val loss: 0.6266968018038978, acc: 0.7628083491461101, test loss: 0.6381731656149727, acc: 0.7574615142946906
epoch: 116, train loss: 0.20004146780632184, acc: 0.880446549391069, val loss: 0.6454086968749478, acc: 0.7596457938013915, test loss: 0.643930160396323, acc: 0.748664781652529
epoch: 117, train loss: 0.194522067372144, acc: 0.8805142083897158, val loss: 0.7333428116254909, acc: 0.7318153067678684, test loss: 0.7567824642874705, acc: 0.727301288092994
epoch: 118, train loss: 0.2133591404867753, acc: 0.8703653585926928, val loss: 0.6321589830628985, acc: 0.763124604680582, test loss: 0.6386615652503213, acc: 0.7587181903864278
epoch: 119, train loss: 0.19264366356139254, acc: 0.8811907983761841, val loss: 0.68381477050733, acc: 0.7507906388361796, test loss: 0.6945332900331037, acc: 0.7521206409048068
epoch: 120, train loss: 0.20004232775858516, acc: 0.877469553450609, val loss: 0.5908594539390193, acc: 0.7685009487666035, test loss: 0.6377938400826437, acc: 0.7533773169965441
epoch: 121, train loss: 0.19459489649300324, acc: 0.8784844384303112, val loss: 0.6881905194704175, acc: 0.7561669829222012, test loss: 0.6659265309806294, acc: 0.7596606974552309
epoch: 122, train loss: 0.20003626131640398, acc: 0.8812584573748309, val loss: 0.6311171551733964, acc: 0.7628083491461101, test loss: 0.6546337275320803, acc: 0.7596606974552309
epoch: 123, train loss: 0.19574086382198722, acc: 0.8799729364005413, val loss: 0.6291240380580934, acc: 0.7700822264389627, test loss: 0.6525705611222651, acc: 0.7675149230285894
epoch: 124, train loss: 0.1756788353031156, acc: 0.8882273342354533, val loss: 0.6714936920844023, acc: 0.7609108159392789, test loss: 0.6840588737125858, acc: 0.7565190072258875
epoch: 125, train loss: 0.16754556766059628, acc: 0.8934370771312584, val loss: 0.6403566207560009, acc: 0.7697659709044908, test loss: 0.6748483574618868, acc: 0.7665724159597863
Epoch   125: reducing learning rate of group 0 to 7.5000e-04.
epoch: 126, train loss: 0.13324861104256083, acc: 0.9106224627875508, val loss: 0.5964614079769166, acc: 0.7871600253004427, test loss: 0.638145915487447, acc: 0.7863650644046497
epoch: 127, train loss: 0.11040754666065493, acc: 0.9246278755074425, val loss: 0.669634801386882, acc: 0.7688172043010753, test loss: 0.7173144136102853, acc: 0.7700282752120641
epoch: 128, train loss: 0.10729505399163908, acc: 0.9238159675236807, val loss: 0.655864348221549, acc: 0.7798861480075902, test loss: 0.6831756612994632, acc: 0.7835375431982406
epoch: 129, train loss: 0.09643029516943415, acc: 0.9322733423545332, val loss: 0.6779930187432845, acc: 0.784629981024668, test loss: 0.7036315691834383, acc: 0.7788250078542256
epoch: 130, train loss: 0.09506010412363303, acc: 0.9286874154262517, val loss: 0.6712203704428628, acc: 0.7849462365591398, test loss: 0.6965865033848122, acc: 0.7794533459000943
epoch: 131, train loss: 0.09770841965494685, acc: 0.9286197564276049, val loss: 0.6887906183554054, acc: 0.7764073371283997, test loss: 0.7173794312481606, acc: 0.7750549795790135
epoch: 132, train loss: 0.09571532391122617, acc: 0.9314614343707713, val loss: 0.7392268968035646, acc: 0.7694497153700189, test loss: 0.7467438131992149, acc: 0.7788250078542256
epoch: 133, train loss: 0.09341685407781149, acc: 0.935723951285521, val loss: 0.6952555680259883, acc: 0.7827324478178368, test loss: 0.7034589127478568, acc: 0.7863650644046497
epoch: 134, train loss: 0.10625594105868927, acc: 0.9253044654939107, val loss: 0.779132426501376, acc: 0.765022137887413, test loss: 0.7946246170301015, acc: 0.7650015708451147
epoch: 135, train loss: 0.09938443766879132, acc: 0.9276048714479026, val loss: 0.6826531573385471, acc: 0.7789373814041746, test loss: 0.7464003121744265, acc: 0.7618598806157713
epoch: 136, train loss: 0.10085100061757961, acc: 0.9285520974289581, val loss: 0.7215491469188403, acc: 0.7643896268184693, test loss: 0.7346066139495638, acc: 0.7687715991203268
epoch: 137, train loss: 0.10769761294852413, acc: 0.9209742895805142, val loss: 0.725447417888967, acc: 0.775774826059456, test loss: 0.7604083923135506, acc: 0.764373232799246
epoch: 138, train loss: 0.10443839892765182, acc: 0.926725304465494, val loss: 0.7536348091658726, acc: 0.7659709044908286, test loss: 0.8034686552740439, acc: 0.7587181903864278
epoch: 139, train loss: 0.1113472969630413, acc: 0.9207036535859269, val loss: 0.7189148619987782, acc: 0.7710309930423782, test loss: 0.6982750750696588, acc: 0.7662582469368521
epoch: 140, train loss: 0.11776400701885133, acc: 0.9223951285520974, val loss: 0.6775246079702456, acc: 0.7789373814041746, test loss: 0.69960258698411, acc: 0.7753691486019478
epoch: 141, train loss: 0.09939794313762443, acc: 0.9290257104194858, val loss: 0.6821289588800342, acc: 0.7820999367488931, test loss: 0.6922884866422053, acc: 0.7709707822808671
epoch: 142, train loss: 0.11070197599666529, acc: 0.9211772665764547, val loss: 0.7040446926565731, acc: 0.7764073371283997, test loss: 0.6925702945189097, acc: 0.7681432610744581
epoch: 143, train loss: 0.10233628911562315, acc: 0.926725304465494, val loss: 0.7245264914426375, acc: 0.7697659709044908, test loss: 0.7504335900278838, acc: 0.7640590637763116
epoch: 144, train loss: 0.16337437355469947, acc: 0.8996617050067659, val loss: 0.6603819081332396, acc: 0.763124604680582, test loss: 0.6955989741919515, acc: 0.7609173735469683
epoch: 145, train loss: 0.13325906281985836, acc: 0.9119756427604871, val loss: 0.6562232613789741, acc: 0.777988614800759, test loss: 0.6640316546805954, acc: 0.7690857681432611
epoch: 146, train loss: 0.09841555520710668, acc: 0.9322056833558864, val loss: 0.7043088259084703, acc: 0.7805186590765338, test loss: 0.7299593313287872, acc: 0.7659440779139177
epoch: 147, train loss: 0.12346205104026485, acc: 0.9154939106901218, val loss: 0.6696157726134626, acc: 0.7700822264389627, test loss: 0.6908639979519786, acc: 0.7678290920515237
epoch: 148, train loss: 0.10900083366524382, acc: 0.9234776725304465, val loss: 0.6885354126050154, acc: 0.7735610373181531, test loss: 0.7240065924596232, acc: 0.763430725730443
epoch: 149, train loss: 0.1058355423207212, acc: 0.9238159675236807, val loss: 0.6953837717733076, acc: 0.7707147375079064, test loss: 0.731133032511736, acc: 0.7646874018221803
epoch: 150, train loss: 0.08944755349212473, acc: 0.9348443843031123, val loss: 0.703320095917607, acc: 0.7760910815939279, test loss: 0.7483583182012215, acc: 0.7769399937166196
epoch: 151, train loss: 0.09441665946550072, acc: 0.9306495263870095, val loss: 0.6829843278326315, acc: 0.7710309930423782, test loss: 0.7404701116509756, acc: 0.7593465284322966
epoch: 152, train loss: 0.09648151536029795, acc: 0.9328822733423545, val loss: 0.717073546867443, acc: 0.7741935483870968, test loss: 0.7311633023457672, acc: 0.7706566132579328
epoch: 153, train loss: 0.07743217093978425, acc: 0.9403924221921516, val loss: 0.73588498750116, acc: 0.7786211258697027, test loss: 0.767799626571219, acc: 0.7709707822808671
epoch: 154, train loss: 0.07605056111197994, acc: 0.9414749661705006, val loss: 0.7084680849204404, acc: 0.7884250474383302, test loss: 0.7764901426128348, acc: 0.7712849513038015
epoch: 155, train loss: 0.07310407210912692, acc: 0.9447902571041948, val loss: 0.737611511098969, acc: 0.7732447817836812, test loss: 0.7991723432280229, acc: 0.764373232799246
epoch: 156, train loss: 0.0800853247148097, acc: 0.9397834912043301, val loss: 0.7522788225737348, acc: 0.7833649588867805, test loss: 0.8092004789184033, acc: 0.7687715991203268
epoch: 157, train loss: 0.07425565989801461, acc: 0.9421515561569689, val loss: 0.7418829993610515, acc: 0.7808349146110057, test loss: 0.7768101019564193, acc: 0.7703424442349984
epoch: 158, train loss: 0.07646613301903372, acc: 0.9414749661705006, val loss: 0.76477924243775, acc: 0.7741935483870968, test loss: 0.7744576559851764, acc: 0.7687715991203268
epoch: 159, train loss: 0.0772161222843098, acc: 0.9407983761840325, val loss: 0.7874438874258564, acc: 0.773877292852625, test loss: 0.7988454440911161, acc: 0.7675149230285894
epoch: 160, train loss: 0.08976497051075444, acc: 0.9337618403247632, val loss: 0.7551402160444266, acc: 0.7741935483870968, test loss: 0.7606051177900616, acc: 0.7700282752120641
epoch: 161, train loss: 0.10190295304835892, acc: 0.9303112313937754, val loss: 0.725918243686886, acc: 0.7719797596457938, test loss: 0.7525044220481846, acc: 0.7631165567075087
epoch: 162, train loss: 0.1130432499237409, acc: 0.9200947225981055, val loss: 0.7550532964872907, acc: 0.7571157495256167, test loss: 0.7942757117384978, acc: 0.7628023876845743
epoch: 163, train loss: 0.08891949778238717, acc: 0.9349120433017591, val loss: 0.7385181626664318, acc: 0.7754585705249842, test loss: 0.7705845028733144, acc: 0.7712849513038015
epoch: 164, train loss: 0.08030972158150679, acc: 0.9389715832205683, val loss: 0.780110615103551, acc: 0.773877292852625, test loss: 0.8043155250705116, acc: 0.760603204524034
epoch: 165, train loss: 0.08566233346160597, acc: 0.9346414073071718, val loss: 0.8018405722486001, acc: 0.7586970271979759, test loss: 0.7990152549339172, acc: 0.7536914860194784
epoch: 166, train loss: 0.09089168644685384, acc: 0.9313261163734776, val loss: 0.762621351166061, acc: 0.7703984819734345, test loss: 0.7957773613847354, acc: 0.7650015708451147
epoch: 167, train loss: 0.11464844686212333, acc: 0.9222598105548038, val loss: 0.7163866644490753, acc: 0.7729285262492094, test loss: 0.7495413345620949, acc: 0.7681432610744581
epoch: 168, train loss: 0.10971651444052812, acc: 0.9238159675236807, val loss: 0.694318507231918, acc: 0.7741935483870968, test loss: 0.7306844355960406, acc: 0.7681432610744581
epoch: 169, train loss: 0.07721813004942805, acc: 0.9380243572395128, val loss: 0.7022308268779294, acc: 0.7881087919038583, test loss: 0.7678607582783796, acc: 0.7750549795790135
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.04609018508168454, acc: 0.9474966170500677, val loss: 0.6712590304452509, acc: 0.775774826059456, test loss: 0.6978692095346358, acc: 0.7728557964184731
epoch: 171, train loss: 0.052880861569846276, acc: 0.9419485791610284, val loss: 0.6653774745248677, acc: 0.7722960151802657, test loss: 0.7005264501769402, acc: 0.7659440779139177
epoch: 172, train loss: 0.04889128524028723, acc: 0.9443166441136671, val loss: 0.7230026615638661, acc: 0.7609108159392789, test loss: 0.7504147324141114, acc: 0.7518064718818724
epoch: 173, train loss: 0.04825222477699972, acc: 0.9460757780784844, val loss: 0.7003252287592637, acc: 0.7703984819734345, test loss: 0.7095254160044817, acc: 0.7631165567075087
epoch: 174, train loss: 0.045651506951156905, acc: 0.9484438430311232, val loss: 0.6500691251616022, acc: 0.7814674256799494, test loss: 0.7200792270975596, acc: 0.7637448947533774
epoch: 175, train loss: 0.04120450526153283, acc: 0.9504736129905278, val loss: 0.6657957934187154, acc: 0.7786211258697027, test loss: 0.70244526458618, acc: 0.7766258246936852
epoch: 176, train loss: 0.04678258265246397, acc: 0.9468200270635995, val loss: 0.7021270223541549, acc: 0.7675521821631879, test loss: 0.7270750390108823, acc: 0.7687715991203268
Epoch   176: reducing learning rate of group 0 to 3.7500e-04.
epoch: 177, train loss: 0.03338101229735011, acc: 0.9606224627875507, val loss: 0.6551685565487634, acc: 0.7890575585072739, test loss: 0.6983431687416463, acc: 0.7825950361294376
epoch: 178, train loss: 0.028238996615348552, acc: 0.9636671177266577, val loss: 0.6709739216364765, acc: 0.7783048703352309, test loss: 0.7166820740572432, acc: 0.7788250078542256
epoch: 179, train loss: 0.027245246864482257, acc: 0.966914749661705, val loss: 0.6783342534872944, acc: 0.7849462365591398, test loss: 0.7115655460336693, acc: 0.7851083883129123
epoch: 180, train loss: 0.02472031931966261, acc: 0.9663734776725305, val loss: 0.6882735809746006, acc: 0.7890575585072739, test loss: 0.7319335210184762, acc: 0.782909205152372
epoch: 181, train loss: 0.02368591066616233, acc: 0.9690798376184032, val loss: 0.6926558718961828, acc: 0.7877925363693865, test loss: 0.7390584393638205, acc: 0.782909205152372
epoch: 182, train loss: 0.02550687941146639, acc: 0.9671853856562923, val loss: 0.6940466947452997, acc: 0.7871600253004427, test loss: 0.7360114295865393, acc: 0.7803958529688972
epoch: 183, train loss: 0.023979574675559594, acc: 0.9698917456021651, val loss: 0.7094113270290888, acc: 0.7802024035420619, test loss: 0.748892826963587, acc: 0.7807100219918316
epoch: 184, train loss: 0.02523470475100213, acc: 0.9666441136671178, val loss: 0.6947390674262919, acc: 0.782416192283365, test loss: 0.7475657077319511, acc: 0.7750549795790135
epoch: 185, train loss: 0.025837646129368445, acc: 0.9677943166441136, val loss: 0.7217910428502603, acc: 0.7776723592662872, test loss: 0.744979879783453, acc: 0.7756833176248822
epoch: 186, train loss: 0.027638605561142685, acc: 0.9625845737483085, val loss: 0.7091268335694682, acc: 0.7789373814041746, test loss: 0.7532053302003271, acc: 0.7785108388312912
epoch: 187, train loss: 0.02613341278612049, acc: 0.9661705006765899, val loss: 0.7202297943743778, acc: 0.7814674256799494, test loss: 0.7463797984016867, acc: 0.7822808671065034
epoch: 188, train loss: 0.023488590139416784, acc: 0.9682679296346414, val loss: 0.7173281355344517, acc: 0.7830487033523087, test loss: 0.7465654913824084, acc: 0.7807100219918316
epoch: 189, train loss: 0.021364474926693352, acc: 0.969553450608931, val loss: 0.7400215777470144, acc: 0.7754585705249842, test loss: 0.7672112386555482, acc: 0.7756833176248822
epoch: 190, train loss: 0.02377035558823602, acc: 0.9682002706359946, val loss: 0.7638103665617884, acc: 0.7735610373181531, test loss: 0.7793195448392148, acc: 0.7693999371661954
epoch: 191, train loss: 0.02664003954275293, acc: 0.9662381596752369, val loss: 0.7234535283638208, acc: 0.7776723592662872, test loss: 0.7621384104429982, acc: 0.7750549795790135
epoch: 192, train loss: 0.025234536885590127, acc: 0.9678619756427604, val loss: 0.7182769196309446, acc: 0.7805186590765338, test loss: 0.7868251554478349, acc: 0.7737983034872762
epoch: 193, train loss: 0.023772660936751934, acc: 0.9694857916102841, val loss: 0.7374166591796296, acc: 0.7764073371283997, test loss: 0.7747307525578438, acc: 0.7690857681432611
epoch: 194, train loss: 0.026606741486084478, acc: 0.965764546684709, val loss: 0.7309830193275293, acc: 0.7839974699557243, test loss: 0.8002046531302128, acc: 0.7684574300973924
epoch: 195, train loss: 0.031430674472342324, acc: 0.9613667117726657, val loss: 0.7239919131047963, acc: 0.7726122707147375, test loss: 0.769120852302314, acc: 0.7693999371661954
epoch: 196, train loss: 0.032117377039100545, acc: 0.9600135317997294, val loss: 0.752495145134199, acc: 0.7748260594560404, test loss: 0.7815388392398389, acc: 0.7737983034872762
epoch: 197, train loss: 0.028199070534531495, acc: 0.9640730717185385, val loss: 0.7300637163443328, acc: 0.7754585705249842, test loss: 0.7657221716003826, acc: 0.7675149230285894
epoch: 198, train loss: 0.029887225334211197, acc: 0.9644790257104194, val loss: 0.7300629268938194, acc: 0.7748260594560404, test loss: 0.7561972412417379, acc: 0.7706566132579328
epoch: 199, train loss: 0.03314231547939116, acc: 0.9581190798376183, val loss: 0.7087291547725806, acc: 0.7754585705249842, test loss: 0.7445409058251172, acc: 0.7747408105560791
epoch: 200, train loss: 0.03363481682192166, acc: 0.9623815967523681, val loss: 0.7827464569828919, acc: 0.7605945604048071, test loss: 0.7924002005624726, acc: 0.7562048382029531
best val acc 0.7890575585072739 at epoch 177.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9963    0.9966    0.9964      5337
           1     0.9895    0.9321    0.9599       810
           2     0.9650    0.9976    0.9810      2100
           3     0.9891    0.9837    0.9864       737
           4     0.9239    0.9867    0.9543       677
           5     0.9814    0.9955    0.9884      1323
           6     0.9837    0.9356    0.9590      1164
           7     0.9722    0.9952    0.9836       421
           8     0.9950    0.9950    0.9950       401
           9     0.9706    1.0000    0.9851       396
          10     0.9872    0.9872    0.9872       627
          11     0.9863    0.9863    0.9863       291
          12     0.9789    0.7126    0.8248       261
          13     0.8819    0.9532    0.9162       235

    accuracy                         0.9809     14780
   macro avg     0.9715    0.9612    0.9645     14780
weighted avg     0.9813    0.9809    0.9805     14780

train confusion matrix:
[[9.96627319e-01 0.00000000e+00 5.62113547e-04 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.74742365e-04 2.06108301e-03
  3.74742365e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 9.32098765e-01 0.00000000e+00 0.00000000e+00
  6.66666667e-02 0.00000000e+00 1.23456790e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [9.52380952e-04 0.00000000e+00 9.97619048e-01 0.00000000e+00
  0.00000000e+00 9.52380952e-04 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.76190476e-04 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 1.35685210e-03 9.83717775e-01
  0.00000000e+00 5.42740841e-03 2.71370421e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 6.78426052e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.03397341e-02 0.00000000e+00 0.00000000e+00
  9.86706056e-01 2.95420975e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.51171580e-03
  7.55857899e-04 9.95464853e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.26757370e-03
  0.00000000e+00 0.00000000e+00]
 [9.45017182e-03 0.00000000e+00 1.71821306e-03 8.59106529e-04
  0.00000000e+00 1.20274914e-02 9.35567010e-01 0.00000000e+00
  0.00000000e+00 9.45017182e-03 2.57731959e-03 0.00000000e+00
  2.57731959e-03 2.57731959e-02]
 [4.75059382e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.95249406e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.49376559e-03 0.00000000e+00 0.00000000e+00
  9.95012469e-01 0.00000000e+00 0.00000000e+00 2.49376559e-03
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.59489633e-03 0.00000000e+00 7.97448166e-03
  0.00000000e+00 1.59489633e-03 1.59489633e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.87240829e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.43642612e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 3.43642612e-03 0.00000000e+00 3.43642612e-03
  0.00000000e+00 3.43642612e-03 0.00000000e+00 9.86254296e-01
  0.00000000e+00 0.00000000e+00]
 [1.53256705e-02 0.00000000e+00 2.68199234e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.83141762e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  7.12643678e-01 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 4.68085106e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.53191489e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8404    0.8845    0.8619      1143
           1     0.8650    0.8150    0.8393       173
           2     0.8032    0.7800    0.7914       450
           3     0.7987    0.8038    0.8013       158
           4     0.8493    0.8552    0.8522       145
           5     0.7973    0.8339    0.8152       283
           6     0.5779    0.5663    0.5720       249
           7     0.7711    0.7111    0.7399        90
           8     0.6923    0.6353    0.6626        85
           9     0.8452    0.8452    0.8452        84
          10     0.8205    0.7164    0.7649       134
          11     0.7778    0.6774    0.7241        62
          12     0.1639    0.1786    0.1709        56
          13     0.7297    0.5400    0.6207        50

    accuracy                         0.7891      3162
   macro avg     0.7380    0.7031    0.7187      3162
weighted avg     0.7886    0.7891    0.7880      3162

validation confusion matrix:
[[8.84514436e-01 5.24934383e-03 3.23709536e-02 2.62467192e-03
  0.00000000e+00 6.99912511e-03 2.71216098e-02 9.62379703e-03
  8.74890639e-03 1.74978128e-03 8.74890639e-04 5.24934383e-03
  1.39982502e-02 8.74890639e-04]
 [4.62427746e-02 8.15028902e-01 5.78034682e-03 5.78034682e-03
  8.67052023e-02 1.73410405e-02 0.00000000e+00 0.00000000e+00
  1.15606936e-02 0.00000000e+00 5.78034682e-03 5.78034682e-03
  0.00000000e+00 0.00000000e+00]
 [8.88888889e-02 2.22222222e-03 7.80000000e-01 1.11111111e-02
  0.00000000e+00 1.77777778e-02 5.11111111e-02 0.00000000e+00
  2.22222222e-03 2.22222222e-03 4.44444444e-03 0.00000000e+00
  4.00000000e-02 0.00000000e+00]
 [3.79746835e-02 6.32911392e-03 3.16455696e-02 8.03797468e-01
  0.00000000e+00 1.89873418e-02 4.43037975e-02 6.32911392e-03
  0.00000000e+00 6.32911392e-03 3.16455696e-02 0.00000000e+00
  1.26582278e-02 0.00000000e+00]
 [1.37931034e-02 4.13793103e-02 6.89655172e-03 0.00000000e+00
  8.55172414e-01 8.27586207e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.59363958e-02 1.76678445e-02 7.06713781e-03 2.47349823e-02
  2.12014134e-02 8.33922261e-01 1.06007067e-02 3.53356890e-03
  1.06007067e-02 0.00000000e+00 7.06713781e-03 3.53356890e-03
  1.41342756e-02 0.00000000e+00]
 [1.48594378e-01 0.00000000e+00 7.63052209e-02 2.81124498e-02
  4.01606426e-03 3.61445783e-02 5.66265060e-01 1.60642570e-02
  4.01606426e-03 2.40963855e-02 2.81124498e-02 4.01606426e-03
  3.61445783e-02 2.81124498e-02]
 [2.44444444e-01 0.00000000e+00 0.00000000e+00 1.11111111e-02
  0.00000000e+00 0.00000000e+00 2.22222222e-02 7.11111111e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.11111111e-02
  0.00000000e+00 0.00000000e+00]
 [2.23529412e-01 3.52941176e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 8.23529412e-02 1.17647059e-02 0.00000000e+00
  6.35294118e-01 0.00000000e+00 0.00000000e+00 1.17647059e-02
  0.00000000e+00 0.00000000e+00]
 [7.14285714e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 5.95238095e-02 0.00000000e+00
  0.00000000e+00 8.45238095e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.38095238e-02]
 [3.73134328e-02 0.00000000e+00 2.98507463e-02 2.98507463e-02
  0.00000000e+00 3.73134328e-02 9.70149254e-02 7.46268657e-03
  2.23880597e-02 1.49253731e-02 7.16417910e-01 0.00000000e+00
  7.46268657e-03 0.00000000e+00]
 [2.09677419e-01 0.00000000e+00 1.61290323e-02 1.61290323e-02
  0.00000000e+00 3.22580645e-02 0.00000000e+00 0.00000000e+00
  4.83870968e-02 0.00000000e+00 0.00000000e+00 6.77419355e-01
  0.00000000e+00 0.00000000e+00]
 [2.32142857e-01 0.00000000e+00 2.67857143e-01 3.57142857e-02
  0.00000000e+00 3.57142857e-02 1.42857143e-01 1.78571429e-02
  1.78571429e-02 0.00000000e+00 5.35714286e-02 1.78571429e-02
  1.78571429e-01 0.00000000e+00]
 [1.60000000e-01 0.00000000e+00 2.00000000e-02 2.00000000e-02
  0.00000000e+00 2.00000000e-02 2.00000000e-01 0.00000000e+00
  0.00000000e+00 2.00000000e-02 0.00000000e+00 0.00000000e+00
  2.00000000e-02 5.40000000e-01]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8592    0.8742    0.8667      1145
           1     0.8618    0.7486    0.8012       175
           2     0.7952    0.8093    0.8022       451
           3     0.8255    0.7736    0.7987       159
           4     0.8857    0.8493    0.8671       146
           5     0.8379    0.8556    0.8467       284
           6     0.4795    0.5600    0.5166       250
           7     0.7474    0.7802    0.7634        91
           8     0.6344    0.6782    0.6556        87
           9     0.8072    0.7791    0.7929        86
          10     0.7810    0.6029    0.6805       136
          11     0.6269    0.6562    0.6412        64
          12     0.2600    0.2281    0.2430        57
          13     0.6977    0.5769    0.6316        52

    accuracy                         0.7826      3183
   macro avg     0.7214    0.6980    0.7077      3183
weighted avg     0.7860    0.7826    0.7831      3183

test confusion matrix:
[[8.74235808e-01 3.49344978e-03 2.18340611e-02 5.24017467e-03
  8.73362445e-04 4.36681223e-03 3.49344978e-02 1.48471616e-02
  1.74672489e-02 2.62008734e-03 1.74672489e-03 4.36681223e-03
  1.13537118e-02 2.62008734e-03]
 [2.85714286e-02 7.48571429e-01 2.85714286e-02 0.00000000e+00
  8.57142857e-02 4.00000000e-02 1.71428571e-02 5.71428571e-03
  2.85714286e-02 0.00000000e+00 1.14285714e-02 5.71428571e-03
  0.00000000e+00 0.00000000e+00]
 [8.20399113e-02 4.43458980e-03 8.09312639e-01 2.21729490e-03
  0.00000000e+00 4.43458980e-03 6.20842572e-02 0.00000000e+00
  2.21729490e-03 2.21729490e-03 2.21729490e-03 0.00000000e+00
  2.66075388e-02 2.21729490e-03]
 [6.28930818e-03 6.28930818e-03 3.77358491e-02 7.73584906e-01
  0.00000000e+00 3.77358491e-02 7.54716981e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.14465409e-02 1.88679245e-02
  1.25786164e-02 0.00000000e+00]
 [6.84931507e-03 5.47945205e-02 1.36986301e-02 0.00000000e+00
  8.49315068e-01 6.84931507e-02 6.84931507e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [5.28169014e-02 7.04225352e-03 2.81690141e-02 7.04225352e-03
  0.00000000e+00 8.55633803e-01 2.11267606e-02 0.00000000e+00
  1.40845070e-02 0.00000000e+00 0.00000000e+00 7.04225352e-03
  3.52112676e-03 3.52112676e-03]
 [1.52000000e-01 8.00000000e-03 1.00000000e-01 2.00000000e-02
  0.00000000e+00 2.40000000e-02 5.60000000e-01 1.20000000e-02
  4.00000000e-03 2.40000000e-02 3.60000000e-02 1.60000000e-02
  1.60000000e-02 2.80000000e-02]
 [1.42857143e-01 0.00000000e+00 1.09890110e-02 0.00000000e+00
  0.00000000e+00 1.09890110e-02 1.09890110e-02 7.80219780e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.19780220e-02
  2.19780220e-02 0.00000000e+00]
 [2.06896552e-01 0.00000000e+00 1.14942529e-02 0.00000000e+00
  0.00000000e+00 2.29885057e-02 1.14942529e-02 0.00000000e+00
  6.78160920e-01 0.00000000e+00 1.14942529e-02 4.59770115e-02
  0.00000000e+00 1.14942529e-02]
 [8.13953488e-02 0.00000000e+00 2.32558140e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.30232558e-02 1.16279070e-02
  0.00000000e+00 7.79069767e-01 0.00000000e+00 1.16279070e-02
  0.00000000e+00 0.00000000e+00]
 [5.14705882e-02 1.47058824e-02 1.47058824e-02 7.35294118e-02
  0.00000000e+00 2.94117647e-02 1.39705882e-01 7.35294118e-03
  1.47058824e-02 2.20588235e-02 6.02941176e-01 2.20588235e-02
  7.35294118e-03 0.00000000e+00]
 [1.09375000e-01 0.00000000e+00 3.12500000e-02 1.56250000e-02
  0.00000000e+00 3.12500000e-02 4.68750000e-02 1.56250000e-02
  1.56250000e-02 1.56250000e-02 3.12500000e-02 6.56250000e-01
  3.12500000e-02 0.00000000e+00]
 [2.28070175e-01 0.00000000e+00 2.10526316e-01 1.75438596e-02
  0.00000000e+00 3.50877193e-02 2.28070175e-01 0.00000000e+00
  0.00000000e+00 3.50877193e-02 1.75438596e-02 0.00000000e+00
  2.28070175e-01 0.00000000e+00]
 [3.84615385e-02 0.00000000e+00 5.76923077e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.26923077e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.76923077e-01]]
---------------------------------------
program finished.
