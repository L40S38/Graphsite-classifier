seed:  666
save trained model at:  ../trained_models/trained_classifier_model_7.pt
save loss at:  ./results/train_classifier_results_7.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  100
learning rate decay at epoch:  60
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(80, 160)
  )
  (fc1): Linear(in_features=160, out_features=80, bias=True)
  (fc2): Linear(in_features=80, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [20, 60, 100]
loss function:
FocalLoss(gamma=0, alpha=1, reduction=mean)
begin training...
epoch: 1, train loss: 2.289797097636807, acc: 0.3318869540393948, val loss: 1.9941625037701058, acc: 0.4096610621830798, test loss: 1.9952228494059623, acc: 0.4003181336161188
epoch: 2, train loss: 1.9950532142754591, acc: 0.4043962318013132, val loss: 1.8388656209493148, acc: 0.44542300507072324, test loss: 1.8323089105579784, acc: 0.4546659597030753
epoch: 3, train loss: 1.8740665291705336, acc: 0.4405937767627748, val loss: 1.8084651058376202, acc: 0.44435548438751, test loss: 1.7922045668766835, acc: 0.4567868504772004
epoch: 4, train loss: 1.7885750920507795, acc: 0.46605766485869254, val loss: 1.7027733436036752, acc: 0.4785161462503336, test loss: 1.675948889106354, acc: 0.4848886532343584
epoch: 5, train loss: 1.7423455184710015, acc: 0.4746788467028262, val loss: 1.7369410518903812, acc: 0.4638377368561516, test loss: 1.7157721112414104, acc: 0.46712619300106045
epoch: 6, train loss: 1.7129936528621046, acc: 0.48569797316585783, val loss: 1.6619643526201984, acc: 0.4931945556445156, test loss: 1.6616103257253088, acc: 0.4992046659597031
epoch: 7, train loss: 1.663673095848776, acc: 0.5004282043962318, val loss: 1.62555445465432, acc: 0.49693087803576197, test loss: 1.6201603303911325, acc: 0.5021208907741251
epoch: 8, train loss: 1.6298139771971403, acc: 0.5096203254353411, val loss: 1.6129475317543462, acc: 0.5022684814518281, test loss: 1.5830031510754583, acc: 0.5106044538706257
epoch: 9, train loss: 1.590789029811268, acc: 0.514987153868113, val loss: 1.6504573833474738, acc: 0.489191353082466, test loss: 1.6405269477678381, acc: 0.4851537645811241
epoch: 10, train loss: 1.561009182230323, acc: 0.5292035398230088, val loss: 1.5535298432863518, acc: 0.5188150520416333, test loss: 1.5405388869511107, acc: 0.5198833510074231
epoch: 11, train loss: 1.5454309008511482, acc: 0.5309163574079361, val loss: 1.511475333538824, acc: 0.5350947424606352, test loss: 1.4981469359655775, acc: 0.5357900318133616
epoch: 12, train loss: 1.521785342485742, acc: 0.5407936054810163, val loss: 1.5265902018973374, acc: 0.5284227381905524, test loss: 1.5226978494658323, acc: 0.524390243902439
epoch: 13, train loss: 1.4894514782783341, acc: 0.5509563231515844, val loss: 1.5126053950994658, acc: 0.5300240192153723, test loss: 1.5079643597405636, acc: 0.5363202545068929
epoch: 14, train loss: 1.4906261876348832, acc: 0.5508421353125892, val loss: 1.6714720567576498, acc: 0.5148118494795837, test loss: 1.6737064422757097, acc: 0.5119300106044539
epoch: 15, train loss: 1.4902177958770237, acc: 0.550214102198116, val loss: 1.472916058740649, acc: 0.5414998665599146, test loss: 1.4614825314454032, acc: 0.5463944856839873
epoch: 16, train loss: 1.4326305145949185, acc: 0.563459891521553, val loss: 1.467161120240899, acc: 0.5575126768081131, test loss: 1.4443139488851278, acc: 0.5455991516436903
epoch: 17, train loss: 1.4174746492676078, acc: 0.5672851841278904, val loss: 1.477343858004189, acc: 0.5356285028022418, test loss: 1.470400939437769, acc: 0.5381760339342524
epoch: 18, train loss: 1.4108104811918045, acc: 0.5753925206965458, val loss: 1.4503854370957092, acc: 0.5511075527088337, test loss: 1.4442429001485555, acc: 0.5509013785790032
epoch: 19, train loss: 1.3893152182149167, acc: 0.5796174707393662, val loss: 1.4848439134150337, acc: 0.5561782759540966, test loss: 1.4655249257861367, acc: 0.559119830328738
epoch 20, gamma increased to 1.
epoch: 20, train loss: 1.148614572399928, acc: 0.5836140451041965, val loss: 1.1407986235103196, acc: 0.5791299706431812, test loss: 1.1441029850018238, acc: 0.5713149522799575
epoch: 21, train loss: 1.1183519502962926, acc: 0.5937767627747644, val loss: 1.1219340564411164, acc: 0.5890045369629037, test loss: 1.1189621473294173, acc: 0.5792682926829268
epoch: 22, train loss: 1.108998325386287, acc: 0.5915500999143591, val loss: 1.1172234831983832, acc: 0.5903389378169202, test loss: 1.1288308116310095, acc: 0.5803287380699894
epoch: 23, train loss: 1.0909847336124427, acc: 0.6006280331144733, val loss: 1.1574782966962966, acc: 0.577261809447558, test loss: 1.1442951837501243, acc: 0.5718451749734889
epoch: 24, train loss: 1.1233070423968547, acc: 0.5909791607193834, val loss: 1.1812068545598808, acc: 0.5665866026154257, test loss: 1.1677500716196265, acc: 0.5662778366914104
epoch: 25, train loss: 1.0850489934103984, acc: 0.5997145304025121, val loss: 1.0663571838127444, acc: 0.600480384307446, test loss: 1.0711102692977241, acc: 0.5970307529162248
epoch: 26, train loss: 1.0661651894394888, acc: 0.6054810162717671, val loss: 1.0997839743275943, acc: 0.5983453429410195, test loss: 1.0876791788184124, acc: 0.5927889713679746
epoch: 27, train loss: 1.0604611425749615, acc: 0.6067370825007137, val loss: 1.1097824247099095, acc: 0.5828662930344275, test loss: 1.1308651612646885, acc: 0.5787380699893956
epoch: 28, train loss: 1.0315366484320507, acc: 0.6193548387096774, val loss: 1.051183352509848, acc: 0.6148919135308246, test loss: 1.0642890060440986, acc: 0.6076352067868505
epoch: 29, train loss: 1.039737727721691, acc: 0.6098201541535826, val loss: 1.0572429918625657, acc: 0.600480384307446, test loss: 1.0756909880127405, acc: 0.5967656415694592
epoch: 30, train loss: 1.0138385819667344, acc: 0.6239223522694833, val loss: 1.0531662522171668, acc: 0.6023485455030692, test loss: 1.0616021330763654, acc: 0.6073700954400848
epoch: 31, train loss: 1.0012019625330801, acc: 0.6261490151298886, val loss: 1.138688884864402, acc: 0.5775286896183613, test loss: 1.1404046835348185, acc: 0.5792682926829268
epoch: 32, train loss: 1.0070707930995844, acc: 0.6251213245789323, val loss: 1.0396892114547147, acc: 0.614358153189218, test loss: 1.04326132701538, acc: 0.6079003181336161
epoch: 33, train loss: 0.9898862087893343, acc: 0.6351127604910077, val loss: 1.0588363306281214, acc: 0.6026154256738724, test loss: 1.035272820756898, acc: 0.6079003181336161
epoch: 34, train loss: 0.9760158127592252, acc: 0.6331715672280902, val loss: 1.0527923670265558, acc: 0.6015479049906592, test loss: 1.0603949067954168, acc: 0.6084305408271474
epoch: 35, train loss: 0.9684004597346714, acc: 0.6387096774193548, val loss: 1.107087490747603, acc: 0.589271417133707, test loss: 1.111505864407326, acc: 0.5988865323435844
epoch: 36, train loss: 0.96247595306672, acc: 0.6386525834998573, val loss: 1.0337542949882164, acc: 0.6127568721643982, test loss: 1.0204340160082659, acc: 0.619034994697773
epoch: 37, train loss: 0.949705406205844, acc: 0.6403654010847845, val loss: 1.100670131645681, acc: 0.5855350947424607, test loss: 1.1144887050919923, acc: 0.5909331919406151
epoch: 38, train loss: 0.9570120986175646, acc: 0.640822152440765, val loss: 1.0514370353501226, acc: 0.6060848678943155, test loss: 1.0557154969471256, acc: 0.6031283138918345
epoch: 39, train loss: 0.9330281487063888, acc: 0.6515558093063089, val loss: 1.0012051241538478, acc: 0.6223645583133173, test loss: 1.0231902245372881, acc: 0.6219512195121951
epoch: 40, train loss: 0.927656429636795, acc: 0.6529260633742506, val loss: 0.9823070999651488, acc: 0.6330397651454497, test loss: 1.0019457261863216, acc: 0.6306998939554613
epoch: 41, train loss: 0.9139667210393791, acc: 0.6537824721667143, val loss: 0.9907831574937074, acc: 0.6266346410461703, test loss: 0.9943244297962047, acc: 0.6280487804878049
epoch: 42, train loss: 0.9053793895765267, acc: 0.6553811019126463, val loss: 0.983436443215852, acc: 0.63063784360822, test loss: 0.9780861486938573, acc: 0.6344114528101803
epoch: 43, train loss: 0.9028787566054864, acc: 0.6559520411076221, val loss: 1.154470964848661, acc: 0.5834000533760342, test loss: 1.1523176405108746, acc: 0.5925238600212089
epoch: 44, train loss: 0.9041561525411821, acc: 0.6560091350271197, val loss: 1.0251428778088185, acc: 0.6204963971176941, test loss: 1.0532857732075016, acc: 0.616118769883351
epoch: 45, train loss: 0.8937183687491584, acc: 0.6612046817013988, val loss: 1.0605093301503792, acc: 0.6172938350680545, test loss: 1.0642675397757635, acc: 0.6155885471898197
epoch: 46, train loss: 0.8748945211261604, acc: 0.670282614901513, val loss: 1.0342279728934516, acc: 0.6199626367760875, test loss: 1.0454133631442284, acc: 0.6142629904559915
epoch: 47, train loss: 0.8686895440110063, acc: 0.6667427918926634, val loss: 0.9886896505271845, acc: 0.6351748065118762, test loss: 1.0294422938993095, acc: 0.6304347826086957
epoch: 48, train loss: 0.8664176765626205, acc: 0.6702255209820154, val loss: 1.0487310309266293, acc: 0.6357085668534828, test loss: 1.0496958874695375, acc: 0.6362672322375398
epoch: 49, train loss: 0.8794276308679867, acc: 0.6674850128461319, val loss: 1.0055500758562783, acc: 0.6218307979717107, test loss: 1.0226953398764196, acc: 0.6163838812301167
epoch: 50, train loss: 0.8881357408338978, acc: 0.6624607479303454, val loss: 0.9489230714481863, acc: 0.6434480918067788, test loss: 0.9659896341892215, acc: 0.6420996818663839
epoch: 51, train loss: 0.8384381812877257, acc: 0.677248073080217, val loss: 0.9297090597333417, acc: 0.6442487323191887, test loss: 0.9798781965342711, acc: 0.6410392364793213
epoch: 52, train loss: 0.8472305663680813, acc: 0.6795889237796174, val loss: 1.0044040760740713, acc: 0.6290365625834, test loss: 1.0287221732094078, acc: 0.620625662778367
epoch: 53, train loss: 0.8487616944986855, acc: 0.6766200399657436, val loss: 0.9462804927296533, acc: 0.6538564184681078, test loss: 0.947227898989098, acc: 0.6561505832449629
epoch: 54, train loss: 0.8265913353761264, acc: 0.6841564373394233, val loss: 1.0275849468841023, acc: 0.6207632772884975, test loss: 1.0395313790848248, acc: 0.6113467656415694
epoch: 55, train loss: 0.8168304354947804, acc: 0.6835854981444476, val loss: 0.9963150033322149, acc: 0.6319722444622364, test loss: 1.0294233981523888, acc: 0.6216861081654295
epoch: 56, train loss: 0.8047205387065658, acc: 0.6891236083357123, val loss: 1.052074863588075, acc: 0.6269015212169736, test loss: 1.0743561124751069, acc: 0.6118769883351007
epoch: 57, train loss: 0.7934527144342227, acc: 0.6959177847559235, val loss: 0.912677758529659, acc: 0.6581265012009607, test loss: 0.9244063173599486, acc: 0.6603923647932132
epoch: 58, train loss: 0.7685835325612431, acc: 0.7044247787610619, val loss: 1.150704131878501, acc: 0.6002135041366427, test loss: 1.1672873312443464, acc: 0.5965005302226936
epoch: 59, train loss: 0.8144224651874626, acc: 0.6878675421067656, val loss: 0.9521523053912694, acc: 0.6458500133440085, test loss: 0.989055127885157, acc: 0.6394485683987274
epoch 60, gamma increased to 2.
epoch: 60, train loss: 0.5980969516542888, acc: 0.7172138167285184, val loss: 0.7132246939379405, acc: 0.6874833199893248, test loss: 0.7528263961270152, acc: 0.6884941675503712
epoch: 61, train loss: 0.5526785868016645, acc: 0.7324578932343705, val loss: 0.8142011504444339, acc: 0.655724579663731, test loss: 0.8460507059147856, acc: 0.6455461293743372
epoch: 62, train loss: 0.5445821272435272, acc: 0.7330288324293462, val loss: 0.7407052840491692, acc: 0.6832132372564719, test loss: 0.7482332120138919, acc: 0.6816012725344645
epoch: 63, train loss: 0.5306513026888425, acc: 0.7383956608621182, val loss: 0.7460654509808815, acc: 0.6821457165732586, test loss: 0.7637765959995549, acc: 0.6802757158006363
epoch: 64, train loss: 0.5276453301765699, acc: 0.7409077933200114, val loss: 0.7472741301294514, acc: 0.6826794769148652, test loss: 0.7844101725302218, acc: 0.6702014846235419
epoch: 65, train loss: 0.519130929027528, acc: 0.7407365115615187, val loss: 0.7455731867344054, acc: 0.685882038964505, test loss: 0.7641942109181797, acc: 0.6839872746553552
epoch: 66, train loss: 0.5266679413159371, acc: 0.7395375392520697, val loss: 0.7348630777638469, acc: 0.6829463570856685, test loss: 0.7470050083111857, acc: 0.6866383881230117
epoch: 67, train loss: 0.5142327189037128, acc: 0.7439337710533828, val loss: 0.767619825344834, acc: 0.6802775553776355, test loss: 0.7880355328289853, acc: 0.6770943796394485
epoch: 68, train loss: 0.5120329830056424, acc: 0.7434199257779046, val loss: 0.7413852123187197, acc: 0.6832132372564719, test loss: 0.7751571268696548, acc: 0.6778897136797455
epoch: 69, train loss: 0.5066546169598035, acc: 0.74564658863831, val loss: 0.7621522404080491, acc: 0.6853482786228983, test loss: 0.776260931696644, acc: 0.6845174973488866
epoch: 70, train loss: 0.4967672892920875, acc: 0.7466171852697687, val loss: 0.7758101475916451, acc: 0.6760074726447824, test loss: 0.8077350115346251, acc: 0.6622481442205727
epoch: 71, train loss: 0.5033012853906116, acc: 0.7437624892948901, val loss: 0.7286762861623934, acc: 0.6816119562316519, test loss: 0.760232381577962, acc: 0.6810710498409331
epoch: 72, train loss: 0.48815465033513766, acc: 0.7524407650585213, val loss: 0.7774903821983368, acc: 0.669602348545503, test loss: 0.7898940730322457, acc: 0.6712619300106044
epoch: 73, train loss: 0.47239435894333426, acc: 0.7551812731944048, val loss: 0.828379348910647, acc: 0.677341873498799, test loss: 0.816373520160404, acc: 0.6762990455991517
epoch: 74, train loss: 0.4826960082772865, acc: 0.7515843562660577, val loss: 0.7980052790770316, acc: 0.6749399519615693, test loss: 0.7915044375719978, acc: 0.6808059384941676
epoch: 75, train loss: 0.4763470657631359, acc: 0.754210676562946, val loss: 0.7844915494684668, acc: 0.6698692287163064, test loss: 0.7889447671105475, acc: 0.6720572640509014
epoch: 76, train loss: 0.48729844799458283, acc: 0.7494718812446475, val loss: 0.7823546581151233, acc: 0.6666666666666666, test loss: 0.8077211313004964, acc: 0.6675503711558854
epoch: 77, train loss: 0.4711433467305527, acc: 0.7562660576648587, val loss: 0.7667758455268854, acc: 0.6877502001601281, test loss: 0.7696938153162741, acc: 0.6842523860021209
epoch: 78, train loss: 0.4764620311322159, acc: 0.7563231515843563, val loss: 0.7750785738937627, acc: 0.678142514011209, test loss: 0.8022867505648988, acc: 0.6781548250265111
epoch: 79, train loss: 0.48375594703054686, acc: 0.7508992292320867, val loss: 0.747436554743062, acc: 0.6872164398185214, test loss: 0.7844126996913113, acc: 0.6823966065747614
epoch: 80, train loss: 0.48407338934900285, acc: 0.7573508421353126, val loss: 0.7552322257128212, acc: 0.6816119562316519, test loss: 0.7726311574937935, acc: 0.6805408271474019
epoch: 81, train loss: 0.44948282046145177, acc: 0.7659720239794462, val loss: 0.7466870914117667, acc: 0.6978916466506538, test loss: 0.7553909076486387, acc: 0.6990986214209968
epoch: 82, train loss: 0.43710714574034404, acc: 0.7687696260348272, val loss: 0.7756084832694647, acc: 0.6906858820389645, test loss: 0.7776572963465815, acc: 0.694591728525981
epoch: 83, train loss: 0.4520255402602572, acc: 0.7615186982586355, val loss: 0.759991944361153, acc: 0.682412596744062, test loss: 0.7746151350715127, acc: 0.6882290562036055
epoch: 84, train loss: 0.45520804329936654, acc: 0.7632886097630602, val loss: 0.8454103131341336, acc: 0.6706698692287163, test loss: 0.8177819091467184, acc: 0.6879639448568399
epoch: 85, train loss: 0.44779718860094797, acc: 0.761575792178133, val loss: 0.8357179967250574, acc: 0.6677341873498799, test loss: 0.8403990117009346, acc: 0.6749734888653235
epoch: 86, train loss: 0.43667185110192347, acc: 0.7660291178989438, val loss: 0.801807009317985, acc: 0.6840138777688818, test loss: 0.8292486580406747, acc: 0.6866383881230117
epoch: 87, train loss: 0.41688251967366136, acc: 0.7753354267770483, val loss: 0.842986241830073, acc: 0.6661329063250601, test loss: 0.8717803888078206, acc: 0.6702014846235419
epoch: 88, train loss: 0.4341436309199179, acc: 0.7643733942335141, val loss: 0.7706860092311724, acc: 0.6776087536696024, test loss: 0.8072749835183234, acc: 0.6808059384941676
epoch: 89, train loss: 0.4295592624410166, acc: 0.7687125321153297, val loss: 0.7596734751692956, acc: 0.6888177208433414, test loss: 0.7781240014736625, acc: 0.6919406150583245
epoch: 90, train loss: 0.41307027394384305, acc: 0.7755638024550385, val loss: 0.7782998773489821, acc: 0.6877502001601281, test loss: 0.8112963750278962, acc: 0.689289501590668
epoch: 91, train loss: 0.3989618732542642, acc: 0.7790465315443905, val loss: 0.7585313899821843, acc: 0.6936215639178009, test loss: 0.7898230651276994, acc: 0.6922057264050901
epoch: 92, train loss: 0.3970165039483663, acc: 0.7799029403368541, val loss: 0.8076429421786534, acc: 0.6792100346944222, test loss: 0.8205899863581895, acc: 0.6831919406150583
epoch: 93, train loss: 0.40906436336472685, acc: 0.7777333713959463, val loss: 0.8257087115640795, acc: 0.6778756338404056, test loss: 0.8565062111281135, acc: 0.6821314952279958
epoch: 94, train loss: 0.41424125300589, acc: 0.7766485869254924, val loss: 0.7956895717532277, acc: 0.6749399519615693, test loss: 0.8053231976317955, acc: 0.6821314952279958
epoch: 95, train loss: 0.4032039971018804, acc: 0.7764202112475022, val loss: 0.7889703945188418, acc: 0.6856151587937016, test loss: 0.806817721460935, acc: 0.6927359490986215
epoch: 96, train loss: 0.43545521497386136, acc: 0.7663145874964317, val loss: 0.8466773155310137, acc: 0.6663997864958634, test loss: 0.8720364168618163, acc: 0.6662248144220573
epoch: 97, train loss: 0.4390517412050772, acc: 0.7618041678561234, val loss: 0.7788567582479884, acc: 0.6933546837469976, test loss: 0.7993285092164704, acc: 0.6959172852598091
epoch: 98, train loss: 0.397055935689527, acc: 0.7825292606337425, val loss: 0.7904226893833997, acc: 0.6960234854550307, test loss: 0.8102066398046176, acc: 0.6869034994697774
epoch: 99, train loss: 0.38521187251511896, acc: 0.7821296031972594, val loss: 0.8195200480554529, acc: 0.677341873498799, test loss: 0.8399425812010295, acc: 0.6755037115588547
epoch 100, gamma increased to 3.
epoch: 100, train loss: 0.31334339367247566, acc: 0.7828147302312304, val loss: 0.6870152144105014, acc: 0.6818788364024553, test loss: 0.7069146027620625, acc: 0.6906150583244963
best val loss 0.6870152144105014 at epoch 100.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.8903    0.9676    0.9274      5337
           1     0.7291    0.7098    0.7193      2502
           2     0.9533    0.8321    0.8886       810
           3     0.8008    0.7560    0.7777      1840
           4     0.9396    0.8440    0.8892       737
           5     0.8281    0.9823    0.8986       677
           6     0.7082    0.9448    0.8096      1323
           7     0.5656    0.7277    0.6365       907
           8     0.9547    0.7007    0.8082       421
           9     0.8582    0.8454    0.8518       401
          10     0.8886    0.9672    0.9262       396
          11     0.9811    0.9398    0.9600       332
          12     0.6963    0.6373    0.6655       295
          13     0.8051    0.7663    0.7852       291
          14     0.0000    0.0000    0.0000       261
          15     0.7850    0.3178    0.4524       494
          16     0.9000    0.0352    0.0677       256
          17     0.7982    0.7745    0.7862       235

    accuracy                         0.8159     17515
   macro avg     0.7824    0.7082    0.7139     17515
weighted avg     0.8104    0.8159    0.8010     17515

train confusion matrix:
[[5164   25    1   23    0    0   13   23   12   25    3    0    2   34
     0    0    0   12]
 [ 162 1776    6   99   17    1  346   75    0    4    1    0   10    1
     0    1    0    3]
 [   4    2  674    0    0  110    6    3    0    5    0    0    3    2
     0    1    0    0]
 [ 107  253    1 1391    1    0    3   27    0    4    2    3    0    4
     0   35    0    9]
 [   1   43    2   11  622    0    4   26    0    0    5    0   23    0
     0    0    0    0]
 [   1    0    4    0    0  665    6    0    0    1    0    0    0    0
     0    0    0    0]
 [   8   34    4    0    0   19 1250    3    0    0    0    0    2    2
     0    1    0    0]
 [  56   91    2   11    4    0   12  660    0    0   20    0   34    0
     0    0    1   16]
 [ 111    1    0    1    0    0    0    5  295    1    0    0    2    5
     0    0    0    0]
 [  48    4    1    0    0    1    4    0    0  339    0    1    0    3
     0    0    0    0]
 [   5    1    0    0    0    0    0    4    0    0  383    0    0    1
     0    1    0    1]
 [   2    1    1    2    0    0    0    0    0    8    0  312    0    2
     0    4    0    0]
 [   5    3    9    0   12    0    3   74    0    0    1    0  188    0
     0    0    0    0]
 [  41    3    0    1    1    1    6    0    2    6    6    0    1  223
     0    0    0    0]
 [  53  136    0   47    1    0    0   21    0    0    3    0    0    0
     0    0    0    0]
 [  11   56    0  148    1    6  111    0    0    2    0    2    0    0
     0  157    0    0]
 [  14    7    0    2    2    0    1  205    0    0    6    0    5    0
     0    0    9    5]
 [   7    0    2    1    1    0    0   41    0    0    1    0    0    0
     0    0    0  182]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.7916    0.8609    0.8248      1143
           1     0.5663    0.5336    0.5495       536
           2     0.8239    0.6763    0.7429       173
           3     0.6457    0.6523    0.6490       394
           4     0.8295    0.6772    0.7456       158
           5     0.7228    0.9172    0.8085       145
           6     0.6138    0.8481    0.7122       283
           7     0.3536    0.4794    0.4070       194
           8     0.8846    0.5111    0.6479        90
           9     0.5176    0.5176    0.5176        85
          10     0.8132    0.8810    0.8457        84
          11     0.9333    0.7887    0.8550        71
          12     0.5400    0.4286    0.4779        63
          13     0.5806    0.5806    0.5806        62
          14     0.0000    0.0000    0.0000        56
          15     0.6410    0.2381    0.3472       105
          16     0.2500    0.0182    0.0339        55
          17     0.5918    0.5800    0.5859        50

    accuracy                         0.6819      3747
   macro avg     0.6166    0.5660    0.5740      3747
weighted avg     0.6732    0.6819    0.6685      3747

validation confusion matrix:
[[984  30   2  34   0   1   8  32   5  21   5   1   1  13   0   0   0   6]
 [ 57 286   5  43   7   5  70  43   0   7   1   0   6   1   0   3   0   2]
 [  7   2 117   1   1  30   9   1   0   2   0   0   1   2   0   0   0   0]
 [ 35  56   4 257   0   2   6  18   0   2   0   1   0   0   0  10   1   2]
 [  2  24   0   4 107   0   7   3   1   0   3   0   6   1   0   0   0   0]
 [  1   1   2   0   0 133   8   0   0   0   0   0   0   0   0   0   0   0]
 [  4  17   6   0   2   5 240   2   0   2   0   0   2   2   0   1   0   0]
 [ 36  24   0  10   4   0   7  93   0   1   6   0   4   3   0   0   1   5]
 [ 35   1   0   0   0   0   1   4  46   1   0   0   1   1   0   0   0   0]
 [ 24   5   3   1   0   2   4   0   0  44   0   0   0   2   0   0   0   0]
 [  1   1   0   0   1   0   0   5   0   0  74   0   0   0   0   0   0   2]
 [  0   4   1   6   0   0   0   0   0   4   0  56   0   0   0   0   0   0]
 [  7  10   0   0   3   0   2  13   0   0   0   0  27   1   0   0   0   0]
 [ 15   1   1   0   1   1   3   0   0   1   0   1   1  36   0   0   0   1]
 [ 15  19   1   9   1   0   0  10   0   0   0   1   0   0   0   0   0   0]
 [  7  14   0  28   0   5  25   0   0   0   0   0   0   0   0  25   0   1]
 [  8  10   0   5   2   0   1  25   0   0   1   0   1   0   0   0   1   1]
 [  5   0   0   0   0   0   0  14   0   0   1   0   0   0   0   0   1  29]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.7928    0.8821    0.8351      1145
           1     0.5706    0.5642    0.5674       537
           2     0.8690    0.7200    0.7875       175
           3     0.6921    0.6658    0.6787       395
           4     0.8211    0.6352    0.7163       159
           5     0.7308    0.9110    0.8110       146
           6     0.6361    0.8310    0.7206       284
           7     0.3772    0.5590    0.4504       195
           8     0.8462    0.6044    0.7051        91
           9     0.5946    0.5057    0.5466        87
          10     0.7667    0.8023    0.7841        86
          11     1.0000    0.7917    0.8837        72
          12     0.4545    0.3906    0.4202        64
          13     0.5000    0.4062    0.4483        64
          14     0.0000    0.0000    0.0000        57
          15     0.5789    0.2056    0.3034       107
          16     0.0000    0.0000    0.0000        56
          17     0.5778    0.5000    0.5361        52

    accuracy                         0.6906      3772
   macro avg     0.6005    0.5542    0.5664      3772
weighted avg     0.6762    0.6906    0.6761      3772

test confusion matrix:
[[1010   34    3   14    0    3   10   34    6   14    6    0    1    9
     0    0    0    1]
 [  68  303    1   40    5    3   69   27    0    4    0    0    6    8
     0    0    0    3]
 [   6    5  126    0    0   28    6    1    0    1    0    0    1    1
     0    0    0    0]
 [  33   56    1  263    1    0    1   17    0    2    3    0    0    1
     0   13    0    4]
 [   2   26    1    3  101    0    3   14    1    1    1    0    4    1
     0    0    0    1]
 [   0    1    2    0    1  133    8    0    0    1    0    0    0    0
     0    0    0    0]
 [  10   12    6    2    1   11  236    2    0    1    0    0    1    1
     0    0    0    1]
 [  21   26    2    7    1    0    9  109    1    0    5    0    9    0
     0    0    0    5]
 [  32    0    0    0    0    0    0    3   55    0    0    0    1    0
     0    0    0    0]
 [  25    3    1    2    1    0    4    0    1   44    0    0    4    2
     0    0    0    0]
 [   4    0    0    1    1    0    0   10    0    0   69    0    0    1
     0    0    0    0]
 [   5    0    1    5    0    0    0    0    0    3    0   57    0    0
     0    1    0    0]
 [   3    7    1    1    4    1    0   21    0    0    0    0   25    1
     0    0    0    0]
 [  19    5    0    1    3    0    5    0    0    3    1    0    0   26
     0    1    0    0]
 [  14   26    0    6    2    0    0    6    1    0    1    0    0    0
     0    0    0    1]
 [   8   21    0   28    2    3   20    2    0    0    0    0    0    1
     0   22    0    0]
 [  10    5    0    6    0    0    0   28    0    0    2    0    1    0
     0    1    0    3]
 [   4    1    0    1    0    0    0   15    0    0    2    0    2    0
     0    0    1   26]]
---------------------------------------
program finished.
