seed:  666
save trained model at:  ../trained_models/trained_classifier_model_16.pt
save loss at:  ./results/train_classifier_results_16.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  15
number of pockets in training set:  16504
number of pockets in validation set:  3531
number of pockets in test set:  3552
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=96, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=96, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=96, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=15, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b08bc58b4f0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0758329087302227, acc: 0.36003393116820165, val loss: 1.9362693837667382, acc: 0.39280657037666383, test loss: 1.9376854982462015, acc: 0.38710585585585583
epoch: 2, train loss: 1.76482080956772, acc: 0.44134755210857973, val loss: 2.550058823339462, acc: 0.38459359954687056, test loss: 2.565316029497095, acc: 0.3820382882882883
epoch: 3, train loss: 1.6250473144732802, acc: 0.4903659718856035, val loss: 1.7591279089805325, acc: 0.4474653072783914, test loss: 1.7603067664412764, acc: 0.4375
epoch: 4, train loss: 1.548473927966611, acc: 0.5126030053320407, val loss: 1.5163767442457468, acc: 0.5038232795242141, test loss: 1.5052319324768342, acc: 0.5067567567567568
epoch: 5, train loss: 1.4628347192524824, acc: 0.5351429956374212, val loss: 1.4091963952339193, acc: 0.5434721042197678, test loss: 1.3874310759810713, acc: 0.558277027027027
epoch: 6, train loss: 1.415386750636838, acc: 0.5521085797382452, val loss: 1.3655029004911834, acc: 0.5613140753327669, test loss: 1.339445348258491, acc: 0.571509009009009
epoch: 7, train loss: 1.3762700672121921, acc: 0.5548957828405235, val loss: 1.3766632978114688, acc: 0.5431888983290852, test loss: 1.3617209228309426, acc: 0.5464527027027027
epoch: 8, train loss: 1.3405520542119813, acc: 0.5704677653902084, val loss: 1.3612610667192777, acc: 0.5556499575191164, test loss: 1.3528375281943932, acc: 0.5546171171171171
epoch: 9, train loss: 1.3016140170890154, acc: 0.5827678138633059, val loss: 1.3734783875101586, acc: 0.5491362220334183, test loss: 1.3695264893609125, acc: 0.5546171171171171
epoch: 10, train loss: 1.2682634819726948, acc: 0.5938560349006301, val loss: 1.593985865521924, acc: 0.4692721608609459, test loss: 1.5816557729566418, acc: 0.47635135135135137
epoch: 11, train loss: 1.2737028453168624, acc: 0.5935530780416869, val loss: 1.297529380134468, acc: 0.5743415463041631, test loss: 1.2887232840598166, acc: 0.5884009009009009
epoch: 12, train loss: 1.239226581977515, acc: 0.605550169655841, val loss: 1.4663266195044982, acc: 0.5293118096856415, test loss: 1.501010731533841, acc: 0.5112612612612613
epoch: 13, train loss: 1.1795943639143196, acc: 0.6222733882695104, val loss: 1.2645498140443658, acc: 0.5981308411214953, test loss: 1.3163435630970173, acc: 0.589527027027027
epoch: 14, train loss: 1.1684306264212567, acc: 0.6262118274357732, val loss: 1.5041861550999038, acc: 0.508637779665817, test loss: 1.454818347552875, acc: 0.5329391891891891
epoch: 15, train loss: 1.1550182746673636, acc: 0.6296655356277266, val loss: 1.2742937606443125, acc: 0.5944491645426225, test loss: 1.2578945009558051, acc: 0.5990990990990991
epoch: 16, train loss: 1.1163413973612746, acc: 0.6410567135239942, val loss: 1.1917015407619513, acc: 0.6063438119512886, test loss: 1.18634396845156, acc: 0.6165540540540541
epoch: 17, train loss: 1.1015783598627562, acc: 0.6471764420746485, val loss: 1.2058263109953131, acc: 0.6165392240158595, test loss: 1.195950984954834, acc: 0.6151463963963963
epoch: 18, train loss: 1.0926930553059062, acc: 0.6471764420746485, val loss: 1.201851247694566, acc: 0.6202209005947323, test loss: 1.2409279539778426, acc: 0.6261261261261262
epoch: 19, train loss: 1.0896910764150578, acc: 0.6551745031507513, val loss: 1.159000203751937, acc: 0.6236193712829227, test loss: 1.1817836868870366, acc: 0.6131756756756757
epoch: 20, train loss: 1.0750169341447453, acc: 0.6562651478429472, val loss: 1.1085649282795214, acc: 0.6505239308977627, test loss: 1.1116446074064787, acc: 0.6506193693693694
epoch: 21, train loss: 1.0443462022461478, acc: 0.6655356277266117, val loss: 1.137226707906299, acc: 0.6349476069102238, test loss: 1.1389571116851256, acc: 0.6331644144144144
epoch: 22, train loss: 1.0156262784517338, acc: 0.6739578284052351, val loss: 1.1362706887691632, acc: 0.6258850184083828, test loss: 1.156110634674897, acc: 0.6323198198198198
epoch: 23, train loss: 1.0179080599092825, acc: 0.6724430441105187, val loss: 1.152233282927581, acc: 0.6338147833474936, test loss: 1.1202839516304635, acc: 0.6348536036036037
epoch: 24, train loss: 1.0196489384610041, acc: 0.6713523994183228, val loss: 1.1513892001992276, acc: 0.6290002832058906, test loss: 1.127037793666393, acc: 0.6334459459459459
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.7792480497237726, acc: 0.6860155113911779, val loss: 0.8705972989805357, acc: 0.6516567544604928, test loss: 0.9040210483310459, acc: 0.6548423423423423
epoch: 26, train loss: 0.744098566627826, acc: 0.6911657779932138, val loss: 0.8462980969773917, acc: 0.6635514018691588, test loss: 0.8379889393711949, acc: 0.6683558558558559
epoch: 27, train loss: 0.7548532975107157, acc: 0.6908628211342704, val loss: 0.8501738141393972, acc: 0.6615689606343812, test loss: 0.8509106378297548, acc: 0.6632882882882883
epoch: 28, train loss: 0.739700193261846, acc: 0.6943771206980126, val loss: 0.8001858125965915, acc: 0.6757292551685075, test loss: 0.8126978455363093, acc: 0.6756756756756757
epoch: 29, train loss: 0.7287509512450587, acc: 0.7027387300048473, val loss: 0.8310628210103941, acc: 0.6661002548853017, test loss: 0.8251389565768542, acc: 0.670045045045045
epoch: 30, train loss: 0.7348768494398457, acc: 0.6943771206980126, val loss: 0.9024935323703867, acc: 0.6332483715661286, test loss: 0.9092230345751788, acc: 0.6345720720720721
epoch: 31, train loss: 0.7070596687029821, acc: 0.7031022782355792, val loss: 0.9379987258878723, acc: 0.6343811951288587, test loss: 0.9712258598825954, acc: 0.6230292792792793
epoch: 32, train loss: 0.7229921664072944, acc: 0.6983155598642753, val loss: 0.8641162873394798, acc: 0.6465590484282073, test loss: 0.8674364648423754, acc: 0.6635698198198198
epoch: 33, train loss: 0.6754923396126714, acc: 0.717644207464857, val loss: 0.8686308524521509, acc: 0.6471254602095724, test loss: 0.8841092672433939, acc: 0.6466779279279279
epoch: 34, train loss: 0.6848819538105823, acc: 0.7148570043625788, val loss: 0.8776697140852745, acc: 0.6530727839139054, test loss: 0.8953627949362403, acc: 0.643581081081081
epoch: 35, train loss: 0.6831072893208574, acc: 0.7132816286960737, val loss: 0.7974045070025915, acc: 0.6822429906542056, test loss: 0.8027895356083775, acc: 0.6897522522522522
epoch: 36, train loss: 0.6716907624474903, acc: 0.7188560349006301, val loss: 0.792623086746743, acc: 0.6771452846219201, test loss: 0.8223083373662587, acc: 0.665259009009009
epoch: 37, train loss: 0.674947771064529, acc: 0.7143116820164809, val loss: 0.8127663901560505, acc: 0.6709147550269046, test loss: 0.8165392972327568, acc: 0.6686373873873874
epoch: 38, train loss: 0.6418871848374041, acc: 0.7255816771691711, val loss: 0.8271770538401921, acc: 0.6697819314641744, test loss: 0.8349814973435961, acc: 0.673704954954955
epoch: 39, train loss: 0.6401739509083725, acc: 0.7276417838099855, val loss: 0.7944025761748265, acc: 0.6873406966864911, test loss: 0.7957910052290907, acc: 0.6880630630630631
epoch: 40, train loss: 0.6454905248416377, acc: 0.7286112457586039, val loss: 0.8395302837207613, acc: 0.664684225431889, test loss: 0.8679938337824367, acc: 0.665259009009009
epoch: 41, train loss: 0.6602275851162325, acc: 0.7209767329132332, val loss: 0.8243100415272742, acc: 0.6740300198244124, test loss: 0.8478431486868644, acc: 0.6745495495495496
epoch: 42, train loss: 0.6504023871951119, acc: 0.7255816771691711, val loss: 0.8359676379044716, acc: 0.670631549136222, test loss: 0.8381781277355848, acc: 0.6810247747747747
epoch: 43, train loss: 0.6245706665013175, acc: 0.7337009209888512, val loss: 0.7452586572928525, acc: 0.6941376380628717, test loss: 0.7479559086464547, acc: 0.701295045045045
epoch: 44, train loss: 0.6204068761858845, acc: 0.7339432864760058, val loss: 0.7617031638327215, acc: 0.6930048145001416, test loss: 0.7658113692257855, acc: 0.6936936936936937
epoch: 45, train loss: 0.5931285851451302, acc: 0.7461827435773146, val loss: 0.7780702407148734, acc: 0.6949872557349193, test loss: 0.7941827344464826, acc: 0.6976351351351351
epoch: 46, train loss: 0.6174961879192136, acc: 0.7386694134755211, val loss: 0.866659558585736, acc: 0.6615689606343812, test loss: 0.8691119438893086, acc: 0.665259009009009
epoch: 47, train loss: 0.5940164306241219, acc: 0.7477581192438196, val loss: 0.7745094484325442, acc: 0.6927216086094591, test loss: 0.7916970091897089, acc: 0.6987612612612613
epoch: 48, train loss: 0.5908086923968139, acc: 0.7465462918080465, val loss: 0.7981168349043771, acc: 0.6771452846219201, test loss: 0.8319707131600594, acc: 0.6790540540540541
epoch: 49, train loss: 0.6014270769469205, acc: 0.7455162384876394, val loss: 0.8419040778107969, acc: 0.6581704899461909, test loss: 0.8466196736773929, acc: 0.6694819819819819
epoch: 50, train loss: 0.5792334399868202, acc: 0.7528477944740669, val loss: 0.75204793905881, acc: 0.7046162560181252, test loss: 0.7436860284289798, acc: 0.7080518018018018
epoch: 51, train loss: 0.5813386129685761, acc: 0.7530295685894328, val loss: 0.7578390974122045, acc: 0.7003681676578872, test loss: 0.7494226479315543, acc: 0.7094594594594594
epoch: 52, train loss: 0.5877027029674491, acc: 0.747515753756665, val loss: 0.8140810641645542, acc: 0.6692155196828095, test loss: 0.788741203041764, acc: 0.6801801801801802
epoch: 53, train loss: 0.588195853105056, acc: 0.7488487639360155, val loss: 0.7565219201436633, acc: 0.6918719909374115, test loss: 0.7901483119071067, acc: 0.6984797297297297
epoch: 54, train loss: 0.5544788842344538, acc: 0.7606640814348037, val loss: 0.8013229230685195, acc: 0.6898895497026338, test loss: 0.8283764317228988, acc: 0.6990427927927928
epoch: 55, train loss: 0.5476258524805957, acc: 0.7632089190499273, val loss: 0.752600998859573, acc: 0.6941376380628717, test loss: 0.7705005149583559, acc: 0.6880630630630631
epoch: 56, train loss: 0.5648486704082092, acc: 0.755998545807077, val loss: 0.7887238885957302, acc: 0.6842254318889833, test loss: 0.8081365544516761, acc: 0.6844031531531531
epoch: 57, train loss: 0.5432849546340964, acc: 0.7656931652932623, val loss: 0.7977351054899485, acc: 0.6811101670914755, test loss: 0.8057348728179932, acc: 0.6849662162162162
epoch: 58, train loss: 0.5454106460019794, acc: 0.7660567135239942, val loss: 0.8906082610711266, acc: 0.6743132257150949, test loss: 0.9246033655630576, acc: 0.6649774774774775
epoch: 59, train loss: 0.5654577120553506, acc: 0.7572103732428502, val loss: 0.7643394859814164, acc: 0.7006513735485698, test loss: 0.7769055387995265, acc: 0.7057995495495496
epoch: 60, train loss: 0.5287404820806935, acc: 0.7689045079980611, val loss: 0.7516480201501341, acc: 0.7080147267063155, test loss: 0.7516556731215468, acc: 0.71875
epoch: 61, train loss: 0.5243646879104867, acc: 0.7765390208434318, val loss: 0.830828585289788, acc: 0.6847918436703483, test loss: 0.85160468290518, acc: 0.6939752252252253
epoch: 62, train loss: 0.5384195820544452, acc: 0.7655113911778962, val loss: 0.7329441105735366, acc: 0.7071651090342679, test loss: 0.7400022702174144, acc: 0.7083333333333334
epoch: 63, train loss: 0.513017711221045, acc: 0.778296170625303, val loss: 0.9922398596504058, acc: 0.6290002832058906, test loss: 1.0171846162091505, acc: 0.6323198198198198
epoch: 64, train loss: 0.544395696897271, acc: 0.7652690256907416, val loss: 0.8199593344615222, acc: 0.6737468139337298, test loss: 0.8495051001643276, acc: 0.6768018018018018
epoch: 65, train loss: 0.5122561084663643, acc: 0.7783567619970916, val loss: 0.6967003587554245, acc: 0.7227414330218068, test loss: 0.7136044663351935, acc: 0.7266328828828829
epoch: 66, train loss: 0.5120131193551403, acc: 0.7745395055744062, val loss: 0.7632491654763645, acc: 0.7026338147833475, test loss: 0.8046919504801432, acc: 0.7010135135135135
epoch: 67, train loss: 0.48931071364469336, acc: 0.7878090159961222, val loss: 0.7357227712810562, acc: 0.7133956386292835, test loss: 0.7521583937309884, acc: 0.7088963963963963
epoch: 68, train loss: 0.5107907836898345, acc: 0.7796291808046534, val loss: 0.7719732224519243, acc: 0.7094307561597282, test loss: 0.8111510405669341, acc: 0.7088963963963963
epoch: 69, train loss: 0.4797687457564729, acc: 0.7884149297140087, val loss: 0.7275246930372206, acc: 0.7128292268479184, test loss: 0.7488626252423536, acc: 0.7094594594594594
epoch: 70, train loss: 0.5022120712540765, acc: 0.7810227823557926, val loss: 0.7535266087569046, acc: 0.7080147267063155, test loss: 0.7840362497278162, acc: 0.6979166666666666
epoch: 71, train loss: 0.4746278044065687, acc: 0.7876272418807562, val loss: 0.7487070822776528, acc: 0.7165109034267912, test loss: 0.7990424675984426, acc: 0.7038288288288288
epoch: 72, train loss: 0.48350650145772933, acc: 0.7834464372273389, val loss: 0.8252424889264448, acc: 0.6881903143585386, test loss: 0.8535775902035, acc: 0.6903153153153153
epoch: 73, train loss: 0.4902242966144338, acc: 0.7839311682016481, val loss: 0.8666310636830444, acc: 0.6757292551685075, test loss: 0.8816335426794516, acc: 0.6807432432432432
epoch: 74, train loss: 0.5008044833126891, acc: 0.7808410082404266, val loss: 0.732127281026643, acc: 0.7165109034267912, test loss: 0.7585728232925003, acc: 0.7094594594594594
epoch: 75, train loss: 0.4675355193918906, acc: 0.7946558410082404, val loss: 0.7385380067220324, acc: 0.7201925800056641, test loss: 0.7539767145036577, acc: 0.7317004504504504
epoch: 76, train loss: 0.44266046308454127, acc: 0.8028356761997092, val loss: 0.7664318707265452, acc: 0.7091475502690456, test loss: 0.7875177849520434, acc: 0.7043918918918919
Epoch    76: reducing learning rate of group 0 to 1.5000e-03.
epoch: 77, train loss: 0.38088286226372436, acc: 0.8256180319922443, val loss: 0.7204698221908349, acc: 0.7371849334466157, test loss: 0.7344410559078595, acc: 0.7350788288288288
epoch: 78, train loss: 0.34011007096371026, acc: 0.843189529810955, val loss: 0.7135957786699964, acc: 0.7371849334466157, test loss: 0.7202252804696023, acc: 0.7483108108108109
epoch: 79, train loss: 0.3293946470760109, acc: 0.8442801745031507, val loss: 0.7449567386497016, acc: 0.7360521098838856, test loss: 0.781390735694954, acc: 0.7395833333333334
epoch: 80, train loss: 0.3253430337566111, acc: 0.8462796897721765, val loss: 0.7444790071010995, acc: 0.7366185216652507, test loss: 0.7835265118796546, acc: 0.7432432432432432
epoch: 81, train loss: 0.32501969583625073, acc: 0.8457949587978671, val loss: 0.7163803074725186, acc: 0.7456811101670915, test loss: 0.7498068788030126, acc: 0.7466216216216216
epoch: 82, train loss: 0.30268584776144264, acc: 0.8564590402326708, val loss: 0.7107588172937985, acc: 0.7320872274143302, test loss: 0.7482444681562819, acc: 0.7331081081081081
epoch: 83, train loss: 0.3232642463613816, acc: 0.8495516238487639, val loss: 0.7222810976230467, acc: 0.7462475219484566, test loss: 0.7556177968377465, acc: 0.7460585585585585
epoch: 84, train loss: 0.30288329849067497, acc: 0.8560349006301503, val loss: 0.7772073452346422, acc: 0.72982158028887, test loss: 0.8284916308549073, acc: 0.7274774774774775
epoch: 85, train loss: 0.3014543768392277, acc: 0.8573679108095007, val loss: 0.7914140315273267, acc: 0.735202492211838, test loss: 0.8361149736352869, acc: 0.7370495495495496
epoch: 86, train loss: 0.3056535111921711, acc: 0.8551260300533204, val loss: 0.7713197256005611, acc: 0.7417162276975361, test loss: 0.7879410853257051, acc: 0.7454954954954955
epoch: 87, train loss: 0.29843496130007424, acc: 0.8571861366941348, val loss: 0.7960388171237307, acc: 0.735768903993203, test loss: 0.8315934941575334, acc: 0.7274774774774775
epoch: 88, train loss: 0.32451053847113226, acc: 0.8448860882210373, val loss: 0.7451053042143069, acc: 0.7408666100254885, test loss: 0.8163105313842361, acc: 0.7353603603603603
epoch: 89, train loss: 0.29653788403117187, acc: 0.8561560833737276, val loss: 0.7967152774789724, acc: 0.7218918153497593, test loss: 0.8645547671361012, acc: 0.714527027027027
epoch: 90, train loss: 0.30922147011907136, acc: 0.8528841492971401, val loss: 0.7337286006759497, acc: 0.7462475219484566, test loss: 0.7794278237196777, acc: 0.7370495495495496
epoch: 91, train loss: 0.2887463859430055, acc: 0.8600945225399903, val loss: 0.77785598796206, acc: 0.7366185216652507, test loss: 0.8347536475808771, acc: 0.7226914414414415
epoch: 92, train loss: 0.29394077694190685, acc: 0.8599127484246243, val loss: 0.8343655994545465, acc: 0.724440668365902, test loss: 0.88270928193857, acc: 0.7207207207207207
epoch: 93, train loss: 0.28973069950266483, acc: 0.8628211342704799, val loss: 0.7627372664387049, acc: 0.7366185216652507, test loss: 0.7920753837705733, acc: 0.7438063063063063
epoch: 94, train loss: 0.303159344488131, acc: 0.8560954920019389, val loss: 0.7585212641145852, acc: 0.741149815916171, test loss: 0.8034380448831094, acc: 0.7378941441441441
epoch: 95, train loss: 0.27753535849170896, acc: 0.8634270479883664, val loss: 0.8021103293589849, acc: 0.7323704333050127, test loss: 0.8776886130238438, acc: 0.7212837837837838
epoch: 96, train loss: 0.2793887942104853, acc: 0.8654871546291808, val loss: 0.8120386355053565, acc: 0.7281223449447749, test loss: 0.8470956409299696, acc: 0.7269144144144144
epoch: 97, train loss: 0.2714283514083501, acc: 0.8660930683470673, val loss: 0.7760200257073108, acc: 0.7468139337298216, test loss: 0.8115196056194134, acc: 0.7432432432432432
epoch: 98, train loss: 0.2732662484735987, acc: 0.8710009694619486, val loss: 0.7790749320832638, acc: 0.7434154630416313, test loss: 0.8499818449621802, acc: 0.7435247747747747
epoch: 99, train loss: 0.26483311398307846, acc: 0.8733034415899176, val loss: 0.8077030721719526, acc: 0.7425658453695837, test loss: 0.8747368288469745, acc: 0.7260698198198198
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.2008710035957645, acc: 0.8694255937954435, val loss: 0.6233863588501664, acc: 0.7473803455111866, test loss: 0.6827434483949129, acc: 0.7454954954954955
epoch: 101, train loss: 0.17769264132578974, acc: 0.8782719340765875, val loss: 0.6435208525545568, acc: 0.7417162276975361, test loss: 0.713292375341192, acc: 0.7350788288288288
epoch: 102, train loss: 0.17863032944099566, acc: 0.877484246243335, val loss: 0.6627258738253347, acc: 0.745964316057774, test loss: 0.7247891866409026, acc: 0.7449324324324325
epoch: 103, train loss: 0.17480753024862164, acc: 0.8774236548715463, val loss: 0.6198648403738146, acc: 0.7496459926366469, test loss: 0.6740805189888757, acc: 0.745213963963964
epoch: 104, train loss: 0.1688522373335603, acc: 0.879423170140572, val loss: 0.7038262886691788, acc: 0.7275559331634098, test loss: 0.7573919285525073, acc: 0.7294481981981982
epoch: 105, train loss: 0.17452284494361656, acc: 0.87905962190984, val loss: 0.7018356656461018, acc: 0.7374681393372983, test loss: 0.7134026654131778, acc: 0.7463400900900901
epoch: 106, train loss: 0.1782391622221938, acc: 0.8787566650508968, val loss: 0.6482447552903791, acc: 0.7470971396205041, test loss: 0.7140029024433445, acc: 0.7390202702702703
epoch: 107, train loss: 0.18616703065257598, acc: 0.8737275811924382, val loss: 0.6615550477860569, acc: 0.7360521098838856, test loss: 0.7007665505280366, acc: 0.7215653153153153
epoch: 108, train loss: 0.16000368730338868, acc: 0.8846946194861852, val loss: 0.6451760864365848, acc: 0.7456811101670915, test loss: 0.7132537117949477, acc: 0.7418355855855856
epoch: 109, train loss: 0.18215613943306613, acc: 0.8735458070770722, val loss: 0.6494675689025832, acc: 0.735202492211838, test loss: 0.6818127535484932, acc: 0.7317004504504504
epoch: 110, train loss: 0.17140585030968536, acc: 0.8787566650508968, val loss: 0.6502051068851273, acc: 0.7436986689323138, test loss: 0.7363925893027503, acc: 0.7255067567567568
epoch: 111, train loss: 0.17804902194630332, acc: 0.8745758603974794, val loss: 0.7083142808799668, acc: 0.7340696686491079, test loss: 0.7181525219668139, acc: 0.7339527027027027
epoch: 112, train loss: 0.16557418323522394, acc: 0.8782719340765875, val loss: 0.7318890727771574, acc: 0.7323704333050127, test loss: 0.7559002294196738, acc: 0.7342342342342343
epoch: 113, train loss: 0.15782918870607973, acc: 0.8837857489093554, val loss: 0.6752237255395691, acc: 0.7422826394789012, test loss: 0.7383871690647023, acc: 0.7345157657657657
epoch: 114, train loss: 0.16328586826495506, acc: 0.8831192438196801, val loss: 0.6680101827736517, acc: 0.7312376097422827, test loss: 0.728569381945842, acc: 0.7291666666666666
epoch: 115, train loss: 0.1750998673612957, acc: 0.8736669898206495, val loss: 0.6528954858720488, acc: 0.741149815916171, test loss: 0.6979059829368247, acc: 0.7302927927927928
epoch: 116, train loss: 0.19145001036937065, acc: 0.8666383906931653, val loss: 0.6760885442697573, acc: 0.723874256584537, test loss: 0.7218273899576686, acc: 0.7271959459459459
epoch: 117, train loss: 0.1865921362447207, acc: 0.8663960252060107, val loss: 0.6907814968344645, acc: 0.7363353157745681, test loss: 0.7401071443214072, acc: 0.7246621621621622
epoch: 118, train loss: 0.18128093492042885, acc: 0.8706980126030053, val loss: 0.700798212080831, acc: 0.7332200509770603, test loss: 0.7460026182569899, acc: 0.7294481981981982
epoch: 119, train loss: 0.1700705106013964, acc: 0.8803926320891905, val loss: 0.690975774418224, acc: 0.7386009629000283, test loss: 0.7477267679867444, acc: 0.7280405405405406
epoch: 120, train loss: 0.163614462727028, acc: 0.8825739214735822, val loss: 0.6971376074572455, acc: 0.7377513452279807, test loss: 0.7533148645280717, acc: 0.7393018018018018
epoch: 121, train loss: 0.17612390113986354, acc: 0.8739699466795928, val loss: 0.6589310107775364, acc: 0.7473803455111866, test loss: 0.7211896782522803, acc: 0.740990990990991
epoch: 122, train loss: 0.1690804945342008, acc: 0.8751211827435773, val loss: 0.748917258349602, acc: 0.7255734919286321, test loss: 0.8129835085825877, acc: 0.7142454954954955
epoch: 123, train loss: 0.17136262529865956, acc: 0.8736063984488609, val loss: 0.6875242949274841, acc: 0.741149815916171, test loss: 0.7337367717210237, acc: 0.7460585585585585
epoch: 124, train loss: 0.1752601126502338, acc: 0.8761512360639845, val loss: 0.7267752497569592, acc: 0.7037666383460776, test loss: 0.7494295771057541, acc: 0.706081081081081
epoch: 125, train loss: 0.1931940280533045, acc: 0.8609428017450315, val loss: 0.6663259196234168, acc: 0.7369017275559332, test loss: 0.7120313204086579, acc: 0.7367680180180181
epoch: 126, train loss: 0.15316224377015467, acc: 0.8842098885118759, val loss: 0.6802112875225118, acc: 0.7360521098838856, test loss: 0.7634253813339783, acc: 0.732545045045045
epoch: 127, train loss: 0.151666980372201, acc: 0.8836039747939893, val loss: 0.6937246481982685, acc: 0.735768903993203, test loss: 0.7541524719547581, acc: 0.740990990990991
Epoch   127: reducing learning rate of group 0 to 7.5000e-04.
epoch: 128, train loss: 0.11073716538829591, acc: 0.9082646631119728, val loss: 0.6683919786630423, acc: 0.7609742282639479, test loss: 0.7143703580976607, acc: 0.7668918918918919
epoch: 129, train loss: 0.08844698262385124, acc: 0.9238972370334464, val loss: 0.6872024676915236, acc: 0.7635230812800906, test loss: 0.7574277708122322, acc: 0.7606981981981982
epoch: 130, train loss: 0.08013188581456851, acc: 0.928926320891905, val loss: 0.7315606795434944, acc: 0.7578589634664401, test loss: 0.7938517675743447, acc: 0.7525337837837838
epoch: 131, train loss: 0.08801443456259966, acc: 0.9216553562772661, val loss: 0.7212117090673563, acc: 0.7578589634664401, test loss: 0.8016736818863465, acc: 0.7587274774774775
epoch: 132, train loss: 0.08446948191262077, acc: 0.9251090644692196, val loss: 0.7634625640140988, acc: 0.75672613990371, test loss: 0.8241277020256799, acc: 0.7559121621621622
epoch: 133, train loss: 0.07617757073953438, acc: 0.9315317498788173, val loss: 0.761656317856723, acc: 0.7553101104502974, test loss: 0.84094373599903, acc: 0.7536599099099099
epoch: 134, train loss: 0.07556873882100008, acc: 0.9323800290838584, val loss: 0.7840182251850562, acc: 0.7575757575757576, test loss: 0.8531250792580682, acc: 0.7505630630630631
epoch: 135, train loss: 0.07894068552245229, acc: 0.9304411051866214, val loss: 0.7787467442501439, acc: 0.7538940809968847, test loss: 0.8335133135855735, acc: 0.7576013513513513
epoch: 136, train loss: 0.08277303382024798, acc: 0.9276539020843432, val loss: 0.7523685342291198, acc: 0.7533276692155196, test loss: 0.819080586905952, acc: 0.7525337837837838
epoch: 137, train loss: 0.07971085346892108, acc: 0.9298957828405235, val loss: 0.7569511880688274, acc: 0.762673463608043, test loss: 0.8324059613116153, acc: 0.7528153153153153
epoch: 138, train loss: 0.0992327670643934, acc: 0.9168080465341736, val loss: 0.7766791295140973, acc: 0.7519116397621071, test loss: 0.8219552491162274, acc: 0.7505630630630631
epoch: 139, train loss: 0.09613791772652412, acc: 0.9186863790596219, val loss: 0.7383436299153612, acc: 0.7507788161993769, test loss: 0.8166516213803678, acc: 0.7480292792792793
epoch: 140, train loss: 0.08255105277489992, acc: 0.9264420746485701, val loss: 0.7549578364663136, acc: 0.7558765222316625, test loss: 0.8401160336829521, acc: 0.7491554054054054
epoch: 141, train loss: 0.07644362931857107, acc: 0.9323194377120698, val loss: 0.8022156747533322, acc: 0.7479467572925517, test loss: 0.834189307582271, acc: 0.7474662162162162
epoch: 142, train loss: 0.09022249352734064, acc: 0.9225036354823073, val loss: 0.781945858822771, acc: 0.7521948456527896, test loss: 0.8282922613728154, acc: 0.745777027027027
epoch: 143, train loss: 0.08253680758261299, acc: 0.929108095007271, val loss: 0.8332023080433101, acc: 0.7349192863211554, test loss: 0.9050985961347013, acc: 0.7148085585585585
epoch: 144, train loss: 0.13640677774351198, acc: 0.8999030538051381, val loss: 0.7412938793804111, acc: 0.7394505805720759, test loss: 0.7828139131133621, acc: 0.736204954954955
epoch: 145, train loss: 0.08384055343975272, acc: 0.9274721279689773, val loss: 0.7326817471661672, acc: 0.7485131690739167, test loss: 0.8134103293891426, acc: 0.754786036036036
epoch: 146, train loss: 0.07541810871759665, acc: 0.9309258361609307, val loss: 0.8070345370231354, acc: 0.7456811101670915, test loss: 0.8547785368051615, acc: 0.7466216216216216
epoch: 147, train loss: 0.08853659769082266, acc: 0.9240790111488124, val loss: 0.7724909727042, acc: 0.7476635514018691, test loss: 0.838106235942325, acc: 0.745777027027027
epoch: 148, train loss: 0.08797149844124545, acc: 0.9254120213281629, val loss: 0.7521609209374509, acc: 0.7516284338714245, test loss: 0.8129771120913394, acc: 0.754786036036036
epoch: 149, train loss: 0.08644630627411405, acc: 0.9286233640329618, val loss: 0.7687419112766475, acc: 0.7451146983857264, test loss: 0.8355820221943898, acc: 0.740990990990991
epoch: 150, train loss: 0.08119142647801934, acc: 0.9272903538536112, val loss: 0.7925157281299786, acc: 0.7436986689323138, test loss: 0.8401732337367427, acc: 0.7415540540540541
epoch: 151, train loss: 0.07226751343145338, acc: 0.9334706737760543, val loss: 0.8188676650785448, acc: 0.7391673746813934, test loss: 0.8564268629830163, acc: 0.7390202702702703
epoch: 152, train loss: 0.07263576159532707, acc: 0.93141056713524, val loss: 0.7689138073922555, acc: 0.7470971396205041, test loss: 0.833113618799158, acc: 0.7519707207207207
epoch: 153, train loss: 0.07123937540503884, acc: 0.9357731459040233, val loss: 0.8225111418153909, acc: 0.7451146983857264, test loss: 0.8861767622801635, acc: 0.745213963963964
epoch: 154, train loss: 0.10372164598077001, acc: 0.9145055744062045, val loss: 0.7359858988972839, acc: 0.7487963749645993, test loss: 0.8199080916138383, acc: 0.745213963963964
epoch: 155, train loss: 0.08875919763636254, acc: 0.9251090644692196, val loss: 0.7624635783176319, acc: 0.7502124044180118, test loss: 0.7942058041289046, acc: 0.7522522522522522
epoch: 156, train loss: 0.07814715229881312, acc: 0.9301987396994668, val loss: 0.7608733635318748, acc: 0.7516284338714245, test loss: 0.8547060897758415, acc: 0.75
epoch: 157, train loss: 0.07113757379212197, acc: 0.9329253514299564, val loss: 0.8308527709674106, acc: 0.7496459926366469, test loss: 0.8816608910088066, acc: 0.7443693693693694
epoch: 158, train loss: 0.06961873564842079, acc: 0.9372879301987397, val loss: 0.8122516678737467, acc: 0.7442650807136788, test loss: 0.9178811429857133, acc: 0.7390202702702703
epoch: 159, train loss: 0.09434734194589597, acc: 0.9213523994183228, val loss: 0.7655470255784494, acc: 0.7451146983857264, test loss: 0.8172791798909506, acc: 0.7384572072072072
epoch: 160, train loss: 0.08776083169301852, acc: 0.9242001938923897, val loss: 0.7843191474802735, acc: 0.7519116397621071, test loss: 0.8493987794394966, acc: 0.7469031531531531
epoch: 161, train loss: 0.06698357840070895, acc: 0.9387421231216675, val loss: 0.8292435064966703, acc: 0.7519116397621071, test loss: 0.888162357313139, acc: 0.7435247747747747
epoch: 162, train loss: 0.06559704379930233, acc: 0.9400145419292293, val loss: 0.7932647682728493, acc: 0.762673463608043, test loss: 0.8669935078234285, acc: 0.7485923423423423
epoch: 163, train loss: 0.07589875531648757, acc: 0.9368031992244304, val loss: 0.8141681113212043, acc: 0.7504956103086944, test loss: 0.8775698794974937, acc: 0.7480292792792793
epoch: 164, train loss: 0.0776864097020722, acc: 0.9311682016480853, val loss: 0.7991229816996926, acc: 0.756159728122345, test loss: 0.8861640799152959, acc: 0.7404279279279279
epoch: 165, train loss: 0.06972669743243433, acc: 0.9371667474551624, val loss: 0.7971226765933236, acc: 0.7504956103086944, test loss: 0.8726860349242752, acc: 0.7393018018018018
epoch: 166, train loss: 0.08438799783796924, acc: 0.9284415899175957, val loss: 0.8273515653691256, acc: 0.7417162276975361, test loss: 0.8680984189918449, acc: 0.7390202702702703
epoch: 167, train loss: 0.07454618726014166, acc: 0.9322588463402811, val loss: 0.7875954737510671, acc: 0.7541772868875672, test loss: 0.8215077793275988, acc: 0.7460585585585585
epoch: 168, train loss: 0.06339321747761702, acc: 0.941650508967523, val loss: 0.807749495733159, acc: 0.7493627867459643, test loss: 0.8995620946626406, acc: 0.7398648648648649
epoch: 169, train loss: 0.09514024597287929, acc: 0.9231701405719825, val loss: 0.7888364921849094, acc: 0.7448314924950439, test loss: 0.8774009663779456, acc: 0.7398648648648649
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.07510174517991527, acc: 0.9129907901114881, val loss: 0.6800975078092827, acc: 0.7479467572925517, test loss: 0.7468513712152705, acc: 0.7412725225225225
epoch: 171, train loss: 0.0642611513792804, acc: 0.9220794958797868, val loss: 0.6466384772469794, acc: 0.757292551685075, test loss: 0.7084662957234426, acc: 0.7423986486486487
epoch: 172, train loss: 0.04597757213536363, acc: 0.9353490063015026, val loss: 0.6516125786577937, acc: 0.7632398753894081, test loss: 0.7230696688901197, acc: 0.75
epoch: 173, train loss: 0.0473913599240855, acc: 0.935106640814348, val loss: 0.6642137365481632, acc: 0.7519116397621071, test loss: 0.7245297109758532, acc: 0.7536599099099099
epoch: 174, train loss: 0.04861478608551242, acc: 0.9354095976732913, val loss: 0.7279607421850152, acc: 0.7448314924950439, test loss: 0.7622890826818105, acc: 0.7376126126126126
epoch: 175, train loss: 0.04079667547057684, acc: 0.9401357246728066, val loss: 0.6817318809096585, acc: 0.7490795808552818, test loss: 0.7610879771344297, acc: 0.7485923423423423
epoch: 176, train loss: 0.044957676137672486, acc: 0.9385603490063015, val loss: 0.7254882444912704, acc: 0.7470971396205041, test loss: 0.7781746355262963, acc: 0.7350788288288288
epoch: 177, train loss: 0.03923493811532935, acc: 0.9430441105186621, val loss: 0.6746311822446673, acc: 0.7536108751062022, test loss: 0.7373048711467434, acc: 0.7530968468468469
epoch: 178, train loss: 0.07140950844869184, acc: 0.9171715947649055, val loss: 0.6530713693712764, acc: 0.7462475219484566, test loss: 0.697506906749966, acc: 0.7384572072072072
Epoch   178: reducing learning rate of group 0 to 3.7500e-04.
epoch: 179, train loss: 0.0430301502717174, acc: 0.9415293262239457, val loss: 0.6456384227350798, acc: 0.75672613990371, test loss: 0.7159627869322494, acc: 0.7567567567567568
epoch: 180, train loss: 0.031629871096691346, acc: 0.9514057198254968, val loss: 0.6598604076207781, acc: 0.7604078164825828, test loss: 0.7332883074476912, acc: 0.7598536036036037
epoch: 181, train loss: 0.02662498625993035, acc: 0.9571013087736306, val loss: 0.6993223035413191, acc: 0.7589917870291703, test loss: 0.7591658916559305, acc: 0.7561936936936937
epoch: 182, train loss: 0.02560976137990837, acc: 0.9573436742607853, val loss: 0.6924722796963821, acc: 0.7604078164825828, test loss: 0.7637430773125039, acc: 0.7576013513513513
epoch: 183, train loss: 0.02431402673066327, acc: 0.9595249636451769, val loss: 0.7114021767978458, acc: 0.7643726989521382, test loss: 0.7910488854657423, acc: 0.7576013513513513
epoch: 184, train loss: 0.021775306937868482, acc: 0.9631604459524964, val loss: 0.7246095328653747, acc: 0.7606910223732654, test loss: 0.8009635521484925, acc: 0.754222972972973
epoch: 185, train loss: 0.022226210077299087, acc: 0.9624333494910324, val loss: 0.7255010617687492, acc: 0.7555933163409799, test loss: 0.7966144718565382, acc: 0.7601351351351351
epoch: 186, train loss: 0.021243826618442475, acc: 0.9647964129907901, val loss: 0.7210849732861104, acc: 0.7606910223732654, test loss: 0.8019513611320976, acc: 0.7581644144144144
epoch: 187, train loss: 0.021130605295148798, acc: 0.9649175957343674, val loss: 0.7425574268507438, acc: 0.7618238459359955, test loss: 0.809649412696426, acc: 0.7528153153153153
epoch: 188, train loss: 0.024679684959590376, acc: 0.961282113427048, val loss: 0.7509522624746391, acc: 0.7558765222316625, test loss: 0.81368600033425, acc: 0.7567567567567568
epoch: 189, train loss: 0.021560179794652336, acc: 0.9626757149781872, val loss: 0.747372703388844, acc: 0.7555933163409799, test loss: 0.8104679047524392, acc: 0.7567567567567568
epoch: 190, train loss: 0.02175519799464213, acc: 0.9626151236063984, val loss: 0.7704122441242384, acc: 0.7538940809968847, test loss: 0.8404522683169391, acc: 0.7581644144144144
epoch: 191, train loss: 0.02293830696977332, acc: 0.9633422200678623, val loss: 0.7490100565864155, acc: 0.7530444633248372, test loss: 0.822814527932588, acc: 0.7581644144144144
epoch: 192, train loss: 0.022405786109321684, acc: 0.9649175957343674, val loss: 0.7679342284684679, acc: 0.7519116397621071, test loss: 0.82669265957566, acc: 0.7530968468468469
epoch: 193, train loss: 0.024995602903886728, acc: 0.9620698012603005, val loss: 0.7588819664755545, acc: 0.7558765222316625, test loss: 0.8228197677715404, acc: 0.7522522522522522
epoch: 194, train loss: 0.023479738828450284, acc: 0.9621303926320892, val loss: 0.7313425878869411, acc: 0.7553101104502974, test loss: 0.8100862771541149, acc: 0.7511261261261262
epoch: 195, train loss: 0.020120779356647914, acc: 0.9644328647600582, val loss: 0.7641007837442999, acc: 0.7584253752478052, test loss: 0.8387321212270238, acc: 0.7570382882882883
epoch: 196, train loss: 0.021032228459158513, acc: 0.965341735336888, val loss: 0.7733024358411782, acc: 0.7482299631832342, test loss: 0.8425152602496447, acc: 0.7561936936936937
epoch: 197, train loss: 0.020422817415987123, acc: 0.9665535627726611, val loss: 0.7734012874183558, acc: 0.7493627867459643, test loss: 0.8388203693939759, acc: 0.7502815315315315
epoch: 198, train loss: 0.02865323986654078, acc: 0.9580101793504605, val loss: 0.7915565463511474, acc: 0.7428490512602662, test loss: 0.8486954414092742, acc: 0.7494369369369369
epoch: 199, train loss: 0.03365255585026088, acc: 0.9541323315559864, val loss: 0.7318512848503222, acc: 0.7504956103086944, test loss: 0.8084213647756491, acc: 0.7449324324324325
epoch: 200, train loss: 0.04556954966454441, acc: 0.9457101308773631, val loss: 0.6975119044374927, acc: 0.75672613990371, test loss: 0.7596529711474169, acc: 0.7497184684684685
best val acc 0.7643726989521382 at epoch 183.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9978    0.9974    0.9976      5337
           1     0.9860    0.9257    0.9549      2502
           2     0.9973    0.9025    0.9475       810
           3     0.9743    0.9902    0.9822      1840
           4     0.9946    0.9973    0.9959       737
           5     0.8952    0.9970    0.9434       677
           6     0.9044    0.9940    0.9471      1323
           7     0.9834    0.9791    0.9812       907
           8     0.9717    0.9786    0.9751       421
           9     1.0000    1.0000    1.0000       401
          10     1.0000    0.9975    0.9987       396
          11     0.9970    1.0000    0.9985       332
          12     0.9861    0.9593    0.9725       295
          13     0.9966    0.9966    0.9966       291
          14     0.9915    0.9957    0.9936       235

    accuracy                         0.9787     16504
   macro avg     0.9784    0.9807    0.9790     16504
weighted avg     0.9798    0.9787    0.9787     16504

train confusion matrix:
[[9.97376803e-01 0.00000000e+00 0.00000000e+00 3.74742365e-04
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  2.24845419e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 9.25659472e-01 0.00000000e+00 1.83852918e-02
  0.00000000e+00 0.00000000e+00 5.55555556e-02 3.99680256e-04
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 9.02469136e-01 0.00000000e+00
  0.00000000e+00 9.75308642e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [2.71739130e-03 6.52173913e-03 0.00000000e+00 9.90217391e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.43478261e-04
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 2.71370421e-03 0.00000000e+00 0.00000000e+00
  9.97286296e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 2.95420975e-03 0.00000000e+00
  0.00000000e+00 9.97045790e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 6.04686319e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.93953137e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [1.10253583e-03 1.10253583e-02 0.00000000e+00 0.00000000e+00
  2.20507166e-03 0.00000000e+00 0.00000000e+00 9.79051819e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.41014333e-03 0.00000000e+00 2.20507166e-03]
 [1.42517815e-02 2.37529691e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.37529691e-03
  9.78622328e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.37529691e-03 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.52525253e-03
  0.00000000e+00 0.00000000e+00 9.97474747e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.38983051e-03 0.00000000e+00 0.00000000e+00 3.72881356e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.59322034e-01 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.43642612e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.96563574e-01 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.25531915e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.95744681e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8709    0.8731    0.8720      1143
           1     0.6277    0.6418    0.6347       536
           2     0.8741    0.7225    0.7911       173
           3     0.7531    0.7665    0.7597       394
           4     0.8310    0.7468    0.7867       158
           5     0.8125    0.8966    0.8525       145
           6     0.7363    0.8092    0.7710       283
           7     0.4636    0.5258    0.4928       194
           8     0.7609    0.7778    0.7692        90
           9     0.6410    0.5882    0.6135        85
          10     0.7717    0.8452    0.8068        84
          11     0.9322    0.7746    0.8462        71
          12     0.6905    0.4603    0.5524        63
          13     0.8302    0.7097    0.7652        62
          14     0.7273    0.6400    0.6809        50

    accuracy                         0.7644      3531
   macro avg     0.7549    0.7185    0.7330      3531
weighted avg     0.7682    0.7644    0.7647      3531

validation confusion matrix:
[[8.73140857e-01 3.23709536e-02 1.74978128e-03 2.01224847e-02
  1.74978128e-03 8.74890639e-04 5.24934383e-03 3.06211724e-02
  1.39982502e-02 1.13735783e-02 4.37445319e-03 8.74890639e-04
  2.62467192e-03 0.00000000e+00 8.74890639e-04]
 [8.95522388e-02 6.41791045e-01 1.30597015e-02 9.70149254e-02
  1.86567164e-02 0.00000000e+00 8.20895522e-02 4.29104478e-02
  0.00000000e+00 3.73134328e-03 1.86567164e-03 0.00000000e+00
  3.73134328e-03 1.86567164e-03 3.73134328e-03]
 [1.73410405e-02 2.89017341e-02 7.22543353e-01 1.73410405e-02
  5.78034682e-03 1.27167630e-01 3.46820809e-02 5.78034682e-03
  0.00000000e+00 1.73410405e-02 0.00000000e+00 0.00000000e+00
  5.78034682e-03 1.73410405e-02 0.00000000e+00]
 [3.55329949e-02 1.24365482e-01 2.53807107e-03 7.66497462e-01
  0.00000000e+00 0.00000000e+00 1.01522843e-02 3.55329949e-02
  2.53807107e-03 0.00000000e+00 7.61421320e-03 5.07614213e-03
  5.07614213e-03 0.00000000e+00 5.07614213e-03]
 [6.32911392e-03 1.45569620e-01 0.00000000e+00 1.89873418e-02
  7.46835443e-01 0.00000000e+00 1.26582278e-02 3.16455696e-02
  6.32911392e-03 6.32911392e-03 6.32911392e-03 0.00000000e+00
  1.26582278e-02 6.32911392e-03 0.00000000e+00]
 [1.37931034e-02 2.06896552e-02 2.06896552e-02 0.00000000e+00
  6.89655172e-03 8.96551724e-01 2.75862069e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 6.89655172e-03 0.00000000e+00
  0.00000000e+00 6.89655172e-03 0.00000000e+00]
 [2.12014134e-02 1.27208481e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.47349823e-02 8.09187279e-01 1.06007067e-02
  0.00000000e+00 3.53356890e-03 0.00000000e+00 0.00000000e+00
  3.53356890e-03 0.00000000e+00 0.00000000e+00]
 [1.44329897e-01 1.08247423e-01 1.03092784e-02 5.15463918e-02
  3.60824742e-02 0.00000000e+00 3.09278351e-02 5.25773196e-01
  1.03092784e-02 5.15463918e-03 4.12371134e-02 0.00000000e+00
  1.03092784e-02 5.15463918e-03 2.06185567e-02]
 [1.33333333e-01 4.44444444e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.44444444e-02
  7.77777778e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [1.88235294e-01 9.41176471e-02 2.35294118e-02 2.35294118e-02
  0.00000000e+00 0.00000000e+00 7.05882353e-02 0.00000000e+00
  1.17647059e-02 5.88235294e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [1.19047619e-02 1.19047619e-02 0.00000000e+00 0.00000000e+00
  1.19047619e-02 0.00000000e+00 0.00000000e+00 5.95238095e-02
  1.19047619e-02 0.00000000e+00 8.45238095e-01 0.00000000e+00
  0.00000000e+00 1.19047619e-02 3.57142857e-02]
 [4.22535211e-02 8.45070423e-02 0.00000000e+00 2.81690141e-02
  1.40845070e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.63380282e-02 0.00000000e+00 7.74647887e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [3.17460317e-02 1.42857143e-01 0.00000000e+00 6.34920635e-02
  0.00000000e+00 0.00000000e+00 3.17460317e-02 2.38095238e-01
  0.00000000e+00 0.00000000e+00 1.58730159e-02 0.00000000e+00
  4.60317460e-01 1.58730159e-02 0.00000000e+00]
 [1.12903226e-01 1.61290323e-02 1.61290323e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.22580645e-02 3.22580645e-02
  0.00000000e+00 4.83870968e-02 1.61290323e-02 1.61290323e-02
  0.00000000e+00 7.09677419e-01 0.00000000e+00]
 [1.00000000e-01 2.00000000e-02 0.00000000e+00 0.00000000e+00
  2.00000000e-02 0.00000000e+00 0.00000000e+00 2.20000000e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 6.40000000e-01]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8536    0.8655    0.8595      1145
           1     0.6327    0.6480    0.6403       537
           2     0.8759    0.7257    0.7938       175
           3     0.7378    0.7266    0.7321       395
           4     0.8881    0.7484    0.8123       159
           5     0.7665    0.8767    0.8179       146
           6     0.7175    0.7782    0.7466       284
           7     0.5268    0.6051    0.5632       195
           8     0.6923    0.7912    0.7385        91
           9     0.6341    0.5977    0.6154        87
          10     0.7978    0.8256    0.8114        86
          11     0.9677    0.8333    0.8955        72
          12     0.6939    0.5312    0.6018        64
          13     0.7500    0.5625    0.6429        64
          14     0.6750    0.5192    0.5870        52

    accuracy                         0.7576      3552
   macro avg     0.7473    0.7090    0.7239      3552
weighted avg     0.7617    0.7576    0.7578      3552

test confusion matrix:
[[8.65502183e-01 4.10480349e-02 2.62008734e-03 1.65938865e-02
  0.00000000e+00 8.73362445e-04 4.36681223e-03 2.09606987e-02
  1.92139738e-02 1.39737991e-02 3.49344978e-03 8.73362445e-04
  0.00000000e+00 6.11353712e-03 4.36681223e-03]
 [8.00744879e-02 6.48044693e-01 1.86219739e-03 9.12476723e-02
  3.72439479e-03 9.31098696e-03 9.68342644e-02 4.65549348e-02
  7.44878957e-03 3.72439479e-03 1.86219739e-03 0.00000000e+00
  1.86219739e-03 3.72439479e-03 3.72439479e-03]
 [6.85714286e-02 1.71428571e-02 7.25714286e-01 5.71428571e-03
  5.71428571e-03 1.37142857e-01 5.71428571e-03 0.00000000e+00
  5.71428571e-03 1.14285714e-02 5.71428571e-03 0.00000000e+00
  1.14285714e-02 0.00000000e+00 0.00000000e+00]
 [8.35443038e-02 1.13924051e-01 0.00000000e+00 7.26582278e-01
  5.06329114e-03 2.53164557e-03 1.01265823e-02 3.79746835e-02
  2.53164557e-03 5.06329114e-03 0.00000000e+00 0.00000000e+00
  5.06329114e-03 5.06329114e-03 2.53164557e-03]
 [1.88679245e-02 9.43396226e-02 0.00000000e+00 3.14465409e-02
  7.48427673e-01 0.00000000e+00 1.25786164e-02 5.03144654e-02
  0.00000000e+00 0.00000000e+00 6.28930818e-03 0.00000000e+00
  1.88679245e-02 6.28930818e-03 1.25786164e-02]
 [0.00000000e+00 2.05479452e-02 3.42465753e-02 6.84931507e-03
  6.84931507e-03 8.76712329e-01 5.47945205e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [2.11267606e-02 1.12676056e-01 7.04225352e-03 1.76056338e-02
  3.52112676e-03 2.81690141e-02 7.78169014e-01 7.04225352e-03
  3.52112676e-03 1.05633803e-02 0.00000000e+00 3.52112676e-03
  3.52112676e-03 0.00000000e+00 3.52112676e-03]
 [7.69230769e-02 1.33333333e-01 1.02564103e-02 5.12820513e-02
  2.56410256e-02 0.00000000e+00 3.58974359e-02 6.05128205e-01
  1.02564103e-02 5.12820513e-03 3.07692308e-02 0.00000000e+00
  1.53846154e-02 0.00000000e+00 0.00000000e+00]
 [1.64835165e-01 2.19780220e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.09890110e-02 1.09890110e-02
  7.91208791e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [1.37931034e-01 9.19540230e-02 4.59770115e-02 4.59770115e-02
  1.14942529e-02 0.00000000e+00 3.44827586e-02 1.14942529e-02
  0.00000000e+00 5.97701149e-01 0.00000000e+00 0.00000000e+00
  2.29885057e-02 0.00000000e+00 0.00000000e+00]
 [6.97674419e-02 0.00000000e+00 0.00000000e+00 1.16279070e-02
  0.00000000e+00 0.00000000e+00 1.16279070e-02 5.81395349e-02
  0.00000000e+00 0.00000000e+00 8.25581395e-01 0.00000000e+00
  1.16279070e-02 0.00000000e+00 1.16279070e-02]
 [4.16666667e-02 6.94444444e-02 1.38888889e-02 1.38888889e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.77777778e-02 0.00000000e+00 8.33333333e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [3.12500000e-02 1.56250000e-01 0.00000000e+00 1.56250000e-02
  0.00000000e+00 0.00000000e+00 3.12500000e-02 2.03125000e-01
  0.00000000e+00 1.56250000e-02 0.00000000e+00 0.00000000e+00
  5.31250000e-01 0.00000000e+00 1.56250000e-02]
 [1.87500000e-01 7.81250000e-02 0.00000000e+00 3.12500000e-02
  3.12500000e-02 0.00000000e+00 1.56250000e-02 3.12500000e-02
  1.56250000e-02 1.56250000e-02 3.12500000e-02 0.00000000e+00
  0.00000000e+00 5.62500000e-01 0.00000000e+00]
 [1.53846154e-01 1.92307692e-02 0.00000000e+00 5.76923077e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.92307692e-01
  0.00000000e+00 0.00000000e+00 5.76923077e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 5.19230769e-01]]
---------------------------------------
program finished.
