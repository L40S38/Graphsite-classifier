seed:  1
save trained model at:  ../trained_models/trained_classifier_model_101.pt
save loss at:  ./results/train_classifier_results_101.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['6c67A01', '4qnyB00', '5tlbA00', '3m7nE00', '3h1qA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1odiC00', '4u79A00', '4ee1A00', '1sehA00', '3gv5D00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b5965a71730>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0249737426912033, acc: 0.39061205161595836; test loss: 1.7574539032987713, acc: 0.4285038997872843
epoch: 2, train loss: 1.7201649820196343, acc: 0.47561264354208593; test loss: 1.604181180916563, acc: 0.49822736941621365
epoch: 3, train loss: 1.6189413682742981, acc: 0.4980466437788564; test loss: 1.56185138234008, acc: 0.5152446230205625
epoch: 4, train loss: 1.5721462005212403, acc: 0.5187048656327691; test loss: 1.5321048674812219, acc: 0.5154809737650674
epoch: 5, train loss: 1.5353468422976078, acc: 0.5305433881851545; test loss: 1.4815355947017106, acc: 0.5424249586386197
epoch: 6, train loss: 1.4747523560077465, acc: 0.5483011720137327; test loss: 1.4211354668724765, acc: 0.5532970928858426
epoch: 7, train loss: 1.4470303294304998, acc: 0.5578903752811649; test loss: 1.4726743504838216, acc: 0.5454975183171827
epoch: 8, train loss: 1.4306554142731922, acc: 0.5661773410678347; test loss: 1.3760406783330923, acc: 0.5840226896714724
epoch: 9, train loss: 1.4005264432187248, acc: 0.5745826920800284; test loss: 1.4165604819476083, acc: 0.5544788466083668
epoch: 10, train loss: 1.367257642678231, acc: 0.5807979164200308; test loss: 1.3681604560738447, acc: 0.5885133538170645
epoch: 11, train loss: 1.3524884501758998, acc: 0.583875932283651; test loss: 1.3240749789760682, acc: 0.5901678090285984
epoch: 12, train loss: 1.3301549747566845, acc: 0.5907422753640346; test loss: 1.2927407311705714, acc: 0.5948948239186953
epoch: 13, train loss: 1.3044266291337563, acc: 0.601811293950515; test loss: 1.3414672145153776, acc: 0.5852044433939967
epoch: 14, train loss: 1.3070158224014385, acc: 0.6005090564697526; test loss: 1.3985370734824945, acc: 0.569368943512172
epoch: 15, train loss: 1.2933399481393464, acc: 0.604238191073754; test loss: 1.356181917785612, acc: 0.5729142046797447
epoch: 16, train loss: 1.2790514179499344, acc: 0.6114596898307091; test loss: 1.2145372755672317, acc: 0.6173481446466557
epoch: 17, train loss: 1.2650658032788855, acc: 0.6167278323665206; test loss: 1.2072263570351398, acc: 0.6277475774048689
epoch: 18, train loss: 1.2468411737015026, acc: 0.6169054102048065; test loss: 1.2852227126585622, acc: 0.597021980619239
epoch: 19, train loss: 1.2279934765841816, acc: 0.6217000118385225; test loss: 1.3179256442139207, acc: 0.6145119357125975
epoch: 20, train loss: 1.2154884404143709, acc: 0.6292766662720493; test loss: 1.2520704369149402, acc: 0.6064760103994328
epoch: 21, train loss: 1.2002994015158401, acc: 0.6312892151059548; test loss: 1.3166556000512308, acc: 0.5944221224296856
epoch: 22, train loss: 1.1729811001236063, acc: 0.644193204688055; test loss: 1.2420285989476896, acc: 0.6083668163554715
epoch: 23, train loss: 1.1776444543758566, acc: 0.6411743814371966; test loss: 1.216937663734903, acc: 0.6201843535807138
epoch: 24, train loss: 1.1604575897174798, acc: 0.648218302355866; test loss: 1.1534696013559491, acc: 0.6490191444103048
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.920476375563802, acc: 0.6518290517343436; test loss: 1.1284585142778014, acc: 0.5603876152209879
epoch: 26, train loss: 0.9210752443941553, acc: 0.6544335266958684; test loss: 0.8911060209506456, acc: 0.659418577168518
epoch: 27, train loss: 0.8996073397515271, acc: 0.657215579495679; test loss: 0.8940085610214121, acc: 0.6513826518553534
epoch: 28, train loss: 0.8997769787196678, acc: 0.6576299277850124; test loss: 1.0851346279424687, acc: 0.6145119357125975
epoch: 29, train loss: 0.8929808427627882, acc: 0.6582810465253937; test loss: 1.093804671166496, acc: 0.5913495627511227
epoch: 30, train loss: 0.868432006836502, acc: 0.6698236060139695; test loss: 0.9283846418673308, acc: 0.635074450484519
epoch: 31, train loss: 0.9020059197079123, acc: 0.6606487510358707; test loss: 1.128872884528571, acc: 0.5870952493500354
epoch: 32, train loss: 0.8911969658890123, acc: 0.6570380016573931; test loss: 0.8917366028170134, acc: 0.6520917040888679
epoch: 33, train loss: 0.8562619809682686, acc: 0.6739078962945424; test loss: 0.896564260382259, acc: 0.6483100921767904
epoch: 34, train loss: 0.8457424333706063, acc: 0.673493548005209; test loss: 0.9144622346408765, acc: 0.6405105176081305
epoch: 35, train loss: 0.8415982853142738, acc: 0.6778146087368296; test loss: 0.9582347180144045, acc: 0.6194753013471992
epoch: 36, train loss: 0.8368710751955193, acc: 0.6751509411625429; test loss: 0.8302585534298851, acc: 0.6728905696052943
epoch: 37, train loss: 0.835027905777529, acc: 0.6801231206345448; test loss: 0.8750443632116388, acc: 0.6606003308910423
epoch: 38, train loss: 0.8249161562313339, acc: 0.6790576536048302; test loss: 0.9020962390570572, acc: 0.6506735996218388
epoch: 39, train loss: 0.8075093025778459, acc: 0.6871078489404522; test loss: 0.8382870528861948, acc: 0.6676908532261877
epoch: 40, train loss: 0.8067263791557457, acc: 0.6883508938084527; test loss: 1.0943744868934648, acc: 0.5913495627511227
epoch: 41, train loss: 0.8014746801915558, acc: 0.6899490943530248; test loss: 0.8682254204877413, acc: 0.6667454502481683
epoch: 42, train loss: 0.8027773157257829, acc: 0.685687226234166; test loss: 0.8699042561543915, acc: 0.6613093831245569
epoch: 43, train loss: 0.7851186416495463, acc: 0.6947436959867409; test loss: 1.054910263928418, acc: 0.5956038761522099
epoch: 44, train loss: 0.78971510834139, acc: 0.6913105244465491; test loss: 0.8694971795060679, acc: 0.6532734578113921
epoch: 45, train loss: 0.7796608501811754, acc: 0.6964010891440748; test loss: 0.8921086294547911, acc: 0.642637674308674
epoch: 46, train loss: 0.7557055202407696, acc: 0.7023795430330295; test loss: 0.8612566624823769, acc: 0.658000472701489
epoch: 47, train loss: 0.7643041810834705, acc: 0.6956907777909317; test loss: 0.902713138931498, acc: 0.6603639801465374
Epoch    47: reducing learning rate of group 0 to 1.5000e-03.
epoch: 48, train loss: 0.7046663846238006, acc: 0.7212027938913224; test loss: 0.8223332026077483, acc: 0.6797447411959348
epoch: 49, train loss: 0.6850149596994891, acc: 0.7239256540783711; test loss: 0.7465445376039819, acc: 0.7078704797920113
epoch: 50, train loss: 0.6672620251995297, acc: 0.7338700130223748; test loss: 0.7500181988152846, acc: 0.7031434649019145
epoch: 51, train loss: 0.6646441000374701, acc: 0.732922931218184; test loss: 0.7812775952459311, acc: 0.7010163082013708
epoch: 52, train loss: 0.6550423421492249, acc: 0.7355274061797088; test loss: 0.7959000097891028, acc: 0.6896714724651383
epoch: 53, train loss: 0.6571702113853795, acc: 0.7375399550136142; test loss: 0.7585609332237704, acc: 0.7005436067123612
epoch: 54, train loss: 0.6452980652015788, acc: 0.7373623771753285; test loss: 0.7239405120470033, acc: 0.7090522335145356
epoch: 55, train loss: 0.6428891370123457, acc: 0.7403812004261868; test loss: 0.7487881043875254, acc: 0.7088158827700307
epoch: 56, train loss: 0.6423913655555417, acc: 0.7400852373623772; test loss: 0.7743323030158493, acc: 0.6986528007563224
epoch: 57, train loss: 0.6381261526896176, acc: 0.7447022611578075; test loss: 0.7859026333086035, acc: 0.7026707634129048
epoch: 58, train loss: 0.6158216900835097, acc: 0.749200899727714; test loss: 0.8727298286107099, acc: 0.6766721815173717
epoch: 59, train loss: 0.6196497731679034, acc: 0.7464188469279034; test loss: 0.7718624573344458, acc: 0.6986528007563224
epoch: 60, train loss: 0.6159675800900523, acc: 0.7487865514383805; test loss: 0.7518766077943684, acc: 0.7021980619238951
epoch: 61, train loss: 0.6056441411444798, acc: 0.7534627678465727; test loss: 0.7473955460401664, acc: 0.7078704797920113
epoch: 62, train loss: 0.5922383913503281, acc: 0.7557712797442879; test loss: 0.7494891874898948, acc: 0.7147246513826518
epoch: 63, train loss: 0.594722695713059, acc: 0.7604474961524802; test loss: 0.7884988902562393, acc: 0.69463483809974
epoch: 64, train loss: 0.5911426947457531, acc: 0.7568367467740026; test loss: 0.7507203002258892, acc: 0.7128338454266131
epoch: 65, train loss: 0.5827830567633144, acc: 0.7629335858884811; test loss: 0.7271213688148277, acc: 0.7246513826518554
epoch: 66, train loss: 0.5776422744222375, acc: 0.7609802296673375; test loss: 0.9666064354692107, acc: 0.6537461593004018
epoch: 67, train loss: 0.5733348903563427, acc: 0.7659524091393394; test loss: 0.722210100034652, acc: 0.725124084140865
epoch: 68, train loss: 0.5794133658209848, acc: 0.7634071267905765; test loss: 0.7478239698574577, acc: 0.7121247931930985
epoch: 69, train loss: 0.5832874156165098, acc: 0.7645909790458151; test loss: 0.775241977468052, acc: 0.7102339872370598
epoch: 70, train loss: 0.5672864892842064, acc: 0.7666035278797206; test loss: 0.736551274985068, acc: 0.7218151737177972
epoch: 71, train loss: 0.5690155615111674, acc: 0.7648869421096247; test loss: 0.8563348134621692, acc: 0.6780902859844008
epoch: 72, train loss: 0.5616202792698524, acc: 0.7665443352669586; test loss: 0.7126006973012349, acc: 0.7291420467974474
epoch: 73, train loss: 0.553375417242477, acc: 0.771990055641056; test loss: 0.7887799351019826, acc: 0.7130701961711179
epoch: 74, train loss: 0.5438743014229506, acc: 0.7744761453770569; test loss: 0.8403330558694809, acc: 0.6844717560860317
epoch: 75, train loss: 0.5489595741989683, acc: 0.7739434118621996; test loss: 0.7416989814020787, acc: 0.7213424722287876
epoch: 76, train loss: 0.5387013095983517, acc: 0.7779685095300106; test loss: 0.8373576834026563, acc: 0.6958165918222642
epoch: 77, train loss: 0.5233840105116431, acc: 0.7831774594530603; test loss: 0.7293882231373112, acc: 0.7300874497754668
epoch: 78, train loss: 0.528842263596713, acc: 0.7784420504321061; test loss: 0.7960303936464458, acc: 0.7135428976601277
epoch: 79, train loss: 0.5263017059092241, acc: 0.7810465253936308; test loss: 0.7485284275885762, acc: 0.7218151737177972
epoch: 80, train loss: 0.5159417152531799, acc: 0.7861370900911566; test loss: 0.7145028507081196, acc: 0.7333963601985346
epoch: 81, train loss: 0.5057623230085805, acc: 0.7894518764058246; test loss: 0.8017328823466471, acc: 0.7003072559678563
epoch: 82, train loss: 0.5196514488381437, acc: 0.7855451639635374; test loss: 0.8766708148058299, acc: 0.6917986291656819
epoch: 83, train loss: 0.5102189675059927, acc: 0.7853675861252516; test loss: 0.7877470452969758, acc: 0.7142519498936422
epoch: 84, train loss: 0.5160706943557913, acc: 0.7834734225168699; test loss: 0.7152444684063112, acc: 0.7341054124320492
epoch: 85, train loss: 0.5035572680013483, acc: 0.7903397655972535; test loss: 0.7960663724244275, acc: 0.6972346962892934
epoch: 86, train loss: 0.5036132793405627, acc: 0.7900438025334439; test loss: 0.7275259812034295, acc: 0.7359962183880879
epoch: 87, train loss: 0.5053211943565833, acc: 0.7901029951462057; test loss: 0.7661337749356779, acc: 0.7196880170172536
epoch: 88, train loss: 0.48979397002544517, acc: 0.7914052326269682; test loss: 0.7566220572698598, acc: 0.722051524462302
epoch: 89, train loss: 0.4749517183028919, acc: 0.8011720137326862; test loss: 0.7281085264877178, acc: 0.735759867643583
epoch: 90, train loss: 0.4827363088742874, acc: 0.7986859239966853; test loss: 0.8080466445583847, acc: 0.7026707634129048
epoch: 91, train loss: 0.4885966820986127, acc: 0.794009707588493; test loss: 1.0304460721048614, acc: 0.6563460174899551
epoch: 92, train loss: 0.500658666638068, acc: 0.7899846099206819; test loss: 0.8154989178088844, acc: 0.7007799574568659
epoch: 93, train loss: 0.4703048067190555, acc: 0.8023558659879247; test loss: 0.7102519255444052, acc: 0.7499409123138738
epoch: 94, train loss: 0.46749189378552797, acc: 0.802888599502782; test loss: 0.8268987930742343, acc: 0.7057433230914677
epoch: 95, train loss: 0.46642428351123194, acc: 0.803717296081449; test loss: 0.7871701871776828, acc: 0.718978964783739
epoch: 96, train loss: 0.4626928483351767, acc: 0.8042500295963064; test loss: 0.7107281174340786, acc: 0.7359962183880879
epoch: 97, train loss: 0.45277633262453976, acc: 0.8080975494258317; test loss: 0.7660022126785888, acc: 0.7381233750886316
epoch: 98, train loss: 0.4463707102965191, acc: 0.8114715283532615; test loss: 0.7500624645407165, acc: 0.7329236587095249
epoch: 99, train loss: 0.45261484213774056, acc: 0.8087486681662128; test loss: 0.7437202396248566, acc: 0.7359962183880879
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3663453965888752, acc: 0.8055522670770687; test loss: 0.6041078923357713, acc: 0.7310328527534862
epoch: 101, train loss: 0.33464928310947956, acc: 0.8194625310761217; test loss: 0.6579499872151742, acc: 0.7232332781848263
epoch: 102, train loss: 0.34635655381728364, acc: 0.8089262460044986; test loss: 0.6878441174073694, acc: 0.7185062632947293
epoch: 103, train loss: 0.33814851823425496, acc: 0.8145495442168817; test loss: 0.6489985752150793, acc: 0.7104703379815647
epoch: 104, train loss: 0.34212872716919607, acc: 0.8108204096128803; test loss: 0.6457361932117531, acc: 0.7393051288111557
Epoch   104: reducing learning rate of group 0 to 7.5000e-04.
epoch: 105, train loss: 0.29851959414124785, acc: 0.8334319876879366; test loss: 0.5905544870825586, acc: 0.7612857480501064
epoch: 106, train loss: 0.2664839111165322, acc: 0.8478157925890849; test loss: 0.6241705658249301, acc: 0.7494682108248641
epoch: 107, train loss: 0.26751996526509836, acc: 0.8453888954658458; test loss: 0.6389008786936963, acc: 0.7466320018908059
epoch: 108, train loss: 0.26034101652255126, acc: 0.8478157925890849; test loss: 0.6909184011548609, acc: 0.7279602930749232
epoch: 109, train loss: 0.2629714129880517, acc: 0.8452113176275601; test loss: 0.6336834030888431, acc: 0.7549042779484756
epoch: 110, train loss: 0.25848840876515833, acc: 0.8476382147507991; test loss: 0.6282572311815204, acc: 0.7487591585913496
epoch: 111, train loss: 0.25213017622592165, acc: 0.8479341778146088; test loss: 0.6626229870547585, acc: 0.7397778303001654
epoch: 112, train loss: 0.2587259349857433, acc: 0.8495323783591808; test loss: 0.7036036871984703, acc: 0.7248877333963601
epoch: 113, train loss: 0.2465288095133, acc: 0.8513673493548005; test loss: 0.6689936205219981, acc: 0.7454502481682818
epoch: 114, train loss: 0.25017397610749154, acc: 0.8485852965549899; test loss: 0.640011532090849, acc: 0.7454502481682818
epoch: 115, train loss: 0.24262600448641874, acc: 0.8526695868355629; test loss: 0.6637158012367597, acc: 0.7513590167809029
epoch: 116, train loss: 0.24822681755219125, acc: 0.8521960459334675; test loss: 0.6583398097346393, acc: 0.7466320018908059
epoch: 117, train loss: 0.24774191871230752, acc: 0.8500651118740381; test loss: 0.6619511770317386, acc: 0.7409595840226897
epoch: 118, train loss: 0.24643301540427592, acc: 0.8519592754824198; test loss: 0.6545093388907933, acc: 0.7459229496572914
epoch: 119, train loss: 0.24055050589198035, acc: 0.8560435657629928; test loss: 0.6776116150430235, acc: 0.7504136138028835
epoch: 120, train loss: 0.23614397634764994, acc: 0.8522552385462294; test loss: 0.6827942560367679, acc: 0.7331600094540298
epoch: 121, train loss: 0.23033587234647115, acc: 0.8585296554989937; test loss: 0.688480636465496, acc: 0.7352871661545733
epoch: 122, train loss: 0.23963110254927755, acc: 0.8539718243163253; test loss: 0.6916479106435143, acc: 0.7426140392342235
epoch: 123, train loss: 0.23381505627504845, acc: 0.856635491890612; test loss: 0.6704373525913881, acc: 0.7447411959347672
epoch: 124, train loss: 0.22622434934441318, acc: 0.8575825736948028; test loss: 0.6816594507135811, acc: 0.7449775466792721
epoch: 125, train loss: 0.24106232317591667, acc: 0.8528471646738487; test loss: 0.7081381569965826, acc: 0.7300874497754668
epoch: 126, train loss: 0.2294252101621699, acc: 0.8604238191073754; test loss: 0.7368274332360719, acc: 0.7355235168990782
epoch: 127, train loss: 0.23345021509596395, acc: 0.8533207055759441; test loss: 0.7112465366858507, acc: 0.7390687780666509
epoch: 128, train loss: 0.23691976476494653, acc: 0.8491772226826092; test loss: 0.683535650700042, acc: 0.7402505317891751
epoch: 129, train loss: 0.23196268937242542, acc: 0.8562803362140405; test loss: 0.710794322683974, acc: 0.7274875915859135
epoch: 130, train loss: 0.22712274704569965, acc: 0.8561027583757547; test loss: 0.6916433152113364, acc: 0.7497045615693689
epoch: 131, train loss: 0.21650573436439396, acc: 0.8607789747839469; test loss: 0.6864317947740224, acc: 0.7459229496572914
epoch: 132, train loss: 0.2154223112237514, acc: 0.8643305315496626; test loss: 0.6805511755572513, acc: 0.7440321437012527
epoch: 133, train loss: 0.20521316249807783, acc: 0.8655143838049012; test loss: 0.7265959933868567, acc: 0.7329236587095249
epoch: 134, train loss: 0.21996737029604113, acc: 0.8578785367586125; test loss: 0.7532977210351182, acc: 0.726778539352399
epoch: 135, train loss: 0.22574713391863666, acc: 0.8557476026991832; test loss: 0.7307827972514892, acc: 0.7359962183880879
epoch: 136, train loss: 0.20827596960363268, acc: 0.8637977980348053; test loss: 0.7058779410058177, acc: 0.7307965020089813
epoch: 137, train loss: 0.2140558088434029, acc: 0.8609565526222327; test loss: 0.7434400346819545, acc: 0.7362325691325927
epoch: 138, train loss: 0.2114615666593929, acc: 0.8637386054220433; test loss: 0.7008789295677991, acc: 0.7449775466792721
epoch: 139, train loss: 0.21706549554438764, acc: 0.8613117082988043; test loss: 0.7080967034382957, acc: 0.7345781139210589
epoch: 140, train loss: 0.21753134544860742, acc: 0.862436367941281; test loss: 0.7463042080698213, acc: 0.7274875915859135
epoch: 141, train loss: 0.2146271825616453, acc: 0.8637977980348053; test loss: 0.693276110171032, acc: 0.7523044197589223
epoch: 142, train loss: 0.19234908988091373, acc: 0.8701906002130934; test loss: 0.7235063836524402, acc: 0.7331600094540298
epoch: 143, train loss: 0.20262920381586214, acc: 0.8640937610986149; test loss: 0.7202594839802479, acc: 0.7393051288111557
epoch: 144, train loss: 0.19893772240163193, acc: 0.8699538297620457; test loss: 0.767171300068952, acc: 0.7383597258331364
epoch: 145, train loss: 0.20725017050934977, acc: 0.8627323310050906; test loss: 0.7338515893417282, acc: 0.7317419049870008
epoch: 146, train loss: 0.2019592121612169, acc: 0.8675269326388066; test loss: 0.710062095781388, acc: 0.7423776884897187
epoch: 147, train loss: 0.19379583063761524, acc: 0.8700130223748076; test loss: 0.7159859058395455, acc: 0.7419049870007091
epoch: 148, train loss: 0.18329129849395287, acc: 0.8751627796850953; test loss: 0.7337248879272418, acc: 0.7494682108248641
epoch: 149, train loss: 0.20249327517648102, acc: 0.8683556292174737; test loss: 0.7954556770680842, acc: 0.7241786811628457
epoch: 150, train loss: 0.2082283136861429, acc: 0.8633242571327099; test loss: 0.7302920979491027, acc: 0.7381233750886316
epoch: 151, train loss: 0.19481155353917926, acc: 0.872084763821475; test loss: 0.7778868991921231, acc: 0.7128338454266131
epoch: 152, train loss: 0.20303033086249317, acc: 0.8665206582218539; test loss: 0.7378605651787848, acc: 0.7333963601985346
epoch: 153, train loss: 0.17912987966191435, acc: 0.879483840416716; test loss: 0.7522265495799783, acc: 0.7345781139210589
epoch: 154, train loss: 0.19063458879685224, acc: 0.8735053865277613; test loss: 0.74409801276461, acc: 0.7329236587095249
epoch: 155, train loss: 0.18890988427717417, acc: 0.8748076240085237; test loss: 0.7215053674544152, acc: 0.7416686362562042
Epoch   155: reducing learning rate of group 0 to 3.7500e-04.
epoch: 156, train loss: 0.16122417334333544, acc: 0.8879483840416716; test loss: 0.7288459452915574, acc: 0.7402505317891751
epoch: 157, train loss: 0.14239111016301045, acc: 0.8986622469515805; test loss: 0.7634431760211386, acc: 0.7369416213661073
epoch: 158, train loss: 0.13546255233549742, acc: 0.9014442997513911; test loss: 0.7685823343065551, acc: 0.7489955093358545
epoch: 159, train loss: 0.13689052987090933, acc: 0.9036936190363443; test loss: 0.7758256713990708, acc: 0.7423776884897187
epoch: 160, train loss: 0.1317688931748615, acc: 0.9066532496744406; test loss: 0.797809519178336, acc: 0.7381233750886316
epoch: 161, train loss: 0.13283459737507058, acc: 0.9050550491298686; test loss: 0.8103169613880796, acc: 0.7419049870007091
epoch: 162, train loss: 0.1274418748815525, acc: 0.9086066058955843; test loss: 0.7894289090655369, acc: 0.7473410541243205
epoch: 163, train loss: 0.13245511529705886, acc: 0.9035160411980585; test loss: 0.7805230538632622, acc: 0.7409595840226897
epoch: 164, train loss: 0.12781050017144788, acc: 0.9062389013851071; test loss: 0.7971939005994876, acc: 0.7447411959347672
epoch: 165, train loss: 0.12827035703750225, acc: 0.9073635610275838; test loss: 0.8009929066430139, acc: 0.7407232332781848
epoch: 166, train loss: 0.12779092111114188, acc: 0.9068900201254884; test loss: 0.8036920709323725, acc: 0.7452138974237769
epoch: 167, train loss: 0.12754670677240662, acc: 0.9068308275127264; test loss: 0.8091175866110109, acc: 0.7456865989127865
epoch: 168, train loss: 0.12406042669459294, acc: 0.9073043684148219; test loss: 0.8371698721112779, acc: 0.7447411959347672
epoch: 169, train loss: 0.1255104688752667, acc: 0.9083698354445365; test loss: 0.8191020957010909, acc: 0.7430867407232333
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.09073925274895595, acc: 0.9074819462531076; test loss: 0.7409759558450243, acc: 0.7393051288111557
epoch: 171, train loss: 0.09312334992860329, acc: 0.9065940570616787; test loss: 0.707790446264528, acc: 0.7485228078468447
epoch: 172, train loss: 0.08879296288795692, acc: 0.907126790576536; test loss: 0.7478455872829854, acc: 0.7359962183880879
epoch: 173, train loss: 0.08655324593809098, acc: 0.90878418373387; test loss: 0.7219346735500551, acc: 0.7428503899787284
epoch: 174, train loss: 0.09122624638744736, acc: 0.9103823842784421; test loss: 0.7217033102111708, acc: 0.74048688253368
epoch: 175, train loss: 0.08335262613651723, acc: 0.9116254291464425; test loss: 0.7321925838910442, acc: 0.7378870243441267
epoch: 176, train loss: 0.0829286462054798, acc: 0.9102639990529182; test loss: 0.7302093988782378, acc: 0.7426140392342235
epoch: 177, train loss: 0.08858785834205324, acc: 0.9091393394104416; test loss: 0.7153259166108044, acc: 0.7485228078468447
epoch: 178, train loss: 0.08318805194728768, acc: 0.9133420149165384; test loss: 0.7372043130527967, acc: 0.7416686362562042
epoch: 179, train loss: 0.08983637661189882, acc: 0.907126790576536; test loss: 0.7437194848393075, acc: 0.7378870243441267
epoch: 180, train loss: 0.08988427553104059, acc: 0.9065940570616787; test loss: 0.7230326746181235, acc: 0.7461593004017962
epoch: 181, train loss: 0.09011345470926811, acc: 0.9026281520066296; test loss: 0.7420315350575584, acc: 0.7426140392342235
epoch: 182, train loss: 0.0884031704553658, acc: 0.9044039303894874; test loss: 0.750853480246861, acc: 0.7359962183880879
epoch: 183, train loss: 0.08697539649535964, acc: 0.9062980939978691; test loss: 0.7421115670072302, acc: 0.7383597258331364
epoch: 184, train loss: 0.08422743059307553, acc: 0.9093169172487273; test loss: 0.721027491870881, acc: 0.7433230914677381
epoch: 185, train loss: 0.083575200224658, acc: 0.9097312655380608; test loss: 0.7318800720191402, acc: 0.7414322855116994
epoch: 186, train loss: 0.08651050475767919, acc: 0.9077187167041553; test loss: 0.7052195868687099, acc: 0.7437957929567478
epoch: 187, train loss: 0.0908051310463235, acc: 0.9049366639043447; test loss: 0.7296148678901995, acc: 0.743559442212243
epoch: 188, train loss: 0.08967227907283526, acc: 0.904640700840535; test loss: 0.7032576036689919, acc: 0.7395414795556606
epoch: 189, train loss: 0.09074936150213211, acc: 0.9035160411980585; test loss: 0.7199587152235842, acc: 0.7395414795556606
epoch: 190, train loss: 0.0840516509088, acc: 0.9084882206700604; test loss: 0.7173714570135857, acc: 0.7454502481682818
epoch: 191, train loss: 0.08465058637011809, acc: 0.9088433763466319; test loss: 0.7248692043113303, acc: 0.7426140392342235
epoch: 192, train loss: 0.08002878073790348, acc: 0.9142890967207292; test loss: 0.7495071174010293, acc: 0.7428503899787284
epoch: 193, train loss: 0.0840868680065689, acc: 0.9115662365336806; test loss: 0.7561253385041178, acc: 0.7487591585913496
epoch: 194, train loss: 0.082628812860092, acc: 0.9086657985083462; test loss: 0.7484938792375435, acc: 0.7445048451902624
epoch: 195, train loss: 0.08680657781171025, acc: 0.9039895821001539; test loss: 0.7353882138482845, acc: 0.7331600094540298
epoch: 196, train loss: 0.08346388641953455, acc: 0.9105599621167279; test loss: 0.7118902022718566, acc: 0.7390687780666509
epoch: 197, train loss: 0.08139388401617421, acc: 0.9126317035633953; test loss: 0.7467686394038028, acc: 0.7397778303001654
epoch: 198, train loss: 0.09156504863057459, acc: 0.9048774712915828; test loss: 0.7492142230504738, acc: 0.7218151737177972
epoch: 199, train loss: 0.09240798726968967, acc: 0.902272996330058; test loss: 0.7451235721703541, acc: 0.7362325691325927
epoch: 200, train loss: 0.08015301089892166, acc: 0.9105599621167279; test loss: 0.7281217455046872, acc: 0.7416686362562042
best test acc 0.7612857480501064 at epoch 105.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9036    0.9516    0.9270      6100
           1     0.9756    0.8629    0.9158       926
           2     0.8383    0.9050    0.8704      2400
           3     0.8716    0.8695    0.8705       843
           4     0.8989    0.9651    0.9308       774
           5     0.8914    0.9504    0.9200      1512
           6     0.7424    0.7150    0.7285      1330
           7     0.8862    0.8254    0.8547       481
           8     0.8364    0.7926    0.8139       458
           9     0.8487    0.9558    0.8991       452
          10     0.9094    0.7978    0.8499       717
          11     0.8685    0.6547    0.7466       333
          12     0.6667    0.0134    0.0262       299
          13     0.8349    0.6766    0.7474       269

    accuracy                         0.8768     16894
   macro avg     0.8552    0.7811    0.7929     16894
weighted avg     0.8732    0.8768    0.8679     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8038    0.8892    0.8443      1525
           1     0.8763    0.7328    0.7981       232
           2     0.7276    0.7554    0.7412       601
           3     0.7742    0.7962    0.7850       211
           4     0.7952    0.8608    0.8267       194
           5     0.7835    0.8042    0.7937       378
           6     0.5229    0.5135    0.5182       333
           7     0.7576    0.6198    0.6818       121
           8     0.6374    0.5043    0.5631       115
           9     0.6929    0.7719    0.7303       114
          10     0.8210    0.7389    0.7778       180
          11     0.7500    0.5000    0.6000        84
          12     0.0000    0.0000    0.0000        75
          13     0.7292    0.5147    0.6034        68

    accuracy                         0.7613      4231
   macro avg     0.6908    0.6430    0.6617      4231
weighted avg     0.7465    0.7613    0.7513      4231

---------------------------------------
program finished.
