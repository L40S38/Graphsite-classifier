seed:  666
number of classes: 10
number of epochs to train: 60
batch size: 64
number of workers to load data:  36
device:  cuda
number of gpus:  2
number of classes after further clustering:  23
number of pockets in training set:  10520
number of pockets in validation set:  2245
number of pockets in test set:  2279
model architecture:
MoNet(
  (conv1): GINMolecularConv(nn=Sequential(
    (0): Linear(in_features=5, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
  ))(edge_transformer=Sequential(
    (0): Linear(in_features=1, out_features=5, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=5, out_features=5, bias=True)
    (3): ELU(alpha=1.0)
  ))
  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): GINMolecularConv(nn=Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
  ))(edge_transformer=Sequential(
    (0): Linear(in_features=1, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ELU(alpha=1.0)
  ))
  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): GINMolecularConv(nn=Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
  ))(edge_transformer=Sequential(
    (0): Linear(in_features=1, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ELU(alpha=1.0)
  ))
  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv4): GINMolecularConv(nn=Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
  ))(edge_transformer=Sequential(
    (0): Linear(in_features=1, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ELU(alpha=1.0)
  ))
  (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=32, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=23, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)
loss function:
NLLLoss()
begin training...
epoch: 1, train loss: 9.652506876898356, acc: 0.06074144486692015, val loss: 3.280291361160958, acc: 0.07661469933184856
epoch: 2, train loss: 3.260794030577511, acc: 0.07214828897338403, val loss: 3.1445003204728024, acc: 0.060133630289532294
epoch: 3, train loss: 3.1393209678591885, acc: 0.08127376425855513, val loss: 3.088093734584035, acc: 0.09665924276169265
epoch: 4, train loss: 3.0916426227119937, acc: 0.10399239543726235, val loss: 3.0755678638849067, acc: 0.11804008908685969
epoch: 5, train loss: 3.0906855064653174, acc: 0.13678707224334602, val loss: 3.088626264835519, acc: 0.09888641425389755
epoch: 6, train loss: 3.077711834381742, acc: 0.14610266159695817, val loss: 3.051809975192913, acc: 0.1443207126948775
epoch: 7, train loss: 3.0078361500352053, acc: 0.1596958174904943, val loss: 3.027399823713409, acc: 0.14253897550111358
epoch: 8, train loss: 3.014057169577015, acc: 0.15019011406844107, val loss: 3.020735393282034, acc: 0.14120267260579064
epoch: 9, train loss: 3.005926155772046, acc: 0.15399239543726237, val loss: 3.0084057629506678, acc: 0.1403118040089087
epoch: 10, train loss: 2.997484380664028, acc: 0.16967680608365018, val loss: 2.99654221534729, acc: 0.1354120267260579
epoch: 11, train loss: 2.976566323037383, acc: 0.14486692015209127, val loss: 3.0009695540557724, acc: 0.12962138084632516
epoch: 12, train loss: 2.9514261862171014, acc: 0.14714828897338403, val loss: 2.9986422207413908, acc: 0.13452115812917595
epoch: 13, train loss: 2.937949702222991, acc: 0.15218631178707223, val loss: 3.002352284641733, acc: 0.11848552338530066
epoch: 14, train loss: 2.912597641201527, acc: 0.15285171102661596, val loss: 2.9862759672984778, acc: 0.09665924276169265
epoch: 15, train loss: 2.9106251914238297, acc: 0.14629277566539925, val loss: 3.1173507815745465, acc: 0.11804008908685969
epoch: 16, train loss: 2.9200334904311727, acc: 0.14790874524714828, val loss: 3.073265807825101, acc: 0.13229398663697103
epoch: 17, train loss: 2.9067358698681733, acc: 0.15076045627376425, val loss: 3.0253902779390125, acc: 0.10779510022271716
epoch: 18, train loss: 2.8869120697558155, acc: 0.1518060836501901, val loss: 3.740761646767767, acc: 0.12115812917594655
epoch: 19, train loss: 2.906115298942015, acc: 0.1467680608365019, val loss: 3.076328517493267, acc: 0.12160356347438753
epoch: 20, train loss: 2.9011325736462843, acc: 0.15009505703422052, val loss: 3.1293767115586584, acc: 0.1242761692650334
epoch: 21, train loss: 2.883173800602612, acc: 0.1408745247148289, val loss: 3.1139324163806466, acc: 0.12115812917594655
epoch: 22, train loss: 2.895136002261376, acc: 0.14543726235741444, val loss: 3.2294395461645315, acc: 0.11358574610244988
epoch: 23, train loss: 2.891325910009812, acc: 0.14914448669201522, val loss: 3.730591376587118, acc: 0.10734966592427617
epoch: 24, train loss: 2.92268080403143, acc: 0.14429657794676806, val loss: 3.070529339414397, acc: 0.15055679287305124
epoch: 25, train loss: 2.903515683743437, acc: 0.14258555133079848, val loss: 2.9881127389342854, acc: 0.15456570155902005
epoch: 26, train loss: 2.900135814463684, acc: 0.14258555133079848, val loss: 2.9217132040546305, acc: 0.14164810690423163
epoch: 27, train loss: 2.9552277807953695, acc: 0.15779467680608364, val loss: 2.9087912474549427, acc: 0.14743875278396437
epoch: 28, train loss: 2.883475483506351, acc: 0.15199619771863118, val loss: 2.938845952422688, acc: 0.14565701559020044
epoch: 29, train loss: 2.9107590729746073, acc: 0.14287072243346008, val loss: 2.8866958894283576, acc: 0.1465478841870824
epoch: 30, train loss: 2.8930334209036013, acc: 0.14192015209125475, val loss: 3.3644610023710935, acc: 0.14610244988864143
epoch: 31, train loss: 2.886865773944347, acc: 0.14429657794676806, val loss: 3.377291694781297, acc: 0.13452115812917595
epoch: 32, train loss: 2.906261879228367, acc: 0.1408745247148289, val loss: 3.214022221172307, acc: 0.14966592427616926
epoch: 33, train loss: 2.9434877281406533, acc: 0.14543726235741444, val loss: 2.890729247010365, acc: 0.15634743875278395
epoch: 34, train loss: 2.8950469457604586, acc: 0.14201520912547527, val loss: 2.8845722148572417, acc: 0.14610244988864143
epoch: 35, train loss: 2.890293346702373, acc: 0.1443916349809886, val loss: 2.884978498594798, acc: 0.14565701559020044
epoch: 36, train loss: 2.886529803729329, acc: 0.14486692015209127, val loss: 2.885620477842063, acc: 0.14521158129175946
epoch: 37, train loss: 2.8852796547313155, acc: 0.1452471482889734, val loss: 2.8875305343576954, acc: 0.14521158129175946
epoch: 38, train loss: 2.8841715734721136, acc: 0.1452471482889734, val loss: 2.8855300919250286, acc: 0.14610244988864143
epoch: 39, train loss: 2.9019565206970097, acc: 0.14923954372623574, val loss: 2.8852154274029296, acc: 0.1465478841870824
epoch: 40, train loss: 2.8883760037077697, acc: 0.1449619771863118, val loss: 2.884107430528161, acc: 0.14521158129175946
epoch: 41, train loss: 2.8853651309647943, acc: 0.1447718631178707, val loss: 2.8875152871444123, acc: 0.14521158129175946
epoch: 42, train loss: 2.8827590574329798, acc: 0.14515209125475284, val loss: 2.887680629632521, acc: 0.14565701559020044
epoch: 43, train loss: 2.8773142424826386, acc: 0.148574144486692, val loss: 14.61733202992675, acc: 0.10512249443207126
epoch: 44, train loss: 2.9864311919919437, acc: 0.13488593155893536, val loss: 2.8943620671142716, acc: 0.1492204899777283
epoch: 45, train loss: 2.8958404332512684, acc: 0.14334600760456273, val loss: 2.8873324433520007, acc: 0.14565701559020044
epoch: 46, train loss: 2.8894304915526066, acc: 0.1449619771863118, val loss: 2.8875382590134584, acc: 0.14565701559020044
epoch: 47, train loss: 2.886053876187865, acc: 0.14515209125475284, val loss: 2.8910911464478763, acc: 0.14565701559020044
epoch: 48, train loss: 2.910269142557006, acc: 0.15465779467680607, val loss: 2.888264953744969, acc: 0.14565701559020044
epoch: 49, train loss: 2.8880812733798877, acc: 0.14543726235741444, val loss: 2.8859197113190036, acc: 0.14565701559020044
epoch: 50, train loss: 2.881987239018139, acc: 0.14505703422053232, val loss: 2.885270740619481, acc: 0.1465478841870824
epoch: 51, train loss: 2.8802788107114146, acc: 0.14515209125475284, val loss: 2.885864866231226, acc: 0.14610244988864143
epoch: 52, train loss: 2.8880637308490593, acc: 0.14581749049429657, val loss: 3.2012009958381906, acc: 0.11581291759465479
epoch: 53, train loss: 2.8870075231269285, acc: 0.14011406844106464, val loss: 2.885391771023417, acc: 0.14565701559020044
epoch: 54, train loss: 2.878547882943099, acc: 0.14505703422053232, val loss: 2.8829414670345246, acc: 0.1443207126948775
epoch: 55, train loss: 2.8792273474283543, acc: 0.14486692015209127, val loss: 2.8874429568947027, acc: 0.14565701559020044
epoch: 56, train loss: 2.873265095627353, acc: 0.14809885931558936, val loss: 3.811733068920722, acc: 0.058797327394209356
epoch: 57, train loss: 2.9045517259677553, acc: 0.15275665399239544, val loss: 2.8962937896660548, acc: 0.15189309576837418
epoch: 58, train loss: 2.893601231701927, acc: 0.14401140684410646, val loss: 2.892127635165684, acc: 0.14565701559020044
epoch: 59, train loss: 2.885663230700185, acc: 0.1452471482889734, val loss: 2.887849192098945, acc: 0.14565701559020044
epoch: 60, train loss: 2.880950077586301, acc: 0.14515209125475284, val loss: 2.885339873617635, acc: 0.14565701559020044
best val loss 2.8829414670345246 at epoch 54.
