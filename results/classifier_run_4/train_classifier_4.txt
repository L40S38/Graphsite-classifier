seed:  666
save trained model at:  ../trained_models/trained_classifier_model_4.pt
save loss at:  ./results/train_classifier_results_4.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  120
learning rate decay at epoch:  60
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=80, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(80, 160)
  )
  (fc1): Linear(in_features=160, out_features=80, bias=True)
  (fc2): Linear(in_features=80, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [20, 60, 100]
loss function:
FocalLoss(gamma=0, alpha=1, reduction=mean)
begin training...
epoch: 1, train loss: 2.3486490221592553, acc: 0.3209249214958607, val loss: 2.109089189953953, acc: 0.3784360821990926, test loss: 2.1094267939712186, acc: 0.3828207847295864
epoch: 2, train loss: 2.0657250081611434, acc: 0.3890379674564659, val loss: 2.207565669221881, acc: 0.2839605017347211, test loss: 2.2016789212838717, acc: 0.2990455991516437
epoch: 3, train loss: 1.9081540337365728, acc: 0.4311732800456751, val loss: 1.8268000531075699, acc: 0.4502268481451828, test loss: 1.8057348275614442, acc: 0.4509544008483563
epoch: 4, train loss: 1.797198115190097, acc: 0.4592063945189837, val loss: 1.8479360178816882, acc: 0.4448892447291166, test loss: 1.8323963907086078, acc: 0.4533404029692471
epoch: 5, train loss: 1.7131190387922937, acc: 0.4860405366828433, val loss: 1.7520821505557513, acc: 0.4526287696824126, test loss: 1.7120701171939725, acc: 0.4766702014846235
epoch: 6, train loss: 1.6547824381148513, acc: 0.5001427347987439, val loss: 1.755338157371804, acc: 0.47371230317587404, test loss: 1.7636594001207726, acc: 0.48462354188759277
epoch: 7, train loss: 1.601928571003558, acc: 0.5174992863260063, val loss: 2.0426082742160117, acc: 0.4174005871363758, test loss: 2.03623323349533, acc: 0.4345174973488865
epoch: 8, train loss: 1.5457124400336233, acc: 0.529717385098487, val loss: 2.2679807560139094, acc: 0.3971176941553243, test loss: 2.2809817024910566, acc: 0.4029692470837752
epoch: 9, train loss: 1.5222106493713854, acc: 0.5410790750785042, val loss: 1.4740272704715693, acc: 0.5340272217774219, test loss: 1.4520538050448275, acc: 0.5487804878048781
epoch: 10, train loss: 1.475972957363341, acc: 0.5530687981729946, val loss: 1.534332191781868, acc: 0.5308246597277823, test loss: 1.5258647204709583, acc: 0.5352598091198303
epoch: 11, train loss: 1.4420298526738324, acc: 0.5604910077076791, val loss: 1.5470864618813989, acc: 0.5222844942620763, test loss: 1.5424679448217753, acc: 0.532608695652174
epoch: 12, train loss: 1.4138880215472505, acc: 0.5723094490436769, val loss: 1.5438496005799187, acc: 0.5308246597277823, test loss: 1.5349170420860057, acc: 0.5395015906680806
epoch: 13, train loss: 1.3915255442708347, acc: 0.5777904653154439, val loss: 1.4246991239358877, acc: 0.578062449959968, test loss: 1.444883579161109, acc: 0.5670731707317073
epoch: 14, train loss: 1.3791907892205393, acc: 0.5872109620325435, val loss: 1.450166707297214, acc: 0.548705631171604, test loss: 1.4665589684014102, acc: 0.5418875927889714
epoch: 15, train loss: 1.3482065615502896, acc: 0.591835569511847, val loss: 1.6212112532000496, acc: 0.5145449693087804, test loss: 1.6094397305178112, acc: 0.5241251325556734
epoch: 16, train loss: 1.344391404122514, acc: 0.5933771053382815, val loss: 1.4881840698999629, acc: 0.5481718708299973, test loss: 1.4948541918336902, acc: 0.5503711558854719
epoch: 17, train loss: 1.3022194055975964, acc: 0.6083357122466457, val loss: 1.325312346434765, acc: 0.5988791032826261, test loss: 1.3203307763137089, acc: 0.6002120890774125
epoch: 18, train loss: 1.2832039359770875, acc: 0.6097630602340851, val loss: 1.299663246743737, acc: 0.6159594342140379, test loss: 1.2985080689160216, acc: 0.619034994697773
epoch: 19, train loss: 1.2714544321515375, acc: 0.6166143305737939, val loss: 1.4709384630990723, acc: 0.5615158793701628, test loss: 1.4365756317009224, acc: 0.573170731707317
epoch 20, gamma increased to 1.
epoch: 20, train loss: 1.0559232703692703, acc: 0.6149015129888666, val loss: 1.1244760544611225, acc: 0.5879370162796904, test loss: 1.1152634542289241, acc: 0.6076352067868505
epoch: 21, train loss: 1.0048915216931609, acc: 0.6340279760205538, val loss: 1.0966900880094335, acc: 0.5991459834534294, test loss: 1.0843294534046097, acc: 0.6015376458112407
epoch: 22, train loss: 0.9919046349162004, acc: 0.6361975449614616, val loss: 1.1034807098812187, acc: 0.6020816653322658, test loss: 1.104315780758984, acc: 0.6039236479321315
epoch: 23, train loss: 0.9940854678928529, acc: 0.6316300314016557, val loss: 1.1873583868722581, acc: 0.5732586068855084, test loss: 1.175845108001887, acc: 0.5861611876988335
epoch: 24, train loss: 0.983383595456813, acc: 0.6356266057664859, val loss: 1.0466435339917237, acc: 0.6068855084067254, test loss: 1.0340896234279726, acc: 0.616914103923648
epoch: 25, train loss: 0.9597619301892334, acc: 0.6421353125892092, val loss: 1.0710159485773434, acc: 0.614358153189218, test loss: 1.0761155753474474, acc: 0.616914103923648
epoch: 26, train loss: 0.9463099313980166, acc: 0.6506423065943477, val loss: 1.4885730749134831, acc: 0.5022684814518281, test loss: 1.5022800889020254, acc: 0.5023860021208908
epoch: 27, train loss: 0.9640901905279516, acc: 0.6430488153011704, val loss: 1.0438125636185014, acc: 0.6159594342140379, test loss: 1.0566188378905441, acc: 0.6203605514316013
epoch: 28, train loss: 0.9317515261779674, acc: 0.6526405937767628, val loss: 1.1543157221572953, acc: 0.5793968508139845, test loss: 1.157919303989107, acc: 0.5837751855779427
epoch: 29, train loss: 0.9258743025968662, acc: 0.6532686268912361, val loss: 0.972751086827816, acc: 0.6394448892447291, test loss: 0.9768593546441605, acc: 0.6394485683987274
epoch: 30, train loss: 0.8957669815811606, acc: 0.66211818441336, val loss: 1.2338494972002993, acc: 0.5975447024286096, test loss: 1.1893847054413749, acc: 0.6041887592788971
epoch: 31, train loss: 0.9098416035834838, acc: 0.6595489580359691, val loss: 1.0810580667827108, acc: 0.6052842273819056, test loss: 1.068034447895507, acc: 0.6134676564156946
epoch: 32, train loss: 0.8931311285499162, acc: 0.6601769911504425, val loss: 1.0493401473192714, acc: 0.6148919135308246, test loss: 1.070512490712446, acc: 0.616914103923648
epoch: 33, train loss: 0.8876458932425749, acc: 0.6666286040536683, val loss: 0.9971224659119793, acc: 0.6346410461702695, test loss: 1.0158125522407215, acc: 0.6320254506892895
epoch: 34, train loss: 0.8720546421989727, acc: 0.6744504710248359, val loss: 1.1337796091747054, acc: 0.6090205497731519, test loss: 1.1622092708676643, acc: 0.6052492046659597
epoch: 35, train loss: 0.8710588241964281, acc: 0.6721667142449329, val loss: 1.0024323997288855, acc: 0.6258340005337604, test loss: 0.9909048318104425, acc: 0.6357370095440085
epoch: 36, train loss: 0.8658675024382155, acc: 0.671881244647445, val loss: 1.187648598913005, acc: 0.5844675740592474, test loss: 1.2059283433511678, acc: 0.5845705196182397
epoch: 37, train loss: 0.8611654118813687, acc: 0.676049100770768, val loss: 1.0232327905756204, acc: 0.622097678142514, test loss: 1.0304428787777558, acc: 0.6240721102863203
epoch: 38, train loss: 0.9058544998777412, acc: 0.66211818441336, val loss: 1.0816409590405276, acc: 0.6180944755804644, test loss: 1.0706621184708078, acc: 0.6275185577942736
epoch: 39, train loss: 0.8646205168501364, acc: 0.6773622609192121, val loss: 1.1199824363987447, acc: 0.6148919135308246, test loss: 1.1468686326816757, acc: 0.6079003181336161
epoch: 40, train loss: 0.8475436927589592, acc: 0.6800456751355981, val loss: 0.9439449179862892, acc: 0.6575927408593542, test loss: 0.9394527146571009, acc: 0.6542948038176034
epoch: 41, train loss: 0.821078840480884, acc: 0.6894661718526977, val loss: 0.9641620710050707, acc: 0.6458500133440085, test loss: 0.9695931661672582, acc: 0.6460763520678685
epoch: 42, train loss: 0.8124318993945752, acc: 0.6925492435055667, val loss: 1.0905134509905898, acc: 0.6183613557512677, test loss: 1.1080631304646853, acc: 0.6155885471898197
epoch: 43, train loss: 0.8763347761446566, acc: 0.6699400513845275, val loss: 0.9998578695033052, acc: 0.6378436082199093, test loss: 1.0059387069491794, acc: 0.6391834570519618
epoch: 44, train loss: 0.8070868526871736, acc: 0.6913502711961176, val loss: 1.0780182190822034, acc: 0.6295703229250067, test loss: 1.0926085992430832, acc: 0.626458112407211
epoch: 45, train loss: 0.7903643326966244, acc: 0.6990008564087925, val loss: 0.9094957746058296, acc: 0.6615959434214038, test loss: 0.8974297542207946, acc: 0.6678154825026511
epoch: 46, train loss: 0.7885676926998898, acc: 0.7012846131886954, val loss: 1.0227814409170972, acc: 0.6375767280491059, test loss: 1.0104203916936008, acc: 0.6389183457051962
epoch: 47, train loss: 0.7834978638633197, acc: 0.7019126463031687, val loss: 1.0076210675698967, acc: 0.6226314384841206, test loss: 1.042766927787886, acc: 0.6195652173913043
epoch: 48, train loss: 0.791758807710059, acc: 0.6946617185269769, val loss: 0.9552257353062625, acc: 0.6466506538564185, test loss: 0.9500250887137582, acc: 0.6553552492046659
epoch: 49, train loss: 0.7622098200905844, acc: 0.7096203254353411, val loss: 1.0066048105716832, acc: 0.6487856952228449, test loss: 1.051137653413532, acc: 0.6495227995758218
epoch: 50, train loss: 0.7775843858242443, acc: 0.6994576077647731, val loss: 1.1663965618320868, acc: 0.6071523885775287, test loss: 1.180104203087654, acc: 0.5988865323435844
epoch: 51, train loss: 0.7670859495061416, acc: 0.7011133314302027, val loss: 0.9293885895943114, acc: 0.6610621830797971, test loss: 0.9592461525319363, acc: 0.6553552492046659
epoch: 52, train loss: 0.7667918810700132, acc: 0.705623751070511, val loss: 0.9569085412321964, acc: 0.6511876167600748, test loss: 0.9852772105410142, acc: 0.6519088016967126
epoch: 53, train loss: 0.7544021671852179, acc: 0.7068798172994576, val loss: 0.9905705910095508, acc: 0.6394448892447291, test loss: 1.0076977164975407, acc: 0.637592788971368
epoch: 54, train loss: 0.7553883531554372, acc: 0.7087639166428775, val loss: 0.9424904319869063, acc: 0.6578596210301575, test loss: 0.9400163526120393, acc: 0.6646341463414634
epoch: 55, train loss: 0.7344600336945741, acc: 0.7165286896945475, val loss: 0.9765856278111275, acc: 0.6549239391513211, test loss: 0.9834342576792485, acc: 0.6508483563096501
epoch: 56, train loss: 0.7241506102151133, acc: 0.7193833856694262, val loss: 1.2763976901888943, acc: 0.5834000533760342, test loss: 1.2739637191323434, acc: 0.5710498409331919
epoch: 57, train loss: 0.738296358581274, acc: 0.7149871538681131, val loss: 0.993591420135722, acc: 0.6589271417133707, test loss: 1.0397315280687771, acc: 0.6466065747613998
epoch: 58, train loss: 0.733253625645285, acc: 0.7161290322580646, val loss: 0.9183783817520325, acc: 0.6706698692287163, test loss: 0.9410620274245423, acc: 0.6662248144220573
epoch: 59, train loss: 0.7145435124780326, acc: 0.7226377390807879, val loss: 1.2405444485109776, acc: 0.5793968508139845, test loss: 1.2540909096140371, acc: 0.5877518557794273
epoch 60, gamma increased to 2.
epoch: 60, train loss: 0.5237710235665398, acc: 0.7479303454182129, val loss: 0.7225836738765097, acc: 0.6949559647718174, test loss: 0.7526097554290788, acc: 0.6983032873806999
epoch: 61, train loss: 0.4725797386364089, acc: 0.7662003996574365, val loss: 0.7922730197071679, acc: 0.6805444355484388, test loss: 0.8347278225333415, acc: 0.6736479321314952
epoch: 62, train loss: 0.47138503163057566, acc: 0.7658007422209535, val loss: 0.7386194199029369, acc: 0.6877502001601281, test loss: 0.7545435466543867, acc: 0.6948568398727466
epoch: 63, train loss: 0.45409928257724674, acc: 0.7727662003996575, val loss: 0.7149627136172438, acc: 0.7072324526287697, test loss: 0.7437929677305899, acc: 0.6983032873806999
epoch: 64, train loss: 0.44680914196020666, acc: 0.7740222666286041, val loss: 0.7304407324446084, acc: 0.6981585268214572, test loss: 0.761745384869621, acc: 0.694591728525981
epoch: 65, train loss: 0.4424527040437872, acc: 0.7731658578361404, val loss: 0.7534704684320372, acc: 0.6925540432345877, test loss: 0.7628949449525027, acc: 0.6972428419936373
epoch: 66, train loss: 0.4336700422982301, acc: 0.774421924065087, val loss: 0.7551352362140261, acc: 0.6930878035761943, test loss: 0.7800685578161181, acc: 0.6940615058324496
epoch: 67, train loss: 0.4242786202368109, acc: 0.7807593491293178, val loss: 0.788682003609174, acc: 0.682412596744062, test loss: 0.8035405936195641, acc: 0.6802757158006363
epoch: 68, train loss: 0.4316098442307003, acc: 0.7760205538110191, val loss: 0.7751183464649934, acc: 0.6976247664798505, test loss: 0.7808616869270106, acc: 0.6969777306468717
epoch: 69, train loss: 0.4333648446420925, acc: 0.7745932058235798, val loss: 0.7624493071897398, acc: 0.6901521216973578, test loss: 0.7928750206531674, acc: 0.6874337221633086
epoch: 70, train loss: 0.4199722591397288, acc: 0.781387382243791, val loss: 0.8239984083786499, acc: 0.6837469975980784, test loss: 0.852875592094211, acc: 0.679745493107105
epoch: 71, train loss: 0.4142163030248079, acc: 0.780188409934342, val loss: 0.8462314805508551, acc: 0.6792100346944222, test loss: 0.863409625889767, acc: 0.6731177094379639
epoch: 72, train loss: 0.4272057754327119, acc: 0.7747073936625749, val loss: 0.8198963403383637, acc: 0.6701361088871097, test loss: 0.8045464631482121, acc: 0.676033934252386
epoch: 73, train loss: 0.41156522596083195, acc: 0.7837282329431916, val loss: 0.8173195497175837, acc: 0.6882839605017347, test loss: 0.83803997742662, acc: 0.6847826086956522
epoch: 74, train loss: 0.40166872550411425, acc: 0.7843562660576648, val loss: 0.8065636586213958, acc: 0.692820923405391, test loss: 0.830382412842703, acc: 0.689289501590668
epoch: 75, train loss: 0.4070959533720809, acc: 0.7810448187268056, val loss: 0.801089759823606, acc: 0.6826794769148652, test loss: 0.8351455053367898, acc: 0.6831919406150583
epoch: 76, train loss: 0.407021666081878, acc: 0.7805309734513274, val loss: 0.7913945232164329, acc: 0.685081398452095, test loss: 0.8126517055143354, acc: 0.6884941675503712
epoch: 77, train loss: 0.3975847007358479, acc: 0.7861832714815872, val loss: 0.8356607637311224, acc: 0.6797437950360288, test loss: 0.8019527955373802, acc: 0.6813361611876988
epoch: 78, train loss: 0.3847213339370011, acc: 0.7864116471595775, val loss: 0.8293561939878848, acc: 0.6786762743528156, test loss: 0.8569012052307452, acc: 0.6831919406150583
epoch: 79, train loss: 0.39431586553292924, acc: 0.7853268626891237, val loss: 0.7509754576109109, acc: 0.7002935681878837, test loss: 0.8017309737230789, acc: 0.6951219512195121
epoch: 80, train loss: 0.37663197300790074, acc: 0.7885812161004853, val loss: 0.866944951663693, acc: 0.6733386709367494, test loss: 0.9055572548448597, acc: 0.66118769883351
epoch: 81, train loss: 0.40490314076364026, acc: 0.781387382243791, val loss: 0.8442477928596336, acc: 0.6778756338404056, test loss: 0.8313756148357533, acc: 0.6839872746553552
epoch: 82, train loss: 0.37147886382999196, acc: 0.7886954039394805, val loss: 0.7709567876125546, acc: 0.696290365625834, test loss: 0.7712677564246784, acc: 0.6959172852598091
epoch: 83, train loss: 0.3614172818590633, acc: 0.7952041107622039, val loss: 0.7859715151729538, acc: 0.7016279690419002, test loss: 0.8163476705803977, acc: 0.6983032873806999
epoch: 84, train loss: 0.3617431553387485, acc: 0.7962888952326577, val loss: 1.0193799447466094, acc: 0.6426474512943688, test loss: 1.0278790495175698, acc: 0.6399787910922587
epoch: 85, train loss: 0.38532781065650373, acc: 0.7858978018840993, val loss: 0.8327237679731188, acc: 0.6829463570856685, test loss: 0.8886416471649202, acc: 0.6691410392364793
epoch: 86, train loss: 0.3676086575141676, acc: 0.7905224093634028, val loss: 0.8157711389512102, acc: 0.6874833199893248, test loss: 0.8114681309632253, acc: 0.6914103923647932
epoch: 87, train loss: 0.34601352783975486, acc: 0.7995432486440194, val loss: 0.9937860020835272, acc: 0.6447824926607953, test loss: 0.9880895832057722, acc: 0.6458112407211029
epoch: 88, train loss: 0.3529822865418628, acc: 0.7958321438766771, val loss: 0.859719516946565, acc: 0.6725380304243395, test loss: 0.896012632869587, acc: 0.6736479321314952
epoch: 89, train loss: 0.3609816107534185, acc: 0.792634884384813, val loss: 0.7960661873996878, acc: 0.6901521216973578, test loss: 0.82104369640856, acc: 0.6895546129374337
epoch: 90, train loss: 0.35074591119562326, acc: 0.7990864972880388, val loss: 0.8016762543208191, acc: 0.6976247664798505, test loss: 0.817147617501899, acc: 0.7022799575821845
epoch: 91, train loss: 0.36127649109907367, acc: 0.7948044533257208, val loss: 0.7990594814706237, acc: 0.6960234854550307, test loss: 0.8160567225479498, acc: 0.6980381760339343
epoch: 92, train loss: 0.34897861925493195, acc: 0.7985726520125607, val loss: 0.8241195882006203, acc: 0.6952228449426208, test loss: 0.8351510359904672, acc: 0.6940615058324496
epoch: 93, train loss: 0.3589723709750577, acc: 0.7931487296602912, val loss: 0.8319682396027702, acc: 0.6866826794769149, test loss: 0.838638020977615, acc: 0.676829268292683
epoch: 94, train loss: 0.3413044844887238, acc: 0.7989723094490436, val loss: 0.8170765458598975, acc: 0.6957566052842273, test loss: 0.8528529381575033, acc: 0.7030752916224814
epoch: 95, train loss: 0.3422826668040738, acc: 0.799200685127034, val loss: 0.8928127492749917, acc: 0.6744061916199626, test loss: 0.9334257748827323, acc: 0.6720572640509014
epoch: 96, train loss: 0.33883131551735746, acc: 0.8007993148729661, val loss: 0.8833731943873173, acc: 0.685882038964505, test loss: 0.8883883599132647, acc: 0.689289501590668
epoch: 97, train loss: 0.3332484624546118, acc: 0.803083071652869, val loss: 0.8393523249271109, acc: 0.699759807846277, test loss: 0.8727534785629709, acc: 0.6967126193001061
epoch: 98, train loss: 0.33569158547917605, acc: 0.8042820439623181, val loss: 0.922116296654674, acc: 0.6605284227381906, test loss: 0.9700667440954474, acc: 0.6561505832449629
epoch: 99, train loss: 0.372318278031454, acc: 0.7909220667998859, val loss: 0.7862531678785412, acc: 0.7008273285294903, test loss: 0.812794912152174, acc: 0.7028101802757158
epoch 100, gamma increased to 3.
epoch: 100, train loss: 0.2535825680748858, acc: 0.8048529831572937, val loss: 0.7289442410938639, acc: 0.6981585268214572, test loss: 0.7549361834976099, acc: 0.7030752916224814
epoch: 101, train loss: 0.2601715019761308, acc: 0.8008564087924636, val loss: 0.7353313371344251, acc: 0.6994929276754737, test loss: 0.7577820112444816, acc: 0.7014846235418876
epoch: 102, train loss: 0.2339886076478253, acc: 0.8180987724807308, val loss: 0.7576943940724378, acc: 0.6978916466506538, test loss: 0.8079368109920497, acc: 0.6935312831389183
epoch: 103, train loss: 0.23666851912486087, acc: 0.8110762203825292, val loss: 0.76091399700571, acc: 0.7029623698959168, test loss: 0.7965513466524546, acc: 0.7022799575821845
epoch: 104, train loss: 0.24203888058254047, acc: 0.8079931487296603, val loss: 0.7780986355564452, acc: 0.6874833199893248, test loss: 0.8368268124245637, acc: 0.6805408271474019
epoch: 105, train loss: 0.25136538355853466, acc: 0.8074793034541822, val loss: 0.7611740290848198, acc: 0.6938884440886042, test loss: 0.7853501797480902, acc: 0.7006892895015907
epoch: 106, train loss: 0.22944588949943726, acc: 0.8122180987724807, val loss: 0.8214083828224893, acc: 0.6744061916199626, test loss: 0.8555958568101724, acc: 0.6731177094379639
epoch: 107, train loss: 0.23359960696627813, acc: 0.8066228946617185, val loss: 0.8673240357091785, acc: 0.6778756338404056, test loss: 0.885523677883573, acc: 0.6741781548250265
epoch: 108, train loss: 0.24748521510818977, acc: 0.8047958892377962, val loss: 0.7225182535236665, acc: 0.6922871630637843, test loss: 0.750276737223248, acc: 0.6967126193001061
epoch: 109, train loss: 0.22577444984168962, acc: 0.815129888666857, val loss: 0.7915464892589731, acc: 0.6866826794769149, test loss: 0.8280763540136473, acc: 0.6850477200424178
epoch: 110, train loss: 0.23229635890606232, acc: 0.8079360548101627, val loss: 0.7657738500892368, acc: 0.6909527622097679, test loss: 0.8318225304622792, acc: 0.6882290562036055
epoch: 111, train loss: 0.2436446670843743, acc: 0.801198972309449, val loss: 0.8219752374063151, acc: 0.6792100346944222, test loss: 0.8670797891606709, acc: 0.6694061505832449
epoch: 112, train loss: 0.22354628529134832, acc: 0.8139309163574079, val loss: 0.7860614392227384, acc: 0.688550840672538, test loss: 0.8123972375471291, acc: 0.6800106044538706
epoch: 113, train loss: 0.2425637598203449, acc: 0.8057664858692549, val loss: 0.8434387959255547, acc: 0.6765412329863891, test loss: 0.8332398529396583, acc: 0.6741781548250265
epoch: 114, train loss: 0.23988914701586753, acc: 0.8100485298315729, val loss: 0.852059071433681, acc: 0.6650653856418468, test loss: 0.8699934742989242, acc: 0.6670201484623541
epoch: 115, train loss: 0.2573193797597605, acc: 0.802683414216386, val loss: 0.7524841809100967, acc: 0.6866826794769149, test loss: 0.7534617336026431, acc: 0.6988335100742312
epoch: 116, train loss: 0.2409852735330267, acc: 0.803882386525835, val loss: 0.7493857963898992, acc: 0.6946890846010142, test loss: 0.8064935318612092, acc: 0.6914103923647932
epoch: 117, train loss: 0.25151556789653834, acc: 0.7989723094490436, val loss: 0.7948949187477016, acc: 0.6976247664798505, test loss: 0.8309809683742099, acc: 0.6953870625662778
epoch: 118, train loss: 0.24570635576742295, acc: 0.8067370825007136, val loss: 0.913455244443052, acc: 0.6509207365892714, test loss: 0.9371920130032876, acc: 0.6458112407211029
epoch: 119, train loss: 0.22813666502033478, acc: 0.8066799885812161, val loss: 0.7625709440283753, acc: 0.7026954897251134, test loss: 0.7749942781310825, acc: 0.6951219512195121
epoch: 120, train loss: 0.27769352603377256, acc: 0.7870396802740508, val loss: 0.810407825428739, acc: 0.6728049105951428, test loss: 0.824926220220321, acc: 0.6765641569459173
best val loss 0.7149627136172438 at epoch 63.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9011    0.9453    0.9226      5337
           1     0.6434    0.8086    0.7166      2502
           2     0.9707    0.7778    0.8636       810
           3     0.7622    0.8082    0.7845      1840
           4     0.9092    0.7748    0.8366       737
           5     0.8095    0.9793    0.8864       677
           6     0.7560    0.8594    0.8044      1323
           7     0.5095    0.6207    0.5596       907
           8     0.9458    0.6627    0.7793       421
           9     0.8850    0.7481    0.8108       401
          10     0.9061    0.9015    0.9038       396
          11     0.9968    0.9247    0.9594       332
          12     0.6100    0.5356    0.5704       295
          13     0.8750    0.5773    0.6957       291
          14     0.0000    0.0000    0.0000       261
          15     0.7612    0.2065    0.3248       494
          16     1.0000    0.0078    0.0155       256
          17     0.7979    0.6553    0.7196       235

    accuracy                         0.7962     17515
   macro avg     0.7800    0.6552    0.6752     17515
weighted avg     0.7985    0.7962    0.7812     17515

validation report:
              precision    recall  f1-score   support

           0     0.8300    0.8801    0.8544      1143
           1     0.5498    0.7108    0.6200       536
           2     0.8657    0.6705    0.7557       173
           3     0.6347    0.7234    0.6762       394
           4     0.8095    0.6456    0.7183       158
           5     0.7191    0.8828    0.7926       145
           6     0.6968    0.7633    0.7285       283
           7     0.3976    0.5103    0.4470       194
           8     0.8846    0.5111    0.6479        90
           9     0.7031    0.5294    0.6040        85
          10     0.8690    0.8690    0.8690        84
          11     1.0000    0.7887    0.8819        71
          12     0.5714    0.3810    0.4571        63
          13     0.8462    0.5323    0.6535        62
          14     0.0000    0.0000    0.0000        56
          15     0.6364    0.1333    0.2205       105
          16     0.0000    0.0000    0.0000        55
          17     0.7027    0.5200    0.5977        50

    accuracy                         0.7072      3747
   macro avg     0.6509    0.5584    0.5847      3747
weighted avg     0.7002    0.7072    0.6925      3747

test report: 
              precision    recall  f1-score   support

           0     0.8130    0.8734    0.8421      1145
           1     0.5288    0.7169    0.6087       537
           2     0.8425    0.7029    0.7664       175
           3     0.6949    0.7266    0.7104       395
           4     0.8065    0.6289    0.7067       159
           5     0.7485    0.8562    0.7987       146
           6     0.6918    0.7430    0.7165       284
           7     0.4116    0.5846    0.4831       195
           8     0.9167    0.6044    0.7285        91
           9     0.6226    0.3793    0.4714        87
          10     0.8140    0.8140    0.8140        86
          11     1.0000    0.7500    0.8571        72
          12     0.3810    0.2500    0.3019        64
          13     0.8333    0.3125    0.4545        64
          14     0.0000    0.0000    0.0000        57
          15     0.6087    0.1308    0.2154       107
          16     1.0000    0.0179    0.0351        56
          17     0.6667    0.5000    0.5714        52

    accuracy                         0.6983      3772
   macro avg     0.6878    0.5328    0.5601      3772
weighted avg     0.7063    0.6983    0.6826      3772

