seed:  666
save trained model at:  ../trained_models/trained_classifier_model_19.pt
save loss at:  ./results/train_classifier_results_19.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  15
number of pockets in training set:  17282
number of pockets in validation set:  3698
number of pockets in test set:  3720
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=15, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b3b60e7dbe0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0609258763865457, acc: 0.37026964471704665, val loss: 1.816398337443111, acc: 0.42239048134126556, test loss: 1.8214582617564867, acc: 0.4266129032258065
epoch: 2, train loss: 1.7807156670329445, acc: 0.4407475986575628, val loss: 1.6968514689759604, acc: 0.4645754461871282, test loss: 1.7054048527953445, acc: 0.46236559139784944
epoch: 3, train loss: 1.6743352148298616, acc: 0.47644948501330864, val loss: 1.7157112828714258, acc: 0.4656571119524067, test loss: 1.731555804898662, acc: 0.4575268817204301
epoch: 4, train loss: 1.6057085206064137, acc: 0.49820622613123483, val loss: 1.5571688320135155, acc: 0.5013520822065982, test loss: 1.5517423604124336, acc: 0.5045698924731182
epoch: 5, train loss: 1.538127684811174, acc: 0.5185163754195117, val loss: 1.5174972223165295, acc: 0.5151433207138995, test loss: 1.5069490114847819, acc: 0.5120967741935484
epoch: 6, train loss: 1.4990359416591281, acc: 0.5255757435482005, val loss: 1.7583941898841096, acc: 0.4637641968631693, test loss: 1.7351013916794973, acc: 0.46639784946236557
epoch: 7, train loss: 1.469851454158801, acc: 0.5370906145122092, val loss: 1.5197673814886, acc: 0.5167658193618172, test loss: 1.5041838861280872, acc: 0.5185483870967742
epoch: 8, train loss: 1.4186871782550519, acc: 0.5496470315935655, val loss: 1.589216653948671, acc: 0.5094645754461872, test loss: 1.586302806997812, acc: 0.510752688172043
epoch: 9, train loss: 1.399403144634235, acc: 0.5565906723758824, val loss: 1.3757590020005932, acc: 0.5578691184424013, test loss: 1.3683156690289897, acc: 0.553494623655914
epoch: 10, train loss: 1.351171906106713, acc: 0.572966091887513, val loss: 1.7081288700815533, acc: 0.4805300162249865, test loss: 1.690436977468511, acc: 0.4774193548387097
epoch: 11, train loss: 1.329747314958602, acc: 0.5826871889827566, val loss: 1.2816183912618666, acc: 0.5881557598702001, test loss: 1.3060996470912811, acc: 0.585752688172043
epoch: 12, train loss: 1.293380271021297, acc: 0.5931605138294179, val loss: 1.442349112568964, acc: 0.5373174689021093, test loss: 1.441383138779671, acc: 0.5473118279569893
epoch: 13, train loss: 1.2493720212588086, acc: 0.6101724337460942, val loss: 1.2896075123641992, acc: 0.588967009194159, test loss: 1.2829520322943246, acc: 0.5959677419354839
epoch: 14, train loss: 1.2068608230275835, acc: 0.6231917602129383, val loss: 1.3005322009695743, acc: 0.597079502433748, test loss: 1.2950067776505665, acc: 0.5932795698924731
epoch: 15, train loss: 1.1933504865064952, acc: 0.6296724916097673, val loss: 1.3758283939279048, acc: 0.5822065981611682, test loss: 1.3717258479005547, acc: 0.5766129032258065
epoch: 16, train loss: 1.1829374997707693, acc: 0.6311769471126027, val loss: 1.256734417089583, acc: 0.6032990805840995, test loss: 1.2588589278600548, acc: 0.6024193548387097
epoch: 17, train loss: 1.1685771610488247, acc: 0.6383520425876634, val loss: 1.2076314826344334, acc: 0.6173607355327204, test loss: 1.2049361044360745, acc: 0.6166666666666667
epoch: 18, train loss: 1.1381083852010612, acc: 0.6443698645990047, val loss: 1.48266115131863, acc: 0.5554353704705246, test loss: 1.471254075470791, acc: 0.5594086021505377
epoch: 19, train loss: 1.1166194259754805, acc: 0.6529915519037148, val loss: 1.1339260743978283, acc: 0.6457544618712818, test loss: 1.1247161183305967, acc: 0.6537634408602151
epoch: 20, train loss: 1.0866511851780227, acc: 0.6656058326582571, val loss: 1.1894767898169643, acc: 0.638182801514332, test loss: 1.174350231950001, acc: 0.6352150537634409
epoch: 21, train loss: 1.0803928337719633, acc: 0.6650850595995834, val loss: 1.1311849540733143, acc: 0.6295294753921038, test loss: 1.1432688564382574, acc: 0.635752688172043
epoch: 22, train loss: 1.036863800400786, acc: 0.6764842032172202, val loss: 1.2254713624796396, acc: 0.6057328285559762, test loss: 1.2110987309486636, acc: 0.6231182795698925
epoch: 23, train loss: 1.0451748640969623, acc: 0.6760212938317325, val loss: 1.1042545864297095, acc: 0.6481882098431585, test loss: 1.112215144147155, acc: 0.65
epoch: 24, train loss: 1.0208910749185114, acc: 0.6844693901168846, val loss: 1.158884701437406, acc: 0.6398053001622499, test loss: 1.15253131774164, acc: 0.6403225806451613
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.7972381064122287, acc: 0.68597384561972, val loss: 1.0091243918487482, acc: 0.6151974040021634, test loss: 1.026426984417823, acc: 0.614247311827957
epoch: 26, train loss: 0.8039616949993377, acc: 0.68597384561972, val loss: 0.920440895870739, acc: 0.6354786371011357, test loss: 0.9335630914216401, acc: 0.6349462365591397
epoch: 27, train loss: 0.7632283855610293, acc: 0.6977201712764727, val loss: 1.2431524724429075, acc: 0.5535424553812872, test loss: 1.2545847010868851, acc: 0.5494623655913978
epoch: 28, train loss: 0.7784177020128341, acc: 0.6933803957875246, val loss: 0.9148670305362452, acc: 0.649269875608437, test loss: 0.9337924111273981, acc: 0.6529569892473118
epoch: 29, train loss: 0.7565932164257316, acc: 0.6996875361647957, val loss: 0.9152086847469315, acc: 0.6343969713358573, test loss: 0.9289895216623942, acc: 0.6475806451612903
epoch: 30, train loss: 0.7429565600658536, acc: 0.7096979516259693, val loss: 0.8404218699237087, acc: 0.6741481882098431, test loss: 0.8509987467078752, acc: 0.678763440860215
epoch: 31, train loss: 0.7278976184518066, acc: 0.7116653165142923, val loss: 0.9374205965554798, acc: 0.6481882098431585, test loss: 0.9437888473592778, acc: 0.6481182795698924
epoch: 32, train loss: 0.7308385494737324, acc: 0.7065733132739266, val loss: 0.8126214855487054, acc: 0.6790156841535966, test loss: 0.8321509397158059, acc: 0.6774193548387096
epoch: 33, train loss: 0.7101459897595028, acc: 0.7197083670871427, val loss: 0.926057067286201, acc: 0.6552190373174689, test loss: 0.9291423454079577, acc: 0.6440860215053763
epoch: 34, train loss: 0.7073930118967159, acc: 0.7153685915981947, val loss: 0.8203340347809944, acc: 0.6857760951865873, test loss: 0.8133878010575489, acc: 0.6801075268817204
epoch: 35, train loss: 0.7048914597867995, acc: 0.7150792732322648, val loss: 0.8087372207332134, acc: 0.6709031909140075, test loss: 0.8222645251981674, acc: 0.6747311827956989
epoch: 36, train loss: 0.6796306556276309, acc: 0.7242795972688346, val loss: 0.8293302694870498, acc: 0.6790156841535966, test loss: 0.8366290389850576, acc: 0.6811827956989247
epoch: 37, train loss: 0.6905128338204229, acc: 0.7287351001041547, val loss: 0.9109513958187733, acc: 0.6430502974580855, test loss: 0.9218422187271939, acc: 0.6524193548387097
epoch: 38, train loss: 0.6714936665205089, acc: 0.7334220576322185, val loss: 0.871814652608497, acc: 0.6563007030827475, test loss: 0.8867735073130618, acc: 0.6513440860215054
epoch: 39, train loss: 0.6681563264310201, acc: 0.7297187825483161, val loss: 0.8268180253249107, acc: 0.6738777717685235, test loss: 0.8484904340518418, acc: 0.671236559139785
epoch: 40, train loss: 0.651563016018538, acc: 0.7334220576322185, val loss: 0.9609507021225743, acc: 0.6473769605191996, test loss: 0.9961653729920746, acc: 0.649731182795699
epoch: 41, train loss: 0.6480613510761254, acc: 0.7397291980094897, val loss: 0.8414529494300541, acc: 0.6746890210924824, test loss: 0.8863710711079259, acc: 0.6588709677419354
epoch: 42, train loss: 0.6526661241872624, acc: 0.7360837865987733, val loss: 0.8111982986435754, acc: 0.6771227690643591, test loss: 0.7966668913441319, acc: 0.6908602150537635
epoch: 43, train loss: 0.6371870513674756, acc: 0.7460363383867608, val loss: 0.8002806355851351, acc: 0.6879394267171444, test loss: 0.8153351594043035, acc: 0.685752688172043
epoch: 44, train loss: 0.6297101885228642, acc: 0.7470200208309223, val loss: 0.9293412420799952, acc: 0.6590048674959438, test loss: 0.9122914498852145, acc: 0.6620967741935484
epoch: 45, train loss: 0.6357085398813183, acc: 0.743432473093392, val loss: 0.8427289082850554, acc: 0.6768523526230394, test loss: 0.8590962056190737, acc: 0.6717741935483871
epoch: 46, train loss: 0.6562802882440636, acc: 0.7379932878139104, val loss: 0.8742681743648647, acc: 0.6530557057869119, test loss: 0.8879860877990723, acc: 0.657258064516129
epoch: 47, train loss: 0.6273760795248368, acc: 0.7426223816687884, val loss: 0.8215643281611705, acc: 0.6784748512709573, test loss: 0.8409641758088142, acc: 0.6623655913978495
epoch: 48, train loss: 0.6091680154523396, acc: 0.7486980673533156, val loss: 0.8294548662112429, acc: 0.6949702541914549, test loss: 0.8714436679758052, acc: 0.6798387096774193
epoch: 49, train loss: 0.613450710930188, acc: 0.750028931836593, val loss: 0.8669875492077379, acc: 0.6690102758247701, test loss: 0.8660742205958213, acc: 0.668010752688172
epoch: 50, train loss: 0.6019106765144361, acc: 0.75222775141766, val loss: 0.8136111065269871, acc: 0.7028123309897242, test loss: 0.821460053741291, acc: 0.6989247311827957
epoch: 51, train loss: 0.5687872689916171, acc: 0.7662307603286657, val loss: 0.9364799656308361, acc: 0.6457544618712818, test loss: 0.9353966046405094, acc: 0.6483870967741936
epoch: 52, train loss: 0.5644108277444085, acc: 0.767735215831501, val loss: 1.044115813504431, acc: 0.6092482422931315, test loss: 1.0567375700960877, acc: 0.6043010752688172
epoch: 53, train loss: 0.5642119056647912, acc: 0.7686610346024766, val loss: 0.7891267964232607, acc: 0.6944294213088156, test loss: 0.798610492419171, acc: 0.7029569892473119
epoch: 54, train loss: 0.575515869156021, acc: 0.7629903946302511, val loss: 0.8183886099661151, acc: 0.6871281773931855, test loss: 0.8502109814715642, acc: 0.6806451612903226
epoch: 55, train loss: 0.5903378229507651, acc: 0.7575512093507696, val loss: 0.8301702859660881, acc: 0.6844240129799892, test loss: 0.8376582914783108, acc: 0.6881720430107527
epoch: 56, train loss: 0.5546357677347364, acc: 0.7704548084712417, val loss: 0.7952242958797127, acc: 0.6971335857220119, test loss: 0.7959222644887944, acc: 0.7032258064516129
epoch: 57, train loss: 0.5494910760784492, acc: 0.7751996296724916, val loss: 0.8227100168581973, acc: 0.6873985938345052, test loss: 0.8270636209877589, acc: 0.6905913978494623
epoch: 58, train loss: 0.5567709158904685, acc: 0.7705126721444278, val loss: 0.8235615736217097, acc: 0.6841535965386696, test loss: 0.8086571431929065, acc: 0.6903225806451613
epoch: 59, train loss: 0.5591308866823566, acc: 0.7715542182617753, val loss: 0.9242173696995555, acc: 0.6541373715521904, test loss: 0.9626906446231309, acc: 0.6559139784946236
epoch: 60, train loss: 0.5424090816927458, acc: 0.7757204027311654, val loss: 0.8376387359645961, acc: 0.7006489994591671, test loss: 0.8170919628553494, acc: 0.7005376344086022
epoch: 61, train loss: 0.5435608889256179, acc: 0.7757204027311654, val loss: 0.8606666735921956, acc: 0.6703623580313683, test loss: 0.8759204387664795, acc: 0.6650537634408602
Epoch    61: reducing learning rate of group 0 to 1.5000e-03.
epoch: 62, train loss: 0.47708842424231124, acc: 0.7973035528295336, val loss: 0.7350076455693428, acc: 0.725797728501893, test loss: 0.7512384604382258, acc: 0.7201612903225807
epoch: 63, train loss: 0.40668099157071586, acc: 0.825193843305173, val loss: 0.7653913143327135, acc: 0.7152514872904273, test loss: 0.7698009778094548, acc: 0.7188172043010753
epoch: 64, train loss: 0.38659468731550745, acc: 0.8332947575512093, val loss: 0.7618301919242896, acc: 0.7255273120605733, test loss: 0.7644744242391278, acc: 0.7263440860215054
epoch: 65, train loss: 0.3910898956356053, acc: 0.8307487559310265, val loss: 0.7919506329082037, acc: 0.714710654407788, test loss: 0.7847733677074473, acc: 0.7158602150537634
epoch: 66, train loss: 0.3809480553437074, acc: 0.8350885314199745, val loss: 0.8019108233289373, acc: 0.7282314764737696, test loss: 0.8269791346724316, acc: 0.725
epoch: 67, train loss: 0.37690989342901304, acc: 0.8365929869228098, val loss: 0.8039117612472543, acc: 0.722823147647377, test loss: 0.8138784608533306, acc: 0.7263440860215054
epoch: 68, train loss: 0.3656994461730431, acc: 0.8395440342552946, val loss: 0.8567681050803482, acc: 0.722823147647377, test loss: 0.867423774350074, acc: 0.725268817204301
epoch: 69, train loss: 0.3515342574348026, acc: 0.8466612660571693, val loss: 0.8130621487156903, acc: 0.7244456462952947, test loss: 0.8052203562951857, acc: 0.7260752688172043
epoch: 70, train loss: 0.3486133117895686, acc: 0.8475292211549589, val loss: 0.8424850417962907, acc: 0.7182260681449432, test loss: 0.8523690321112192, acc: 0.7225806451612903
epoch: 71, train loss: 0.3701855433634235, acc: 0.8398333526212244, val loss: 0.9405568645347319, acc: 0.6974040021633315, test loss: 0.8963817288798671, acc: 0.6983870967741935
epoch: 72, train loss: 0.34211884115308183, acc: 0.8512324962388612, val loss: 0.907231632086185, acc: 0.7060573282855598, test loss: 0.9223571023633403, acc: 0.7045698924731183
epoch: 73, train loss: 0.3425485935942082, acc: 0.8521583150098369, val loss: 0.8178285825181614, acc: 0.7298539751216874, test loss: 0.8265151316119779, acc: 0.7212365591397849
epoch: 74, train loss: 0.33373083873502, acc: 0.8514060872584192, val loss: 0.8516203938077243, acc: 0.7144402379664684, test loss: 0.8628964265187581, acc: 0.7217741935483871
epoch: 75, train loss: 0.33314115247741893, acc: 0.851926860317093, val loss: 0.8398412556955143, acc: 0.7325581395348837, test loss: 0.8392220902186568, acc: 0.728225806451613
epoch: 76, train loss: 0.3131186101167873, acc: 0.8622844578173823, val loss: 0.8795977292543105, acc: 0.7141698215251487, test loss: 0.8715768511577319, acc: 0.7215053763440861
epoch: 77, train loss: 0.32058767880392963, acc: 0.8556879990741812, val loss: 0.9071956672431198, acc: 0.7155219037317468, test loss: 0.9006213111262168, acc: 0.7201612903225807
epoch: 78, train loss: 0.315332514298016, acc: 0.8625737761833121, val loss: 0.8246924427150069, acc: 0.7090319091400757, test loss: 0.8358624078894175, acc: 0.7099462365591398
epoch: 79, train loss: 0.3253693614461579, acc: 0.8552250896886935, val loss: 0.8904637364582992, acc: 0.6987560843699296, test loss: 0.8934957550417992, acc: 0.7032258064516129
epoch: 80, train loss: 0.3119846316188757, acc: 0.8633838676079157, val loss: 0.9342511664731493, acc: 0.7128177393185505, test loss: 0.9549592320637036, acc: 0.7083333333333334
epoch: 81, train loss: 0.3009376459245314, acc: 0.8665085059599583, val loss: 0.8558519428133513, acc: 0.7217414818820984, test loss: 0.8721037177629368, acc: 0.7161290322580646
epoch: 82, train loss: 0.3087742822084172, acc: 0.8603749566022451, val loss: 0.897143487997349, acc: 0.722823147647377, test loss: 0.8690971656512189, acc: 0.7225806451612903
epoch: 83, train loss: 0.3165626895157307, acc: 0.8598541835435713, val loss: 0.9212019113413767, acc: 0.7120064899945917, test loss: 0.9292370514203143, acc: 0.7118279569892473
epoch: 84, train loss: 0.30591643948878366, acc: 0.8632102765883578, val loss: 0.9066244583635346, acc: 0.7044348296376419, test loss: 0.9010630397386449, acc: 0.714516129032258
epoch: 85, train loss: 0.2970675598304452, acc: 0.8689387802337692, val loss: 1.0952177166358532, acc: 0.6565711195240671, test loss: 1.105014080642372, acc: 0.6548387096774193
epoch: 86, train loss: 0.32301170903766424, acc: 0.8590440921189677, val loss: 0.9032162159052716, acc: 0.722823147647377, test loss: 0.9304197070419148, acc: 0.7147849462365592
epoch: 87, train loss: 0.2850386419842991, acc: 0.8698067353315588, val loss: 0.9402732529338468, acc: 0.7138994050838291, test loss: 0.923801484159244, acc: 0.7188172043010753
epoch: 88, train loss: 0.2941599601487183, acc: 0.8693438259460711, val loss: 0.8533854258905043, acc: 0.7230935640886966, test loss: 0.8702302871211882, acc: 0.7258064516129032
epoch: 89, train loss: 0.28694906607641, acc: 0.87165837287351, val loss: 0.929658277154678, acc: 0.7065981611681991, test loss: 0.9429046635986656, acc: 0.7129032258064516
epoch: 90, train loss: 0.27664399436400067, acc: 0.8744358291864368, val loss: 0.9384504276201363, acc: 0.7184964845862628, test loss: 0.9264495013862528, acc: 0.7260752688172043
epoch: 91, train loss: 0.3168747888065546, acc: 0.8591598194653397, val loss: 1.0055335216615058, acc: 0.6846944294213089, test loss: 1.0220104991748769, acc: 0.6811827956989247
epoch: 92, train loss: 0.30967548932414984, acc: 0.8615322300659646, val loss: 0.8948372109249645, acc: 0.714710654407788, test loss: 0.9144087391514932, acc: 0.7118279569892473
epoch: 93, train loss: 0.2749513924301202, acc: 0.8743201018400648, val loss: 0.9141356705200872, acc: 0.7201189832341807, test loss: 0.9355640185776577, acc: 0.7204301075268817
epoch: 94, train loss: 0.26383220114071687, acc: 0.8775604675384794, val loss: 0.9949110032675265, acc: 0.696322336398053, test loss: 0.977149772644043, acc: 0.7051075268817204
epoch: 95, train loss: 0.3160819130050903, acc: 0.8582340006943641, val loss: 0.8731331421659726, acc: 0.7295835586803677, test loss: 0.889871015856343, acc: 0.7241935483870968
epoch: 96, train loss: 0.26648505338501177, acc: 0.88056937854415, val loss: 0.9035389843729782, acc: 0.7287723093564089, test loss: 0.8979052656440325, acc: 0.7317204301075269
epoch: 97, train loss: 0.2626111719042956, acc: 0.8816109246614975, val loss: 0.8744264583835221, acc: 0.7244456462952947, test loss: 0.8839857409077306, acc: 0.7217741935483871
epoch: 98, train loss: 0.2663026030586351, acc: 0.8791227867145006, val loss: 0.9445504234441878, acc: 0.7171444023796647, test loss: 0.9353156946038688, acc: 0.7236559139784946
epoch: 99, train loss: 0.2441684225204021, acc: 0.889364656868418, val loss: 0.8965614126978081, acc: 0.7309356408869659, test loss: 0.8857696128147905, acc: 0.7255376344086022
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.17957554735521822, acc: 0.8881495197315126, val loss: 0.7985504135175161, acc: 0.7239048134126554, test loss: 0.7788808566267772, acc: 0.7231182795698925
epoch: 101, train loss: 0.18731032662846372, acc: 0.882768198125217, val loss: 0.769808085819655, acc: 0.714710654407788, test loss: 0.817669956145748, acc: 0.7150537634408602
epoch: 102, train loss: 0.17224251815613573, acc: 0.8868186552482351, val loss: 0.8279806724298703, acc: 0.7163331530557058, test loss: 0.8301456677016391, acc: 0.717741935483871
epoch: 103, train loss: 0.16175346589173975, acc: 0.8937044323573661, val loss: 0.7934895414091433, acc: 0.7320173066522444, test loss: 0.8160152937776299, acc: 0.7271505376344086
epoch: 104, train loss: 0.17270795320308452, acc: 0.8882073834046985, val loss: 0.8053395066021455, acc: 0.7190373174689021, test loss: 0.8107455463819606, acc: 0.7172043010752688
epoch: 105, train loss: 0.17786090607107025, acc: 0.8867607915750492, val loss: 0.757978519212239, acc: 0.717685235262304, test loss: 0.7714349469830913, acc: 0.7231182795698925
epoch: 106, train loss: 0.1745102646111918, acc: 0.8856035181113298, val loss: 0.8792959149300311, acc: 0.7160627366143861, test loss: 0.8704694055741833, acc: 0.7137096774193549
epoch: 107, train loss: 0.15585912803562824, acc: 0.8978706168267562, val loss: 0.8019717403719269, acc: 0.7201189832341807, test loss: 0.8046692971260317, acc: 0.7247311827956989
epoch: 108, train loss: 0.15427606757792758, acc: 0.8976970258071982, val loss: 0.8610045029189923, acc: 0.709572742022715, test loss: 0.8732782656146634, acc: 0.7104838709677419
epoch: 109, train loss: 0.1570425920206534, acc: 0.8935308413378081, val loss: 0.9235952029426785, acc: 0.7141698215251487, test loss: 0.9420312635360225, acc: 0.7118279569892473
epoch: 110, train loss: 0.1638857931288397, acc: 0.8926628862400185, val loss: 0.8342008701590604, acc: 0.7071389940508382, test loss: 0.8480111845078007, acc: 0.7016129032258065
epoch: 111, train loss: 0.17076199105383, acc: 0.8865872005554912, val loss: 0.8034763049924613, acc: 0.7157923201730665, test loss: 0.8259095335519442, acc: 0.7266129032258064
epoch: 112, train loss: 0.1479700664902241, acc: 0.898217798865872, val loss: 0.9725001274797966, acc: 0.6957815035154138, test loss: 0.9588174455909318, acc: 0.6983870967741935
Epoch   112: reducing learning rate of group 0 to 7.5000e-04.
epoch: 113, train loss: 0.12764183953516517, acc: 0.9107742159472283, val loss: 0.8308790128510858, acc: 0.7317468902109249, test loss: 0.8409684909287319, acc: 0.7325268817204301
epoch: 114, train loss: 0.09423319727247918, acc: 0.9251244068973499, val loss: 0.882353281961898, acc: 0.7379664683612763, test loss: 0.9115418070106096, acc: 0.7319892473118279
epoch: 115, train loss: 0.08513704530602459, acc: 0.9340354125679898, val loss: 0.9809301438494075, acc: 0.7314764737696052, test loss: 0.9701446497312156, acc: 0.7325268817204301
epoch: 116, train loss: 0.08175318407625763, acc: 0.9364078231686147, val loss: 0.9625743657850201, acc: 0.7339102217414819, test loss: 0.9459671025635094, acc: 0.7314516129032258
epoch: 117, train loss: 0.07937100801706148, acc: 0.9376229603055202, val loss: 1.0381584479526418, acc: 0.7187669010275824, test loss: 1.075867812351514, acc: 0.7196236559139785
epoch: 118, train loss: 0.08021944754723252, acc: 0.9373336419395903, val loss: 0.9715684132037, acc: 0.7376960519199567, test loss: 0.9895911452590779, acc: 0.7333333333333333
epoch: 119, train loss: 0.08900448973256672, acc: 0.9309107742159473, val loss: 1.0296092832842796, acc: 0.7166035694970254, test loss: 1.0265463952095277, acc: 0.7190860215053764
epoch: 120, train loss: 0.08739677020354536, acc: 0.9320680476796667, val loss: 0.9683713156059022, acc: 0.72796106003245, test loss: 0.9771675817428097, acc: 0.728494623655914
epoch: 121, train loss: 0.08119471732709381, acc: 0.9360606411294989, val loss: 0.9839548740985266, acc: 0.7255273120605733, test loss: 1.018578468343263, acc: 0.728225806451613
epoch: 122, train loss: 0.0840493353956097, acc: 0.9345561856266635, val loss: 0.9767212763291684, acc: 0.7244456462952947, test loss: 0.9853116122625207, acc: 0.7295698924731183
epoch: 123, train loss: 0.09267610775218657, acc: 0.9272653628052309, val loss: 0.9927313593415198, acc: 0.732287723093564, test loss: 1.004408153923609, acc: 0.7279569892473118
epoch: 124, train loss: 0.08563241661422687, acc: 0.9313158199282491, val loss: 0.9914688010548436, acc: 0.7268793942671714, test loss: 1.0125864269912883, acc: 0.7298387096774194
epoch: 125, train loss: 0.07963681446482994, acc: 0.9350190950121514, val loss: 0.998967500168804, acc: 0.7336398053001623, test loss: 1.0148392123560752, acc: 0.7311827956989247
epoch: 126, train loss: 0.0720425797485185, acc: 0.9395324615206573, val loss: 1.0392778436837162, acc: 0.7152514872904273, test loss: 1.0666988121565952, acc: 0.7153225806451613
epoch: 127, train loss: 0.08741753913195731, acc: 0.9284804999421363, val loss: 1.0115810408986794, acc: 0.7230935640886966, test loss: 1.035462244608069, acc: 0.7169354838709677
epoch: 128, train loss: 0.09641433681117041, acc: 0.924603633838676, val loss: 0.9797025883500804, acc: 0.7160627366143861, test loss: 0.9912384274185345, acc: 0.7123655913978495
epoch: 129, train loss: 0.08762490484221586, acc: 0.9306214558500173, val loss: 0.969776018946283, acc: 0.7347214710654408, test loss: 1.010817043755644, acc: 0.7263440860215054
epoch: 130, train loss: 0.08183801577172888, acc: 0.9362342321490569, val loss: 1.0064049262237136, acc: 0.7244456462952947, test loss: 1.0325797219430246, acc: 0.7244623655913979
epoch: 131, train loss: 0.07355719368731088, acc: 0.9409790533503066, val loss: 1.0027609758598859, acc: 0.7290427257977286, test loss: 1.0180106875716999, acc: 0.7311827956989247
epoch: 132, train loss: 0.07647225326480048, acc: 0.940284689272075, val loss: 1.0349686683481483, acc: 0.722011898323418, test loss: 1.0243779038870207, acc: 0.7384408602150537
epoch: 133, train loss: 0.07780361102764398, acc: 0.9366392778613586, val loss: 1.1326482294700544, acc: 0.7106544077879935, test loss: 1.1688503378181048, acc: 0.7021505376344086
epoch: 134, train loss: 0.09048868076415156, acc: 0.9321837750260387, val loss: 0.9688002770497646, acc: 0.7241752298539751, test loss: 0.9872939284129809, acc: 0.728763440860215
epoch: 135, train loss: 0.08713361248934438, acc: 0.9320101840064807, val loss: 1.015891759701198, acc: 0.725797728501893, test loss: 1.0000816924597626, acc: 0.7365591397849462
epoch: 136, train loss: 0.07520193601614099, acc: 0.9400532345793311, val loss: 0.9933908740013209, acc: 0.7233639805300163, test loss: 1.0219933294480847, acc: 0.7225806451612903
epoch: 137, train loss: 0.09055026533109503, acc: 0.9312000925818771, val loss: 0.9459069515319048, acc: 0.7233639805300163, test loss: 0.9797318525211786, acc: 0.7260752688172043
epoch: 138, train loss: 0.09520596197848656, acc: 0.9282490452493924, val loss: 0.9282265477982516, acc: 0.7379664683612763, test loss: 0.9475273932180097, acc: 0.7301075268817204
epoch: 139, train loss: 0.08325385847841889, acc: 0.9362342321490569, val loss: 0.928245283578653, acc: 0.7360735532720389, test loss: 0.9515412017863284, acc: 0.725
epoch: 140, train loss: 0.06858885396190463, acc: 0.9439301006827914, val loss: 1.008270540902781, acc: 0.7347214710654408, test loss: 1.0239401535321306, acc: 0.7298387096774194
epoch: 141, train loss: 0.0775535907374324, acc: 0.938028006017822, val loss: 0.9695751210558795, acc: 0.732287723093564, test loss: 0.997473261433263, acc: 0.7327956989247312
epoch: 142, train loss: 0.07511653709402653, acc: 0.940284689272075, val loss: 1.0817038980672784, acc: 0.7230935640886966, test loss: 1.1093162577639344, acc: 0.718010752688172
epoch: 143, train loss: 0.07520281876034378, acc: 0.9405161439648189, val loss: 1.032419034247143, acc: 0.7174148188209843, test loss: 1.0718490785168062, acc: 0.7137096774193549
epoch: 144, train loss: 0.08896004816435538, acc: 0.9323573660455966, val loss: 1.0453495792339402, acc: 0.7060573282855598, test loss: 1.0722703344078475, acc: 0.7129032258064516
epoch: 145, train loss: 0.09664982227232842, acc: 0.9277861358639047, val loss: 1.0169198906312058, acc: 0.7155219037317468, test loss: 1.0361890490337085, acc: 0.7153225806451613
epoch: 146, train loss: 0.09040381654988366, acc: 0.9299849554449716, val loss: 1.0013928484310648, acc: 0.719848566792861, test loss: 1.010539359431113, acc: 0.7233870967741935
epoch: 147, train loss: 0.06971539152160368, acc: 0.9430621455850018, val loss: 1.026751678398714, acc: 0.7230935640886966, test loss: 1.0207440248099706, acc: 0.7276881720430107
epoch: 148, train loss: 0.08063752017678134, acc: 0.9376229603055202, val loss: 0.9583438725778385, acc: 0.733098972417523, test loss: 0.9923790177991313, acc: 0.7368279569892473
epoch: 149, train loss: 0.07757908718813736, acc: 0.9398796435597732, val loss: 1.0156115709349167, acc: 0.7117360735532721, test loss: 1.0215709065878262, acc: 0.7153225806451613
epoch: 150, train loss: 0.08328732889037402, acc: 0.9346719129730355, val loss: 1.028139337325109, acc: 0.7125473228772309, test loss: 1.0800395975830734, acc: 0.7126344086021505
epoch: 151, train loss: 0.07762313453691203, acc: 0.9377386876518922, val loss: 1.0167924486153057, acc: 0.727149810708491, test loss: 1.0359976973584903, acc: 0.7306451612903225
epoch: 152, train loss: 0.08442960151497819, acc: 0.9340354125679898, val loss: 1.009993617247091, acc: 0.7190373174689021, test loss: 1.0353162668084586, acc: 0.7223118279569892
epoch: 153, train loss: 0.07090030961017162, acc: 0.9428306908922578, val loss: 1.0432985721245658, acc: 0.7203893996755003, test loss: 1.0629267918166294, acc: 0.7185483870967742
epoch: 154, train loss: 0.08054794552801191, acc: 0.9359449137831269, val loss: 1.028144303755608, acc: 0.7133585722011898, test loss: 1.0586050997498215, acc: 0.7163978494623656
epoch: 155, train loss: 0.09345131242420783, acc: 0.926860317092929, val loss: 0.9513501812794068, acc: 0.7312060573282856, test loss: 0.9912358607015302, acc: 0.7233870967741935
epoch: 156, train loss: 0.08523822711695953, acc: 0.937854414998264, val loss: 0.9793975517000618, acc: 0.7309356408869659, test loss: 0.9851651063529394, acc: 0.7271505376344086
epoch: 157, train loss: 0.07013948944025108, acc: 0.9428306908922578, val loss: 1.0681261412835623, acc: 0.7163331530557058, test loss: 1.0864809533601165, acc: 0.7190860215053764
epoch: 158, train loss: 0.06904723320555982, acc: 0.9421941904872121, val loss: 0.9897158202640168, acc: 0.733098972417523, test loss: 1.0020614167695405, acc: 0.7266129032258064
epoch: 159, train loss: 0.05864554641604727, acc: 0.9509894688114802, val loss: 1.0350276804924528, acc: 0.7379664683612763, test loss: 1.0477228262091196, acc: 0.7319892473118279
epoch: 160, train loss: 0.06793666181241097, acc: 0.9438722370096053, val loss: 1.0264553051500336, acc: 0.7282314764737696, test loss: 1.0301910887482346, acc: 0.7231182795698925
epoch: 161, train loss: 0.08731855257626499, acc: 0.9333989121629441, val loss: 0.9956340213412527, acc: 0.724986479177934, test loss: 0.9996954564125308, acc: 0.7239247311827957
epoch: 162, train loss: 0.07106401310441891, acc: 0.9432357366045596, val loss: 1.071002888640821, acc: 0.7190373174689021, test loss: 1.0761391234654252, acc: 0.7217741935483871
epoch: 163, train loss: 0.06328586073019565, acc: 0.9457238745515565, val loss: 1.048770608097363, acc: 0.73526230394808, test loss: 1.0708890412443428, acc: 0.7258064516129032
Epoch   163: reducing learning rate of group 0 to 3.7500e-04.
epoch: 164, train loss: 0.05138516371765563, acc: 0.9555028353199861, val loss: 1.0568545371406848, acc: 0.732287723093564, test loss: 1.0747432990740704, acc: 0.7330645161290322
epoch: 165, train loss: 0.03994727825355563, acc: 0.962272885082745, val loss: 1.0905950970750296, acc: 0.7290427257977286, test loss: 1.0972735897187265, acc: 0.7309139784946237
epoch: 166, train loss: 0.03860659931601301, acc: 0.9636037495660225, val loss: 1.1373887339303659, acc: 0.7252568956192537, test loss: 1.154202530973701, acc: 0.7212365591397849
epoch: 167, train loss: 0.03906096741358807, acc: 0.9647031593565559, val loss: 1.112639346594679, acc: 0.7306652244456463, test loss: 1.1182014316640874, acc: 0.728494623655914
epoch: 168, train loss: 0.03119124474857309, acc: 0.968985071172318, val loss: 1.1446203407949858, acc: 0.7309356408869659, test loss: 1.1533649188216015, acc: 0.735752688172043
epoch: 169, train loss: 0.030940120979607237, acc: 0.9693901168846198, val loss: 1.1832847719001667, acc: 0.7266089778258518, test loss: 1.1764406824624667, acc: 0.7228494623655914
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.020737857238267005, acc: 0.9688114801527601, val loss: 1.0885810502610895, acc: 0.7360735532720389, test loss: 1.1229973793029786, acc: 0.7327956989247312
epoch: 171, train loss: 0.021132792801116712, acc: 0.9684642981136442, val loss: 1.0882477543688, acc: 0.7287723093564089, test loss: 1.1308731448265814, acc: 0.7309139784946237
epoch: 172, train loss: 0.020184850148850603, acc: 0.9695058442309917, val loss: 1.066054672341272, acc: 0.7309356408869659, test loss: 1.0831554705096829, acc: 0.7341397849462366
epoch: 173, train loss: 0.018711651800966223, acc: 0.9718782548316167, val loss: 1.1170635614090831, acc: 0.7155219037317468, test loss: 1.123462881580476, acc: 0.7166666666666667
epoch: 174, train loss: 0.020787290534886853, acc: 0.9703159356555954, val loss: 1.0571668779869348, acc: 0.7268793942671714, test loss: 1.0612871739172167, acc: 0.7298387096774194
epoch: 175, train loss: 0.018512486390180573, acc: 0.9715310727925008, val loss: 1.0377615015206303, acc: 0.7266089778258518, test loss: 1.0503489427669075, acc: 0.7290322580645161
epoch: 176, train loss: 0.019063963375562265, acc: 0.9703159356555954, val loss: 1.0392212714292735, acc: 0.722823147647377, test loss: 1.0854170178854337, acc: 0.7239247311827957
epoch: 177, train loss: 0.019116357087437138, acc: 0.9708367087142692, val loss: 1.04717155402902, acc: 0.7298539751216874, test loss: 1.076110739349037, acc: 0.7338709677419355
epoch: 178, train loss: 0.024923209321262962, acc: 0.966901978937623, val loss: 1.0564615986939054, acc: 0.7195781503515414, test loss: 1.0983500475524575, acc: 0.7233870967741935
epoch: 179, train loss: 0.02847988873408902, acc: 0.962330748755931, val loss: 1.013045855894032, acc: 0.7222823147647377, test loss: 1.0318912793231267, acc: 0.7268817204301076
epoch: 180, train loss: 0.02986733720677413, acc: 0.9616363846776993, val loss: 0.9991924344945302, acc: 0.7255273120605733, test loss: 1.0374753388025428, acc: 0.7263440860215054
epoch: 181, train loss: 0.021123466560566, acc: 0.9678856613817846, val loss: 1.011896924884852, acc: 0.7317468902109249, test loss: 1.0403756003226003, acc: 0.7263440860215054
epoch: 182, train loss: 0.02070631702958558, acc: 0.9695058442309917, val loss: 1.0294354479012455, acc: 0.7295835586803677, test loss: 1.0507211756962602, acc: 0.7276881720430107
epoch: 183, train loss: 0.02440131695878794, acc: 0.9669598426108089, val loss: 1.0258936237296006, acc: 0.7247160627366144, test loss: 1.0420636320626864, acc: 0.7236559139784946
epoch: 184, train loss: 0.026225505640736512, acc: 0.964761023029742, val loss: 1.0362738585588156, acc: 0.725797728501893, test loss: 1.0637861985032278, acc: 0.7241935483870968
epoch: 185, train loss: 0.024675134433173235, acc: 0.9660918875130193, val loss: 1.0221181653395675, acc: 0.727149810708491, test loss: 1.0453560900944536, acc: 0.7174731182795699
epoch: 186, train loss: 0.027550531557061966, acc: 0.962504339775489, val loss: 0.9925029642714752, acc: 0.7306652244456463, test loss: 1.0218381348476615, acc: 0.728494623655914
epoch: 187, train loss: 0.02102586954419066, acc: 0.968869343825946, val loss: 0.9973954051942552, acc: 0.730124391563007, test loss: 1.040741724609047, acc: 0.7279569892473118
epoch: 188, train loss: 0.02107817207138346, acc: 0.9708367087142692, val loss: 1.0314613687083036, acc: 0.7241752298539751, test loss: 1.0441304037647863, acc: 0.7274193548387097
epoch: 189, train loss: 0.022136655596361132, acc: 0.9670177062839949, val loss: 1.0287724434845895, acc: 0.7368848025959979, test loss: 1.055695032304333, acc: 0.735752688172043
epoch: 190, train loss: 0.02180962605810789, acc: 0.9660340238398334, val loss: 1.0471362061859015, acc: 0.730124391563007, test loss: 1.0746699594682263, acc: 0.728225806451613
epoch: 191, train loss: 0.03162010870343653, acc: 0.9597268834625622, val loss: 1.001208807727077, acc: 0.7303948080043267, test loss: 1.0212145605394918, acc: 0.7327956989247312
epoch: 192, train loss: 0.032131616094495295, acc: 0.9576437912278671, val loss: 0.9639649716114727, acc: 0.7268793942671714, test loss: 1.0119389667305896, acc: 0.7239247311827957
epoch: 193, train loss: 0.03354872036227342, acc: 0.9587432010184006, val loss: 0.9891579543145171, acc: 0.7325581395348837, test loss: 1.0206058973907142, acc: 0.7303763440860215
epoch: 194, train loss: 0.02332851866741083, acc: 0.9655711144543455, val loss: 0.998780337071148, acc: 0.7268793942671714, test loss: 1.081508135539229, acc: 0.7201612903225807
epoch: 195, train loss: 0.039113426566151556, acc: 0.9559657447054739, val loss: 0.9658831815064566, acc: 0.7217414818820984, test loss: 0.9758131268203899, acc: 0.7206989247311828
epoch: 196, train loss: 0.03438938385701893, acc: 0.9571230181691934, val loss: 0.9501432231337783, acc: 0.7255273120605733, test loss: 0.9737555180826495, acc: 0.7220430107526882
epoch: 197, train loss: 0.02720901909549539, acc: 0.9625622034486749, val loss: 0.9507816151195504, acc: 0.7290427257977286, test loss: 0.9659777995078794, acc: 0.728225806451613
epoch: 198, train loss: 0.02512115226947948, acc: 0.9655711144543455, val loss: 1.006979078908686, acc: 0.7341806381828015, test loss: 1.0004063073024956, acc: 0.7314516129032258
epoch: 199, train loss: 0.02486554226750643, acc: 0.9658604328202754, val loss: 0.9948168119396501, acc: 0.7341806381828015, test loss: 1.020760533630207, acc: 0.7354838709677419
epoch: 200, train loss: 0.026328683929847024, acc: 0.9656289781275316, val loss: 0.9882904616738087, acc: 0.7274202271498107, test loss: 1.0114723241457375, acc: 0.728763440860215
best val acc 0.7379664683612763 at epoch 114.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9899    0.9929    0.9914      5337
           1     0.8933    0.9037    0.8985      2502
           2     0.9855    0.9259    0.9548       810
           3     0.9660    0.9871    0.9764      2100
           4     0.9681    0.9891    0.9785       737
           5     0.9082    0.9941    0.9492       677
           6     0.8678    0.9773    0.9193      1323
           7     0.9292    0.9021    0.9154      1164
           8     0.9780    0.9525    0.9651       421
           9     0.9900    0.9875    0.9888       401
          10     0.9469    0.9899    0.9679       396
          11     0.9684    0.9282    0.9479       627
          12     0.9794    0.9794    0.9794       291
          13     0.9286    0.0996    0.1799       261
          14     0.8927    0.8851    0.8889       235

    accuracy                         0.9500     17282
   macro avg     0.9461    0.8996    0.9001     17282
weighted avg     0.9508    0.9500    0.9449     17282

train confusion matrix:
[[9.92879895e-01 7.49484729e-04 1.87371182e-04 1.12422709e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.68634064e-03
  1.68634064e-03 3.74742365e-04 0.00000000e+00 0.00000000e+00
  3.74742365e-04 1.87371182e-04 7.49484729e-04]
 [5.99520384e-03 9.03677058e-01 0.00000000e+00 1.43884892e-02
  1.19904077e-03 0.00000000e+00 7.23421263e-02 2.39808153e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 9.25925926e-01 0.00000000e+00
  2.46913580e-03 6.91358025e-02 2.46913580e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [9.52380952e-04 8.09523810e-03 0.00000000e+00 9.87142857e-01
  0.00000000e+00 2.38095238e-03 9.52380952e-04 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.76190476e-04 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 6.78426052e-03 0.00000000e+00 0.00000000e+00
  9.89145183e-01 0.00000000e+00 0.00000000e+00 1.35685210e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.35685210e-03
  1.35685210e-03 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 5.90841950e-03 0.00000000e+00
  0.00000000e+00 9.94091581e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [7.55857899e-04 1.51171580e-02 7.55857899e-04 0.00000000e+00
  0.00000000e+00 5.29100529e-03 9.77324263e-01 7.55857899e-04
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [8.59106529e-03 2.31958763e-02 0.00000000e+00 3.43642612e-03
  3.43642612e-03 0.00000000e+00 8.59106529e-03 9.02061856e-01
  0.00000000e+00 0.00000000e+00 1.71821306e-02 1.46048110e-02
  0.00000000e+00 8.59106529e-04 1.80412371e-02]
 [4.27553444e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.52494062e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.75059382e-03 0.00000000e+00 0.00000000e+00]
 [4.98753117e-03 0.00000000e+00 2.49376559e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.49376559e-03 0.00000000e+00
  0.00000000e+00 9.87531172e-01 0.00000000e+00 2.49376559e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [2.52525253e-03 0.00000000e+00 0.00000000e+00 2.52525253e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.05050505e-03
  0.00000000e+00 0.00000000e+00 9.89898990e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [1.59489633e-03 1.59489633e-03 0.00000000e+00 1.59489633e-03
  2.23285486e-02 0.00000000e+00 1.59489633e-03 3.98724083e-02
  0.00000000e+00 3.18979266e-03 0.00000000e+00 9.28229665e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [3.43642612e-03 0.00000000e+00 6.87285223e-03 3.43642612e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 6.87285223e-03 0.00000000e+00
  9.79381443e-01 0.00000000e+00 0.00000000e+00]
 [7.66283525e-03 7.50957854e-01 0.00000000e+00 9.19540230e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.98084291e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.96168582e-02 0.00000000e+00]
 [4.25531915e-03 0.00000000e+00 8.51063830e-03 0.00000000e+00
  4.25531915e-03 0.00000000e+00 0.00000000e+00 9.78723404e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 8.85106383e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8602    0.8723    0.8662      1143
           1     0.5974    0.5896    0.5934       536
           2     0.8313    0.7688    0.7988       173
           3     0.6801    0.7511    0.7138       450
           4     0.7562    0.7658    0.7610       158
           5     0.7500    0.8897    0.8139       145
           6     0.7418    0.8021    0.7708       283
           7     0.5000    0.4940    0.4970       249
           8     0.8158    0.6889    0.7470        90
           9     0.6757    0.5882    0.6289        85
          10     0.8471    0.8571    0.8521        84
          11     0.8065    0.7463    0.7752       134
          12     0.7059    0.5806    0.6372        62
          13     0.0588    0.0179    0.0274        56
          14     0.5714    0.4800    0.5217        50

    accuracy                         0.7380      3698
   macro avg     0.6799    0.6595    0.6670      3698
weighted avg     0.7306    0.7380    0.7331      3698

validation confusion matrix:
[[8.72265967e-01 3.58705162e-02 8.74890639e-03 2.71216098e-02
  0.00000000e+00 8.74890639e-04 8.74890639e-04 2.79965004e-02
  7.87401575e-03 6.99912511e-03 1.74978128e-03 8.74890639e-04
  5.24934383e-03 2.62467192e-03 8.74890639e-04]
 [7.27611940e-02 5.89552239e-01 9.32835821e-03 1.26865672e-01
  3.17164179e-02 1.30597015e-02 8.02238806e-02 5.22388060e-02
  1.86567164e-03 1.86567164e-03 0.00000000e+00 9.32835821e-03
  5.59701493e-03 1.86567164e-03 3.73134328e-03]
 [1.15606936e-02 1.15606936e-02 7.68786127e-01 1.73410405e-02
  1.73410405e-02 9.24855491e-02 4.62427746e-02 0.00000000e+00
  0.00000000e+00 2.31213873e-02 0.00000000e+00 5.78034682e-03
  5.78034682e-03 0.00000000e+00 0.00000000e+00]
 [4.44444444e-02 1.42222222e-01 0.00000000e+00 7.51111111e-01
  2.22222222e-03 0.00000000e+00 6.66666667e-03 2.66666667e-02
  0.00000000e+00 2.22222222e-03 0.00000000e+00 6.66666667e-03
  6.66666667e-03 6.66666667e-03 4.44444444e-03]
 [0.00000000e+00 1.07594937e-01 6.32911392e-03 3.16455696e-02
  7.65822785e-01 0.00000000e+00 3.16455696e-02 3.79746835e-02
  0.00000000e+00 0.00000000e+00 1.26582278e-02 6.32911392e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 2.06896552e-02 2.06896552e-02 0.00000000e+00
  6.89655172e-03 8.89655172e-01 5.51724138e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  6.89655172e-03 0.00000000e+00 0.00000000e+00]
 [2.12014134e-02 6.00706714e-02 1.06007067e-02 0.00000000e+00
  1.06007067e-02 5.65371025e-02 8.02120141e-01 1.41342756e-02
  3.53356890e-03 1.41342756e-02 0.00000000e+00 7.06713781e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [8.43373494e-02 1.36546185e-01 4.01606426e-03 9.63855422e-02
  2.81124498e-02 4.01606426e-03 1.60642570e-02 4.93975904e-01
  4.01606426e-03 8.03212851e-03 2.40963855e-02 3.21285141e-02
  4.01606426e-03 3.21285141e-02 3.21285141e-02]
 [2.55555556e-01 3.33333333e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.11111111e-02
  6.88888889e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.11111111e-02 0.00000000e+00]
 [2.47058824e-01 5.88235294e-02 1.17647059e-02 1.17647059e-02
  0.00000000e+00 1.17647059e-02 3.52941176e-02 2.35294118e-02
  0.00000000e+00 5.88235294e-01 0.00000000e+00 1.17647059e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 3.57142857e-02 0.00000000e+00 2.38095238e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.57142857e-02
  0.00000000e+00 0.00000000e+00 8.57142857e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 4.76190476e-02]
 [3.73134328e-02 4.47761194e-02 7.46268657e-03 7.46268657e-03
  4.47761194e-02 0.00000000e+00 0.00000000e+00 8.20895522e-02
  0.00000000e+00 2.23880597e-02 0.00000000e+00 7.46268657e-01
  0.00000000e+00 0.00000000e+00 7.46268657e-03]
 [2.41935484e-01 3.22580645e-02 3.22580645e-02 3.22580645e-02
  0.00000000e+00 0.00000000e+00 3.22580645e-02 1.61290323e-02
  1.61290323e-02 1.61290323e-02 0.00000000e+00 0.00000000e+00
  5.80645161e-01 0.00000000e+00 0.00000000e+00]
 [1.42857143e-01 2.85714286e-01 0.00000000e+00 2.67857143e-01
  1.78571429e-02 1.78571429e-02 3.57142857e-02 1.25000000e-01
  1.78571429e-02 0.00000000e+00 3.57142857e-02 3.57142857e-02
  0.00000000e+00 1.78571429e-02 0.00000000e+00]
 [4.00000000e-02 0.00000000e+00 0.00000000e+00 1.40000000e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.20000000e-01
  0.00000000e+00 0.00000000e+00 2.00000000e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 4.80000000e-01]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8402    0.8585    0.8492      1145
           1     0.5912    0.6220    0.6062       537
           2     0.7874    0.7829    0.7851       175
           3     0.7209    0.7273    0.7241       451
           4     0.7871    0.7673    0.7771       159
           5     0.7803    0.9247    0.8464       146
           6     0.7448    0.7606    0.7526       284
           7     0.5055    0.5480    0.5259       250
           8     0.7717    0.7802    0.7760        91
           9     0.5698    0.5632    0.5665        87
          10     0.9041    0.7674    0.8302        86
          11     0.8182    0.6618    0.7317       136
          12     0.5962    0.4844    0.5345        64
          13     0.0000    0.0000    0.0000        57
          14     0.5854    0.4615    0.5161        52

    accuracy                         0.7320      3720
   macro avg     0.6668    0.6473    0.6548      3720
weighted avg     0.7249    0.7320    0.7274      3720

test confusion matrix:
[[0.85851528 0.03668122 0.00349345 0.02445415 0.00174672 0.
  0.00174672 0.02969432 0.01310044 0.01222707 0.00174672 0.00262009
  0.00524017 0.00436681 0.00436681]
 [0.07635009 0.62197393 0.01489758 0.09683426 0.01489758 0.00931099
  0.09124767 0.03910615 0.         0.01303538 0.         0.01303538
  0.00744879 0.         0.0018622 ]
 [0.02857143 0.01714286 0.78285714 0.         0.01142857 0.08571429
  0.02857143 0.         0.00571429 0.00571429 0.         0.00571429
  0.02857143 0.         0.        ]
 [0.06208426 0.13968958 0.00443459 0.72727273 0.00665188 0.
  0.         0.04656319 0.         0.00221729 0.00221729 0.
  0.         0.00665188 0.00221729]
 [0.01886792 0.11949686 0.00628931 0.03144654 0.7672956  0.
  0.01257862 0.04402516 0.         0.         0.         0.
  0.         0.         0.        ]
 [0.00684932 0.00684932 0.03424658 0.         0.         0.92465753
  0.02739726 0.         0.         0.         0.         0.
  0.         0.         0.        ]
 [0.02464789 0.07042254 0.02112676 0.00352113 0.01760563 0.05985915
  0.76056338 0.00704225 0.         0.02112676 0.         0.01056338
  0.         0.         0.00352113]
 [0.112      0.12       0.012      0.072      0.024      0.
  0.032      0.548      0.012      0.004      0.008      0.012
  0.012      0.004      0.028     ]
 [0.17582418 0.02197802 0.         0.01098901 0.         0.
  0.         0.         0.78021978 0.         0.         0.
  0.         0.01098901 0.        ]
 [0.22988506 0.05747126 0.05747126 0.01149425 0.         0.
  0.01149425 0.02298851 0.         0.56321839 0.         0.03448276
  0.01149425 0.         0.        ]
 [0.05813953 0.         0.         0.02325581 0.02325581 0.
  0.         0.08139535 0.         0.         0.76744186 0.
  0.02325581 0.01162791 0.01162791]
 [0.04411765 0.07352941 0.01470588 0.00735294 0.02205882 0.
  0.00735294 0.11764706 0.         0.02941176 0.         0.66176471
  0.         0.01470588 0.00735294]
 [0.21875    0.078125   0.         0.078125   0.015625   0.
  0.015625   0.03125    0.03125    0.03125    0.015625   0.
  0.484375   0.         0.        ]
 [0.15789474 0.50877193 0.         0.14035088 0.01754386 0.01754386
  0.01754386 0.10526316 0.         0.01754386 0.01754386 0.
  0.         0.         0.        ]
 [0.07692308 0.03846154 0.01923077 0.09615385 0.         0.
  0.         0.30769231 0.         0.         0.         0.
  0.         0.         0.46153846]]
---------------------------------------
program finished.
