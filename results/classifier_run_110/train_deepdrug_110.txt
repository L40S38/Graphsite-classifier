seed:  10
save trained model at:  ../trained_models/trained_classifier_model_110.pt
save loss at:  ./results/train_classifier_results_110.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4ho4A00', '4cvlA00', '5mhiA02', '1b6sB00', '3ihlA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['2aqxA00', '1w7aB00', '1b2mD00', '2czdA00', '3hskA00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b8ddbd85d60>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.037139854657519, acc: 0.3898425476500533; test loss: 1.7491349089204828, acc: 0.4594658473174191
epoch: 2, train loss: 1.7401237998669763, acc: 0.4656682845980822; test loss: 1.6205264041114433, acc: 0.48853698889151503
epoch: 3, train loss: 1.62973330818832, acc: 0.5034331715401917; test loss: 1.5891935199858365, acc: 0.5147719215315528
epoch: 4, train loss: 1.5765997284161801, acc: 0.5146205753521961; test loss: 1.5712173226256654, acc: 0.5246986528007563
epoch: 5, train loss: 1.5368630619208263, acc: 0.5340357523381082; test loss: 1.524010696018934, acc: 0.5315528243913968
epoch: 6, train loss: 1.4882285711651542, acc: 0.5502545282348763; test loss: 1.437342515653314, acc: 0.558733160009454
epoch: 7, train loss: 1.4492419130550667, acc: 0.5583639161832603; test loss: 1.4186594866038327, acc: 0.5703143464901914
epoch: 8, train loss: 1.448519379129858, acc: 0.5646975257487865; test loss: 1.4394459467468406, acc: 0.5570787047979201
epoch: 9, train loss: 1.3950768203104236, acc: 0.5768912039777436; test loss: 1.3975593603634997, acc: 0.5766958165918222
epoch: 10, train loss: 1.3798512963694314, acc: 0.5808571090327928; test loss: 1.35831370254522, acc: 0.5892224060505791
epoch: 11, train loss: 1.3673691235591183, acc: 0.5915117793299396; test loss: 1.3188416864313546, acc: 0.5918222642401324
epoch: 12, train loss: 1.330817424899603, acc: 0.5945897951935598; test loss: 1.309059033690263, acc: 0.6026943984873553
epoch: 13, train loss: 1.3101557654658196, acc: 0.6008050195335622; test loss: 1.3758805539134096, acc: 0.5674781375561333
epoch: 14, train loss: 1.3024253496068658, acc: 0.604238191073754; test loss: 1.3102310624029079, acc: 0.5963129283857244
epoch: 15, train loss: 1.2888538887673784, acc: 0.6067242808097549; test loss: 1.2884173954499583, acc: 0.6026943984873553
epoch: 16, train loss: 1.2553409749380269, acc: 0.6187995738131881; test loss: 1.2500360790253586, acc: 0.6164027416686363
epoch: 17, train loss: 1.255138459524008, acc: 0.6178524920089973; test loss: 1.2553311433388645, acc: 0.6093122193334909
epoch: 18, train loss: 1.25371178600426, acc: 0.618030069847283; test loss: 1.2804207576417213, acc: 0.6026943984873553
epoch: 19, train loss: 1.229005626126607, acc: 0.6264354208594768; test loss: 1.2368222421800743, acc: 0.6171117939021508
epoch: 20, train loss: 1.2029409887551612, acc: 0.6322362969101456; test loss: 1.2448964142624692, acc: 0.6114393760340345
epoch: 21, train loss: 1.2096248636503142, acc: 0.6305197111400497; test loss: 1.2349252251129403, acc: 0.6166390924131411
epoch: 22, train loss: 1.1966985817052662, acc: 0.6353143127737658; test loss: 1.2322469478914626, acc: 0.6166390924131411
epoch: 23, train loss: 1.179147145431169, acc: 0.6399905291819581; test loss: 1.3516680345104761, acc: 0.5821318837154337
epoch: 24, train loss: 1.1717808562452954, acc: 0.6427133893690068; test loss: 1.2272628724673655, acc: 0.6208934058142284
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.931875153351101, acc: 0.6481591097431041; test loss: 1.0702521634761168, acc: 0.6005672417868116
epoch: 26, train loss: 0.9165642125177231, acc: 0.6560317272404403; test loss: 0.9630890080224986, acc: 0.6225478610257622
epoch: 27, train loss: 0.9058822662121617, acc: 0.6602935953592992; test loss: 0.9665607484174209, acc: 0.6256204207043252
epoch: 28, train loss: 0.8988421336636679, acc: 0.6527761335385344; test loss: 0.9265556197390717, acc: 0.6379106594185772
epoch: 29, train loss: 0.9010277611624775, acc: 0.6553806085000592; test loss: 0.9528438288166404, acc: 0.650437248877334
epoch: 30, train loss: 0.8871527979243249, acc: 0.6619509885166331; test loss: 1.0845863053883764, acc: 0.595367525407705
epoch: 31, train loss: 0.8787905730142161, acc: 0.6662128566354919; test loss: 0.9237349031665201, acc: 0.6369652564405578
epoch: 32, train loss: 0.8765564335793931, acc: 0.6672783236652066; test loss: 0.9078144443933914, acc: 0.6530371070668872
epoch: 33, train loss: 0.8669336511846886, acc: 0.6640227299633006; test loss: 0.9881099968254073, acc: 0.6225478610257622
epoch: 34, train loss: 0.8592709690858108, acc: 0.6717769622351131; test loss: 1.0153243112327415, acc: 0.6126211297565587
epoch: 35, train loss: 0.8479545142462341, acc: 0.6766307564815911; test loss: 0.9160906587247729, acc: 0.6494918458993146
epoch: 36, train loss: 0.8513306915244929, acc: 0.6729016218775897; test loss: 0.9511797895839496, acc: 0.6416922713306547
epoch: 37, train loss: 0.8330749779519802, acc: 0.6740854741328283; test loss: 1.019604956728999, acc: 0.6036398014653747
epoch: 38, train loss: 0.8413565110719965, acc: 0.6744406298093998; test loss: 0.9220430634033593, acc: 0.6603639801465374
epoch: 39, train loss: 0.8123096920716575, acc: 0.6820172842429265; test loss: 0.8889944507731413, acc: 0.6561096667454502
epoch: 40, train loss: 0.8151457502148456, acc: 0.6829051734343554; test loss: 0.8972466525612247, acc: 0.6502008981328291
epoch: 41, train loss: 0.809401904363893, acc: 0.6837338700130223; test loss: 1.126826873036373, acc: 0.578586622547861
epoch: 42, train loss: 0.8204708025979618, acc: 0.6833195217236889; test loss: 0.9069400770786495, acc: 0.644292129520208
epoch: 43, train loss: 0.8063708587738273, acc: 0.6852136853320706; test loss: 0.9032457412430311, acc: 0.6563460174899551
epoch: 44, train loss: 0.7868016174139604, acc: 0.6900674795785486; test loss: 0.8789096096113652, acc: 0.6587095249350036
epoch: 45, train loss: 0.7839018421524783, acc: 0.6956315851781698; test loss: 0.8483093602526022, acc: 0.6714724651382652
epoch: 46, train loss: 0.7795461005426179, acc: 0.6932046880549307; test loss: 0.8890540353008208, acc: 0.6542188607894115
epoch: 47, train loss: 0.7607311966420067, acc: 0.7005445720374097; test loss: 0.8696557041939988, acc: 0.661073032380052
epoch: 48, train loss: 0.7633814902450681, acc: 0.6982952527524565; test loss: 0.8282037862675378, acc: 0.6733632710943039
epoch: 49, train loss: 0.7582588597587033, acc: 0.7026755060968392; test loss: 0.9222383003825415, acc: 0.6565823682344599
epoch: 50, train loss: 0.7527319720839415, acc: 0.7058127145732213; test loss: 0.925307098051471, acc: 0.6542188607894115
epoch: 51, train loss: 0.7463422337128197, acc: 0.7039185509648396; test loss: 0.9190817732417649, acc: 0.6452375324982274
epoch: 52, train loss: 0.7351138727176934, acc: 0.7105481235941754; test loss: 0.854296509747921, acc: 0.6724178681162846
epoch: 53, train loss: 0.7167983244364962, acc: 0.714928376938558; test loss: 1.1106382988497203, acc: 0.5729142046797447
epoch: 54, train loss: 0.7471043550927243, acc: 0.7045696697052208; test loss: 0.8785756048483365, acc: 0.6674545024816828
epoch: 55, train loss: 0.7239171511974224, acc: 0.7140404877471291; test loss: 0.8408677567829339, acc: 0.677853935239896
epoch: 56, train loss: 0.7112791958794561, acc: 0.7180655854149403; test loss: 0.9130775088876225, acc: 0.6516190025998582
epoch: 57, train loss: 0.7055014343082207, acc: 0.7187758967680833; test loss: 0.8748701552470168, acc: 0.659418577168518
epoch: 58, train loss: 0.7198892073249117, acc: 0.7137445246833195; test loss: 1.0773073326918678, acc: 0.5956038761522099
epoch: 59, train loss: 0.7045447567207257, acc: 0.7227418018231325; test loss: 0.807434901760645, acc: 0.6856535098085559
epoch: 60, train loss: 0.6989331706975556, acc: 0.7216171421806559; test loss: 0.7944649484134458, acc: 0.6854171590640511
epoch: 61, train loss: 0.7000697904667855, acc: 0.7220314904699894; test loss: 0.8024289869467606, acc: 0.6816355471519735
epoch: 62, train loss: 0.6861061049210498, acc: 0.7250503137208476; test loss: 0.8122700841904137, acc: 0.6865989127865753
epoch: 63, train loss: 0.6841829975314347, acc: 0.7236888836273233; test loss: 0.8063612415333556, acc: 0.6906168754431576
epoch: 64, train loss: 0.6741010004265768, acc: 0.7315023085118977; test loss: 0.782421874312525, acc: 0.7036161663909242
epoch: 65, train loss: 0.671680287121174, acc: 0.7318574641884693; test loss: 0.7956506814327794, acc: 0.7012526589458756
epoch: 66, train loss: 0.679218451991171, acc: 0.7298449153545638; test loss: 1.0586299956535172, acc: 0.6135665327345781
epoch: 67, train loss: 0.6664282168751569, acc: 0.7360601396945661; test loss: 0.8361715290513171, acc: 0.664145592058615
epoch: 68, train loss: 0.6587707459665522, acc: 0.7355274061797088; test loss: 0.9290650751933903, acc: 0.6421649728196643
epoch: 69, train loss: 0.6545847454719661, acc: 0.740262815200663; test loss: 0.9703483446865492, acc: 0.6376743086740724
epoch: 70, train loss: 0.6561514375350088, acc: 0.7361193323073281; test loss: 0.8558811469721586, acc: 0.6847081068305365
epoch: 71, train loss: 0.6645092004655423, acc: 0.7371847993370427; test loss: 0.834968114738852, acc: 0.6750177263058379
epoch: 72, train loss: 0.6569500956236717, acc: 0.7371847993370427; test loss: 0.7989628899325661, acc: 0.6986528007563224
epoch: 73, train loss: 0.6489063439756452, acc: 0.7405587782644726; test loss: 0.7653284818955387, acc: 0.7081068305365162
epoch: 74, train loss: 0.6486170488277723, acc: 0.7398484669113294; test loss: 1.074592576638882, acc: 0.5989127865752777
epoch: 75, train loss: 0.643470981240456, acc: 0.7416242452941872; test loss: 0.7848226554115961, acc: 0.6948711888442448
epoch: 76, train loss: 0.6381470497873558, acc: 0.7412098970048538; test loss: 0.9217446631952483, acc: 0.6520917040888679
epoch: 77, train loss: 0.6307786234359114, acc: 0.7445838759322837; test loss: 0.7816757031068596, acc: 0.690144173954148
epoch: 78, train loss: 0.6213899754597277, acc: 0.7469515804427608; test loss: 0.7670532829849975, acc: 0.6984164500118175
epoch: 79, train loss: 0.6238115550470561, acc: 0.749970403693619; test loss: 0.8017655535810413, acc: 0.7014890096903805
epoch: 80, train loss: 0.6294592075183065, acc: 0.746714809991713; test loss: 0.8703652626795849, acc: 0.6821082486409832
epoch: 81, train loss: 0.6178321419093007, acc: 0.7479578548597136; test loss: 0.7940302034890615, acc: 0.6986528007563224
epoch: 82, train loss: 0.6049058240688071, acc: 0.7579022138037173; test loss: 0.7960837098673652, acc: 0.6785629874734106
epoch: 83, train loss: 0.6048337635986054, acc: 0.7554161240677163; test loss: 0.8004528904772628, acc: 0.6965256440557788
epoch: 84, train loss: 0.6140522312042599, acc: 0.7497928258553332; test loss: 0.8962094935365559, acc: 0.6577641219569842
Epoch    84: reducing learning rate of group 0 to 1.5000e-03.
epoch: 85, train loss: 0.5375992299622159, acc: 0.7808689475553451; test loss: 0.681634844687779, acc: 0.7352871661545733
epoch: 86, train loss: 0.5082423057805381, acc: 0.7887415650526814; test loss: 0.7101955178837155, acc: 0.7274875915859135
epoch: 87, train loss: 0.5013102762188533, acc: 0.7931218183970641; test loss: 0.7038476875109527, acc: 0.7322146064760104
epoch: 88, train loss: 0.5006926359827736, acc: 0.7934769740736356; test loss: 0.7174515894923867, acc: 0.7298510990309619
epoch: 89, train loss: 0.4925667926363935, acc: 0.7954303302947792; test loss: 0.7676829825166663, acc: 0.718978964783739
epoch: 90, train loss: 0.4942519874195665, acc: 0.7967325677755416; test loss: 0.7268975998205204, acc: 0.723705979673836
epoch: 91, train loss: 0.4812751717435914, acc: 0.7982715757073517; test loss: 0.7324015184712168, acc: 0.7338690616875443
epoch: 92, train loss: 0.48375420635221017, acc: 0.7974428791286847; test loss: 0.7252372533991612, acc: 0.7192153155282439
epoch: 93, train loss: 0.48306173103807154, acc: 0.7991594648987806; test loss: 0.8263871748659859, acc: 0.6903805246986529
epoch: 94, train loss: 0.47492947881992603, acc: 0.8049011483366876; test loss: 0.7613509258630005, acc: 0.7208697707397779
epoch: 95, train loss: 0.46806110392466965, acc: 0.8015271694092577; test loss: 0.736456174316262, acc: 0.7305601512644765
epoch: 96, train loss: 0.46766264708603544, acc: 0.8040724517580206; test loss: 0.7662132177981776, acc: 0.7012526589458756
epoch: 97, train loss: 0.46885501574211125, acc: 0.8069136971705931; test loss: 0.7363531290287386, acc: 0.7258331363743796
epoch: 98, train loss: 0.48231556799209396, acc: 0.801349591570972; test loss: 0.8261294369437232, acc: 0.7017253604348853
epoch: 99, train loss: 0.45709421085128704, acc: 0.8115307209660234; test loss: 0.7353202050190337, acc: 0.7322146064760104
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3555929501606781, acc: 0.8107612170001184; test loss: 0.6663134225968406, acc: 0.7180335618057196
epoch: 101, train loss: 0.34795108966055716, acc: 0.8131881141233575; test loss: 0.6051983041589747, acc: 0.7419049870007091
epoch: 102, train loss: 0.3544717118226947, acc: 0.8072096602344028; test loss: 0.6082336122728752, acc: 0.7397778303001654
epoch: 103, train loss: 0.35758372945476175, acc: 0.8065585414940215; test loss: 0.6597432253406392, acc: 0.7258331363743796
epoch: 104, train loss: 0.3577826988863581, acc: 0.8075648159109743; test loss: 0.6017062414399672, acc: 0.7362325691325927
epoch: 105, train loss: 0.34842221515269284, acc: 0.8103468687107849; test loss: 0.6145000908003437, acc: 0.7213424722287876
epoch: 106, train loss: 0.34461685574462575, acc: 0.8080383568130697; test loss: 0.6556896172750929, acc: 0.7246513826518554
epoch: 107, train loss: 0.3391033102700565, acc: 0.8145495442168817; test loss: 0.6369398708350268, acc: 0.7215788229732923
epoch: 108, train loss: 0.37265621273704175, acc: 0.798863501834971; test loss: 0.6161848915240298, acc: 0.7315055542424959
epoch: 109, train loss: 0.34306126890823774, acc: 0.8090446312300225; test loss: 0.5947107175258113, acc: 0.7381233750886316
epoch: 110, train loss: 0.3335117949449594, acc: 0.8144903516041198; test loss: 0.7346361682533908, acc: 0.7095249350035453
epoch: 111, train loss: 0.344894133863667, acc: 0.8112347579022138; test loss: 0.6440733478188205, acc: 0.7260694871188844
epoch: 112, train loss: 0.3287856655528987, acc: 0.8164437078252634; test loss: 0.7112579921604302, acc: 0.7045615693689435
epoch: 113, train loss: 0.3303146402560932, acc: 0.8138392328637386; test loss: 0.6198252632217299, acc: 0.7352871661545733
epoch: 114, train loss: 0.32531849606172336, acc: 0.8182786788208831; test loss: 0.6027577364312309, acc: 0.7341054124320492
epoch: 115, train loss: 0.3221819455367906, acc: 0.8162661299869777; test loss: 0.6435685034368483, acc: 0.7227605766958166
epoch: 116, train loss: 0.32597855937979103, acc: 0.8131289215105955; test loss: 0.6591372416673668, acc: 0.7180335618057196
epoch: 117, train loss: 0.33695283506034296, acc: 0.8108204096128803; test loss: 0.7060664527609511, acc: 0.711415740959584
epoch: 118, train loss: 0.3302050293359839, acc: 0.8130105362850716; test loss: 0.6323355283051961, acc: 0.7305601512644765
epoch: 119, train loss: 0.33927184455180515, acc: 0.806972889783355; test loss: 0.6875889259600746, acc: 0.7111793902150791
epoch: 120, train loss: 0.349204756939596, acc: 0.8046051852728779; test loss: 0.6728260621368617, acc: 0.7144883006381471
epoch: 121, train loss: 0.3204076379040113, acc: 0.8190481827867883; test loss: 0.7254187224238682, acc: 0.6972346962892934
epoch: 122, train loss: 0.3131855080695608, acc: 0.8219486208121226; test loss: 0.6640030446838076, acc: 0.7201607185062633
epoch: 123, train loss: 0.3201585165654412, acc: 0.8192849532378359; test loss: 0.6421989726276438, acc: 0.7274875915859135
epoch: 124, train loss: 0.3258064891532199, acc: 0.8140168107020244; test loss: 0.6626421124961634, acc: 0.7222878752068069
epoch: 125, train loss: 0.29902688855847986, acc: 0.8263288741565052; test loss: 0.6514362746506711, acc: 0.7345781139210589
epoch: 126, train loss: 0.30847869231120306, acc: 0.8214750799100272; test loss: 0.701759670367339, acc: 0.7187426140392342
epoch: 127, train loss: 0.31474779705557016, acc: 0.8193441458505979; test loss: 0.6420897050434691, acc: 0.7234696289293311
epoch: 128, train loss: 0.3128492290823787, acc: 0.8196401089144075; test loss: 0.7204818659092456, acc: 0.7005436067123612
epoch: 129, train loss: 0.3077060471163059, acc: 0.8234284361311708; test loss: 0.7217713152931193, acc: 0.7010163082013708
epoch: 130, train loss: 0.3023422423785885, acc: 0.8222445838759322; test loss: 0.6768346921514603, acc: 0.7163791065941858
epoch: 131, train loss: 0.3385999673525229, acc: 0.8070912750088789; test loss: 0.6700599180330652, acc: 0.7140155991491374
epoch: 132, train loss: 0.31446052183621703, acc: 0.8172724044039303; test loss: 0.6735973582710676, acc: 0.7253604348853699
epoch: 133, train loss: 0.3020663697387645, acc: 0.8224221617142181; test loss: 0.7120208546952473, acc: 0.7073977783030017
epoch: 134, train loss: 0.2988261894755241, acc: 0.8250858292885048; test loss: 0.6658797550359197, acc: 0.7265421886078941
epoch: 135, train loss: 0.2981035440429874, acc: 0.8222445838759322; test loss: 0.6983572009099457, acc: 0.7182699125502245
Epoch   135: reducing learning rate of group 0 to 7.5000e-04.
epoch: 136, train loss: 0.2558297631944356, acc: 0.8457440511424175; test loss: 0.6411431800724625, acc: 0.7378870243441267
epoch: 137, train loss: 0.23122118019882681, acc: 0.8539718243163253; test loss: 0.6296041866646003, acc: 0.7440321437012527
epoch: 138, train loss: 0.21902422661455026, acc: 0.8600094708180419; test loss: 0.6728916157788065, acc: 0.737414322855117
epoch: 139, train loss: 0.21860135713490958, acc: 0.8630874866816621; test loss: 0.6674327076932438, acc: 0.7376506735996219
epoch: 140, train loss: 0.21534062419591662, acc: 0.8636794128092814; test loss: 0.6919858406351351, acc: 0.7329236587095249
epoch: 141, train loss: 0.21287456858866566, acc: 0.861548478749852; test loss: 0.6963664021961967, acc: 0.7315055542424959
epoch: 142, train loss: 0.21144363605878727, acc: 0.8652776133538534; test loss: 0.6944079612113432, acc: 0.7296147482864571
epoch: 143, train loss: 0.21155210201156285, acc: 0.8659287320942346; test loss: 0.665645066797522, acc: 0.7359962183880879
epoch: 144, train loss: 0.20295484824633336, acc: 0.8687107848940452; test loss: 0.7017418843751482, acc: 0.7315055542424959
epoch: 145, train loss: 0.20346102968424584, acc: 0.8702497928258554; test loss: 0.69332099310249, acc: 0.7369416213661073
epoch: 146, train loss: 0.20576871298425534, acc: 0.8652776133538534; test loss: 0.7582834022204608, acc: 0.7298510990309619
epoch: 147, train loss: 0.2132816937945786, acc: 0.8629691014561383; test loss: 0.7163636295881657, acc: 0.737414322855117
epoch: 148, train loss: 0.22044838616004622, acc: 0.858352077660708; test loss: 0.696223241869819, acc: 0.7449775466792721
epoch: 149, train loss: 0.19634816086516657, acc: 0.8710192967917604; test loss: 0.7413999265149197, acc: 0.7381233750886316
epoch: 150, train loss: 0.20213719201508507, acc: 0.8670533917367113; test loss: 0.7527928683978112, acc: 0.7362325691325927
epoch: 151, train loss: 0.20893391745251652, acc: 0.8629691014561383; test loss: 0.6959386552600144, acc: 0.7409595840226897
epoch: 152, train loss: 0.20068886975247408, acc: 0.8703089854386172; test loss: 0.6920914702369259, acc: 0.7378870243441267
epoch: 153, train loss: 0.19794393517716447, acc: 0.8687107848940452; test loss: 0.6949365520499835, acc: 0.7426140392342235
epoch: 154, train loss: 0.19663820670732624, acc: 0.872143956434237; test loss: 0.7449983452206609, acc: 0.7208697707397779
epoch: 155, train loss: 0.20352480675294382, acc: 0.8677045104770924; test loss: 0.7314100230565677, acc: 0.7258331363743796
epoch: 156, train loss: 0.1956532385129258, acc: 0.8682964366047118; test loss: 0.7344948908195685, acc: 0.723705979673836
epoch: 157, train loss: 0.20860527053949918, acc: 0.8655143838049012; test loss: 0.746019959590644, acc: 0.7213424722287876
epoch: 158, train loss: 0.19047436710503082, acc: 0.8724399194980467; test loss: 0.7443575675367492, acc: 0.737414322855117
epoch: 159, train loss: 0.18277231917778416, acc: 0.8760506688765242; test loss: 0.7575434650041566, acc: 0.7333963601985346
epoch: 160, train loss: 0.19347966562485122, acc: 0.868888362732331; test loss: 0.7358355741358852, acc: 0.7286693453084377
epoch: 161, train loss: 0.1960872671629418, acc: 0.8677045104770924; test loss: 0.7475657001563883, acc: 0.7393051288111557
epoch: 162, train loss: 0.18466299871403719, acc: 0.8757547058127145; test loss: 0.7105005231204536, acc: 0.7367052706216024
epoch: 163, train loss: 0.18704050272699643, acc: 0.8752219722978573; test loss: 0.7526397924562966, acc: 0.7362325691325927
epoch: 164, train loss: 0.17761238065572607, acc: 0.8774712915828105; test loss: 0.7396560681117, acc: 0.7371779721106122
epoch: 165, train loss: 0.1792530820163535, acc: 0.8793654551911921; test loss: 0.7419519960330863, acc: 0.7272512408414087
epoch: 166, train loss: 0.18265885195817358, acc: 0.8774712915828105; test loss: 0.7385867514866121, acc: 0.7419049870007091
epoch: 167, train loss: 0.1806473027739678, acc: 0.8784183733870013; test loss: 0.7402819683861152, acc: 0.7381233750886316
epoch: 168, train loss: 0.19524906219336158, acc: 0.872143956434237; test loss: 0.7478511021847816, acc: 0.7310328527534862
epoch: 169, train loss: 0.17327294354115885, acc: 0.8780040250976678; test loss: 0.7518057940671754, acc: 0.735759867643583
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.13521272143739574, acc: 0.8772345211317627; test loss: 0.676006915720888, acc: 0.7298510990309619
epoch: 171, train loss: 0.12802308452518166, acc: 0.8820291227654788; test loss: 0.7075467157578024, acc: 0.7312692034979911
epoch: 172, train loss: 0.1254590155273002, acc: 0.8857582573694803; test loss: 0.6548393324908566, acc: 0.7359962183880879
epoch: 173, train loss: 0.13360004906218562, acc: 0.8767609802296673; test loss: 0.6594144526848807, acc: 0.7322146064760104
epoch: 174, train loss: 0.1316572864164724, acc: 0.8780040250976678; test loss: 0.6530593814536545, acc: 0.7355235168990782
epoch: 175, train loss: 0.12681943096676507, acc: 0.881969930152717; test loss: 0.6577880837849144, acc: 0.7310328527534862
epoch: 176, train loss: 0.12128240089346223, acc: 0.8830945897951935; test loss: 0.6527995684839427, acc: 0.7341054124320492
epoch: 177, train loss: 0.12656399300885338, acc: 0.8798389960932875; test loss: 0.6661113260542352, acc: 0.7352871661545733
epoch: 178, train loss: 0.12802775831663976, acc: 0.8781224103231917; test loss: 0.6842191530665842, acc: 0.7296147482864571
epoch: 179, train loss: 0.134794621034968, acc: 0.8726766899490943; test loss: 0.6803976078738955, acc: 0.7272512408414087
epoch: 180, train loss: 0.13250501469753193, acc: 0.8708417189534746; test loss: 0.681015622922924, acc: 0.7244150319073505
epoch: 181, train loss: 0.13012307503208742, acc: 0.8777080620338582; test loss: 0.6714502784045586, acc: 0.7331600094540298
epoch: 182, train loss: 0.12544465766840607, acc: 0.8776488694210962; test loss: 0.7211472020028641, acc: 0.7246513826518554
epoch: 183, train loss: 0.12873656651007576, acc: 0.8770569432934769; test loss: 0.7015201472357018, acc: 0.7260694871188844
epoch: 184, train loss: 0.13585175354374618, acc: 0.8683556292174737; test loss: 0.687021756560998, acc: 0.7296147482864571
epoch: 185, train loss: 0.13423443302588023, acc: 0.8736237717532852; test loss: 0.6586450256655555, acc: 0.7315055542424959
epoch: 186, train loss: 0.1261171811506482, acc: 0.8790694921273825; test loss: 0.6835421675121043, acc: 0.723705979673836
Epoch   186: reducing learning rate of group 0 to 3.7500e-04.
epoch: 187, train loss: 0.10396839163529742, acc: 0.8926246004498638; test loss: 0.682304051575495, acc: 0.7456865989127865
epoch: 188, train loss: 0.08740891734866034, acc: 0.9065940570616787; test loss: 0.706379773852899, acc: 0.7381233750886316
epoch: 189, train loss: 0.08399603547881422, acc: 0.9050550491298686; test loss: 0.70422588310972, acc: 0.737414322855117
epoch: 190, train loss: 0.08306723506641597, acc: 0.9060021309340595; test loss: 0.7159479966374386, acc: 0.7416686362562042
epoch: 191, train loss: 0.08305298113813764, acc: 0.9093169172487273; test loss: 0.7203470394702083, acc: 0.7393051288111557
epoch: 192, train loss: 0.08262109059306366, acc: 0.9072451758020599; test loss: 0.7276652106098237, acc: 0.7359962183880879
epoch: 193, train loss: 0.08409548223519644, acc: 0.9056469752574878; test loss: 0.7612020024394516, acc: 0.7369416213661073
epoch: 194, train loss: 0.08553433442662295, acc: 0.9044631230022493; test loss: 0.7070383720999771, acc: 0.7402505317891751
epoch: 195, train loss: 0.08160897257999229, acc: 0.9097904581508228; test loss: 0.7364044592922051, acc: 0.7393051288111557
epoch: 196, train loss: 0.08141194005149828, acc: 0.9123949331123475; test loss: 0.7277477699880255, acc: 0.7414322855116994
epoch: 197, train loss: 0.08292325797769831, acc: 0.9073043684148219; test loss: 0.7363964571115452, acc: 0.7430867407232333
epoch: 198, train loss: 0.08196799348053233, acc: 0.9077779093169173; test loss: 0.7449554884526725, acc: 0.734341763176554
epoch: 199, train loss: 0.08730884038535493, acc: 0.9043447377767254; test loss: 0.7281032915065768, acc: 0.7355235168990782
epoch: 200, train loss: 0.08997509517750261, acc: 0.8994909435302474; test loss: 0.7175675490820445, acc: 0.7414322855116994
best test acc 0.7456865989127865 at epoch 187.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9640    0.9738    0.9688      6100
           1     0.9728    0.9255    0.9485       926
           2     0.8882    0.9633    0.9242      2400
           3     0.9650    0.9478    0.9563       843
           4     0.9297    0.9742    0.9514       774
           5     0.9540    0.9749    0.9643      1512
           6     0.8716    0.8474    0.8593      1330
           7     0.9247    0.8940    0.9091       481
           8     0.8510    0.9476    0.8967       458
           9     0.9631    0.9823    0.9726       452
          10     0.9400    0.9177    0.9287       717
          11     0.9777    0.9219    0.9490       333
          12     0.9038    0.1572    0.2678       299
          13     0.7918    0.7918    0.7918       269

    accuracy                         0.9350     16894
   macro avg     0.9213    0.8728    0.8778     16894
weighted avg     0.9353    0.9350    0.9301     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8316    0.8584    0.8448      1525
           1     0.7729    0.7629    0.7679       232
           2     0.6872    0.7604    0.7220       601
           3     0.7703    0.7630    0.7667       211
           4     0.8256    0.7320    0.7760       194
           5     0.7959    0.8148    0.8052       378
           6     0.4894    0.5556    0.5204       333
           7     0.7182    0.6529    0.6840       121
           8     0.5526    0.5478    0.5502       115
           9     0.7478    0.7544    0.7511       114
          10     0.7754    0.5944    0.6730       180
          11     0.6557    0.4762    0.5517        84
          12     0.0625    0.0133    0.0220        75
          13     0.6349    0.5882    0.6107        68

    accuracy                         0.7457      4231
   macro avg     0.6657    0.6339    0.6461      4231
weighted avg     0.7387    0.7457    0.7404      4231

---------------------------------------
program finished.
