seed:  666
save trained model at:  ../trained_models/trained_classifier_model_11.pt
save loss at:  ./results/train_classifier_results_11.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  1
learning rate decay at epoch:  90
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [20, 60, 130]
loss function:
FocalLoss(gamma=0, alpha=1, reduction=mean)
begin training...
epoch: 1, train loss: 2.278282260200551, acc: 0.3381101912646303, val loss: 1.9548634478083158, acc: 0.4099279423538831, test loss: 1.9590452657903872, acc: 0.41251325556733826
best val acc 0.4099279423538831 at epoch 1.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.5264    0.8173    0.6403      5337
           1     0.2998    0.1727    0.2191      2502
           2     0.3985    0.1284    0.1942       810
           3     0.2497    0.6500    0.3608      1840
           4     1.0000    0.0014    0.0027       737
           5     0.5352    0.2024    0.2937       677
           6     0.3685    0.6909    0.4807      1323
           7     0.0000    0.0000    0.0000       907
           8     0.0000    0.0000    0.0000       421
           9     0.0000    0.0000    0.0000       401
          10     0.0000    0.0000    0.0000       396
          11     0.0000    0.0000    0.0000       332
          12     0.0000    0.0000    0.0000       295
          13     0.0000    0.0000    0.0000       291
          14     0.0000    0.0000    0.0000       261
          15     0.0000    0.0000    0.0000       494
          16     0.0000    0.0000    0.0000       256
          17     0.0000    0.0000    0.0000       235

    accuracy                         0.4080     17515
   macro avg     0.1877    0.1479    0.1218     17515
weighted avg     0.3385    0.4080    0.3211     17515

train confusion matrix:
[[8.17313097e-01 3.11036163e-02 7.68221847e-03 1.14109050e-01
  0.00000000e+00 1.31159828e-03 2.84804197e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.29816147e-01 1.72661871e-01 3.19744205e-03 4.50439648e-01
  0.00000000e+00 3.99680256e-04 1.43485212e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.32098765e-01 3.45679012e-02 1.28395062e-01 4.56790123e-02
  0.00000000e+00 6.91358025e-02 2.90123457e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.71195652e-01 7.06521739e-02 0.00000000e+00 6.50000000e-01
  0.00000000e+00 0.00000000e+00 8.15217391e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.03934871e-01 2.21166893e-01 6.78426052e-03 2.48303935e-01
  1.35685210e-03 1.62822252e-02 2.02170963e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.86115214e-01 1.92023634e-02 8.71491876e-02 3.24963072e-02
  0.00000000e+00 2.02363368e-01 4.72673560e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.14134543e-01 1.03552532e-01 1.13378685e-02 5.51776266e-02
  0.00000000e+00 2.49433107e-02 6.90854119e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.82249173e-01 1.17971334e-01 0.00000000e+00 5.57883131e-01
  0.00000000e+00 1.10253583e-03 4.07938258e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [7.55344418e-01 5.22565321e-02 2.85035629e-02 8.31353919e-02
  0.00000000e+00 1.18764846e-02 6.88836105e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [7.05735661e-01 3.49127182e-02 3.49127182e-02 7.48129676e-02
  0.00000000e+00 2.49376559e-03 1.47132170e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.36363636e-01 2.02020202e-02 0.00000000e+00 3.43434343e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [7.98192771e-01 3.31325301e-02 3.01204819e-03 9.63855422e-02
  0.00000000e+00 0.00000000e+00 6.92771084e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.84745763e-01 2.06779661e-01 0.00000000e+00 3.89830508e-01
  0.00000000e+00 0.00000000e+00 1.18644068e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.76975945e-01 1.03092784e-01 3.43642612e-03 1.71821306e-01
  0.00000000e+00 0.00000000e+00 4.46735395e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.91570881e-01 1.64750958e-01 0.00000000e+00 5.97701149e-01
  0.00000000e+00 0.00000000e+00 4.59770115e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.59109312e-01 9.31174089e-02 2.02429150e-03 4.08906883e-01
  0.00000000e+00 4.04858300e-03 2.32793522e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.77343750e-01 3.12500000e-02 0.00000000e+00 6.52343750e-01
  0.00000000e+00 0.00000000e+00 3.90625000e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.08510638e-01 9.36170213e-02 0.00000000e+00 4.80851064e-01
  0.00000000e+00 4.25531915e-03 1.27659574e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.5313    0.8101    0.6417      1143
           1     0.2646    0.1437    0.1862       536
           2     0.3913    0.1040    0.1644       173
           3     0.2655    0.7183    0.3877       394
           4     0.0000    0.0000    0.0000       158
           5     0.6034    0.2414    0.3448       145
           6     0.3628    0.6961    0.4770       283
           7     0.0000    0.0000    0.0000       194
           8     0.0000    0.0000    0.0000        90
           9     0.0000    0.0000    0.0000        85
          10     0.0000    0.0000    0.0000        84
          11     0.0000    0.0000    0.0000        71
          12     0.0000    0.0000    0.0000        63
          13     0.0000    0.0000    0.0000        62
          14     0.0000    0.0000    0.0000        56
          15     0.0000    0.0000    0.0000       105
          16     0.0000    0.0000    0.0000        55
          17     0.0000    0.0000    0.0000        50

    accuracy                         0.4099      3747
   macro avg     0.1344    0.1508    0.1223      3747
weighted avg     0.2966    0.4099    0.3201      3747

validation confusion matrix:
[[0.81014873 0.03062117 0.00524934 0.12423447 0.         0.00262467
  0.02712161 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.2369403  0.14365672 0.00373134 0.47201493 0.         0.00186567
  0.14179104 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.41618497 0.02312139 0.10404624 0.04624277 0.         0.04046243
  0.3699422  0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.20050761 0.06598985 0.         0.71827411 0.         0.00253807
  0.01269036 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.30379747 0.25949367 0.         0.23417722 0.         0.01265823
  0.18987342 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.17931034 0.02758621 0.06896552 0.02068966 0.         0.24137931
  0.46206897 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.12367491 0.08833922 0.01060071 0.06360424 0.         0.01766784
  0.69611307 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.30412371 0.1185567  0.         0.52061856 0.         0.
  0.05670103 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.78888889 0.02222222 0.03333333 0.07777778 0.         0.01111111
  0.06666667 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.67058824 0.04705882 0.03529412 0.07058824 0.         0.01176471
  0.16470588 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.72619048 0.         0.         0.27380952 0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.8028169  0.02816901 0.         0.12676056 0.         0.
  0.04225352 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.22222222 0.12698413 0.         0.53968254 0.         0.
  0.11111111 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.64516129 0.11290323 0.01612903 0.20967742 0.         0.
  0.01612903 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.30357143 0.125      0.         0.53571429 0.         0.
  0.03571429 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.27619048 0.13333333 0.         0.32380952 0.         0.01904762
  0.24761905 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.2        0.12727273 0.         0.63636364 0.         0.
  0.03636364 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.28       0.1        0.         0.6        0.         0.
  0.02       0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.5321    0.8183    0.6449      1145
           1     0.2990    0.1732    0.2193       537
           2     0.3684    0.1200    0.1810       175
           3     0.2568    0.6911    0.3745       395
           4     0.0000    0.0000    0.0000       159
           5     0.5789    0.2260    0.3251       146
           6     0.3805    0.7007    0.4932       284
           7     0.0000    0.0000    0.0000       195
           8     0.0000    0.0000    0.0000        91
           9     0.0000    0.0000    0.0000        87
          10     0.0000    0.0000    0.0000        86
          11     0.0000    0.0000    0.0000        72
          12     0.0000    0.0000    0.0000        64
          13     0.0000    0.0000    0.0000        64
          14     0.0000    0.0000    0.0000        57
          15     0.0000    0.0000    0.0000       107
          16     0.0000    0.0000    0.0000        56
          17     0.0000    0.0000    0.0000        52

    accuracy                         0.4125      3772
   macro avg     0.1342    0.1516    0.1243      3772
weighted avg     0.2991    0.4125    0.3243      3772

test confusion matrix:
[[0.81834061 0.02620087 0.00786026 0.12227074 0.         0.00087336
  0.02445415 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.20856611 0.17318436 0.00558659 0.45623836 0.         0.
  0.15642458 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.46857143 0.03428571 0.12       0.07428571 0.         0.07428571
  0.22857143 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.24303797 0.05316456 0.         0.69113924 0.         0.
  0.01265823 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.31446541 0.24528302 0.00628931 0.28301887 0.         0.01257862
  0.13836478 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.17123288 0.04109589 0.09589041 0.02054795 0.         0.2260274
  0.44520548 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.13732394 0.09859155 0.00352113 0.04577465 0.         0.01408451
  0.70070423 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.23076923 0.15384615 0.         0.55384615 0.         0.
  0.06153846 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.81318681 0.02197802 0.02197802 0.05494505 0.         0.01098901
  0.07692308 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.64367816 0.09195402 0.03448276 0.04597701 0.         0.
  0.18390805 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.69767442 0.03488372 0.         0.26744186 0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.79166667 0.02777778 0.01388889 0.11111111 0.         0.
  0.05555556 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.296875   0.078125   0.         0.515625   0.         0.
  0.109375   0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.5625     0.125      0.         0.234375   0.         0.
  0.078125   0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.24561404 0.15789474 0.         0.56140351 0.         0.
  0.03508772 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.20560748 0.09345794 0.01869159 0.44859813 0.         0.01869159
  0.21495327 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.25       0.125      0.         0.57142857 0.         0.01785714
  0.03571429 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]
 [0.44230769 0.07692308 0.         0.44230769 0.         0.
  0.03846154 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.        ]]
---------------------------------------
program finished.
