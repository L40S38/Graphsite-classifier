seed:  666
save trained model at:  ../trained_models/trained_classifier_model_11.pt
save loss at:  ./results/train_classifier_results_11.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  1
learning rate decay at epoch:  90
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [20, 60, 130]
loss function:
FocalLoss(gamma=0, alpha=1, reduction=mean)
begin training...
epoch: 1, train loss: 2.3226830825511637, acc: 0.3277761918355695, val loss: 2.126866992136495, acc: 0.37603416066186285, test loss: 2.1259302331938597, acc: 0.3669141039236479
best val acc 0.37603416066186285 at epoch 1.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.5649    0.6730    0.6142      5337
           1     0.3139    0.4085    0.3550      2502
           2     0.2143    0.0037    0.0073       810
           3     0.2816    0.4674    0.3515      1840
           4     0.3902    0.0434    0.0781       737
           5     0.1935    0.0089    0.0169       677
           6     0.2257    0.8050    0.3525      1323
           7     0.0000    0.0000    0.0000       907
           8     0.0000    0.0000    0.0000       421
           9     0.0000    0.0000    0.0000       401
          10     0.0000    0.0000    0.0000       396
          11     0.0000    0.0000    0.0000       332
          12     0.0000    0.0000    0.0000       295
          13     0.0000    0.0000    0.0000       291
          14     0.0000    0.0000    0.0000       261
          15     0.0000    0.0000    0.0000       494
          16     0.0000    0.0000    0.0000       256
          17     0.0000    0.0000    0.0000       235

    accuracy                         0.3757     17515
   macro avg     0.1213    0.1339    0.0986     17515
weighted avg     0.2974    0.3757    0.3057     17515

train confusion matrix:
[[6.73037287e-01 6.95147086e-02 3.74742365e-04 8.05696084e-02
  2.24845419e-03 2.24845419e-03 1.72006745e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.53477218e-01 4.08473221e-01 1.19904077e-03 2.09432454e-01
  3.19744205e-03 0.00000000e+00 2.24220624e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.20987654e-01 6.29629630e-02 3.70370370e-03 1.72839506e-02
  1.11111111e-02 1.11111111e-02 6.72839506e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.07608696e-01 2.88043478e-01 0.00000000e+00 4.67391304e-01
  1.08695652e-03 0.00000000e+00 3.58695652e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.59158752e-01 2.00814111e-01 0.00000000e+00 3.36499322e-01
  4.34192673e-02 0.00000000e+00 1.60108548e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [8.56720827e-02 9.15805022e-02 4.43131462e-03 5.90841950e-03
  4.43131462e-03 8.86262925e-03 7.99113737e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.70370370e-02 1.36054422e-01 1.51171580e-03 1.88964475e-02
  7.55857899e-04 7.55857899e-04 8.04988662e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.73428886e-01 3.09812569e-01 0.00000000e+00 3.36273429e-01
  2.20507166e-03 0.00000000e+00 7.82800441e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.06175772e-01 8.31353919e-02 0.00000000e+00 2.13776722e-02
  0.00000000e+00 2.37529691e-03 4.86935867e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.29177057e-01 6.98254364e-02 0.00000000e+00 2.74314214e-02
  0.00000000e+00 4.98753117e-03 5.68578554e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.13636364e-01 9.34343434e-02 0.00000000e+00 2.55050505e-01
  0.00000000e+00 0.00000000e+00 3.78787879e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.86746988e-01 7.22891566e-02 3.01204819e-03 8.73493976e-02
  2.71084337e-02 0.00000000e+00 1.23493976e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.54237288e-01 3.15254237e-01 0.00000000e+00 2.06779661e-01
  3.38983051e-03 0.00000000e+00 2.20338983e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.57044674e-01 1.78694158e-01 0.00000000e+00 7.56013746e-02
  0.00000000e+00 0.00000000e+00 2.88659794e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.57088123e-01 3.25670498e-01 0.00000000e+00 3.86973180e-01
  3.83141762e-03 0.00000000e+00 1.26436782e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.63967611e-01 3.01619433e-01 0.00000000e+00 2.44939271e-01
  0.00000000e+00 0.00000000e+00 2.89473684e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.65625000e-01 2.61718750e-01 0.00000000e+00 4.17968750e-01
  0.00000000e+00 0.00000000e+00 5.46875000e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.42553191e-01 1.74468085e-01 0.00000000e+00 3.48936170e-01
  8.51063830e-03 0.00000000e+00 2.55319149e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]]
Traceback (most recent call last):
  File "train_classifier.py", line 320, in <module>
    metrics.ConfusionMatrixDisplay(train_confusion_mat).plot()
TypeError: __init__() missing 1 required positional argument: 'display_labels'
