seed:  11
save trained model at:  ../trained_models/trained_classifier_model_111.pt
save loss at:  ./results/train_classifier_results_111.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['5fu0A00', '3zm7C00', '4pd5A00', '5cjpB00', '3mhyB05']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['6m7kA00', '2qv6D00', '2j3eA00', '3vnsA00', '2w5gA00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b529646b730>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0559807917035036, acc: 0.38321297502071744; test loss: 1.7441777236860037, acc: 0.45402978019380763
epoch: 2, train loss: 1.7521834708931066, acc: 0.46264946134722384; test loss: 1.731047185423582, acc: 0.445757504136138
epoch: 3, train loss: 1.6422779134311407, acc: 0.49342961998342605; test loss: 1.5929210269403244, acc: 0.5242259513117467
epoch: 4, train loss: 1.586932931395696, acc: 0.513022374807624; test loss: 1.5764741402826419, acc: 0.5263531080122903
epoch: 5, train loss: 1.531450727444027, acc: 0.537409731265538; test loss: 1.5272777567966924, acc: 0.5440794138501537
epoch: 6, train loss: 1.4966437126089445, acc: 0.5481235941754469; test loss: 1.5147473271702852, acc: 0.5549515480973765
epoch: 7, train loss: 1.4785824789159239, acc: 0.5493074464306854; test loss: 1.4155696412120071, acc: 0.5686598912786576
epoch: 8, train loss: 1.4335343579053739, acc: 0.5655854149402154; test loss: 1.392539171991098, acc: 0.569368943512172
epoch: 9, train loss: 1.4053507387616924, acc: 0.5726885284716468; test loss: 1.3460530832962798, acc: 0.5802410777593949
epoch: 10, train loss: 1.4042644622100942, acc: 0.5760033147863146; test loss: 1.358035343754407, acc: 0.584731741904987
epoch: 11, train loss: 1.3669526069082705, acc: 0.5856517106665088; test loss: 1.3301792731383144, acc: 0.5963129283857244
epoch: 12, train loss: 1.3274473840842886, acc: 0.5909198532023203; test loss: 1.4764653266504622, acc: 0.5681871897896479
epoch: 13, train loss: 1.3440058691425127, acc: 0.5908606605895584; test loss: 1.382894016557539, acc: 0.5620420704325219
epoch: 14, train loss: 1.3151575737455632, acc: 0.5962471883508939; test loss: 1.2482809035441633, acc: 0.6145119357125975
epoch: 15, train loss: 1.3085179423331876, acc: 0.5999763229548952; test loss: 1.370792570307981, acc: 0.5887497045615694
epoch: 16, train loss: 1.2972574167961735, acc: 0.6012785604356576; test loss: 1.2786606751105882, acc: 0.6048215551878988
epoch: 17, train loss: 1.2575753602217228, acc: 0.6121700011838522; test loss: 1.3500953820602066, acc: 0.6008035925313164
epoch: 18, train loss: 1.2463058613172802, acc: 0.6183260329110927; test loss: 1.2367341392177185, acc: 0.6190025998581895
epoch: 19, train loss: 1.228503512195882, acc: 0.6243636794128092; test loss: 1.3094698177446742, acc: 0.6008035925313164
epoch: 20, train loss: 1.2235829245201635, acc: 0.6253107612170001; test loss: 1.2498883284228655, acc: 0.6164027416686363
epoch: 21, train loss: 1.2134841055902232, acc: 0.628625547531668; test loss: 1.2576033015872141, acc: 0.619948002836209
epoch: 22, train loss: 1.221451983545551, acc: 0.629454244110335; test loss: 1.3951837668827989, acc: 0.5904041597731032
epoch: 23, train loss: 1.194542777627209, acc: 0.6382147507991003; test loss: 1.2102482251821187, acc: 0.6296383833609076
epoch: 24, train loss: 1.2047085134718027, acc: 0.6305789037528117; test loss: 1.3099016819798166, acc: 0.5927676672181518
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9479778388611477, acc: 0.64004972179472; test loss: 0.914120437968738, acc: 0.6556369652564406
epoch: 26, train loss: 0.9295062581448776, acc: 0.647389605777199; test loss: 0.9672495623453096, acc: 0.6301110848499173
epoch: 27, train loss: 0.9287968571544244, acc: 0.6473304131644371; test loss: 0.9907034439726153, acc: 0.6147482864571023
epoch: 28, train loss: 0.9271206286188638, acc: 0.6453770569432935; test loss: 1.2043542358165824, acc: 0.5764594658473174
epoch: 29, train loss: 0.9280900742381907, acc: 0.6454362495560554; test loss: 0.9611646992690849, acc: 0.6369652564405578
epoch: 30, train loss: 0.9065797393161389, acc: 0.6510003551556766; test loss: 1.0132603891575993, acc: 0.6097849208225006
epoch: 31, train loss: 0.897071114408627, acc: 0.6590505504912987; test loss: 0.9841887381758089, acc: 0.6187662491136847
epoch: 32, train loss: 0.890159669812366, acc: 0.656386882917012; test loss: 0.9823603393957706, acc: 0.6161663909241314
epoch: 33, train loss: 0.8756839388134043, acc: 0.6583402391381555; test loss: 0.947688671635894, acc: 0.632001890805956
epoch: 34, train loss: 0.8759067704780039, acc: 0.662010181129395; test loss: 0.9417709032995034, acc: 0.650437248877334
epoch: 35, train loss: 0.8576522053974514, acc: 0.6679294424055878; test loss: 0.9340769345245866, acc: 0.6431103757976837
epoch: 36, train loss: 0.915854102713943, acc: 0.6496389250621523; test loss: 0.9657598079391981, acc: 0.6265658236823446
Epoch    36: reducing learning rate of group 0 to 1.5000e-03.
epoch: 37, train loss: 0.8307708218081599, acc: 0.6744998224221617; test loss: 0.8285841788560384, acc: 0.6821082486409832
epoch: 38, train loss: 0.785333805349828, acc: 0.6921392210252161; test loss: 0.819137058030401, acc: 0.6806901441739541
epoch: 39, train loss: 0.7942984606847969, acc: 0.6888836273233101; test loss: 0.8162389987187305, acc: 0.6828173008744978
epoch: 40, train loss: 0.7778216142886994, acc: 0.6914881022848348; test loss: 0.8394895799051247, acc: 0.6688726069487119
epoch: 41, train loss: 0.7628054941449911, acc: 0.6993607197821712; test loss: 0.9372837087049238, acc: 0.6374379579295675
epoch: 42, train loss: 0.7542044956528592, acc: 0.7014324612288386; test loss: 0.8603525025375175, acc: 0.6665090995036634
epoch: 43, train loss: 0.7530958605148502, acc: 0.7032674322244584; test loss: 0.8015005970023761, acc: 0.6891987709761286
epoch: 44, train loss: 0.73836192215944, acc: 0.7032674322244584; test loss: 0.8570191943034076, acc: 0.6702907114157409
epoch: 45, train loss: 0.7245197877723591, acc: 0.7098378122410324; test loss: 0.8374683521115676, acc: 0.6773812337508863
epoch: 46, train loss: 0.7300009444872839, acc: 0.7096010417899846; test loss: 0.833655644466172, acc: 0.6750177263058379
epoch: 47, train loss: 0.7291717215996741, acc: 0.7074109151177933; test loss: 0.9013145333400322, acc: 0.647364689198771
epoch: 48, train loss: 0.7169529783580378, acc: 0.7128566354918906; test loss: 0.7782364789263573, acc: 0.6986528007563224
epoch: 49, train loss: 0.7104787731912978, acc: 0.7148691843257962; test loss: 0.8022139286549765, acc: 0.6889624202316237
epoch: 50, train loss: 0.6986087059514599, acc: 0.7191310524446549; test loss: 1.038493324582725, acc: 0.6185298983691798
epoch: 51, train loss: 0.6921171549439953, acc: 0.7242216171421807; test loss: 0.77815850900719, acc: 0.7003072559678563
epoch: 52, train loss: 0.6871366748883765, acc: 0.7232745353379898; test loss: 0.8545872341851144, acc: 0.6735996218388088
epoch: 53, train loss: 0.676997191710939, acc: 0.7274180182313247; test loss: 0.9023032152351266, acc: 0.6530371070668872
epoch: 54, train loss: 0.6715144778872942, acc: 0.7303776488694211; test loss: 0.7914652482918378, acc: 0.693216733632711
epoch: 55, train loss: 0.6661128204603004, acc: 0.7335740499585651; test loss: 0.832528428754128, acc: 0.6856535098085559
epoch: 56, train loss: 0.6620779639519897, acc: 0.734461939149994; test loss: 0.8123846678439677, acc: 0.6880170172536043
epoch: 57, train loss: 0.6542931211140081, acc: 0.7331597016692317; test loss: 0.7922397746850458, acc: 0.7031434649019145
epoch: 58, train loss: 0.6524740250079296, acc: 0.7339883982478986; test loss: 0.8963632570094066, acc: 0.6646182935476247
epoch: 59, train loss: 0.6397248545426285, acc: 0.7391381555581863; test loss: 0.8066644142266285, acc: 0.6951075395887497
epoch: 60, train loss: 0.6342512118798029, acc: 0.7436367941280928; test loss: 0.8171047390676565, acc: 0.7073977783030017
epoch: 61, train loss: 0.6269451478705458, acc: 0.7429856753877117; test loss: 0.8019437414059767, acc: 0.6920349799101867
epoch: 62, train loss: 0.6306773864532824, acc: 0.7442287202557121; test loss: 0.8011563444667267, acc: 0.7045615693689435
epoch: 63, train loss: 0.6117303874050356, acc: 0.7490825145021901; test loss: 0.898552737656397, acc: 0.664145592058615
epoch: 64, train loss: 0.6215746777218814, acc: 0.7477210844086658; test loss: 0.8924128424329584, acc: 0.6598912786575277
epoch: 65, train loss: 0.6204825192848663, acc: 0.748372203149047; test loss: 0.9949296705831565, acc: 0.6324745922949657
epoch: 66, train loss: 0.6264656943739751, acc: 0.7436959867408547; test loss: 0.8225684133271353, acc: 0.6955802410777594
epoch: 67, train loss: 0.6011167767062842, acc: 0.7510950633360957; test loss: 0.7766373293305706, acc: 0.6934530843772158
epoch: 68, train loss: 0.5797520381777445, acc: 0.7644134012075293; test loss: 0.7831420057689402, acc: 0.7019617111793902
epoch: 69, train loss: 0.580075971223877, acc: 0.7638806676926719; test loss: 0.8041691589625951, acc: 0.6922713306546916
epoch: 70, train loss: 0.5887651441929057, acc: 0.7600331478631467; test loss: 0.7727008067640225, acc: 0.708343181281021
epoch: 71, train loss: 0.562582229931453, acc: 0.7686752693263881; test loss: 1.0281732162384523, acc: 0.6310564878279367
epoch: 72, train loss: 0.5684780677606527, acc: 0.7652420977861962; test loss: 0.7725927206506912, acc: 0.705270621602458
epoch: 73, train loss: 0.5668290913902374, acc: 0.7653012903989582; test loss: 0.7604432467224186, acc: 0.7185062632947293
epoch: 74, train loss: 0.5644351114066124, acc: 0.7657156386882917; test loss: 0.7760008437445868, acc: 0.7014890096903805
epoch: 75, train loss: 0.5607158489109878, acc: 0.770332662483722; test loss: 0.7715889303094245, acc: 0.7071614275584968
epoch: 76, train loss: 0.5535771339987106, acc: 0.770391855096484; test loss: 0.7971603419025072, acc: 0.7130701961711179
epoch: 77, train loss: 0.5561558157293786, acc: 0.7704510477092459; test loss: 0.7877496802984358, acc: 0.7043252186244386
epoch: 78, train loss: 0.5491298295851218, acc: 0.7747129158281046; test loss: 0.7835529234301702, acc: 0.7125974946821082
epoch: 79, train loss: 0.5415649999030542, acc: 0.7777909316917249; test loss: 0.8118862985184051, acc: 0.7003072559678563
epoch: 80, train loss: 0.5178872617346387, acc: 0.7822895702616314; test loss: 0.7788237211299095, acc: 0.7073977783030017
epoch: 81, train loss: 0.5335110265770875, acc: 0.7818752219722979; test loss: 0.7519871508982187, acc: 0.7213424722287876
epoch: 82, train loss: 0.5164811222161204, acc: 0.7853083935124896; test loss: 0.9572701128457235, acc: 0.662727487591586
epoch: 83, train loss: 0.5209916087033496, acc: 0.7806913697170593; test loss: 0.8784458898703446, acc: 0.6958165918222642
epoch: 84, train loss: 0.5163591185434057, acc: 0.7834734225168699; test loss: 0.8210959930392874, acc: 0.7081068305365162
epoch: 85, train loss: 0.5182764134605184, acc: 0.7812832958446786; test loss: 0.7593725529496931, acc: 0.725124084140865
epoch: 86, train loss: 0.5089130099671852, acc: 0.7867882088315378; test loss: 0.759812595939501, acc: 0.7203970692507682
epoch: 87, train loss: 0.5042088954169552, acc: 0.7908724991121108; test loss: 0.8187275332385726, acc: 0.7168518080831955
epoch: 88, train loss: 0.48701902320430984, acc: 0.795548715520303; test loss: 0.7907423478890412, acc: 0.7161427558496809
epoch: 89, train loss: 0.4999165287437814, acc: 0.789096720729253; test loss: 0.7554818725676098, acc: 0.7234696289293311
epoch: 90, train loss: 0.4902863644475385, acc: 0.7902213803717296; test loss: 0.8098932373075727, acc: 0.7170881588277003
epoch: 91, train loss: 0.5008416577346285, acc: 0.7872025571208713; test loss: 0.769630595488741, acc: 0.7166154573386906
epoch: 92, train loss: 0.4768453580297012, acc: 0.7978572274180182; test loss: 0.8699295980601806, acc: 0.6998345544788466
epoch: 93, train loss: 0.491019573192759, acc: 0.7917603883035397; test loss: 0.8513435601230154, acc: 0.7017253604348853
epoch: 94, train loss: 0.47419616318926017, acc: 0.7972061086776371; test loss: 0.8156107333049626, acc: 0.7147246513826518
epoch: 95, train loss: 0.490109929808981, acc: 0.7951935598437315; test loss: 0.8587996923622018, acc: 0.7047979201134483
epoch: 96, train loss: 0.4755012899287935, acc: 0.7977980348052564; test loss: 0.8121925161060107, acc: 0.7133065469156228
Epoch    96: reducing learning rate of group 0 to 7.5000e-04.
epoch: 97, train loss: 0.4060896775723068, acc: 0.8242571327098378; test loss: 0.7552472561759832, acc: 0.7345781139210589
epoch: 98, train loss: 0.3808864938891031, acc: 0.8365691961643187; test loss: 0.757178001329426, acc: 0.7421413377452138
epoch: 99, train loss: 0.37388045699677974, acc: 0.8351485734580324; test loss: 0.7856574645703973, acc: 0.7362325691325927
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.2827290083242335, acc: 0.8398247898662247; test loss: 0.6652318065652777, acc: 0.7378870243441267
epoch: 101, train loss: 0.268928810271396, acc: 0.8450337397892743; test loss: 0.6638536127541708, acc: 0.7445048451902624
epoch: 102, train loss: 0.26896980590434755, acc: 0.847697407363561; test loss: 0.6858277094333174, acc: 0.7329236587095249
epoch: 103, train loss: 0.27452796973871085, acc: 0.8418965313128921; test loss: 0.672417324363016, acc: 0.7322146064760104
epoch: 104, train loss: 0.2621229542025705, acc: 0.8468095181721321; test loss: 0.6715889217385275, acc: 0.734341763176554
epoch: 105, train loss: 0.26228181618945307, acc: 0.8442642358233693; test loss: 0.655295044078074, acc: 0.7359962183880879
epoch: 106, train loss: 0.2581355794920249, acc: 0.8500651118740381; test loss: 0.722633921883118, acc: 0.7253604348853699
epoch: 107, train loss: 0.2643834912728964, acc: 0.8417189534746063; test loss: 0.6800733177804462, acc: 0.7371779721106122
epoch: 108, train loss: 0.26846456703764876, acc: 0.8400615603172724; test loss: 0.6964215638366498, acc: 0.7355235168990782
epoch: 109, train loss: 0.25167670515750135, acc: 0.8476382147507991; test loss: 0.7436479005766264, acc: 0.7265421886078941
epoch: 110, train loss: 0.2556129032455987, acc: 0.8416005682490825; test loss: 0.6918944215921047, acc: 0.7352871661545733
epoch: 111, train loss: 0.25752019557964734, acc: 0.8430803835681306; test loss: 0.6711423795743351, acc: 0.7341054124320492
epoch: 112, train loss: 0.254341272693478, acc: 0.8480525630401325; test loss: 0.7341500850074315, acc: 0.7218151737177972
epoch: 113, train loss: 0.24721768864466762, acc: 0.8495915709719427; test loss: 0.7217170129963986, acc: 0.7326873079650201
epoch: 114, train loss: 0.26114240837362485, acc: 0.8387001302237481; test loss: 0.6676878521724728, acc: 0.7350508154100686
epoch: 115, train loss: 0.24565638951596472, acc: 0.8467503255593702; test loss: 0.7532362746054104, acc: 0.7194516662727488
epoch: 116, train loss: 0.2500723124051413, acc: 0.8487628743932757; test loss: 0.6908253844086933, acc: 0.734341763176554
epoch: 117, train loss: 0.23610876637146122, acc: 0.8562803362140405; test loss: 0.7068902799858322, acc: 0.738832427322146
epoch: 118, train loss: 0.23682100053361088, acc: 0.8498283414229905; test loss: 0.7185683846051065, acc: 0.7359962183880879
epoch: 119, train loss: 0.24070199226978842, acc: 0.8497099561974666; test loss: 0.6920347203602044, acc: 0.7352871661545733
epoch: 120, train loss: 0.24072107018218655, acc: 0.8506570380016574; test loss: 0.7269655332247491, acc: 0.7333963601985346
epoch: 121, train loss: 0.22807203222481162, acc: 0.8568722623416598; test loss: 0.7402882203512569, acc: 0.7303238005199716
epoch: 122, train loss: 0.24797047882993448, acc: 0.8482893334911803; test loss: 0.7345475659982388, acc: 0.7246513826518554
epoch: 123, train loss: 0.24037017717796966, acc: 0.8446193914999408; test loss: 0.7200469086565213, acc: 0.7255967856298747
epoch: 124, train loss: 0.24084198905671717, acc: 0.8471646738487036; test loss: 0.7635390987860567, acc: 0.7213424722287876
epoch: 125, train loss: 0.23248069713182587, acc: 0.8559251805374689; test loss: 0.7408299571203865, acc: 0.7312692034979911
epoch: 126, train loss: 0.2304951037523153, acc: 0.85503729134604; test loss: 0.7024944237686731, acc: 0.7411959347671945
epoch: 127, train loss: 0.2241470968954744, acc: 0.8523144311589913; test loss: 0.7344832094925511, acc: 0.7331600094540298
epoch: 128, train loss: 0.21437600192564174, acc: 0.8636794128092814; test loss: 0.703537556556516, acc: 0.7411959347671945
epoch: 129, train loss: 0.21329554890907487, acc: 0.8610157452349947; test loss: 0.7865753212146402, acc: 0.7163791065941858
epoch: 130, train loss: 0.21514554669112726, acc: 0.8577601515330887; test loss: 0.7562744182155251, acc: 0.7255967856298747
epoch: 131, train loss: 0.2182129243746218, acc: 0.8604238191073754; test loss: 0.7505039551161728, acc: 0.7284329945639328
epoch: 132, train loss: 0.21538620832277725, acc: 0.8611341304605186; test loss: 0.7428372779711852, acc: 0.7300874497754668
epoch: 133, train loss: 0.2255815375665525, acc: 0.8524920089972772; test loss: 0.7170877082780077, acc: 0.7322146064760104
epoch: 134, train loss: 0.22258474601504788, acc: 0.8595951225287084; test loss: 0.7730950554108625, acc: 0.7355235168990782
epoch: 135, train loss: 0.20415840302788607, acc: 0.8668166212856635; test loss: 0.808066357171048, acc: 0.7208697707397779
epoch: 136, train loss: 0.20566847932942733, acc: 0.8657511542559488; test loss: 0.8211297852523612, acc: 0.7187426140392342
epoch: 137, train loss: 0.22004210662457935, acc: 0.858352077660708; test loss: 0.7436814055716673, acc: 0.7296147482864571
epoch: 138, train loss: 0.21947364573494504, acc: 0.8585296554989937; test loss: 0.7856125727919252, acc: 0.7248877333963601
epoch: 139, train loss: 0.2106242306107979, acc: 0.8661655025452824; test loss: 0.7458965472368111, acc: 0.7229969274403214
epoch: 140, train loss: 0.21279138720563365, acc: 0.8629691014561383; test loss: 0.7288949810790836, acc: 0.7338690616875443
epoch: 141, train loss: 0.19657546994149, acc: 0.865691961643187; test loss: 0.7952239815998912, acc: 0.7218151737177972
epoch: 142, train loss: 0.200212194771192, acc: 0.8659287320942346; test loss: 0.7696141007943402, acc: 0.7265421886078941
epoch: 143, train loss: 0.21074692207962237, acc: 0.863146679294424; test loss: 0.7644034051297828, acc: 0.7317419049870008
epoch: 144, train loss: 0.19470256394827395, acc: 0.8686515922812833; test loss: 0.7594164656961648, acc: 0.7274875915859135
epoch: 145, train loss: 0.19505373178868232, acc: 0.8676453178643305; test loss: 0.7805860371658409, acc: 0.7393051288111557
epoch: 146, train loss: 0.19199939746186057, acc: 0.8678228957026163; test loss: 0.7899888346759288, acc: 0.7324509572205152
epoch: 147, train loss: 0.18741494468172387, acc: 0.8706641411151889; test loss: 0.9254109430639943, acc: 0.7017253604348853
Epoch   147: reducing learning rate of group 0 to 3.7500e-04.
epoch: 148, train loss: 0.16323355275829318, acc: 0.8834497454717651; test loss: 0.7608180127789635, acc: 0.7442684944457575
epoch: 149, train loss: 0.13850826201593297, acc: 0.8997277139812951; test loss: 0.8139124775406205, acc: 0.7442684944457575
epoch: 150, train loss: 0.13727584469243592, acc: 0.9019178406534865; test loss: 0.8083463029020702, acc: 0.7468683526353108
epoch: 151, train loss: 0.13553637134029667, acc: 0.9013851071386291; test loss: 0.8503492586270655, acc: 0.7466320018908059
epoch: 152, train loss: 0.1377867595864299, acc: 0.900615603172724; test loss: 0.8279146335559431, acc: 0.7433230914677381
epoch: 153, train loss: 0.12795116399835238, acc: 0.9069492127382502; test loss: 0.8426659878645346, acc: 0.7385960765776413
epoch: 154, train loss: 0.13415410393273264, acc: 0.9005564105599622; test loss: 0.8329089410872472, acc: 0.7411959347671945
epoch: 155, train loss: 0.12493917823712812, acc: 0.9048774712915828; test loss: 0.8483384567861212, acc: 0.7355235168990782
epoch: 156, train loss: 0.1418489226984303, acc: 0.8963537350538653; test loss: 0.8392339742823263, acc: 0.7355235168990782
epoch: 157, train loss: 0.13433157228091291, acc: 0.9019178406534865; test loss: 0.8715186116825574, acc: 0.738832427322146
epoch: 158, train loss: 0.1213411149226294, acc: 0.9093169172487273; test loss: 0.8777151861756554, acc: 0.737414322855117
epoch: 159, train loss: 0.12442062741181435, acc: 0.9083698354445365; test loss: 0.8729657376243611, acc: 0.743559442212243
epoch: 160, train loss: 0.12696878353074922, acc: 0.9051142417426306; test loss: 0.8708080957471775, acc: 0.7352871661545733
epoch: 161, train loss: 0.12513914486635672, acc: 0.906357286610631; test loss: 0.8632561089834179, acc: 0.7421413377452138
epoch: 162, train loss: 0.11621733570703603, acc: 0.9110335030188232; test loss: 0.8764214104891556, acc: 0.7348144646655637
epoch: 163, train loss: 0.1247025435194823, acc: 0.9058245530957737; test loss: 0.8585114818857905, acc: 0.743559442212243
epoch: 164, train loss: 0.13467353374221078, acc: 0.9020362258790103; test loss: 0.869037254345217, acc: 0.7376506735996219
epoch: 165, train loss: 0.11967660999868246, acc: 0.910441576891204; test loss: 0.871633592831894, acc: 0.7350508154100686
epoch: 166, train loss: 0.12283985679447686, acc: 0.9090801467976797; test loss: 0.9135461276204725, acc: 0.7279602930749232
epoch: 167, train loss: 0.11764837268352282, acc: 0.9100272286018705; test loss: 0.8882648153131495, acc: 0.7369416213661073
epoch: 168, train loss: 0.12497572386506127, acc: 0.9067124422872026; test loss: 0.8628373927637297, acc: 0.7371779721106122
epoch: 169, train loss: 0.11198955937484403, acc: 0.9118030069847283; test loss: 0.964974190264102, acc: 0.725124084140865
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.0895515429205523, acc: 0.9076595240913934; test loss: 0.7742175701464132, acc: 0.74048688253368
epoch: 171, train loss: 0.08169349293564429, acc: 0.9142299041079673; test loss: 0.7969891924690169, acc: 0.7348144646655637
epoch: 172, train loss: 0.07993262303931142, acc: 0.9180182313247307; test loss: 0.8010100135000793, acc: 0.7383597258331364
epoch: 173, train loss: 0.07875650902129326, acc: 0.9131052444654907; test loss: 0.7933956916324919, acc: 0.7371779721106122
epoch: 174, train loss: 0.07968727649457782, acc: 0.912868474014443; test loss: 0.7931810776636804, acc: 0.7381233750886316
epoch: 175, train loss: 0.07747355278577411, acc: 0.9123949331123475; test loss: 0.7889559600855323, acc: 0.7281966438194281
epoch: 176, train loss: 0.08598483617318174, acc: 0.90878418373387; test loss: 0.8178241643407502, acc: 0.7284329945639328
epoch: 177, train loss: 0.083807129744095, acc: 0.9107375399550136; test loss: 0.7680900579252808, acc: 0.7355235168990782
epoch: 178, train loss: 0.08816488925819639, acc: 0.9061797087723452; test loss: 0.8315622639751863, acc: 0.7199243677617585
epoch: 179, train loss: 0.07914220110486919, acc: 0.9148218302355866; test loss: 0.794073269852395, acc: 0.7381233750886316
epoch: 180, train loss: 0.07804725794742358, acc: 0.912039777435776; test loss: 0.7857082821578456, acc: 0.7385960765776413
epoch: 181, train loss: 0.0758217401053689, acc: 0.9160056824908251; test loss: 0.8004098814786452, acc: 0.7416686362562042
epoch: 182, train loss: 0.08061062405486777, acc: 0.9118621995974903; test loss: 0.811948251566462, acc: 0.7350508154100686
epoch: 183, train loss: 0.08244504279108517, acc: 0.910441576891204; test loss: 0.8177909822335511, acc: 0.7289056960529425
epoch: 184, train loss: 0.07961433364171679, acc: 0.9124541257251095; test loss: 0.8004749083851449, acc: 0.7322146064760104
epoch: 185, train loss: 0.07600319526566547, acc: 0.9148218302355866; test loss: 0.8339705348888263, acc: 0.7355235168990782
epoch: 186, train loss: 0.08178658620964299, acc: 0.9097904581508228; test loss: 0.8505195097786911, acc: 0.7232332781848263
epoch: 187, train loss: 0.08473787923172915, acc: 0.9094944950870132; test loss: 0.8362153124736009, acc: 0.7265421886078941
epoch: 188, train loss: 0.07906738149600437, acc: 0.9082514502190127; test loss: 0.8234105845541291, acc: 0.7315055542424959
epoch: 189, train loss: 0.07939074211933068, acc: 0.9099088433763466; test loss: 0.8281982576162419, acc: 0.7362325691325927
epoch: 190, train loss: 0.07739152135271228, acc: 0.9132236296910146; test loss: 0.8048824238963117, acc: 0.7341054124320492
epoch: 191, train loss: 0.08873915294216372, acc: 0.9038120042618681; test loss: 0.7935796193664053, acc: 0.7397778303001654
epoch: 192, train loss: 0.10055864441972717, acc: 0.8980111282111992; test loss: 0.7831763498497415, acc: 0.725124084140865
epoch: 193, train loss: 0.08099164389760957, acc: 0.9093169172487273; test loss: 0.8566840190296899, acc: 0.7147246513826518
epoch: 194, train loss: 0.07351638695569776, acc: 0.9158872972653013; test loss: 0.8177598464350925, acc: 0.7390687780666509
epoch: 195, train loss: 0.08237306506757668, acc: 0.9094944950870132; test loss: 0.8051702871978551, acc: 0.7324509572205152
epoch: 196, train loss: 0.08812965870228155, acc: 0.9051734343553924; test loss: 0.766152743891998, acc: 0.7378870243441267
epoch: 197, train loss: 0.081656278753275, acc: 0.9114478513081568; test loss: 0.7840918192934466, acc: 0.7355235168990782
epoch: 198, train loss: 0.08591199847608823, acc: 0.9065348644489167; test loss: 0.8077309074708402, acc: 0.7281966438194281
Epoch   198: reducing learning rate of group 0 to 1.8750e-04.
epoch: 199, train loss: 0.07147237777223076, acc: 0.9189653131289215; test loss: 0.7815041117734646, acc: 0.7352871661545733
epoch: 200, train loss: 0.059151141396129966, acc: 0.9289096720729253; test loss: 0.79909501541199, acc: 0.7416686362562042
best test acc 0.7468683526353108 at epoch 150.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9436    0.9787    0.9608      6100
           1     0.9779    0.9536    0.9656       926
           2     0.8867    0.9592    0.9215      2400
           3     0.9500    0.9241    0.9369       843
           4     0.9438    0.9767    0.9600       774
           5     0.9574    0.9663    0.9618      1512
           6     0.8828    0.8158    0.8480      1330
           7     0.9332    0.9002    0.9164       481
           8     0.9015    0.9389    0.9198       458
           9     0.9403    0.9757    0.9577       452
          10     0.9385    0.9149    0.9266       717
          11     0.8867    0.8228    0.8536       333
          12     0.8462    0.1472    0.2507       299
          13     0.8378    0.6914    0.7576       269

    accuracy                         0.9293     16894
   macro avg     0.9162    0.8547    0.8669     16894
weighted avg     0.9279    0.9293    0.9236     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.7745    0.8852    0.8262      1525
           1     0.8325    0.7284    0.7770       232
           2     0.7248    0.7671    0.7454       601
           3     0.8218    0.7867    0.8039       211
           4     0.8394    0.8351    0.8372       194
           5     0.8125    0.7910    0.8016       378
           6     0.5552    0.4685    0.5081       333
           7     0.7204    0.5537    0.6262       121
           8     0.4907    0.4609    0.4753       115
           9     0.7143    0.7895    0.7500       114
          10     0.7305    0.6778    0.7032       180
          11     0.6102    0.4286    0.5035        84
          12     0.1000    0.0133    0.0235        75
          13     0.6667    0.4118    0.5091        68

    accuracy                         0.7469      4231
   macro avg     0.6710    0.6141    0.6350      4231
weighted avg     0.7324    0.7469    0.7359      4231

---------------------------------------
program finished.
