seed:  22
save trained model at:  ../trained_models/trained_classifier_model_122.pt
save loss at:  ./results/train_classifier_results_122.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['3a0tA00', '1nlyB00', '3c6tA00', '4ww7A00', '4cg8A00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['3hd2A01', '2eklA00', '4i9bA00', '3ovbA00', '4unrB00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b7045a71730>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 1.9962061335663126, acc: 0.4020362258790103; test loss: 1.8294698357497567, acc: 0.42921295202079884
epoch: 2, train loss: 1.7259574381151015, acc: 0.4670889073043684; test loss: 1.721623127129703, acc: 0.48050106357835026
epoch: 3, train loss: 1.6355302478345002, acc: 0.4995856517106665; test loss: 1.625597649391479, acc: 0.4826282202788939
epoch: 4, train loss: 1.5651823151135593, acc: 0.5200071031135314; test loss: 1.5515952270889193, acc: 0.5131174663200189
epoch: 5, train loss: 1.5157387303335081, acc: 0.5364626494613473; test loss: 1.5326537005136662, acc: 0.5284802647128338
epoch: 6, train loss: 1.501112885160165, acc: 0.5427370664141116; test loss: 1.4879079420102794, acc: 0.5426613093831245
epoch: 7, train loss: 1.4518003444975334, acc: 0.5606724280809755; test loss: 1.4011742225692954, acc: 0.5684235405341527
epoch: 8, train loss: 1.4254163929283428, acc: 0.5693145495442169; test loss: 1.4052582599118764, acc: 0.5745686598912787
epoch: 9, train loss: 1.4146446709352314, acc: 0.5704984017994554; test loss: 1.4917644060297628, acc: 0.5355707870479792
epoch: 10, train loss: 1.3760485771324165, acc: 0.5823369243518409; test loss: 1.350999203620862, acc: 0.5790593240368708
epoch: 11, train loss: 1.3618627417309124, acc: 0.5883153782407955; test loss: 1.4760013457816477, acc: 0.5395887497045616
epoch: 12, train loss: 1.3288229202340731, acc: 0.5989700485379424; test loss: 1.32660096600613, acc: 0.5861498463720161
epoch: 13, train loss: 1.3138523702439688, acc: 0.600982597371848; test loss: 1.331517757858911, acc: 0.581659182226424
epoch: 14, train loss: 1.2977761013403128, acc: 0.6106901858648041; test loss: 1.288302517365519, acc: 0.6019853462538407
epoch: 15, train loss: 1.2900480561472614, acc: 0.6114004972179472; test loss: 1.2966403269198623, acc: 0.5930040179626566
epoch: 16, train loss: 1.282373556769556, acc: 0.613176275600805; test loss: 1.406026560993686, acc: 0.56487827936658
epoch: 17, train loss: 1.2731282480287738, acc: 0.6141233574049959; test loss: 1.2794921730630253, acc: 0.6029307492318601
epoch: 18, train loss: 1.2600975263128622, acc: 0.6187403812004262; test loss: 1.2990096688298745, acc: 0.5948948239186953
epoch: 19, train loss: 1.235248002382768, acc: 0.6290398958210015; test loss: 1.2328189155278377, acc: 0.6116757267785393
epoch: 20, train loss: 1.2478235922326022, acc: 0.6235941754469042; test loss: 1.3029273491721716, acc: 0.6022216969983455
epoch: 21, train loss: 1.215530589245841, acc: 0.6363797798034805; test loss: 1.3758432647630245, acc: 0.5665327345781139
epoch: 22, train loss: 1.2018496728273655, acc: 0.6370308985438617; test loss: 1.2327939201207117, acc: 0.6133301819900733
epoch: 23, train loss: 1.2093792319424803, acc: 0.6369125133183379; test loss: 1.2328074392036072, acc: 0.6154573386906169
epoch: 24, train loss: 1.2202852182138795, acc: 0.6361430093524328; test loss: 1.2557736351483821, acc: 0.6119120775230442
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9684899028933541, acc: 0.6417663075648159; test loss: 0.9789344642889807, acc: 0.6263294729378398
epoch: 26, train loss: 0.9506877992256755, acc: 0.6436604711731976; test loss: 0.9650274776336122, acc: 0.6324745922949657
epoch: 27, train loss: 0.9364263692579002, acc: 0.6493429619983426; test loss: 0.9751532387716554, acc: 0.627511226660364
epoch: 28, train loss: 0.9201790598125532, acc: 0.6544927193086303; test loss: 1.202158560118363, acc: 0.571023398723706
epoch: 29, train loss: 0.9274214257555515, acc: 0.6557357641766307; test loss: 1.0886767270068587, acc: 0.5714961002127157
epoch: 30, train loss: 0.9142844036399675, acc: 0.654670297146916; test loss: 0.974167114790662, acc: 0.6265658236823446
epoch: 31, train loss: 0.9071370495259021, acc: 0.6602344027465372; test loss: 0.9482442894829444, acc: 0.6253840699598203
epoch: 32, train loss: 0.889044600900876, acc: 0.6636083816739671; test loss: 1.0045469209562377, acc: 0.6175844953911604
epoch: 33, train loss: 0.9017988836834441, acc: 0.6589913578785368; test loss: 0.960495294394208, acc: 0.6312928385724415
epoch: 34, train loss: 0.9378663777839338, acc: 0.6470344501006274; test loss: 0.9433320827502163, acc: 0.6379106594185772
epoch: 35, train loss: 0.8913023797833766, acc: 0.6626612998697763; test loss: 0.9459020096312964, acc: 0.6327109430394706
epoch: 36, train loss: 0.8848271920679024, acc: 0.6649106191547295; test loss: 0.9848489981682637, acc: 0.6190025998581895
epoch: 37, train loss: 0.8633919986476697, acc: 0.6715993843968273; test loss: 0.9855893205959615, acc: 0.6204207043252187
epoch: 38, train loss: 0.8612600725769954, acc: 0.669409257724636; test loss: 1.0645567304669359, acc: 0.5979673835972583
epoch: 39, train loss: 0.8473203362368036, acc: 0.6729608144903516; test loss: 1.0265787482346183, acc: 0.6003308910423067
epoch: 40, train loss: 0.8596402792743187, acc: 0.6716585770095892; test loss: 0.9128522537639873, acc: 0.644292129520208
epoch: 41, train loss: 0.8373349078649654, acc: 0.6741446667455901; test loss: 0.9955248029709313, acc: 0.6140392342235879
epoch: 42, train loss: 0.8362493620859288, acc: 0.6784065348644489; test loss: 0.9111614685987248, acc: 0.641219569841645
epoch: 43, train loss: 0.8481478989512395, acc: 0.6753285190008287; test loss: 0.8741949758076606, acc: 0.6648546442921295
epoch: 44, train loss: 0.8212498648358306, acc: 0.6832603291109269; test loss: 0.951730562535563, acc: 0.6312928385724415
epoch: 45, train loss: 0.8152978426961062, acc: 0.6853320705575944; test loss: 1.057091418971578, acc: 0.604585204443394
epoch: 46, train loss: 0.8111364958595916, acc: 0.6865751154255949; test loss: 0.8937804025556708, acc: 0.6563460174899551
epoch: 47, train loss: 0.8000559880773852, acc: 0.6883508938084527; test loss: 0.926944230527524, acc: 0.6416922713306547
epoch: 48, train loss: 0.7935296076987077, acc: 0.6907777909316917; test loss: 0.8961230784379909, acc: 0.6535098085558969
epoch: 49, train loss: 0.791822808542745, acc: 0.6950988516633124; test loss: 0.8914249014330372, acc: 0.6471283384542661
epoch: 50, train loss: 0.7860816480338086, acc: 0.6949212738250267; test loss: 0.8804820110769144, acc: 0.6506735996218388
epoch: 51, train loss: 0.7722542421541454, acc: 0.6985912158162662; test loss: 0.8367629895337618, acc: 0.6674545024816828
epoch: 52, train loss: 0.7770961987361183, acc: 0.6954540073398839; test loss: 0.968276744304721, acc: 0.6185298983691798
epoch: 53, train loss: 0.7789959186165237, acc: 0.6951580442760743; test loss: 1.2374464951629927, acc: 0.5544788466083668
epoch: 54, train loss: 0.7784345684650453, acc: 0.6957499704036936; test loss: 0.8507064207032397, acc: 0.661073032380052
epoch: 55, train loss: 0.755535892633287, acc: 0.7065230259263644; test loss: 0.9043888431495878, acc: 0.641219569841645
epoch: 56, train loss: 0.7423714744391294, acc: 0.7066414111518883; test loss: 0.975360117632569, acc: 0.6265658236823446
epoch: 57, train loss: 0.7576514972135472, acc: 0.7073517225050314; test loss: 0.8871631021619096, acc: 0.6511463011108485
epoch: 58, train loss: 0.7397105003921777, acc: 0.7062862554753166; test loss: 0.8437395326298367, acc: 0.662727487591586
epoch: 59, train loss: 0.7355719330509245, acc: 0.7110216644962709; test loss: 0.8701993538341013, acc: 0.6551642637674309
epoch: 60, train loss: 0.7330794122934031, acc: 0.7110216644962709; test loss: 0.9931047644577982, acc: 0.6218388087922477
epoch: 61, train loss: 0.7199633125935954, acc: 0.7165265774831301; test loss: 0.8243689885293866, acc: 0.6705270621602458
epoch: 62, train loss: 0.7117511400657672, acc: 0.7197821711850361; test loss: 0.8768301734234587, acc: 0.6620184353580714
epoch: 63, train loss: 0.740417380593748, acc: 0.7113768201728424; test loss: 0.8231226478280032, acc: 0.6745450248168282
epoch: 64, train loss: 0.7254837874521147, acc: 0.7171185036107494; test loss: 0.9028295389277747, acc: 0.6483100921767904
epoch: 65, train loss: 0.7082403042379379, acc: 0.7193678228957027; test loss: 0.8876977236212864, acc: 0.6542188607894115
epoch: 66, train loss: 0.7070628974202597, acc: 0.7235113057890376; test loss: 0.9700934814240346, acc: 0.6402741668636256
epoch: 67, train loss: 0.7048139713622557, acc: 0.7227418018231325; test loss: 0.9509122635393497, acc: 0.6329472937839754
epoch: 68, train loss: 0.6930711897954696, acc: 0.7269444773292293; test loss: 0.8677850438471636, acc: 0.6532734578113921
epoch: 69, train loss: 0.6948042438836638, acc: 0.7231561501124659; test loss: 0.8576679518813924, acc: 0.6665090995036634
epoch: 70, train loss: 0.6860810329014555, acc: 0.7258790102995146; test loss: 0.8375988820855202, acc: 0.6811628456629638
epoch: 71, train loss: 0.6910074631730607, acc: 0.7226234165976086; test loss: 0.8371549918842609, acc: 0.6806901441739541
epoch: 72, train loss: 0.6691696596904889, acc: 0.7302000710311353; test loss: 1.0318785113269515, acc: 0.6159300401796266
epoch: 73, train loss: 0.6657574510340072, acc: 0.7348762874393275; test loss: 0.8023955319908825, acc: 0.6882533679981092
epoch: 74, train loss: 0.660152748285334, acc: 0.7377175328519001; test loss: 0.9832441908223949, acc: 0.6294020326164027
epoch: 75, train loss: 0.6613685015151949, acc: 0.7349354800520895; test loss: 0.780910363520778, acc: 0.6955802410777594
epoch: 76, train loss: 0.6798456361819629, acc: 0.7275955960696106; test loss: 0.7924107375423329, acc: 0.6880170172536043
epoch: 77, train loss: 0.6570881625611612, acc: 0.7357641766307564; test loss: 0.8066553019710929, acc: 0.6861262112975656
epoch: 78, train loss: 0.6545339079457542, acc: 0.7407955487155203; test loss: 0.7672541602456353, acc: 0.7003072559678563
epoch: 79, train loss: 0.6515714702090244, acc: 0.7406179708772345; test loss: 0.8480494998921967, acc: 0.6707634129047506
epoch: 80, train loss: 0.6603996177474408, acc: 0.7363561027583757; test loss: 0.7700093350380032, acc: 0.7073977783030017
epoch: 81, train loss: 0.6401619744171007, acc: 0.746714809991713; test loss: 0.8515210396796641, acc: 0.6653273457811392
epoch: 82, train loss: 0.6452852292033756, acc: 0.7439327571919024; test loss: 1.0885396486082644, acc: 0.5818955329709289
epoch: 83, train loss: 0.6503213061882192, acc: 0.742689712323902; test loss: 0.8135670407447824, acc: 0.6856535098085559
epoch: 84, train loss: 0.6327544323252379, acc: 0.750029596306381; test loss: 1.0740009797884482, acc: 0.5906405105176081
epoch: 85, train loss: 0.6491745998199782, acc: 0.7437551793536167; test loss: 0.7950411030542369, acc: 0.6889624202316237
epoch: 86, train loss: 0.6163128264221028, acc: 0.7534035752338109; test loss: 0.768148075460796, acc: 0.6991255022453321
epoch: 87, train loss: 0.6188754958486619, acc: 0.7516869894637149; test loss: 0.7700667103479106, acc: 0.7140155991491374
epoch: 88, train loss: 0.6110007004567374, acc: 0.7558896649698118; test loss: 0.8063820547258508, acc: 0.6896714724651383
epoch: 89, train loss: 0.6199645875783676, acc: 0.7507399076595241; test loss: 0.8203400489822457, acc: 0.6806901441739541
epoch: 90, train loss: 0.6172547148249764, acc: 0.755652894518764; test loss: 0.8129618499781104, acc: 0.6783266367289057
epoch: 91, train loss: 0.6073982243327907, acc: 0.7559488575825737; test loss: 0.861899489630652, acc: 0.6776175844953911
epoch: 92, train loss: 0.6018510224890593, acc: 0.759737184799337; test loss: 0.7428611871114833, acc: 0.7133065469156228
epoch: 93, train loss: 0.6098566876652256, acc: 0.7557712797442879; test loss: 0.8327209968089043, acc: 0.6773812337508863
epoch: 94, train loss: 0.6045794473028019, acc: 0.7592636438972417; test loss: 0.8358060844230246, acc: 0.6615457338690617
epoch: 95, train loss: 0.627924033469867, acc: 0.7500887889191429; test loss: 0.8163259533161081, acc: 0.6802174426849444
epoch: 96, train loss: 0.605143898956251, acc: 0.7563632058719072; test loss: 0.7914732394559928, acc: 0.7047979201134483
epoch: 97, train loss: 0.6088530263548506, acc: 0.7577838285781935; test loss: 0.7760870324272097, acc: 0.6986528007563224
epoch: 98, train loss: 0.5896652672817767, acc: 0.760625073990766; test loss: 0.8236931810083752, acc: 0.6830536516190026
Epoch    98: reducing learning rate of group 0 to 1.5000e-03.
epoch: 99, train loss: 0.5247209731807395, acc: 0.7848348526103942; test loss: 0.7361305382651264, acc: 0.7187426140392342
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.40684228222074065, acc: 0.7957854859713508; test loss: 0.6099199963585193, acc: 0.7286693453084377
epoch: 101, train loss: 0.3973306045068486, acc: 0.7981531904818279; test loss: 0.5823371811303031, acc: 0.7248877333963601
epoch: 102, train loss: 0.38446690827498736, acc: 0.8011128211199242; test loss: 0.6041133256421758, acc: 0.723705979673836
epoch: 103, train loss: 0.37791009985922186, acc: 0.802947792115544; test loss: 0.6244537571704407, acc: 0.7274875915859135
epoch: 104, train loss: 0.38584909477049856, acc: 0.7959630638096366; test loss: 0.6578771057660854, acc: 0.706925076813992
epoch: 105, train loss: 0.3802882130504205, acc: 0.8004025097667811; test loss: 0.6238108986349878, acc: 0.7260694871188844
epoch: 106, train loss: 0.3750714968580797, acc: 0.8006984728305907; test loss: 0.6683434440125474, acc: 0.7019617111793902
epoch: 107, train loss: 0.36897785825264784, acc: 0.8043684148218302; test loss: 0.6126128284115793, acc: 0.7211061214842827
epoch: 108, train loss: 0.36714181810620977, acc: 0.8020007103113531; test loss: 0.6380711440300721, acc: 0.7161427558496809
epoch: 109, train loss: 0.3603379860585634, acc: 0.8076240085237363; test loss: 0.6198028630045, acc: 0.7246513826518554
epoch: 110, train loss: 0.36511510420765725, acc: 0.8064993488812596; test loss: 0.6322300061072843, acc: 0.7125974946821082
epoch: 111, train loss: 0.3684707769518338, acc: 0.8015863620220196; test loss: 0.629420148089433, acc: 0.722051524462302
epoch: 112, train loss: 0.3630027744499652, acc: 0.805374689238783; test loss: 0.6019403293774613, acc: 0.7359962183880879
epoch: 113, train loss: 0.37146739329517814, acc: 0.801349591570972; test loss: 0.7471493380877234, acc: 0.688489718742614
epoch: 114, train loss: 0.3758336204924752, acc: 0.7956671007458269; test loss: 0.5949613231486729, acc: 0.7291420467974474
epoch: 115, train loss: 0.35559653207735575, acc: 0.8053154966260211; test loss: 0.6144315887077907, acc: 0.7329236587095249
epoch: 116, train loss: 0.36641755442908913, acc: 0.8033029477921155; test loss: 0.673740026038523, acc: 0.696289293311274
epoch: 117, train loss: 0.3554623646328008, acc: 0.8062625784302119; test loss: 0.6353436295965326, acc: 0.7289056960529425
epoch: 118, train loss: 0.3495118742131955, acc: 0.8083935124896413; test loss: 0.6252611028587415, acc: 0.7227605766958166
epoch: 119, train loss: 0.3540590326645197, acc: 0.8043092222090683; test loss: 0.7480222712282142, acc: 0.6856535098085559
epoch: 120, train loss: 0.358567220791848, acc: 0.8055522670770687; test loss: 0.6182406168969014, acc: 0.7241786811628457
epoch: 121, train loss: 0.3451303423467128, acc: 0.8093997869065941; test loss: 0.6661841967403959, acc: 0.7066887260694871
epoch: 122, train loss: 0.3513888772336692, acc: 0.8051379187877353; test loss: 0.6316304500433774, acc: 0.7225242259513117
epoch: 123, train loss: 0.3467652459842885, acc: 0.8088078607789748; test loss: 0.6990097479683883, acc: 0.7047979201134483
epoch: 124, train loss: 0.3427268138817532, acc: 0.811057180063928; test loss: 0.641295498326382, acc: 0.720633419995273
epoch: 125, train loss: 0.33744235227231345, acc: 0.8116491061915473; test loss: 0.64493293757699, acc: 0.7109430394705744
epoch: 126, train loss: 0.3518388763509362, acc: 0.8041316443707826; test loss: 0.7070346270799919, acc: 0.7021980619238951
epoch: 127, train loss: 0.33684254056732515, acc: 0.8111755652894519; test loss: 0.643393010175755, acc: 0.7258331363743796
epoch: 128, train loss: 0.3330522843955099, acc: 0.8160293595359299; test loss: 0.7155406698778262, acc: 0.7007799574568659
epoch: 129, train loss: 0.3385911929659608, acc: 0.8112939505149758; test loss: 0.7674147106180572, acc: 0.674781375561333
epoch: 130, train loss: 0.34743735084127814, acc: 0.8073872380726885; test loss: 0.655292750616441, acc: 0.706925076813992
epoch: 131, train loss: 0.33487294507588355, acc: 0.811885876642595; test loss: 0.6655050792527295, acc: 0.7173245095722052
epoch: 132, train loss: 0.3323881275606985, acc: 0.8161477447614538; test loss: 0.6486652824394643, acc: 0.7161427558496809
epoch: 133, train loss: 0.3295709253190127, acc: 0.8155558186338345; test loss: 0.7016529224353376, acc: 0.6913259276766722
epoch: 134, train loss: 0.33076722744062437, acc: 0.8153782407955488; test loss: 0.6444057775597515, acc: 0.7222878752068069
epoch: 135, train loss: 0.33216784598277815, acc: 0.8138392328637386; test loss: 0.6493169059992344, acc: 0.7149610021271567
epoch: 136, train loss: 0.3184159538711163, acc: 0.8164437078252634; test loss: 0.6403000955462202, acc: 0.7229969274403214
epoch: 137, train loss: 0.33676211251032145, acc: 0.8115307209660234; test loss: 0.659753081252518, acc: 0.7173245095722052
epoch: 138, train loss: 0.3235049999168078, acc: 0.8162661299869777; test loss: 0.6636278654097159, acc: 0.7170881588277003
epoch: 139, train loss: 0.31856806779297286, acc: 0.8188706049485024; test loss: 0.6789388972742003, acc: 0.7097612857480501
epoch: 140, train loss: 0.32494975575486823, acc: 0.8153190481827868; test loss: 0.6680140928071432, acc: 0.7109430394705744
epoch: 141, train loss: 0.33308775152301484, acc: 0.8108204096128803; test loss: 0.6318576493406375, acc: 0.7270148900969038
epoch: 142, train loss: 0.3182305654005227, acc: 0.8189297975612644; test loss: 0.6704249604603157, acc: 0.722051524462302
epoch: 143, train loss: 0.3030871913195136, acc: 0.8255001775778383; test loss: 0.6853975167766978, acc: 0.7125974946821082
epoch: 144, train loss: 0.3093654169222865, acc: 0.8211199242334557; test loss: 0.6532346561033374, acc: 0.7109430394705744
epoch: 145, train loss: 0.31017706726406596, acc: 0.8214750799100272; test loss: 0.7210582333855301, acc: 0.7196880170172536
epoch: 146, train loss: 0.3052888540095714, acc: 0.8221261986504085; test loss: 0.7821297896046414, acc: 0.6913259276766722
epoch: 147, train loss: 0.3097417086937928, acc: 0.8194033384633598; test loss: 0.6947366444671782, acc: 0.7033798156464193
epoch: 148, train loss: 0.30616366012259827, acc: 0.8218894281993607; test loss: 0.7305391410146023, acc: 0.7040888678799339
epoch: 149, train loss: 0.29185799885123553, acc: 0.8266840298330769; test loss: 0.7286276389058671, acc: 0.7135428976601277
Epoch   149: reducing learning rate of group 0 to 7.5000e-04.
epoch: 150, train loss: 0.2581123723685424, acc: 0.8449745471765123; test loss: 0.6542529715795208, acc: 0.7336327109430395
epoch: 151, train loss: 0.23338600162981643, acc: 0.8577009589203267; test loss: 0.6699255465390523, acc: 0.7341054124320492
epoch: 152, train loss: 0.23451570925013165, acc: 0.8568130697288978; test loss: 0.6989478977880025, acc: 0.734341763176554
epoch: 153, train loss: 0.22930485488035643, acc: 0.8587072333372795; test loss: 0.6910004011143807, acc: 0.74048688253368
epoch: 154, train loss: 0.22517733681804544, acc: 0.8626731383923286; test loss: 0.6921251894198701, acc: 0.7322146064760104
epoch: 155, train loss: 0.2196347400593704, acc: 0.8621995974902332; test loss: 0.6799318199883508, acc: 0.726778539352399
epoch: 156, train loss: 0.21932077268651212, acc: 0.8634426423582336; test loss: 0.6909749788529088, acc: 0.7350508154100686
epoch: 157, train loss: 0.22186058830252425, acc: 0.8597135077542323; test loss: 0.7328129885636783, acc: 0.7170881588277003
epoch: 158, train loss: 0.23191477067444866, acc: 0.8562803362140405; test loss: 0.7439395709117799, acc: 0.7177972110612149
epoch: 159, train loss: 0.22624414437188614, acc: 0.8577601515330887; test loss: 0.7088662953670919, acc: 0.735759867643583
epoch: 160, train loss: 0.21822836502594425, acc: 0.8635018349709956; test loss: 0.7083233312860614, acc: 0.7303238005199716
epoch: 161, train loss: 0.22303255020732413, acc: 0.8584112702734699; test loss: 0.7386589498278717, acc: 0.720633419995273
epoch: 162, train loss: 0.21712838153705324, acc: 0.8611341304605186; test loss: 0.7217602217008644, acc: 0.7248877333963601
epoch: 163, train loss: 0.2172159222797542, acc: 0.8595359299159465; test loss: 0.7160218343529626, acc: 0.7348144646655637
epoch: 164, train loss: 0.22328079987118424, acc: 0.8619036344264236; test loss: 0.7231992576962468, acc: 0.7274875915859135
epoch: 165, train loss: 0.21750315618794444, acc: 0.8606013969456612; test loss: 0.792494611293817, acc: 0.7104703379815647
epoch: 166, train loss: 0.2158340286598892, acc: 0.862377175328519; test loss: 0.7390610809389284, acc: 0.7324509572205152
epoch: 167, train loss: 0.20368089784358795, acc: 0.8675861252515686; test loss: 0.7510935221967109, acc: 0.726778539352399
epoch: 168, train loss: 0.21152688655484086, acc: 0.8628507162306144; test loss: 0.7562705507461017, acc: 0.7279602930749232
epoch: 169, train loss: 0.21894101350282344, acc: 0.8606605895584231; test loss: 0.7425851956858642, acc: 0.7258331363743796
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.17762048249486298, acc: 0.8552148691843258; test loss: 0.6049740723024105, acc: 0.7260694871188844
epoch: 171, train loss: 0.1568422518715543, acc: 0.8651000355155677; test loss: 0.6576932188862183, acc: 0.723705979673836
epoch: 172, train loss: 0.15552931316342455, acc: 0.8654551911921392; test loss: 0.648647486445076, acc: 0.7215788229732923
epoch: 173, train loss: 0.15041490738506472, acc: 0.8710784894045223; test loss: 0.6503823912191605, acc: 0.7293783975419522
epoch: 174, train loss: 0.15467608539863664, acc: 0.8653368059666153; test loss: 0.6578479036037252, acc: 0.7307965020089813
epoch: 175, train loss: 0.15242319684896835, acc: 0.8671717769622351; test loss: 0.657954478680848, acc: 0.7232332781848263
epoch: 176, train loss: 0.14721799444888375, acc: 0.8674085474132828; test loss: 0.681729421804938, acc: 0.722051524462302
epoch: 177, train loss: 0.14855761674504117, acc: 0.8674677400260448; test loss: 0.6574665948984556, acc: 0.7248877333963601
epoch: 178, train loss: 0.1577432557254257, acc: 0.8649224576772819; test loss: 0.6830787013197475, acc: 0.7229969274403214
epoch: 179, train loss: 0.17965420980301497, acc: 0.8536166686397537; test loss: 0.6263333691741467, acc: 0.7286693453084377
epoch: 180, train loss: 0.15648423991563745, acc: 0.8606013969456612; test loss: 0.6326521652524169, acc: 0.7298510990309619
epoch: 181, train loss: 0.15009827549512306, acc: 0.8636202201965195; test loss: 0.6585853455957581, acc: 0.7296147482864571
epoch: 182, train loss: 0.1585761277622283, acc: 0.8601870486563277; test loss: 0.6689980496359447, acc: 0.7177972110612149
epoch: 183, train loss: 0.15177954970640134, acc: 0.8640345684858529; test loss: 0.6768975279004938, acc: 0.7218151737177972
epoch: 184, train loss: 0.15027743023154833, acc: 0.8675269326388066; test loss: 0.6448735322999605, acc: 0.7312692034979911
epoch: 185, train loss: 0.1503679210442939, acc: 0.8633834497454718; test loss: 0.6464044685990387, acc: 0.725124084140865
epoch: 186, train loss: 0.14908192128673367, acc: 0.8649224576772819; test loss: 0.6514976953282421, acc: 0.7293783975419522
epoch: 187, train loss: 0.15487836403996053, acc: 0.8618444418136617; test loss: 0.6602757042447773, acc: 0.7225242259513117
epoch: 188, train loss: 0.14146389787445776, acc: 0.8666982360601397; test loss: 0.6788109127327331, acc: 0.7187426140392342
epoch: 189, train loss: 0.15282286319105925, acc: 0.8640937610986149; test loss: 0.6974242989038131, acc: 0.7173245095722052
epoch: 190, train loss: 0.16229483067841166, acc: 0.858292885047946; test loss: 0.6183517773194788, acc: 0.7296147482864571
epoch: 191, train loss: 0.14303967099832166, acc: 0.8679412809281402; test loss: 0.6963847206997494, acc: 0.7234696289293311
epoch: 192, train loss: 0.14797386833128878, acc: 0.863205871907186; test loss: 0.6840867549920584, acc: 0.7215788229732923
epoch: 193, train loss: 0.14698590831995378, acc: 0.8651592281283296; test loss: 0.6861162865536626, acc: 0.7192153155282439
epoch: 194, train loss: 0.14389871802466345, acc: 0.8657511542559488; test loss: 0.6891348931108122, acc: 0.7203970692507682
epoch: 195, train loss: 0.14144903950448767, acc: 0.8690067479578548; test loss: 0.6850532037770599, acc: 0.7215788229732923
epoch: 196, train loss: 0.14510864551968866, acc: 0.8655735764176631; test loss: 0.6895831610626206, acc: 0.7118884424485937
epoch: 197, train loss: 0.1355656274161396, acc: 0.866461465609092; test loss: 0.6916342149584379, acc: 0.7270148900969038
epoch: 198, train loss: 0.13419338893548854, acc: 0.8703089854386172; test loss: 0.6928875259799727, acc: 0.7187426140392342
epoch: 199, train loss: 0.14414993566218898, acc: 0.8665206582218539; test loss: 0.6887730449064774, acc: 0.7265421886078941
epoch: 200, train loss: 0.14061021521851566, acc: 0.8659287320942346; test loss: 0.7030692283381978, acc: 0.7043252186244386
Epoch   200: reducing learning rate of group 0 to 3.7500e-04.
best test acc 0.74048688253368 at epoch 153.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.8778    0.9700    0.9216      6100
           1     0.9709    0.8639    0.9143       926
           2     0.8475    0.8754    0.8612      2400
           3     0.9173    0.8422    0.8782       843
           4     0.8819    0.9548    0.9169       774
           5     0.9296    0.9173    0.9234      1512
           6     0.7791    0.6947    0.7345      1330
           7     0.8323    0.8565    0.8443       481
           8     0.8392    0.7860    0.8117       458
           9     0.8431    0.9513    0.8940       452
          10     0.9334    0.8020    0.8627       717
          11     0.8711    0.7508    0.8065       333
          12     0.8333    0.0167    0.0328       299
          13     0.8387    0.6766    0.7490       269

    accuracy                         0.8756     16894
   macro avg     0.8711    0.7827    0.7965     16894
weighted avg     0.8752    0.8756    0.8665     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.7737    0.8767    0.8220      1525
           1     0.8817    0.7069    0.7847       232
           2     0.6925    0.7571    0.7234       601
           3     0.7957    0.7014    0.7456       211
           4     0.8021    0.7732    0.7874       194
           5     0.7962    0.7857    0.7909       378
           6     0.5719    0.5255    0.5477       333
           7     0.5984    0.6033    0.6008       121
           8     0.5641    0.5739    0.5690       115
           9     0.7250    0.7632    0.7436       114
          10     0.8143    0.6333    0.7125       180
          11     0.5323    0.3929    0.4521        84
          12     1.0000    0.0267    0.0519        75
          13     0.7111    0.4706    0.5664        68

    accuracy                         0.7405      4231
   macro avg     0.7328    0.6136    0.6356      4231
weighted avg     0.7446    0.7405    0.7311      4231

---------------------------------------
program finished.
