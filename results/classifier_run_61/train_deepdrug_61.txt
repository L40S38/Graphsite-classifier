seed:  21
save trained model at:  ../trained_models/trained_classifier_model_61.pt
save loss at:  ./results/train_classifier_results_61.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['3gruA00', '3fzpA00', '1ffuB00', '3cx8A00', '1qzrA01']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['3pcoC01', '5udrD00', '1w5fB00', '1r0yB00', '3lq3A01']
model architecture:
DeepDruG(
  (embedding_net): JKMCNMMEmbeddingNet(
    (conv0): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=33, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2ac7f120e8b0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.019760448988816, acc: 0.3943411862199597; test loss: 1.7576119435422388, acc: 0.46513826518553536
epoch: 2, train loss: 1.7285927293483245, acc: 0.4670297146916065; test loss: 1.808478277925903, acc: 0.43110375797683764
epoch: 3, train loss: 1.6224191699406798, acc: 0.4960340949449509; test loss: 1.5454707386870667, acc: 0.5079177499409123
epoch: 4, train loss: 1.543777012494777, acc: 0.525216053036581; test loss: 1.5685026900927752, acc: 0.5353344363034743
epoch: 5, train loss: 1.4897906894272879, acc: 0.5441576891203977; test loss: 1.5243041693417634, acc: 0.54195225714961
epoch: 6, train loss: 1.425954402346886, acc: 0.5669468450337398; test loss: 1.5934073347426623, acc: 0.5138265185535335
epoch: 7, train loss: 1.3846783437460177, acc: 0.5821001539007932; test loss: 1.5779646300052363, acc: 0.5412432049160955
epoch: 8, train loss: 1.3551800479208123, acc: 0.584704628862318; test loss: 1.4670767607741872, acc: 0.5398251004490664
epoch: 9, train loss: 1.302186574754707, acc: 0.605066887652421; test loss: 1.3310717299485482, acc: 0.5989127865752777
epoch: 10, train loss: 1.2479315308321013, acc: 0.6223511305789038; test loss: 1.3272252015204216, acc: 0.604585204443394
epoch: 11, train loss: 1.2175051755449031, acc: 0.6304605185272878; test loss: 1.2470054734094125, acc: 0.597021980619239
epoch: 12, train loss: 1.1669501484505902, acc: 0.6472120279389132; test loss: 1.3136436879508069, acc: 0.6041125029543843
epoch: 13, train loss: 1.143574104882619, acc: 0.6549070675979638; test loss: 1.1769265250037169, acc: 0.6294020326164027
epoch: 14, train loss: 1.1103013871731358, acc: 0.6683437906949212; test loss: 1.582629758960613, acc: 0.506972346962893
epoch: 15, train loss: 1.082789247820924, acc: 0.6776962235113058; test loss: 1.2524333627926658, acc: 0.618293547624675
epoch: 16, train loss: 1.0559012565130705, acc: 0.6778738013495915; test loss: 1.2194401060427595, acc: 0.6464192862207516
epoch: 17, train loss: 1.0257032933116057, acc: 0.6888244347105481; test loss: 1.2455484206669158, acc: 0.6246750177263058
epoch: 18, train loss: 1.0066181454162308, acc: 0.6967562448206464; test loss: 1.1816657416168552, acc: 0.644292129520208
epoch: 19, train loss: 1.0012692127112623, acc: 0.6997158754587427; test loss: 1.4810216328799655, acc: 0.5686598912786576
epoch: 20, train loss: 0.9792536776404811, acc: 0.7068781816029359; test loss: 1.0197477568917397, acc: 0.690144173954148
epoch: 21, train loss: 0.9636049512802418, acc: 0.7094234639516989; test loss: 1.2828291361846418, acc: 0.621366107303238
epoch: 22, train loss: 0.9861987439488411, acc: 0.7018468095181721; test loss: 1.227225553169512, acc: 0.6556369652564406
epoch: 23, train loss: 0.949285819133358, acc: 0.7121463241387475; test loss: 0.9389567746532299, acc: 0.7130701961711179
epoch: 24, train loss: 0.9004594499830298, acc: 0.7267077068781816; test loss: 1.0859101649239058, acc: 0.6613093831245569
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.7023699904978452, acc: 0.7295489522907541; test loss: 0.7868928944239011, acc: 0.6948711888442448
epoch: 26, train loss: 0.6840025559252955, acc: 0.7371847993370427; test loss: 0.7793012537648565, acc: 0.6941621366107303
epoch: 27, train loss: 0.661677492642891, acc: 0.7497336332425714; test loss: 0.9054334734495412, acc: 0.662491136847081
epoch: 28, train loss: 0.6888556108328525, acc: 0.7345803243755179; test loss: 1.0840290991360064, acc: 0.584731741904987
epoch: 29, train loss: 0.6870502059426098, acc: 0.735349828341423; test loss: 0.898837579561728, acc: 0.6771448830063814
epoch: 30, train loss: 0.6419798088409611, acc: 0.7533443826210489; test loss: 0.7622168658050739, acc: 0.7081068305365162
epoch: 31, train loss: 0.689668248352305, acc: 0.7354090209541849; test loss: 0.9545104844551564, acc: 0.6331836445284803
epoch: 32, train loss: 0.6605337836748892, acc: 0.7448206463833313; test loss: 0.7246495276527521, acc: 0.7274875915859135
epoch: 33, train loss: 0.6177593060353811, acc: 0.7624008523736238; test loss: 0.7962545650390214, acc: 0.7005436067123612
epoch: 34, train loss: 0.6058675796022863, acc: 0.7651237125606725; test loss: 0.7052441642966797, acc: 0.7326873079650201
epoch: 35, train loss: 0.642855963326494, acc: 0.7539363087486681; test loss: 0.7599481835880338, acc: 0.7135428976601277
epoch: 36, train loss: 0.5935097764804828, acc: 0.7719308630282941; test loss: 0.6391654189555421, acc: 0.7473410541243205
epoch: 37, train loss: 0.5837073359134367, acc: 0.7748313010536285; test loss: 0.742291906037192, acc: 0.7118884424485937
epoch: 38, train loss: 0.5870622798255368, acc: 0.7696815437433409; test loss: 0.8957080103437604, acc: 0.6667454502481683
epoch: 39, train loss: 0.5735673559435983, acc: 0.7771990055641056; test loss: 0.7810003161289483, acc: 0.71756086031671
epoch: 40, train loss: 0.5813302389154551, acc: 0.7713981295134367; test loss: 0.7173750525077616, acc: 0.7263058378633893
epoch: 41, train loss: 0.5469120359073606, acc: 0.7841837338700131; test loss: 0.8142112868526984, acc: 0.7055069723469629
epoch: 42, train loss: 0.5573150503701736, acc: 0.7824079554871552; test loss: 0.7204639275455498, acc: 0.7291420467974474
epoch: 43, train loss: 0.5427751829246804, acc: 0.7870841718953474; test loss: 0.7328331807464946, acc: 0.7348144646655637
epoch: 44, train loss: 0.5689747343123192, acc: 0.7783236652065822; test loss: 0.7691434568559777, acc: 0.7078704797920113
epoch: 45, train loss: 0.5394617371747124, acc: 0.7893334911803007; test loss: 0.7226685653818158, acc: 0.7296147482864571
epoch: 46, train loss: 0.5568214619464531, acc: 0.7812832958446786; test loss: 0.6540735728124782, acc: 0.755849680926495
epoch: 47, train loss: 0.5540887101438605, acc: 0.7848348526103942; test loss: 0.7063209227821388, acc: 0.7350508154100686
epoch: 48, train loss: 0.49672515543731866, acc: 0.8043684148218302; test loss: 0.7866703001665635, acc: 0.6896714724651383
epoch: 49, train loss: 0.5036925420868928, acc: 0.803717296081449; test loss: 0.7017299146749144, acc: 0.738832427322146
epoch: 50, train loss: 0.5138724914059436, acc: 0.7968509530010655; test loss: 0.6215081581182441, acc: 0.7634129047506499
epoch: 51, train loss: 0.5188259254394374, acc: 0.7973244939031608; test loss: 0.754340310266919, acc: 0.7144883006381471
epoch: 52, train loss: 0.5127049106930337, acc: 0.7989818870604949; test loss: 0.693694235669386, acc: 0.7364689198770976
epoch: 53, train loss: 0.48309893737999915, acc: 0.8104060613235469; test loss: 0.6309623756016493, acc: 0.7577404868825337
epoch: 54, train loss: 0.5060125613468024, acc: 0.8022966733751627; test loss: 0.6638983563874186, acc: 0.751122666036398
epoch: 55, train loss: 0.4869216571683162, acc: 0.8105244465490706; test loss: 0.6719576716282158, acc: 0.7478137556133302
epoch: 56, train loss: 0.47697732235336215, acc: 0.8142535811530721; test loss: 0.7734748636456252, acc: 0.7293783975419522
epoch: 57, train loss: 0.4559305045037943, acc: 0.8208239611696461; test loss: 0.6223154123780069, acc: 0.7624675017726306
epoch: 58, train loss: 0.45665444558056567, acc: 0.8194033384633598; test loss: 0.725413380433301, acc: 0.7326873079650201
epoch: 59, train loss: 0.4791822942161018, acc: 0.8130697288978336; test loss: 0.6940127702166188, acc: 0.7475774048688253
epoch: 60, train loss: 0.46263661254887556, acc: 0.8177459453060258; test loss: 0.6506870190208613, acc: 0.7433230914677381
epoch: 61, train loss: 0.4448036399270933, acc: 0.8224221617142181; test loss: 0.6509953373517023, acc: 0.7586858898605531
Epoch    61: reducing learning rate of group 0 to 1.5000e-03.
epoch: 62, train loss: 0.3705554150527393, acc: 0.8533798981887061; test loss: 0.603119278145693, acc: 0.7766485464429213
epoch: 63, train loss: 0.31874176370147783, acc: 0.8714336450810939; test loss: 0.5769252016087341, acc: 0.7934294493027653
epoch: 64, train loss: 0.3033435016260128, acc: 0.8764650171658577; test loss: 0.6035485556716756, acc: 0.7922476955802411
epoch: 65, train loss: 0.2971144263506133, acc: 0.8792470699656683; test loss: 0.6023154858296376, acc: 0.7865752777121248
epoch: 66, train loss: 0.2869874066758667, acc: 0.881969930152717; test loss: 0.6401350705005463, acc: 0.7830300165445521
epoch: 67, train loss: 0.2856586895441182, acc: 0.883508938084527; test loss: 0.6580106424569689, acc: 0.7872843299456393
epoch: 68, train loss: 0.276344974202998, acc: 0.8848703681780514; test loss: 0.6687994822067337, acc: 0.7823209643110376
epoch: 69, train loss: 0.35133290286245916, acc: 0.8566946845033739; test loss: 0.6210457626681778, acc: 0.7865752777121248
epoch: 70, train loss: 0.2884509622824888, acc: 0.8812004261868119; test loss: 0.639446876145401, acc: 0.7884660836681635
epoch: 71, train loss: 0.2792336668285997, acc: 0.8875932283651; test loss: 0.6766088537447449, acc: 0.7837390687780666
epoch: 72, train loss: 0.25854626898795485, acc: 0.8928021782881497; test loss: 0.6494115988601328, acc: 0.7868116284566297
epoch: 73, train loss: 0.26191484268366183, acc: 0.89250621522434; test loss: 0.7058419569590728, acc: 0.7733396360198534
epoch: 74, train loss: 0.26679364592630767, acc: 0.889250621522434; test loss: 0.6851929485079414, acc: 0.7667218151737178
epoch: 75, train loss: 0.2690296717366481, acc: 0.8903752811649106; test loss: 0.6730036986293987, acc: 0.7700307255967856
epoch: 76, train loss: 0.2556183388229798, acc: 0.8936308748668166; test loss: 0.8033439748480483, acc: 0.760340345072087
epoch: 77, train loss: 0.25198038213631524, acc: 0.8948147271220551; test loss: 0.718388141568066, acc: 0.7754667927203971
epoch: 78, train loss: 0.2706065608564038, acc: 0.8858766425950041; test loss: 0.6947765088594149, acc: 0.760340345072087
epoch: 79, train loss: 0.25797553835175396, acc: 0.8948147271220551; test loss: 0.7363202019071388, acc: 0.7806665090995036
epoch: 80, train loss: 0.24805926838943743, acc: 0.896531312892151; test loss: 0.6779667337125032, acc: 0.7879933821791538
epoch: 81, train loss: 0.2301341361254083, acc: 0.9014442997513911; test loss: 0.7644788014412602, acc: 0.7634129047506499
epoch: 82, train loss: 0.24897802331552882, acc: 0.8970640464070084; test loss: 0.6600029284834045, acc: 0.790829591113212
epoch: 83, train loss: 0.22802565586846354, acc: 0.9048182786788209; test loss: 0.8701690550082628, acc: 0.734341763176554
epoch: 84, train loss: 0.22473380997877537, acc: 0.9058837457085356; test loss: 0.6929137748174142, acc: 0.7879933821791538
epoch: 85, train loss: 0.2062733288158873, acc: 0.9142890967207292; test loss: 0.6986985507796938, acc: 0.7924840463247459
epoch: 86, train loss: 0.2007347817578033, acc: 0.9141115188824435; test loss: 0.7056671222933808, acc: 0.795320255258804
epoch: 87, train loss: 0.212051488133638, acc: 0.9113886586953948; test loss: 0.692929581191573, acc: 0.7835027180335618
epoch: 88, train loss: 0.23468260757066772, acc: 0.9038120042618681; test loss: 0.7413068500091438, acc: 0.7686126211297566
epoch: 89, train loss: 0.20786635469893555, acc: 0.9138155558186338; test loss: 0.6586592303356981, acc: 0.7957929567478138
epoch: 90, train loss: 0.21331649897499905, acc: 0.9151769859121581; test loss: 0.6941387979056193, acc: 0.7813755613330182
epoch: 91, train loss: 0.20512221906895578, acc: 0.9138747484313957; test loss: 0.7703414299166307, acc: 0.7733396360198534
epoch: 92, train loss: 0.2158026854441803, acc: 0.9111518882443471; test loss: 0.7335572552945816, acc: 0.7775939494209406
epoch: 93, train loss: 0.22573426277119266, acc: 0.9044631230022493; test loss: 0.6884852652852909, acc: 0.793902150791775
epoch: 94, train loss: 0.2010046186193963, acc: 0.9151177932993962; test loss: 0.6801340172573117, acc: 0.7922476955802411
epoch: 95, train loss: 0.195466178933643, acc: 0.9158281046525394; test loss: 0.652803927497981, acc: 0.7887024344126684
epoch: 96, train loss: 0.1817525159053807, acc: 0.9222209068308275; test loss: 0.6982245507800419, acc: 0.7929567478137556
epoch: 97, train loss: 0.17608407293076456, acc: 0.925950041434829; test loss: 0.7213635813172786, acc: 0.7915386433467265
epoch: 98, train loss: 0.17064123229471284, acc: 0.9290280572984492; test loss: 0.7229712043521929, acc: 0.7875206806901441
epoch: 99, train loss: 0.17453147147419298, acc: 0.9261276192731147; test loss: 0.7210088126401872, acc: 0.7884660836681635
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.15987915042498924, acc: 0.9097904581508228; test loss: 0.5728155273941498, acc: 0.7825573150555424
epoch: 101, train loss: 0.12980400686226923, acc: 0.9229312181839706; test loss: 0.5882530324821409, acc: 0.790829591113212
epoch: 102, train loss: 0.11929868901796957, acc: 0.9297383686515923; test loss: 0.6216080753100564, acc: 0.7794847553769795
epoch: 103, train loss: 0.11678987827821923, acc: 0.9301527169409258; test loss: 0.60120577630183, acc: 0.7924840463247459
epoch: 104, train loss: 0.12987319874604017, acc: 0.9238782999881615; test loss: 0.5860878507230503, acc: 0.7827936658000473
epoch: 105, train loss: 0.1392636727380713, acc: 0.9170119569077779; test loss: 0.5818438665490995, acc: 0.7901205388796975
epoch: 106, train loss: 0.13625001389415528, acc: 0.9194388540310169; test loss: 0.5654302093329595, acc: 0.7955566060033089
epoch: 107, train loss: 0.12320284825380673, acc: 0.9257724635965432; test loss: 0.664888641085362, acc: 0.7787757031434649
epoch: 108, train loss: 0.13338055283208705, acc: 0.9197940097075885; test loss: 0.5878683434567308, acc: 0.7913022926022217
epoch: 109, train loss: 0.12319312692641139, acc: 0.9274298567538771; test loss: 0.6451784936115551, acc: 0.7690853226187663
epoch: 110, train loss: 0.14598158634145034, acc: 0.9180182313247307; test loss: 0.6181668995176751, acc: 0.7723942330418341
epoch: 111, train loss: 0.10991499267013112, acc: 0.9313957618089262; test loss: 0.6041617171754794, acc: 0.7957929567478138
epoch: 112, train loss: 0.11864586697007773, acc: 0.9305670652302592; test loss: 0.5638809398510473, acc: 0.795320255258804
Epoch   112: reducing learning rate of group 0 to 7.5000e-04.
epoch: 113, train loss: 0.08313417210566358, acc: 0.9485024268971233; test loss: 0.5867649286076747, acc: 0.8123375088631529
epoch: 114, train loss: 0.05352508735537487, acc: 0.9651947436959868; test loss: 0.61294953049962, acc: 0.8097376506735996
epoch: 115, train loss: 0.04860458503674301, acc: 0.9686279152361785; test loss: 0.640290170633266, acc: 0.8012290238714252
epoch: 116, train loss: 0.048575056754506886, acc: 0.966615366402273; test loss: 0.6581691808662243, acc: 0.798392814937367
epoch: 117, train loss: 0.04702778799123943, acc: 0.968213566946845; test loss: 0.6650622323229363, acc: 0.8069014417395415
epoch: 118, train loss: 0.04509544795275624, acc: 0.9704036936190363; test loss: 0.6534918214772729, acc: 0.8128102103521626
epoch: 119, train loss: 0.04869024636056545, acc: 0.9669705220788446; test loss: 0.6795172317460253, acc: 0.7991018671708816
epoch: 120, train loss: 0.052674151835754746, acc: 0.9656682845980822; test loss: 0.6652087073212283, acc: 0.8043015835499882
epoch: 121, train loss: 0.047964916803225885, acc: 0.9677400260447496; test loss: 0.6820612721227918, acc: 0.8038288820609785
epoch: 122, train loss: 0.05635126515194497, acc: 0.9640700840535101; test loss: 0.677227628481808, acc: 0.7991018671708816
epoch: 123, train loss: 0.05160769576594239, acc: 0.9636557357641766; test loss: 0.6883475826412644, acc: 0.7955566060033089
epoch: 124, train loss: 0.04952521340568044, acc: 0.9671480999171304; test loss: 0.7019347386900006, acc: 0.7993382179153864
epoch: 125, train loss: 0.05038748763133834, acc: 0.9669113294660826; test loss: 0.723180240926605, acc: 0.7986291656818719
epoch: 126, train loss: 0.08237813678239879, acc: 0.9488575825736948; test loss: 0.7031481987870464, acc: 0.7839754195225715
epoch: 127, train loss: 0.07235834751345638, acc: 0.95270510240322; test loss: 0.6345051854700942, acc: 0.7991018671708816
epoch: 128, train loss: 0.05711469505561292, acc: 0.9604593346750325; test loss: 0.6752974257573989, acc: 0.7974474119593477
epoch: 129, train loss: 0.0622391219305875, acc: 0.9592162898070321; test loss: 0.6619190194313196, acc: 0.8035925313164737
epoch: 130, train loss: 0.04902544421531596, acc: 0.9667929442405587; test loss: 0.6703401831300285, acc: 0.8052469865280075
epoch: 131, train loss: 0.04731417948326529, acc: 0.9671480999171304; test loss: 0.6919822443238783, acc: 0.8002836208934058
epoch: 132, train loss: 0.040576318777040064, acc: 0.9709956197466556; test loss: 0.7220212822686243, acc: 0.7905932403687072
epoch: 133, train loss: 0.048093720566694365, acc: 0.9654315141470344; test loss: 0.7253534932194013, acc: 0.7927203970692508
epoch: 134, train loss: 0.07336800124501072, acc: 0.9538297620456967; test loss: 0.6417320811756957, acc: 0.7998109194043961
epoch: 135, train loss: 0.05518450633589299, acc: 0.9637149283769385; test loss: 0.6868683524065281, acc: 0.7993382179153864
epoch: 136, train loss: 0.052806740921514196, acc: 0.9643068545045579; test loss: 0.6495821809295333, acc: 0.8040652328054834
epoch: 137, train loss: 0.034593255312349705, acc: 0.9757902213803717; test loss: 0.6982427054447813, acc: 0.80146537461593
epoch: 138, train loss: 0.03617733662719344, acc: 0.9753166804782764; test loss: 0.7801687911846654, acc: 0.7839754195225715
epoch: 139, train loss: 0.05754556905200838, acc: 0.9608144903516042; test loss: 0.6977730886898433, acc: 0.795320255258804
epoch: 140, train loss: 0.05665007425198488, acc: 0.9643660471173198; test loss: 0.6627967039537441, acc: 0.8040652328054834
epoch: 141, train loss: 0.051983845233740826, acc: 0.9658458624363679; test loss: 0.679308112139286, acc: 0.8035925313164737
epoch: 142, train loss: 0.08902663208047666, acc: 0.9459571445483603; test loss: 0.6804687555336101, acc: 0.7780666509099504
epoch: 143, train loss: 0.07152435665118197, acc: 0.9537705694329348; test loss: 0.6414212872023505, acc: 0.8028834790829591
epoch: 144, train loss: 0.0549544242953914, acc: 0.9618799573813188; test loss: 0.6530752640417411, acc: 0.7976837627038526
epoch: 145, train loss: 0.05265776322881357, acc: 0.9667337516277968; test loss: 0.6991394542684174, acc: 0.7915386433467265
epoch: 146, train loss: 0.05213246396978155, acc: 0.9653131289215106; test loss: 0.6612242768769342, acc: 0.8026471283384543
epoch: 147, train loss: 0.045135089793956196, acc: 0.9702853083935125; test loss: 0.7217942376814112, acc: 0.8024107775939494
epoch: 148, train loss: 0.06496921430501879, acc: 0.9588019415176986; test loss: 0.6583934357137552, acc: 0.8007563223824155
epoch: 149, train loss: 0.09960284492994774, acc: 0.9365455191192139; test loss: 0.6203111620609768, acc: 0.7967383597258332
epoch: 150, train loss: 0.05693734546203126, acc: 0.9628862317982716; test loss: 0.6796793937683105, acc: 0.7967383597258332
epoch: 151, train loss: 0.04738406390454468, acc: 0.9686279152361785; test loss: 0.6931238370256146, acc: 0.7972110612148429
epoch: 152, train loss: 0.0463281624452227, acc: 0.9700485379424648; test loss: 0.6878955603153027, acc: 0.8026471283384543
epoch: 153, train loss: 0.041652213901588304, acc: 0.9708180419083698; test loss: 0.6595112416119306, acc: 0.812101158118648
epoch: 154, train loss: 0.02946447144531845, acc: 0.9795785485971351; test loss: 0.7047692932042352, acc: 0.8019380761049397
epoch: 155, train loss: 0.04725875093874966, acc: 0.9669705220788446; test loss: 0.6885470944751495, acc: 0.8050106357835027
epoch: 156, train loss: 0.042490454149186944, acc: 0.9705220788445602; test loss: 0.697440873587619, acc: 0.8066650909950366
epoch: 157, train loss: 0.03591092809641363, acc: 0.9769148810228484; test loss: 0.7680054493362924, acc: 0.7946112030252895
epoch: 158, train loss: 0.0712838743670997, acc: 0.9530010654670297; test loss: 0.6755652882984642, acc: 0.7981564641928622
epoch: 159, train loss: 0.0474873000833746, acc: 0.9681543743340831; test loss: 0.6973046867088403, acc: 0.80146537461593
epoch: 160, train loss: 0.038193862043330834, acc: 0.973126553806085; test loss: 0.7522419839005638, acc: 0.7922476955802411
epoch: 161, train loss: 0.04255893610363157, acc: 0.971528353261513; test loss: 0.6769167220612221, acc: 0.8087922476955802
epoch: 162, train loss: 0.03842461309825173, acc: 0.9746655617378951; test loss: 0.6791189876579838, acc: 0.8087922476955802
epoch: 163, train loss: 0.03634823872718258, acc: 0.9754942583165621; test loss: 0.6753937429470025, acc: 0.8113921058851336
Epoch   163: reducing learning rate of group 0 to 3.7500e-04.
epoch: 164, train loss: 0.02154841926764744, acc: 0.9857345803243756; test loss: 0.6818699372121837, acc: 0.8128102103521626
epoch: 165, train loss: 0.016977045870280993, acc: 0.9883390552859003; test loss: 0.7105681948052093, acc: 0.8125738596076577
epoch: 166, train loss: 0.014661889575528537, acc: 0.9908251450219012; test loss: 0.7161377681623083, acc: 0.8170645237532498
epoch: 167, train loss: 0.01337010040614638, acc: 0.9920089972771398; test loss: 0.7311959861722913, acc: 0.8163554715197353
epoch: 168, train loss: 0.011290866237610483, acc: 0.993429619983426; test loss: 0.7267137018396228, acc: 0.815173717797211
epoch: 169, train loss: 0.010333882524980654, acc: 0.9940807387238073; test loss: 0.7360748511571125, acc: 0.8144646655636966
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.008963120861180287, acc: 0.9912394933112347; test loss: 0.6850393097103699, acc: 0.8076104939730561
epoch: 171, train loss: 0.007074355052573939, acc: 0.9929560790813307; test loss: 0.6451354049616349, acc: 0.8170645237532498
epoch: 172, train loss: 0.006401222743915586, acc: 0.9941991239493311; test loss: 0.6482044740423077, acc: 0.8132829118411723
epoch: 173, train loss: 0.008947081953407082, acc: 0.991890612051616; test loss: 0.6308210208832414, acc: 0.8118648073741432
epoch: 174, train loss: 0.006936796025306999, acc: 0.9937255830472357; test loss: 0.629523631388683, acc: 0.8123375088631529
epoch: 175, train loss: 0.006900048549606209, acc: 0.9941399313365692; test loss: 0.6326567565991901, acc: 0.8092649491845899
epoch: 176, train loss: 0.00734109920970509, acc: 0.9930744643068545; test loss: 0.6196278257309137, acc: 0.8116284566296383
epoch: 177, train loss: 0.006758634065019547, acc: 0.9949686279152362; test loss: 0.6178920668255097, acc: 0.8111557551406287
epoch: 178, train loss: 0.005916830380169918, acc: 0.9949686279152362; test loss: 0.6293293690179939, acc: 0.810683053651619
epoch: 179, train loss: 0.006063024296138731, acc: 0.994376701787617; test loss: 0.6218341968775754, acc: 0.8139919640746869
epoch: 180, train loss: 0.007266426095279244, acc: 0.993429619983426; test loss: 0.6262828863363011, acc: 0.813755613330182
epoch: 181, train loss: 0.009943683209552403, acc: 0.9915946489878064; test loss: 0.6288985395172306, acc: 0.8118648073741432
epoch: 182, train loss: 0.01163061290698897, acc: 0.9873327808689476; test loss: 0.6277226864432252, acc: 0.8085558969510754
epoch: 183, train loss: 0.012508959591792676, acc: 0.9889309814135195; test loss: 0.6256671343760354, acc: 0.8111557551406287
epoch: 184, train loss: 0.010789223791502404, acc: 0.9895229075411389; test loss: 0.6277509650223871, acc: 0.8092649491845899
epoch: 185, train loss: 0.015404527309659714, acc: 0.9865040842902806; test loss: 0.6400581210591049, acc: 0.8054833372725124
epoch: 186, train loss: 0.04543292792201945, acc: 0.9632413874748431; test loss: 0.5390282415718199, acc: 0.8057196880170172
epoch: 187, train loss: 0.02828549628858349, acc: 0.973955250384752; test loss: 0.5926505436544525, acc: 0.7948475537697943
epoch: 188, train loss: 0.016550938919542936, acc: 0.9822422161714218; test loss: 0.6144867736491377, acc: 0.8026471283384543
epoch: 189, train loss: 0.01073912255456282, acc: 0.9894637149283769; test loss: 0.6089355772309651, acc: 0.8095012999290948
epoch: 190, train loss: 0.011486437126272625, acc: 0.9886942109624719; test loss: 0.6258424890714739, acc: 0.8069014417395415
epoch: 191, train loss: 0.01768889788300508, acc: 0.9838996093287558; test loss: 0.6612757442127698, acc: 0.8005199716379107
epoch: 192, train loss: 0.01734357697855471, acc: 0.982952527524565; test loss: 0.6087570063641268, acc: 0.8092649491845899
epoch: 193, train loss: 0.01612320290337064, acc: 0.9856753877116136; test loss: 0.6261830861561125, acc: 0.8007563223824155
epoch: 194, train loss: 0.03206399182234556, acc: 0.9725938202912277; test loss: 0.6111777645001539, acc: 0.7943748522807846
epoch: 195, train loss: 0.024190653949699006, acc: 0.978039540665325; test loss: 0.5963307170263844, acc: 0.8038288820609785
epoch: 196, train loss: 0.012958552799717596, acc: 0.9876879365455191; test loss: 0.6286047507729887, acc: 0.8050106357835027
epoch: 197, train loss: 0.011121405505834228, acc: 0.9888717888007577; test loss: 0.6235051764520002, acc: 0.8047742850389978
epoch: 198, train loss: 0.009653736091251634, acc: 0.9908843376346632; test loss: 0.6358386882510599, acc: 0.8116284566296383
epoch: 199, train loss: 0.0080887030599972, acc: 0.9925417307919971; test loss: 0.6384081419859335, acc: 0.8085558969510754
epoch: 200, train loss: 0.00992705407507768, acc: 0.9904107967325678; test loss: 0.6528989410490551, acc: 0.8009926731269204
best test acc 0.8170645237532498 at epoch 166.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9992    0.9998    0.9995      6100
           1     0.9957    0.9957    0.9957       926
           2     0.9963    0.9979    0.9971      2400
           3     0.9964    0.9988    0.9976       843
           4     0.9935    0.9948    0.9942       774
           5     0.9987    0.9987    0.9987      1512
           6     0.9970    0.9962    0.9966      1330
           7     0.9979    1.0000    0.9990       481
           8     0.9956    0.9913    0.9934       458
           9     0.9956    1.0000    0.9978       452
          10     1.0000    0.9958    0.9979       717
          11     1.0000    0.9940    0.9970       333
          12     0.9797    0.9699    0.9748       299
          13     0.9962    0.9851    0.9907       269

    accuracy                         0.9974     16894
   macro avg     0.9958    0.9942    0.9950     16894
weighted avg     0.9974    0.9974    0.9974     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8810    0.8977    0.8892      1525
           1     0.8608    0.8793    0.8699       232
           2     0.8342    0.8369    0.8355       601
           3     0.8431    0.8152    0.8289       211
           4     0.8763    0.8763    0.8763       194
           5     0.8915    0.8042    0.8456       378
           6     0.5681    0.6517    0.6070       333
           7     0.7857    0.8182    0.8016       121
           8     0.6754    0.6696    0.6725       115
           9     0.9184    0.7895    0.8491       114
          10     0.8385    0.7500    0.7918       180
          11     0.8548    0.6310    0.7260        84
          12     0.2000    0.2667    0.2286        75
          13     0.8000    0.6471    0.7154        68

    accuracy                         0.8171      4231
   macro avg     0.7734    0.7381    0.7527      4231
weighted avg     0.8244    0.8171    0.8195      4231

---------------------------------------
program finished.
