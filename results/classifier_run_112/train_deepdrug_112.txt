seed:  12
save trained model at:  ../trained_models/trained_classifier_model_112.pt
save loss at:  ./results/train_classifier_results_112.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['2hxsA00', '5cr2B00', '3lf2C00', '5f8eA00', '5yh3A00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['2fhiA00', '2yyeB00', '3nd6E00', '6cauA00', '3ijpA00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b0b75ec4610>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0419181599142258, acc: 0.39197348170948265; test loss: 1.8925675201348395, acc: 0.41385015362798394
epoch: 2, train loss: 1.7537483261000466, acc: 0.46040014206227065; test loss: 1.5860363005239386, acc: 0.49350035452611674
epoch: 3, train loss: 1.6349832835360592, acc: 0.4928376938558068; test loss: 1.570180716342155, acc: 0.5095722051524463
epoch: 4, train loss: 1.5791176851930118, acc: 0.5174026281520067; test loss: 1.5736666396277872, acc: 0.5209170408886789
epoch: 5, train loss: 1.5167493698680783, acc: 0.5379424647803954; test loss: 1.5220744436273506, acc: 0.5426613093831245
epoch: 6, train loss: 1.4966555733193558, acc: 0.5459334675032556; test loss: 1.5257297750624943, acc: 0.5275348617348145
epoch: 7, train loss: 1.468857865736049, acc: 0.5553450929324021; test loss: 1.425067931646096, acc: 0.5589695107539588
epoch: 8, train loss: 1.433067052226328, acc: 0.5628625547531668; test loss: 1.4294308351022194, acc: 0.5558969510753959
epoch: 9, train loss: 1.4358624462772576, acc: 0.5632769030425003; test loss: 1.3726139255686873, acc: 0.5807137792484046
epoch: 10, train loss: 1.3666832710816843, acc: 0.5845270510240322; test loss: 1.364071650923186, acc: 0.5906405105176081
epoch: 11, train loss: 1.3836718672928168, acc: 0.5759441221735527; test loss: 1.3362730694448264, acc: 0.5906405105176081
epoch: 12, train loss: 1.3365622176719838, acc: 0.587960222564224; test loss: 1.3807725482053492, acc: 0.5774048688253368
epoch: 13, train loss: 1.319284220976393, acc: 0.5945306025807979; test loss: 1.3218887636098589, acc: 0.586386197116521
epoch: 14, train loss: 1.3084228328519902, acc: 0.5992068189889902; test loss: 1.2998327163059755, acc: 0.5963129283857244
epoch: 15, train loss: 1.285445088463534, acc: 0.6098022966733752; test loss: 1.2668727259634798, acc: 0.6154573386906169
epoch: 16, train loss: 1.299060249111279, acc: 0.6039422280099443; test loss: 1.5233257320636666, acc: 0.5589695107539588
epoch: 17, train loss: 1.272599986054791, acc: 0.6112821119924233; test loss: 1.2961135668496493, acc: 0.612148428267549
epoch: 18, train loss: 1.23278021995903, acc: 0.6222327453533799; test loss: 1.239301966989725, acc: 0.6230205625147719
epoch: 19, train loss: 1.2189899251575906, acc: 0.6273825026636676; test loss: 1.2203898519467755, acc: 0.63365634601749
epoch: 20, train loss: 1.2251274067367395, acc: 0.6251331833787144; test loss: 1.1999197252476872, acc: 0.63365634601749
epoch: 21, train loss: 1.1957577385817164, acc: 0.6370900911566236; test loss: 1.2840985213856526, acc: 0.6133301819900733
epoch: 22, train loss: 1.1772199343178587, acc: 0.6409968035989109; test loss: 1.1705924268874455, acc: 0.6376743086740724
epoch: 23, train loss: 1.1740485302750225, acc: 0.6434237007221498; test loss: 1.2326967107069122, acc: 0.612148428267549
epoch: 24, train loss: 1.1587337460110876, acc: 0.6445483603646265; test loss: 1.221617154493537, acc: 0.6159300401796266
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9222974563344944, acc: 0.6467976796495797; test loss: 0.9231861810880303, acc: 0.647364689198771
epoch: 26, train loss: 0.9056623315294635, acc: 0.6529537113768201; test loss: 0.9025219983452744, acc: 0.6613093831245569
epoch: 27, train loss: 0.9004910891403514, acc: 0.6545519119213922; test loss: 0.8550073491346467, acc: 0.6780902859844008
epoch: 28, train loss: 0.8839214997304549, acc: 0.6597016692316798; test loss: 0.9193280038470721, acc: 0.6556369652564406
epoch: 29, train loss: 0.8809159446061822, acc: 0.6644370782526341; test loss: 0.8528903868428868, acc: 0.6773812337508863
epoch: 30, train loss: 0.8693990397202104, acc: 0.6639635373505387; test loss: 1.0272042990801493, acc: 0.6225478610257622
epoch: 31, train loss: 0.8685891878623235, acc: 0.6657393157333965; test loss: 1.1205443512658029, acc: 0.5956038761522099
epoch: 32, train loss: 0.864096397526295, acc: 0.6644370782526341; test loss: 1.0105185678342758, acc: 0.6216024580477428
epoch: 33, train loss: 0.8456461667464135, acc: 0.6739670889073044; test loss: 0.8755465500519873, acc: 0.6752540770503427
epoch: 34, train loss: 0.8326209586330232, acc: 0.6768675269326389; test loss: 1.1640550087766708, acc: 0.5452611675726778
epoch: 35, train loss: 0.8316407101640084, acc: 0.6795903871196874; test loss: 0.9510643816366176, acc: 0.6438194280311983
epoch: 36, train loss: 0.8371819424759047, acc: 0.6758020599029241; test loss: 0.8699285214856228, acc: 0.6683999054597022
epoch: 37, train loss: 0.8151209619576739, acc: 0.6817805137918788; test loss: 0.8185517310081885, acc: 0.6821082486409832
epoch: 38, train loss: 0.8116414660925948, acc: 0.6792352314431159; test loss: 0.8368685818597572, acc: 0.6851808083195462
epoch: 39, train loss: 0.7945048145546749, acc: 0.6855096483958802; test loss: 0.932735211007901, acc: 0.6523280548333728
epoch: 40, train loss: 0.7880661133564307, acc: 0.6896531312892151; test loss: 0.9043701995605274, acc: 0.6587095249350036
epoch: 41, train loss: 0.7894066252115141, acc: 0.6876405824553096; test loss: 1.0414167716130667, acc: 0.6390924131411014
epoch: 42, train loss: 0.812847568775135, acc: 0.6836154847874986; test loss: 1.0826476960282494, acc: 0.6008035925313164
epoch: 43, train loss: 0.8122504650311624, acc: 0.6794720018941636; test loss: 0.8305184575745581, acc: 0.6766721815173717
epoch: 44, train loss: 0.7784106530662795, acc: 0.6933822658932165; test loss: 0.8551449727234449, acc: 0.677853935239896
epoch: 45, train loss: 0.767022503668477, acc: 0.6983544453652184; test loss: 0.8146068044543915, acc: 0.6896714724651383
epoch: 46, train loss: 0.7574763389272747, acc: 0.7008997277139813; test loss: 0.8428906859080297, acc: 0.6799810919404397
epoch: 47, train loss: 0.7503860964780166, acc: 0.7036225879010299; test loss: 0.857364032038051, acc: 0.6712361143937603
epoch: 48, train loss: 0.7661354141121322, acc: 0.6987687936545519; test loss: 0.8222814013112157, acc: 0.6804537934294493
epoch: 49, train loss: 0.7383008777311995, acc: 0.7080028412454126; test loss: 0.8235559225702251, acc: 0.6880170172536043
epoch: 50, train loss: 0.7293038219478097, acc: 0.710844086657985; test loss: 0.7784953525347564, acc: 0.7000709052233515
epoch: 51, train loss: 0.7080472854519647, acc: 0.7160530365810347; test loss: 0.9195670841490228, acc: 0.6589458756795084
epoch: 52, train loss: 0.7198497085609675, acc: 0.7114360127856043; test loss: 0.9005463690206644, acc: 0.662727487591586
epoch: 53, train loss: 0.7093586874132369, acc: 0.7107257014324613; test loss: 0.8713932030027962, acc: 0.6665090995036634
epoch: 54, train loss: 0.7120103133462852, acc: 0.7180063928021783; test loss: 0.9624723590764276, acc: 0.6575277712124793
epoch: 55, train loss: 0.7097869835281507, acc: 0.7195454007339884; test loss: 0.7890214069716278, acc: 0.6917986291656819
epoch: 56, train loss: 0.6975000575334769, acc: 0.7229785722741802; test loss: 0.8502729970832379, acc: 0.677853935239896
epoch: 57, train loss: 0.6845621307516883, acc: 0.728009944358944; test loss: 1.1661424730946002, acc: 0.5889860553060742
epoch: 58, train loss: 0.7034824285420268, acc: 0.7198413637977981; test loss: 0.7839592912467032, acc: 0.7036161663909242
epoch: 59, train loss: 0.6829231473995099, acc: 0.7252278915591335; test loss: 0.7623096021234327, acc: 0.7137792484046325
epoch: 60, train loss: 0.6731647577062223, acc: 0.7300224931928495; test loss: 0.7984069234455373, acc: 0.6974710470337981
epoch: 61, train loss: 0.6626840940703862, acc: 0.728838640937611; test loss: 0.8146705876510213, acc: 0.6974710470337981
epoch: 62, train loss: 0.6693115845239698, acc: 0.72635255120161; test loss: 0.7405837971784915, acc: 0.7166154573386906
epoch: 63, train loss: 0.6718263220360847, acc: 0.7283651000355156; test loss: 0.8120721008613638, acc: 0.6986528007563224
epoch: 64, train loss: 0.6663050078154598, acc: 0.7344027465372321; test loss: 0.7567352711576797, acc: 0.711415740959584
epoch: 65, train loss: 0.6640585527005698, acc: 0.7332780868947555; test loss: 0.8025580428503727, acc: 0.7029071141574096
epoch: 66, train loss: 0.6645568217541941, acc: 0.7300816858056115; test loss: 0.8469012089582573, acc: 0.6847081068305365
epoch: 67, train loss: 0.662309254310929, acc: 0.7341067834734225; test loss: 0.8313079367741094, acc: 0.6941621366107303
epoch: 68, train loss: 0.6649415431198205, acc: 0.7339883982478986; test loss: 0.8415726981414348, acc: 0.6873079650200898
epoch: 69, train loss: 0.645346075434, acc: 0.7389605777199005; test loss: 0.8276362707079006, acc: 0.7033798156464193
epoch: 70, train loss: 0.6435663373043823, acc: 0.7392565407837102; test loss: 0.7811574749393233, acc: 0.7017253604348853
epoch: 71, train loss: 0.6351394705748691, acc: 0.7435184089025689; test loss: 0.7809927014418737, acc: 0.7118884424485937
epoch: 72, train loss: 0.6450941917232131, acc: 0.7404995856517107; test loss: 0.7891607918487996, acc: 0.7026707634129048
epoch: 73, train loss: 0.6340423310326587, acc: 0.7407955487155203; test loss: 0.7490372556908648, acc: 0.7095249350035453
Epoch    73: reducing learning rate of group 0 to 1.5000e-03.
epoch: 74, train loss: 0.5675506234112556, acc: 0.768793654551912; test loss: 0.6907256506656366, acc: 0.7397778303001654
epoch: 75, train loss: 0.5346997060852812, acc: 0.7779685095300106; test loss: 0.7278580670096412, acc: 0.7352871661545733
epoch: 76, train loss: 0.5339082292061353, acc: 0.7805729844915354; test loss: 0.7031317630069979, acc: 0.7369416213661073
epoch: 77, train loss: 0.5227531307235023, acc: 0.7841837338700131; test loss: 0.7503996255464853, acc: 0.723705979673836
epoch: 78, train loss: 0.5149864712043163, acc: 0.7846572747721084; test loss: 0.7678022411691751, acc: 0.7213424722287876
epoch: 79, train loss: 0.5195175323300606, acc: 0.7832366520658222; test loss: 0.6867878035263822, acc: 0.7501772630583786
epoch: 80, train loss: 0.5138628704527842, acc: 0.7860778974783947; test loss: 0.7365805640679335, acc: 0.7312692034979911
epoch: 81, train loss: 0.5198113045721233, acc: 0.7846572747721084; test loss: 0.7073204953516665, acc: 0.7393051288111557
epoch: 82, train loss: 0.5084095704805305, acc: 0.7888599502782053; test loss: 0.7078754832574984, acc: 0.7383597258331364
epoch: 83, train loss: 0.4990552215445207, acc: 0.7892151059547768; test loss: 0.7766946690270197, acc: 0.7154337036161664
epoch: 84, train loss: 0.5173998556010636, acc: 0.7812832958446786; test loss: 0.7230196886664444, acc: 0.7352871661545733
epoch: 85, train loss: 0.508010253685952, acc: 0.7879720610867764; test loss: 0.7226201529425105, acc: 0.7336327109430395
epoch: 86, train loss: 0.49186926860350383, acc: 0.7974428791286847; test loss: 0.7505216693742933, acc: 0.7232332781848263
epoch: 87, train loss: 0.4833708046081575, acc: 0.7989226944477329; test loss: 0.7071423342931752, acc: 0.743559442212243
epoch: 88, train loss: 0.47933071333444427, acc: 0.7963774120989701; test loss: 0.710757434635798, acc: 0.7475774048688253
epoch: 89, train loss: 0.4702693022425053, acc: 0.8009944358944003; test loss: 0.7091931254270369, acc: 0.7468683526353108
epoch: 90, train loss: 0.4673787310756536, acc: 0.8050787261749733; test loss: 0.7639279376246129, acc: 0.7260694871188844
epoch: 91, train loss: 0.4653235320312759, acc: 0.8033029477921155; test loss: 0.7158120217770726, acc: 0.7421413377452138
epoch: 92, train loss: 0.47762170239243973, acc: 0.8011128211199242; test loss: 0.7189690834014756, acc: 0.7331600094540298
epoch: 93, train loss: 0.46385727147668215, acc: 0.8020007103113531; test loss: 0.7331054592718612, acc: 0.7385960765776413
epoch: 94, train loss: 0.4883776599695884, acc: 0.7948384041671599; test loss: 0.7382624768049997, acc: 0.7322146064760104
epoch: 95, train loss: 0.4696452278942417, acc: 0.8072688528471647; test loss: 0.8155725718502512, acc: 0.7333963601985346
epoch: 96, train loss: 0.4585510035217112, acc: 0.8034213330176394; test loss: 0.7291689999304717, acc: 0.7421413377452138
epoch: 97, train loss: 0.4401903087830027, acc: 0.8141351959275482; test loss: 0.756528174558787, acc: 0.7376506735996219
epoch: 98, train loss: 0.45467658636766706, acc: 0.8057890375281165; test loss: 0.7353410940028285, acc: 0.7416686362562042
epoch: 99, train loss: 0.44988553569459966, acc: 0.8101692908724991; test loss: 0.7204307987458372, acc: 0.74048688253368
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3468399084473808, acc: 0.8134840771871671; test loss: 0.5790665235341342, acc: 0.7584495391160482
epoch: 101, train loss: 0.3321715838862295, acc: 0.8181602935953592; test loss: 0.628339073782524, acc: 0.7456865989127865
epoch: 102, train loss: 0.3358393496411135, acc: 0.8133656919616432; test loss: 0.9127472276468982, acc: 0.6691089576932168
epoch: 103, train loss: 0.36342090414489703, acc: 0.8036581034686872; test loss: 0.6254535485486054, acc: 0.7376506735996219
epoch: 104, train loss: 0.3367550927809149, acc: 0.8155558186338345; test loss: 0.6143443751802853, acc: 0.7414322855116994
epoch: 105, train loss: 0.3333693154643975, acc: 0.8153782407955488; test loss: 0.6358681287474734, acc: 0.738832427322146
epoch: 106, train loss: 0.3349527428809801, acc: 0.8130697288978336; test loss: 0.618274761779008, acc: 0.7411959347671945
epoch: 107, train loss: 0.33344547282073067, acc: 0.8168580561145969; test loss: 0.5903550568158901, acc: 0.7567950839045143
epoch: 108, train loss: 0.3185901622088037, acc: 0.8194033384633598; test loss: 0.6433647210308915, acc: 0.7352871661545733
epoch: 109, train loss: 0.34608376658751694, acc: 0.8089854386172606; test loss: 0.6189596470972799, acc: 0.7324509572205152
epoch: 110, train loss: 0.3306503288630439, acc: 0.8126553806085001; test loss: 0.6240066116060723, acc: 0.7367052706216024
epoch: 111, train loss: 0.3293060256811914, acc: 0.8137208476382147; test loss: 0.6224138728835796, acc: 0.7447411959347672
epoch: 112, train loss: 0.32476797822002434, acc: 0.8154966260210725; test loss: 0.6034211738823548, acc: 0.7499409123138738
epoch: 113, train loss: 0.32920297807428955, acc: 0.816798863501835; test loss: 0.6241961284552701, acc: 0.7350508154100686
epoch: 114, train loss: 0.324853163947717, acc: 0.8168580561145969; test loss: 0.6385040732315882, acc: 0.7333963601985346
epoch: 115, train loss: 0.3312930146187484, acc: 0.8170356339528827; test loss: 0.6352166960465375, acc: 0.7359962183880879
epoch: 116, train loss: 0.32373523424966416, acc: 0.8188706049485024; test loss: 0.6095265592702651, acc: 0.7497045615693689
epoch: 117, train loss: 0.3213172827660861, acc: 0.8179235231443116; test loss: 0.6426570467339202, acc: 0.7454502481682818
epoch: 118, train loss: 0.31434054852830345, acc: 0.8205871907185983; test loss: 0.6401711496386058, acc: 0.7494682108248641
epoch: 119, train loss: 0.3099945446958595, acc: 0.8215342725227892; test loss: 0.6341554338107631, acc: 0.7258331363743796
epoch: 120, train loss: 0.3098957677825719, acc: 0.8234876287439328; test loss: 0.6238146381044579, acc: 0.7475774048688253
epoch: 121, train loss: 0.3103405518038987, acc: 0.8199360719782172; test loss: 0.6881305206416488, acc: 0.7279602930749232
epoch: 122, train loss: 0.3113272136713014, acc: 0.8188706049485024; test loss: 0.6448329369302226, acc: 0.7222878752068069
epoch: 123, train loss: 0.3153973020532928, acc: 0.8181011009825974; test loss: 0.6291554196172221, acc: 0.7449775466792721
epoch: 124, train loss: 0.30968063646369365, acc: 0.8181602935953592; test loss: 0.7282597980327963, acc: 0.7130701961711179
Epoch   124: reducing learning rate of group 0 to 7.5000e-04.
epoch: 125, train loss: 0.2634550868738074, acc: 0.8411270273469871; test loss: 0.6119673237736358, acc: 0.757267785393524
epoch: 126, train loss: 0.24054869861945438, acc: 0.8513673493548005; test loss: 0.618027873789833, acc: 0.7612857480501064
epoch: 127, train loss: 0.22910789262829298, acc: 0.85995027820528; test loss: 0.6187979251435113, acc: 0.7586858898605531
epoch: 128, train loss: 0.22812299589313698, acc: 0.8604238191073754; test loss: 0.6477993463292187, acc: 0.7582131883715434
epoch: 129, train loss: 0.22616998879171707, acc: 0.8581744998224221; test loss: 0.6465306867524203, acc: 0.7593949420940675
epoch: 130, train loss: 0.2176860735143192, acc: 0.8637977980348053; test loss: 0.6661723166091367, acc: 0.7553769794374853
epoch: 131, train loss: 0.22108529636362284, acc: 0.8622587901029951; test loss: 0.7206959056955679, acc: 0.7416686362562042
epoch: 132, train loss: 0.22681046692283813, acc: 0.8596543151414704; test loss: 0.6536359691473362, acc: 0.7489955093358545
epoch: 133, train loss: 0.2153597339216853, acc: 0.8636202201965195; test loss: 0.665801642212116, acc: 0.7494682108248641
epoch: 134, train loss: 0.22363860954388198, acc: 0.8637977980348053; test loss: 0.6785906861223416, acc: 0.7518317182699126
epoch: 135, train loss: 0.2070610917406956, acc: 0.8649816502900438; test loss: 0.6953264572636734, acc: 0.7485228078468447
epoch: 136, train loss: 0.2193921596020005, acc: 0.8629099088433764; test loss: 0.6624340158747432, acc: 0.7549042779484756
epoch: 137, train loss: 0.21590086160093366, acc: 0.8639161832603292; test loss: 0.679578646309577, acc: 0.752540770503427
epoch: 138, train loss: 0.21505035627582617, acc: 0.8605422043328993; test loss: 0.663369433081367, acc: 0.7518317182699126
epoch: 139, train loss: 0.22471848155452442, acc: 0.8610157452349947; test loss: 0.6519462038117699, acc: 0.757267785393524
epoch: 140, train loss: 0.20754092612978664, acc: 0.8651592281283296; test loss: 0.6792396358833953, acc: 0.7494682108248641
epoch: 141, train loss: 0.2062170242640878, acc: 0.8684148218302356; test loss: 0.6827592421186587, acc: 0.7560860316709997
epoch: 142, train loss: 0.1997709551710793, acc: 0.8695394814727122; test loss: 0.6930898617582382, acc: 0.7454502481682818
epoch: 143, train loss: 0.20328319758596655, acc: 0.8668758138984255; test loss: 0.7587755034083143, acc: 0.7421413377452138
epoch: 144, train loss: 0.20153942963872928, acc: 0.8698354445365218; test loss: 0.7047067889083054, acc: 0.7426140392342235
epoch: 145, train loss: 0.20289232727302042, acc: 0.866461465609092; test loss: 0.7396174190287274, acc: 0.7419049870007091
epoch: 146, train loss: 0.19692778818788254, acc: 0.8722623416597609; test loss: 0.6930146026318306, acc: 0.7430867407232333
epoch: 147, train loss: 0.1956350119195493, acc: 0.8736829643660471; test loss: 0.6823706589982572, acc: 0.7539588749704561
epoch: 148, train loss: 0.19251169264845783, acc: 0.8714336450810939; test loss: 0.7102377092942536, acc: 0.7501772630583786
epoch: 149, train loss: 0.20326554862383456, acc: 0.8682964366047118; test loss: 0.7096499388901456, acc: 0.738832427322146
epoch: 150, train loss: 0.19948176118598487, acc: 0.8691843257961407; test loss: 0.7757110180682365, acc: 0.7378870243441267
epoch: 151, train loss: 0.19456704300127609, acc: 0.8695986740854741; test loss: 0.710344361684137, acc: 0.7426140392342235
epoch: 152, train loss: 0.19752585479701443, acc: 0.8723807268852847; test loss: 0.7530337929922873, acc: 0.7409595840226897
epoch: 153, train loss: 0.19985912252286772, acc: 0.869717059310998; test loss: 0.7143457158466683, acc: 0.748050106357835
epoch: 154, train loss: 0.1823943047336083, acc: 0.8780040250976678; test loss: 0.7823301433418074, acc: 0.7411959347671945
epoch: 155, train loss: 0.1816108766019281, acc: 0.8782407955487155; test loss: 0.6932265476198857, acc: 0.7508863152918932
epoch: 156, train loss: 0.19058409717092292, acc: 0.8759322836510004; test loss: 0.7012105619504474, acc: 0.7454502481682818
epoch: 157, train loss: 0.19385093583381288, acc: 0.8736829643660471; test loss: 0.744045867392482, acc: 0.7428503899787284
epoch: 158, train loss: 0.19985853721612273, acc: 0.8700130223748076; test loss: 0.7274631147152479, acc: 0.7253604348853699
epoch: 159, train loss: 0.1856057972308657, acc: 0.8774120989700486; test loss: 0.7068485276013282, acc: 0.7567950839045143
epoch: 160, train loss: 0.1815810135635269, acc: 0.8768793654551912; test loss: 0.7334827430083154, acc: 0.7440321437012527
epoch: 161, train loss: 0.21023331139321805, acc: 0.8627915236178525; test loss: 0.7329250931091777, acc: 0.7437957929567478
epoch: 162, train loss: 0.18875365532565092, acc: 0.8737421569788091; test loss: 0.6910915211057922, acc: 0.754195225714961
epoch: 163, train loss: 0.18293478049455175, acc: 0.8792470699656683; test loss: 0.7339246651433766, acc: 0.7390687780666509
epoch: 164, train loss: 0.1928974456659108, acc: 0.8678228957026163; test loss: 0.7521107132568395, acc: 0.7468683526353108
epoch: 165, train loss: 0.18470130388032788, acc: 0.8755771279744288; test loss: 0.7374628605613808, acc: 0.7478137556133302
epoch: 166, train loss: 0.17118687907612448, acc: 0.8845744051142418; test loss: 0.7142562252554987, acc: 0.752777121247932
epoch: 167, train loss: 0.17766546552648654, acc: 0.8813188114123357; test loss: 0.7234378249840769, acc: 0.7437957929567478
epoch: 168, train loss: 0.1727359077680516, acc: 0.8800757665443353; test loss: 0.7814691145632515, acc: 0.738832427322146
epoch: 169, train loss: 0.1777119371624152, acc: 0.8781816029359536; test loss: 0.7448540772424751, acc: 0.7499409123138738
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.12754345426541666, acc: 0.8834497454717651; test loss: 0.6744249326854247, acc: 0.7400141810446703
epoch: 171, train loss: 0.13248599457101481, acc: 0.8816739670889073; test loss: 0.6876430282128446, acc: 0.7329236587095249
epoch: 172, train loss: 0.13392105653513334, acc: 0.8771753285190008; test loss: 0.6326893999401318, acc: 0.7534861734814464
epoch: 173, train loss: 0.12290403721416379, acc: 0.8852255238546229; test loss: 0.6367899844111689, acc: 0.743559442212243
epoch: 174, train loss: 0.1190529645487756, acc: 0.8862317982715757; test loss: 0.6533836616857822, acc: 0.7551406286929804
epoch: 175, train loss: 0.11900081989242153, acc: 0.8833905528590031; test loss: 0.6654209679279224, acc: 0.7437957929567478
Epoch   175: reducing learning rate of group 0 to 3.7500e-04.
epoch: 176, train loss: 0.10338661108067632, acc: 0.8966496981176749; test loss: 0.6430731156114088, acc: 0.7551406286929804
epoch: 177, train loss: 0.09089099573827261, acc: 0.9065348644489167; test loss: 0.6958641213633664, acc: 0.751122666036398
epoch: 178, train loss: 0.08882750205610071, acc: 0.9069492127382502; test loss: 0.697691534152129, acc: 0.7584495391160482
epoch: 179, train loss: 0.08724260067970101, acc: 0.9057061678702498; test loss: 0.6863213602574095, acc: 0.7556133301819901
epoch: 180, train loss: 0.08760942417364848, acc: 0.906416479223393; test loss: 0.6903838123223709, acc: 0.7513590167809029
epoch: 181, train loss: 0.08572639604055544, acc: 0.9111518882443471; test loss: 0.6952082469541675, acc: 0.7537225242259513
epoch: 182, train loss: 0.08731595997917901, acc: 0.9073043684148219; test loss: 0.7243273181854425, acc: 0.7485228078468447
epoch: 183, train loss: 0.08584153096506804, acc: 0.9083698354445365; test loss: 0.7127073829096842, acc: 0.7494682108248641
epoch: 184, train loss: 0.08747483032073186, acc: 0.9081330649934888; test loss: 0.7093710821589463, acc: 0.751122666036398
epoch: 185, train loss: 0.08665992167608631, acc: 0.9110335030188232; test loss: 0.7054515765423865, acc: 0.7440321437012527
epoch: 186, train loss: 0.08280878868719184, acc: 0.912039777435776; test loss: 0.7221991721407738, acc: 0.7485228078468447
epoch: 187, train loss: 0.08041277106194662, acc: 0.9124541257251095; test loss: 0.7054309969786858, acc: 0.7549042779484756
epoch: 188, train loss: 0.07857817638812298, acc: 0.9120989700485379; test loss: 0.7035775406370206, acc: 0.7485228078468447
epoch: 189, train loss: 0.08300877806022842, acc: 0.9091393394104416; test loss: 0.7220473653795139, acc: 0.7501772630583786
epoch: 190, train loss: 0.08511256512218754, acc: 0.9083106428317745; test loss: 0.7126179088573595, acc: 0.7497045615693689
epoch: 191, train loss: 0.08337652910691726, acc: 0.9108559251805375; test loss: 0.7191889843346801, acc: 0.7489955093358545
epoch: 192, train loss: 0.08206193459920239, acc: 0.9131644370782527; test loss: 0.7394637946568604, acc: 0.7489955093358545
epoch: 193, train loss: 0.08159599357686044, acc: 0.9110335030188232; test loss: 0.7252398515756292, acc: 0.7506499645473883
epoch: 194, train loss: 0.08600345123226268, acc: 0.9078962945424411; test loss: 0.7467852593933373, acc: 0.7414322855116994
epoch: 195, train loss: 0.08718783557732543, acc: 0.9080738723807269; test loss: 0.7233258402395463, acc: 0.7475774048688253
epoch: 196, train loss: 0.08322050851365441, acc: 0.9100272286018705; test loss: 0.7189912579634938, acc: 0.7487591585913496
epoch: 197, train loss: 0.08243370836756561, acc: 0.9141707114952053; test loss: 0.7167619293222357, acc: 0.748050106357835
epoch: 198, train loss: 0.0849940842589754, acc: 0.9093169172487273; test loss: 0.7283335306548081, acc: 0.7492318600803592
epoch: 199, train loss: 0.08419065547808678, acc: 0.909612880312537; test loss: 0.7454990950128875, acc: 0.7402505317891751
epoch: 200, train loss: 0.08318834029983964, acc: 0.9098496507635847; test loss: 0.7193481444579944, acc: 0.7518317182699126
best test acc 0.7612857480501064 at epoch 126.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.8970    0.9552    0.9252      6100
           1     0.9478    0.9212    0.9343       926
           2     0.8622    0.9100    0.8855      2400
           3     0.9359    0.8138    0.8706       843
           4     0.9065    0.9522    0.9288       774
           5     0.9288    0.9233    0.9260      1512
           6     0.7432    0.7158    0.7292      1330
           7     0.8636    0.7505    0.8031       481
           8     0.8271    0.8668    0.8465       458
           9     0.8753    0.9469    0.9097       452
          10     0.9031    0.8187    0.8588       717
          11     0.8318    0.8318    0.8318       333
          12     0.7143    0.0334    0.0639       299
          13     0.7769    0.7249    0.7500       269

    accuracy                         0.8814     16894
   macro avg     0.8581    0.7975    0.8045     16894
weighted avg     0.8784    0.8814    0.8734     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8014    0.8813    0.8395      1525
           1     0.7824    0.8060    0.7941       232
           2     0.7516    0.7804    0.7657       601
           3     0.8757    0.7346    0.7990       211
           4     0.8168    0.8505    0.8333       194
           5     0.8462    0.7857    0.8148       378
           6     0.5269    0.5285    0.5277       333
           7     0.7283    0.5537    0.6291       121
           8     0.5385    0.6087    0.5714       115
           9     0.8276    0.8421    0.8348       114
          10     0.7933    0.6611    0.7212       180
          11     0.5570    0.5238    0.5399        84
          12     0.2500    0.0133    0.0253        75
          13     0.5536    0.4559    0.5000        68

    accuracy                         0.7613      4231
   macro avg     0.6892    0.6447    0.6568      4231
weighted avg     0.7526    0.7613    0.7532      4231

---------------------------------------
program finished.
