seed:  666
number of classes (from original clusters): 10
how to merge clusters:  [[0, 9], [1, 5], 2, [3, 8], 4, 6, 7]
whether to further subcluster data according to chemical reaction: False
positive training pair sampling threshold:  13500
negative training pair sampling threshold:  4500
positive validation pair sampling threshold:  3900
negative validation pair sampling threshold:  1300
number of epochs to train: 60
learning rate decay to half at epoch 40.
batch size: 256
similar margin of contrastive loss: 0.0
dissimilar margin of contrastive loss: 2.0
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of classes after merging:  7
number of pockets in training set:  10527
number of pockets in validation set:  2254
number of pockets in test set:  2263
number of train positive pairs: 94500
number of train negative pairs: 94500
number of validation positive pairs: 27300
number of validation negative pairs: 27300
model architecture:
SiameseNet(
  (embedding_net): EmbeddingNet(
    (set2set): Set2Set(32, 64)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.7853360732144149, validation loss: 0.7346901135566907.
epoch: 2, train loss: 0.6269152310891126, validation loss: 0.6815132588257283.
epoch: 3, train loss: 0.5549016060400261, validation loss: 0.6678393638527.
epoch: 4, train loss: 0.5086561494050202, validation loss: 0.6380907370319296.
epoch: 5, train loss: 0.48194104641707486, validation loss: 0.6820337978824155.
epoch: 6, train loss: 0.4578542950746244, validation loss: 0.624845832433456.
epoch: 7, train loss: 0.43900896776794757, validation loss: 0.6062505540393648.
epoch: 8, train loss: 0.42344817558167475, validation loss: 0.6469219034495372.
epoch: 9, train loss: 0.40905148824055987, validation loss: 0.6189176415523766.
epoch: 10, train loss: 0.4035189756363157, validation loss: 0.6073657151106949.
epoch: 11, train loss: 0.3975676039842071, validation loss: 0.6163751677628402.
epoch: 12, train loss: 0.3775356326027522, validation loss: 0.6346072080283811.
epoch: 13, train loss: 0.3745124600526517, validation loss: 0.6726249476841518.
epoch: 14, train loss: 0.3698226436231502, validation loss: 0.6075623336903778.
epoch: 15, train loss: 0.3635634864524559, validation loss: 0.6338847520150545.
epoch: 16, train loss: 0.36254522882693657, validation loss: 0.614582299871759.
epoch: 17, train loss: 0.35494286911575884, validation loss: 0.6229995445279412.
epoch: 18, train loss: 0.35831527443537636, validation loss: 0.6143428791311635.
epoch: 19, train loss: 0.3470402786820023, validation loss: 0.6646481054955786.
epoch: 20, train loss: 0.34554748307081756, validation loss: 0.6226272862472814.
epoch: 21, train loss: 0.35142324647449313, validation loss: 0.6414233278267549.
epoch: 22, train loss: 0.3348937900482662, validation loss: 0.6457798180213341.
epoch: 23, train loss: 0.3388447912710684, validation loss: 0.6193985113905464.
epoch: 24, train loss: 0.3365000763544961, validation loss: 0.6611186629075271.
epoch: 25, train loss: 0.3419449302027465, validation loss: 0.6153981023655706.
epoch: 26, train loss: 0.33593099735401294, validation loss: 0.6200017437218747.
epoch: 27, train loss: 0.3367673014686221, validation loss: 0.6425132230262617.
epoch: 28, train loss: 0.3325418460179889, validation loss: 0.6317864953903924.
epoch: 29, train loss: 0.324331134775959, validation loss: 0.6719692833869012.
epoch: 30, train loss: 0.32872418773994244, validation loss: 0.6437450965070899.
epoch: 31, train loss: 0.33671375987138696, validation loss: 0.6357117308452452.
epoch: 32, train loss: 0.32775401838998947, validation loss: 0.6255840302910997.
epoch: 33, train loss: 0.3239020517339151, validation loss: 0.6212433515276228.
epoch: 34, train loss: 0.33140911635141523, validation loss: 0.6695921339656844.
epoch: 35, train loss: 0.3307509588413138, validation loss: 0.6403963000957782.
epoch: 36, train loss: 0.3335442788058488, validation loss: 0.6091854271521935.
epoch: 37, train loss: 0.32447696572762946, validation loss: 0.6198211228367173.
epoch: 38, train loss: 0.3224157052216706, validation loss: 0.6462437246133993.
epoch: 39, train loss: 0.327688287462507, validation loss: 0.6947580623976041.
epoch: 40, train loss: 0.27580874288649787, validation loss: 0.6204525128039685.
epoch: 41, train loss: 0.27504218064161834, validation loss: 0.632558829897926.
epoch: 42, train loss: 0.27459853268174267, validation loss: 0.594865941477346.
epoch: 43, train loss: 0.2764742887991446, validation loss: 0.6028963898040436.
epoch: 44, train loss: 0.28102051007306134, validation loss: 0.5996810024387234.
epoch: 45, train loss: 0.26944682497700684, validation loss: 0.6099369865110069.
epoch: 46, train loss: 0.27283789068555075, validation loss: 0.6238605173690852.
epoch: 47, train loss: 0.2839535263505562, validation loss: 0.6232188759269295.
epoch: 48, train loss: 0.27265020192867867, validation loss: 0.6254202733022389.
epoch: 49, train loss: 0.270272926592953, validation loss: 0.6113163733569693.
epoch: 50, train loss: 0.2653778073144337, validation loss: 0.6242419820652777.
epoch: 51, train loss: 0.2670541191504746, validation loss: 0.6247576542389699.
epoch: 52, train loss: 0.26986559069598165, validation loss: 0.6207529657664317.
epoch: 53, train loss: 0.26983494586036316, validation loss: 0.6240729932121305.
epoch: 54, train loss: 0.2763517632661042, validation loss: 0.615922749418042.
epoch: 55, train loss: 0.26321537433604086, validation loss: 0.6077140578944168.
epoch: 56, train loss: 0.2696109269762796, validation loss: 0.6312359004317623.
epoch: 57, train loss: 0.26208921327540485, validation loss: 0.6128042854057564.
epoch: 58, train loss: 0.26169604290351667, validation loss: 0.6049665230272454.
epoch: 59, train loss: 0.2666428643483964, validation loss: 0.6122196883596344.
epoch: 60, train loss: 0.2616863645926985, validation loss: 0.6218745351099706.
best validation loss 0.594865941477346 at epoch 42.
