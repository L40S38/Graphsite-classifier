seed:  19
save trained model at:  ../trained_models/trained_classifier_model_119.pt
save loss at:  ./results/train_classifier_results_119.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4p4oA01', '4wtlC02', '6gejL00', '4e01A01', '1nvfA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['3wjrA02', '2pvmA00', '5yijA00', '5mdhB00', '1hdiA00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b035d7f4730>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.009276368314026, acc: 0.3938084527051024; test loss: 1.732214305564828, acc: 0.46206570550697235
epoch: 2, train loss: 1.7334787356044832, acc: 0.4679176038830354; test loss: 1.5921643105445813, acc: 0.5105176081304656
epoch: 3, train loss: 1.6194782309720146, acc: 0.5050313720847638; test loss: 1.4953009176693806, acc: 0.5376979437485228
epoch: 4, train loss: 1.5744541993970431, acc: 0.5176985912158163; test loss: 1.620738131586921, acc: 0.4979910186717088
epoch: 5, train loss: 1.5395594645466653, acc: 0.5304250029596307; test loss: 1.4379927598114302, acc: 0.5613330181990073
epoch: 6, train loss: 1.4960560857353513, acc: 0.5451047709245886; test loss: 1.3872146015329525, acc: 0.5748050106357835
epoch: 7, train loss: 1.4502605629260827, acc: 0.5622706286255476; test loss: 1.396780577992976, acc: 0.584731741904987
epoch: 8, train loss: 1.4253272493732065, acc: 0.5671836154847875; test loss: 1.3755638798312695, acc: 0.5856771448830064
epoch: 9, train loss: 1.4006913499294744, acc: 0.5789037528116491; test loss: 1.3320431915532047, acc: 0.597021980619239
epoch: 10, train loss: 1.3807434206448095, acc: 0.5796140641647922; test loss: 1.3197068734192674, acc: 0.5920586149846372
epoch: 11, train loss: 1.339949459172684, acc: 0.5893216526577483; test loss: 1.2811868405460438, acc: 0.6064760103994328
epoch: 12, train loss: 1.333636004472888, acc: 0.5949449508701314; test loss: 1.3707652393454668, acc: 0.5714961002127157
epoch: 13, train loss: 1.3078265485360494, acc: 0.603468687107849; test loss: 1.2468942807544012, acc: 0.6187662491136847
epoch: 14, train loss: 1.2929153055132558, acc: 0.6052444654907068; test loss: 1.2983700842531656, acc: 0.6005672417868116
epoch: 15, train loss: 1.2953304559278167, acc: 0.6031135314312773; test loss: 1.246180519317234, acc: 0.6147482864571023
epoch: 16, train loss: 1.273286895976626, acc: 0.6106901858648041; test loss: 1.27650022112261, acc: 0.6069487118884425
epoch: 17, train loss: 1.2434845424000436, acc: 0.6212264709364271; test loss: 1.2629982236710262, acc: 0.6142755849680926
epoch: 18, train loss: 1.2391059196417298, acc: 0.6218775896768083; test loss: 1.2870011687475973, acc: 0.6067123611439376
epoch: 19, train loss: 1.2279998696992083, acc: 0.6264354208594768; test loss: 1.2740556865233221, acc: 0.6093122193334909
epoch: 20, train loss: 1.2295972465466816, acc: 0.6225287084171895; test loss: 1.2855578911442533, acc: 0.6064760103994328
epoch: 21, train loss: 1.2046895220644647, acc: 0.6293950514975731; test loss: 1.2583237712024318, acc: 0.6166390924131411
epoch: 22, train loss: 1.1874351575838344, acc: 0.6361430093524328; test loss: 1.219903766052347, acc: 0.6218388087922477
epoch: 23, train loss: 1.1821709558133107, acc: 0.6364389724162425; test loss: 1.1674255795074224, acc: 0.6400378161191208
epoch: 24, train loss: 1.1608141387328885, acc: 0.644134012075293; test loss: 1.2045674173580054, acc: 0.6208934058142284
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9291103876623527, acc: 0.6447259382029122; test loss: 0.9127377669204355, acc: 0.6492554951548097
epoch: 26, train loss: 0.9076132544035092, acc: 0.6523617852492009; test loss: 0.9400239128278689, acc: 0.6431103757976837
epoch: 27, train loss: 0.9120760393436242, acc: 0.6551438380490114; test loss: 0.9520369363265014, acc: 0.6395651146301111
epoch: 28, train loss: 0.8918166492088908, acc: 0.6594648987806322; test loss: 0.9260982261766809, acc: 0.641219569841645
epoch: 29, train loss: 0.902951906104758, acc: 0.6533680596661536; test loss: 0.878491020518199, acc: 0.661073032380052
epoch: 30, train loss: 0.8789556986540186, acc: 0.6610630993252042; test loss: 1.0740988154243392, acc: 0.6064760103994328
epoch: 31, train loss: 0.8732426322337733, acc: 0.6630164555463478; test loss: 1.0183033768490002, acc: 0.6052942566769085
epoch: 32, train loss: 0.921303422687829, acc: 0.6409968035989109; test loss: 0.8739903105598509, acc: 0.6622547861025763
epoch: 33, train loss: 0.873252648170903, acc: 0.663726766899491; test loss: 0.9761998870924443, acc: 0.6348380997400142
epoch: 34, train loss: 0.851613578866103, acc: 0.669350065111874; test loss: 0.8571277916839968, acc: 0.6620184353580714
epoch: 35, train loss: 0.8414598532410025, acc: 0.6720137326861607; test loss: 0.9774592973695132, acc: 0.6324745922949657
epoch: 36, train loss: 0.8254085099667476, acc: 0.6769267195454007; test loss: 0.839021768110127, acc: 0.6790356889624203
epoch: 37, train loss: 0.8173778730764238, acc: 0.6810702024387356; test loss: 0.9029881086396822, acc: 0.6497281966438194
epoch: 38, train loss: 0.8128408528198444, acc: 0.6803598910855925; test loss: 0.886981148495513, acc: 0.6525644055778775
epoch: 39, train loss: 0.8026893847621206, acc: 0.685687226234166; test loss: 0.8419809032630424, acc: 0.6728905696052943
epoch: 40, train loss: 0.8006493443283821, acc: 0.6838522552385462; test loss: 0.8707945558709812, acc: 0.6561096667454502
epoch: 41, train loss: 0.7901596600690142, acc: 0.6920208357996922; test loss: 0.7894008083000895, acc: 0.6906168754431576
epoch: 42, train loss: 0.7705035616240304, acc: 0.6965194743695987; test loss: 0.9058501780723404, acc: 0.6622547861025763
epoch: 43, train loss: 0.7746170682710657, acc: 0.6908369835444537; test loss: 0.8434413640219054, acc: 0.6804537934294493
epoch: 44, train loss: 0.7760620085292982, acc: 0.6948620812122647; test loss: 0.8280638258272918, acc: 0.676199480028362
epoch: 45, train loss: 0.7582870740304547, acc: 0.6978217118503611; test loss: 0.8191068993480288, acc: 0.6887260694871189
epoch: 46, train loss: 0.7598224782415913, acc: 0.7006629572629336; test loss: 0.8661362279027778, acc: 0.6572914204679745
epoch: 47, train loss: 0.752113901511782, acc: 0.7027938913223629; test loss: 0.82196384258401, acc: 0.6797447411959348
epoch: 48, train loss: 0.7435830850579745, acc: 0.7063454480880786; test loss: 0.8207584478927825, acc: 0.6863625620420705
epoch: 49, train loss: 0.7365213830442532, acc: 0.7083579969219841; test loss: 0.8973660200263727, acc: 0.6584731741904987
epoch: 50, train loss: 0.734217137158291, acc: 0.7084171895347461; test loss: 0.8622268070652817, acc: 0.6674545024816828
epoch: 51, train loss: 0.7510024268090605, acc: 0.7031490469989345; test loss: 0.921145604029245, acc: 0.647364689198771
epoch: 52, train loss: 0.7243168065643062, acc: 0.7122647093642713; test loss: 0.9285325563368124, acc: 0.6355471519735287
Epoch    52: reducing learning rate of group 0 to 1.5000e-03.
epoch: 53, train loss: 0.661778000920457, acc: 0.732094234639517; test loss: 0.743932152549529, acc: 0.7078704797920113
epoch: 54, train loss: 0.6298830415195751, acc: 0.7445838759322837; test loss: 0.7464674067423998, acc: 0.7133065469156228
epoch: 55, train loss: 0.6301872482508107, acc: 0.7447614537705695; test loss: 0.8146967070797492, acc: 0.6922713306546916
epoch: 56, train loss: 0.6168130725336397, acc: 0.7469515804427608; test loss: 0.7361952705716895, acc: 0.7085795320255259
epoch: 57, train loss: 0.6092667428293044, acc: 0.7517461820764768; test loss: 0.7209958521870681, acc: 0.7180335618057196
epoch: 58, train loss: 0.6102884940893901, acc: 0.7522197229785723; test loss: 0.6946158067635175, acc: 0.7300874497754668
epoch: 59, train loss: 0.5916677761235942, acc: 0.7611578075056233; test loss: 0.8026160538492403, acc: 0.6877806665090995
epoch: 60, train loss: 0.5885410388927002, acc: 0.7557712797442879; test loss: 0.751113923664945, acc: 0.7092885842590404
epoch: 61, train loss: 0.6019887182364537, acc: 0.7507399076595241; test loss: 0.7247789821228049, acc: 0.7166154573386906
epoch: 62, train loss: 0.5960386380988972, acc: 0.7564815910974311; test loss: 0.7487997135296693, acc: 0.7201607185062633
epoch: 63, train loss: 0.5793033063489332, acc: 0.7609210370545756; test loss: 0.7195504372690733, acc: 0.7244150319073505
epoch: 64, train loss: 0.5760475362886885, acc: 0.764650171658577; test loss: 0.7409477398429912, acc: 0.720633419995273
epoch: 65, train loss: 0.5755203511921826, acc: 0.7665443352669586; test loss: 0.773230535035662, acc: 0.705270621602458
epoch: 66, train loss: 0.5625415222453156, acc: 0.770391855096484; test loss: 0.7377692934638979, acc: 0.7180335618057196
epoch: 67, train loss: 0.5571273826940307, acc: 0.7699775068071505; test loss: 0.7472626139046424, acc: 0.7149610021271567
epoch: 68, train loss: 0.5448694693148877, acc: 0.7760743459216289; test loss: 0.7708844599839898, acc: 0.7076341290475066
epoch: 69, train loss: 0.5466872796026422, acc: 0.7730555226707707; test loss: 0.7738958085803269, acc: 0.709997636492555
epoch: 70, train loss: 0.5555240205227023, acc: 0.7718716704155322; test loss: 0.8858534921232731, acc: 0.6750177263058379
epoch: 71, train loss: 0.5398402353482965, acc: 0.7770214277258198; test loss: 0.7239056510357281, acc: 0.7196880170172536
epoch: 72, train loss: 0.5274642465579463, acc: 0.7798034805256304; test loss: 0.7300936838211785, acc: 0.7274875915859135
epoch: 73, train loss: 0.5423494719098528, acc: 0.7771398129513437; test loss: 0.7494297846922833, acc: 0.7078704797920113
epoch: 74, train loss: 0.5280465289052455, acc: 0.7789747839469634; test loss: 0.7815794207136334, acc: 0.7125974946821082
epoch: 75, train loss: 0.5249250534371933, acc: 0.7842429264827749; test loss: 0.7456602797782103, acc: 0.7253604348853699
epoch: 76, train loss: 0.523748184267947, acc: 0.7842429264827749; test loss: 0.7512821577536922, acc: 0.7246513826518554
epoch: 77, train loss: 0.5229554794957204, acc: 0.7824079554871552; test loss: 0.782517191874054, acc: 0.7017253604348853
epoch: 78, train loss: 0.5142488689398333, acc: 0.78501243044868; test loss: 0.7073193979274576, acc: 0.7326873079650201
epoch: 79, train loss: 0.5002925614781728, acc: 0.7892742985675387; test loss: 0.7903601000760357, acc: 0.7095249350035453
epoch: 80, train loss: 0.4903248000373583, acc: 0.7927666627204925; test loss: 0.7700548199045266, acc: 0.7307965020089813
epoch: 81, train loss: 0.4989635288270815, acc: 0.7920563513673493; test loss: 0.7569363045652998, acc: 0.723705979673836
epoch: 82, train loss: 0.48369524427612987, acc: 0.798034805256304; test loss: 0.818020137707073, acc: 0.7026707634129048
epoch: 83, train loss: 0.4961026134795235, acc: 0.7940689002012549; test loss: 0.7456652678600357, acc: 0.7170881588277003
epoch: 84, train loss: 0.48787144078042105, acc: 0.7954303302947792; test loss: 0.7798986185304213, acc: 0.7166154573386906
epoch: 85, train loss: 0.4777984454164113, acc: 0.7959038711968747; test loss: 0.774273770719351, acc: 0.7173245095722052
epoch: 86, train loss: 0.47726886324929363, acc: 0.7992186575115425; test loss: 0.7833009805839356, acc: 0.7177972110612149
epoch: 87, train loss: 0.4889959004964859, acc: 0.7942464780395406; test loss: 0.7657092873071334, acc: 0.7147246513826518
epoch: 88, train loss: 0.47605770173983925, acc: 0.7995146205753522; test loss: 0.8076480819496582, acc: 0.7185062632947293
epoch: 89, train loss: 0.4759565524741914, acc: 0.7989818870604949; test loss: 0.7238803897110234, acc: 0.7303238005199716
epoch: 90, train loss: 0.45232303430868404, acc: 0.8105836391618326; test loss: 0.7297022878574271, acc: 0.737414322855117
epoch: 91, train loss: 0.45498059284084436, acc: 0.8063217710429739; test loss: 0.7604545010260391, acc: 0.7244150319073505
epoch: 92, train loss: 0.46281828085910753, acc: 0.8034213330176394; test loss: 0.7448930405635018, acc: 0.748050106357835
epoch: 93, train loss: 0.4532870330640631, acc: 0.811885876642595; test loss: 0.7005985231721861, acc: 0.7397778303001654
epoch: 94, train loss: 0.4499203620633529, acc: 0.8112347579022138; test loss: 0.741125995006665, acc: 0.7211061214842827
epoch: 95, train loss: 0.44732896484918844, acc: 0.8083343198768793; test loss: 0.7501403949018969, acc: 0.7348144646655637
epoch: 96, train loss: 0.45567604623857555, acc: 0.8044868000473541; test loss: 0.74115809552412, acc: 0.7341054124320492
epoch: 97, train loss: 0.43330280929256987, acc: 0.8109387948384041; test loss: 0.7222862638668709, acc: 0.7383597258331364
epoch: 98, train loss: 0.44370602081693306, acc: 0.8075056232982124; test loss: 0.7793514693045385, acc: 0.7187426140392342
epoch: 99, train loss: 0.4343893043392577, acc: 0.8170356339528827; test loss: 0.7291214964232583, acc: 0.737414322855117
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.34478511254759503, acc: 0.8124778027702143; test loss: 0.6311230939546394, acc: 0.7289056960529425
epoch: 101, train loss: 0.3238557907121484, acc: 0.8183378714336451; test loss: 0.6474058675980123, acc: 0.7355235168990782
epoch: 102, train loss: 0.318983756668175, acc: 0.8217710429738369; test loss: 0.6284188449425044, acc: 0.7341054124320492
epoch: 103, train loss: 0.310846023077201, acc: 0.82402036225879; test loss: 0.6318429469836854, acc: 0.7248877333963601
Epoch   103: reducing learning rate of group 0 to 7.5000e-04.
epoch: 104, train loss: 0.2775299144635133, acc: 0.8388185154492719; test loss: 0.5933579063336637, acc: 0.748050106357835
epoch: 105, train loss: 0.25494393660808523, acc: 0.8508938084527051; test loss: 0.615692991901588, acc: 0.7508863152918932
epoch: 106, train loss: 0.25195598665096075, acc: 0.8505978453888955; test loss: 0.6398716341995567, acc: 0.7506499645473883
epoch: 107, train loss: 0.2432522998274438, acc: 0.8560435657629928; test loss: 0.6264528629769109, acc: 0.7487591585913496
epoch: 108, train loss: 0.24073282836971585, acc: 0.858352077660708; test loss: 0.6627735437103773, acc: 0.7395414795556606
epoch: 109, train loss: 0.25818413335839235, acc: 0.8487628743932757; test loss: 0.6216435928599431, acc: 0.7508863152918932
epoch: 110, train loss: 0.2448698306056865, acc: 0.8531431277376583; test loss: 0.6135125484513887, acc: 0.7537225242259513
epoch: 111, train loss: 0.2435725912605219, acc: 0.8498875340357523; test loss: 0.6643313141585918, acc: 0.7359962183880879
epoch: 112, train loss: 0.23974313728870322, acc: 0.8548597135077542; test loss: 0.6237854499733496, acc: 0.751122666036398
epoch: 113, train loss: 0.23259881744585728, acc: 0.859121581626613; test loss: 0.6226913629483174, acc: 0.7570314346490191
epoch: 114, train loss: 0.23470334329590425, acc: 0.857523381082041; test loss: 0.6574871106171434, acc: 0.7333963601985346
epoch: 115, train loss: 0.22884666193084802, acc: 0.8553924470226115; test loss: 0.6646928952338143, acc: 0.7359962183880879
epoch: 116, train loss: 0.23369559649710148, acc: 0.8567538771161359; test loss: 0.7163486080061485, acc: 0.7291420467974474
epoch: 117, train loss: 0.24245090309693232, acc: 0.8513081567420386; test loss: 0.6644655494254916, acc: 0.7419049870007091
epoch: 118, train loss: 0.24009732497723493, acc: 0.852610394222801; test loss: 0.6299199466022355, acc: 0.752540770503427
epoch: 119, train loss: 0.23351683387625666, acc: 0.8577601515330887; test loss: 0.668136790468743, acc: 0.7421413377452138
epoch: 120, train loss: 0.2171434341619265, acc: 0.8661063099325205; test loss: 0.6755265680731005, acc: 0.7367052706216024
epoch: 121, train loss: 0.22383145791645712, acc: 0.8601278560435658; test loss: 0.6927112139756165, acc: 0.734341763176554
epoch: 122, train loss: 0.22333724422964019, acc: 0.859121581626613; test loss: 0.6983465707434517, acc: 0.74048688253368
epoch: 123, train loss: 0.23287004457502883, acc: 0.8539718243163253; test loss: 0.6356540573203967, acc: 0.743559442212243
epoch: 124, train loss: 0.21797120943709605, acc: 0.8607789747839469; test loss: 0.6697711768203299, acc: 0.7482864571023399
epoch: 125, train loss: 0.21317654451181356, acc: 0.8612525156860423; test loss: 0.7098696878181229, acc: 0.7426140392342235
epoch: 126, train loss: 0.21983997744823527, acc: 0.860719782171185; test loss: 0.6789464155457257, acc: 0.7407232332781848
epoch: 127, train loss: 0.22450998347335077, acc: 0.8614300935243282; test loss: 0.7483002195063, acc: 0.7246513826518554
epoch: 128, train loss: 0.21997930435502375, acc: 0.8594767373031845; test loss: 0.6776072745119193, acc: 0.7468683526353108
epoch: 129, train loss: 0.21843208568581918, acc: 0.8607789747839469; test loss: 0.6609872060305569, acc: 0.743559442212243
epoch: 130, train loss: 0.20271256882346733, acc: 0.870545755889665; test loss: 0.6714752570756134, acc: 0.7532498227369416
epoch: 131, train loss: 0.20520915496246311, acc: 0.8700130223748076; test loss: 0.708267484030186, acc: 0.7300874497754668
epoch: 132, train loss: 0.20818018456373014, acc: 0.8639161832603292; test loss: 0.6683669228047745, acc: 0.7411959347671945
epoch: 133, train loss: 0.20351453303295833, acc: 0.8678228957026163; test loss: 0.7399110966781386, acc: 0.726778539352399
epoch: 134, train loss: 0.21133098037296236, acc: 0.8647448798389961; test loss: 0.669347145686446, acc: 0.7459229496572914
epoch: 135, train loss: 0.2004908449948716, acc: 0.8691251331833787; test loss: 0.6981287526058774, acc: 0.7390687780666509
epoch: 136, train loss: 0.19639490600364426, acc: 0.8710784894045223; test loss: 0.7088122266428151, acc: 0.7329236587095249
epoch: 137, train loss: 0.1900357938266526, acc: 0.8765242097786197; test loss: 0.7082699932575338, acc: 0.7492318600803592
epoch: 138, train loss: 0.19991395514971325, acc: 0.8717888007576654; test loss: 0.694494303899633, acc: 0.7440321437012527
epoch: 139, train loss: 0.19188230345097324, acc: 0.8742156978809045; test loss: 0.7374982314029784, acc: 0.7385960765776413
epoch: 140, train loss: 0.1944154909204079, acc: 0.871374452468332; test loss: 0.7105389029048234, acc: 0.7466320018908059
epoch: 141, train loss: 0.23107845920373013, acc: 0.8545637504439446; test loss: 0.6889864683320296, acc: 0.7454502481682818
epoch: 142, train loss: 0.19108254565590865, acc: 0.8717296081449035; test loss: 0.6960979241114422, acc: 0.7485228078468447
epoch: 143, train loss: 0.18211928661284227, acc: 0.8774712915828105; test loss: 0.7340878961329144, acc: 0.7461593004017962
epoch: 144, train loss: 0.1886675142953985, acc: 0.8775304841955724; test loss: 0.7459215270912244, acc: 0.7341054124320492
epoch: 145, train loss: 0.17717825826264083, acc: 0.8783591807742394; test loss: 0.7226845223524417, acc: 0.7402505317891751
epoch: 146, train loss: 0.1947072839065178, acc: 0.8707825263407126; test loss: 0.6830169981012274, acc: 0.7452138974237769
epoch: 147, train loss: 0.17922902945704725, acc: 0.8810228483485261; test loss: 0.7419830030708611, acc: 0.7345781139210589
epoch: 148, train loss: 0.19515586828316148, acc: 0.8701906002130934; test loss: 0.7325240083688598, acc: 0.7331600094540298
epoch: 149, train loss: 0.18578539485125284, acc: 0.8733278086894756; test loss: 0.7013990335840898, acc: 0.7499409123138738
epoch: 150, train loss: 0.18164552751731433, acc: 0.8771161359062389; test loss: 0.757460298112199, acc: 0.7355235168990782
epoch: 151, train loss: 0.1811737056097872, acc: 0.8766425950041434; test loss: 0.7262118763744451, acc: 0.7402505317891751
epoch: 152, train loss: 0.18637366462871566, acc: 0.8771161359062389; test loss: 0.7154967193765804, acc: 0.7345781139210589
epoch: 153, train loss: 0.18317866100634966, acc: 0.878655143838049; test loss: 0.7024448160244989, acc: 0.7461593004017962
epoch: 154, train loss: 0.17818743172370147, acc: 0.8796022256422399; test loss: 0.7256937952533001, acc: 0.7437957929567478
Epoch   154: reducing learning rate of group 0 to 3.7500e-04.
epoch: 155, train loss: 0.1545101090572396, acc: 0.8910855925180537; test loss: 0.6805712156784898, acc: 0.7489955093358545
epoch: 156, train loss: 0.13020215706545404, acc: 0.9054693974192021; test loss: 0.7346655614154611, acc: 0.7482864571023399
epoch: 157, train loss: 0.12643630491448138, acc: 0.907126790576536; test loss: 0.7435310877290805, acc: 0.7601039943275821
epoch: 158, train loss: 0.12310479603671627, acc: 0.906416479223393; test loss: 0.7688162427679975, acc: 0.7497045615693689
epoch: 159, train loss: 0.12144508227358432, acc: 0.9078962945424411; test loss: 0.7617484621053834, acc: 0.7520680690144174
epoch: 160, train loss: 0.11947980492238083, acc: 0.9106783473422517; test loss: 0.7690675460934442, acc: 0.7475774048688253
epoch: 161, train loss: 0.11852853658605896, acc: 0.9102639990529182; test loss: 0.7912328582485075, acc: 0.7471047033798156
epoch: 162, train loss: 0.12069868247255258, acc: 0.9084882206700604; test loss: 0.7794283073130468, acc: 0.7428503899787284
epoch: 163, train loss: 0.12028356279277712, acc: 0.9067124422872026; test loss: 0.8035126634028413, acc: 0.7449775466792721
epoch: 164, train loss: 0.11626338921341794, acc: 0.9113294660826329; test loss: 0.79530710659532, acc: 0.748050106357835
epoch: 165, train loss: 0.12023332272251075, acc: 0.9074819462531076; test loss: 0.7851079640110118, acc: 0.7492318600803592
epoch: 166, train loss: 0.11444396363414504, acc: 0.9119213922102521; test loss: 0.7761305865878558, acc: 0.7556133301819901
epoch: 167, train loss: 0.11481208740126611, acc: 0.9115070439209186; test loss: 0.774887406721771, acc: 0.7485228078468447
epoch: 168, train loss: 0.11764946251461968, acc: 0.9093169172487273; test loss: 0.7885461884789139, acc: 0.7530134719924367
epoch: 169, train loss: 0.11462244144999742, acc: 0.9111518882443471; test loss: 0.7992395847803367, acc: 0.7473410541243205
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.08045599453580779, acc: 0.9134604001420623; test loss: 0.7535933136968178, acc: 0.7440321437012527
epoch: 171, train loss: 0.08075087015708128, acc: 0.9127500887889192; test loss: 0.7419563802639324, acc: 0.7440321437012527
epoch: 172, train loss: 0.08129891166905616, acc: 0.9110335030188232; test loss: 0.6984676839724355, acc: 0.7534861734814464
epoch: 173, train loss: 0.08292937526953238, acc: 0.9134604001420623; test loss: 0.6935855756553175, acc: 0.7504136138028835
epoch: 174, train loss: 0.07983382365520683, acc: 0.9106783473422517; test loss: 0.6863068592404676, acc: 0.7473410541243205
epoch: 175, train loss: 0.0764121302041102, acc: 0.9136379779803481; test loss: 0.6938741504652961, acc: 0.7563223824155046
epoch: 176, train loss: 0.08470012826620792, acc: 0.9094353024742512; test loss: 0.6991892810748053, acc: 0.7452138974237769
epoch: 177, train loss: 0.07811226389002938, acc: 0.9107967325677755; test loss: 0.6999863721720858, acc: 0.7440321437012527
epoch: 178, train loss: 0.0710393233153966, acc: 0.9193204688054931; test loss: 0.7086962991728903, acc: 0.7497045615693689
epoch: 179, train loss: 0.078902034779385, acc: 0.9126908961761572; test loss: 0.7327562021155415, acc: 0.74048688253368
epoch: 180, train loss: 0.08396666975189374, acc: 0.9103231916656801; test loss: 0.6959693428754976, acc: 0.7407232332781848
epoch: 181, train loss: 0.0811001303948297, acc: 0.9127500887889192; test loss: 0.6841068153430936, acc: 0.7532498227369416
epoch: 182, train loss: 0.07931769914273272, acc: 0.911270273469871; test loss: 0.7171696909097768, acc: 0.7473410541243205
epoch: 183, train loss: 0.07825222995290534, acc: 0.9102639990529182; test loss: 0.7110517729092253, acc: 0.7376506735996219
epoch: 184, train loss: 0.07608761679020819, acc: 0.9121581626612999; test loss: 0.7179407305087471, acc: 0.7426140392342235
epoch: 185, train loss: 0.0754054216547832, acc: 0.9134012075293003; test loss: 0.7114697561227974, acc: 0.7456865989127865
epoch: 186, train loss: 0.07585406632759, acc: 0.9138747484313957; test loss: 0.7189152171442171, acc: 0.7423776884897187
epoch: 187, train loss: 0.07981516879060303, acc: 0.9085474132828223; test loss: 0.7080888809309138, acc: 0.7482864571023399
epoch: 188, train loss: 0.07517984241328325, acc: 0.9123357404995857; test loss: 0.729981174595219, acc: 0.7407232332781848
epoch: 189, train loss: 0.07913174702385353, acc: 0.9118621995974903; test loss: 0.7088124112749291, acc: 0.7471047033798156
epoch: 190, train loss: 0.07975831351679459, acc: 0.9113294660826329; test loss: 0.707602625506281, acc: 0.7461593004017962
epoch: 191, train loss: 0.07824198425440615, acc: 0.9130460518527288; test loss: 0.7067119892087002, acc: 0.7485228078468447
epoch: 192, train loss: 0.07396647334010611, acc: 0.9172487273588256; test loss: 0.696309865887524, acc: 0.7497045615693689
epoch: 193, train loss: 0.07136063416025068, acc: 0.9155913342014916; test loss: 0.7420399530478838, acc: 0.7426140392342235
epoch: 194, train loss: 0.07452747679570307, acc: 0.914466674559015; test loss: 0.7172487369245233, acc: 0.7445048451902624
epoch: 195, train loss: 0.0800337723164258, acc: 0.9097312655380608; test loss: 0.7215448816570371, acc: 0.7437957929567478
epoch: 196, train loss: 0.07014697159311821, acc: 0.9176038830353972; test loss: 0.7057826972069737, acc: 0.7407232332781848
epoch: 197, train loss: 0.07314995114332234, acc: 0.9146442523973008; test loss: 0.7102138858065766, acc: 0.74048688253368
epoch: 198, train loss: 0.07487711805739572, acc: 0.9123357404995857; test loss: 0.7111115021729295, acc: 0.7428503899787284
epoch: 199, train loss: 0.07347679493380564, acc: 0.9139931336569196; test loss: 0.7383096042295856, acc: 0.7437957929567478
epoch: 200, train loss: 0.06966529977371724, acc: 0.9172487273588256; test loss: 0.7369597118759065, acc: 0.7400141810446703
best test acc 0.7601039943275821 at epoch 157.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9698    0.9833    0.9765      6100
           1     0.9871    0.9104    0.9472       926
           2     0.8783    0.9775    0.9253      2400
           3     0.9656    0.9644    0.9650       843
           4     0.9014    0.9806    0.9394       774
           5     0.9521    0.9729    0.9624      1512
           6     0.8816    0.8737    0.8776      1330
           7     0.9032    0.8919    0.8975       481
           8     0.9653    0.9105    0.9371       458
           9     0.9553    0.9934    0.9740       452
          10     0.9710    0.9331    0.9516       717
          11     0.9433    0.9489    0.9461       333
          12     0.8286    0.0970    0.1737       299
          13     0.8458    0.6320    0.7234       269

    accuracy                         0.9394     16894
   macro avg     0.9249    0.8621    0.8712     16894
weighted avg     0.9385    0.9394    0.9330     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8356    0.8702    0.8526      1525
           1     0.8333    0.7974    0.8150       232
           2     0.7191    0.8136    0.7635       601
           3     0.8144    0.7488    0.7802       211
           4     0.8031    0.7990    0.8010       194
           5     0.7761    0.8069    0.7912       378
           6     0.4957    0.5135    0.5044       333
           7     0.7016    0.7190    0.7102       121
           8     0.6923    0.4696    0.5596       115
           9     0.7712    0.7982    0.7845       114
          10     0.7799    0.6889    0.7316       180
          11     0.4444    0.4286    0.4364        84
          12     0.2727    0.0400    0.0698        75
          13     0.6889    0.4559    0.5487        68

    accuracy                         0.7601      4231
   macro avg     0.6877    0.6393    0.6535      4231
weighted avg     0.7524    0.7601    0.7529      4231

---------------------------------------
program finished.
