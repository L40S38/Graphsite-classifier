seed:  21
save trained model at:  ../trained_models/trained_classifier_model_121.pt
save loss at:  ./results/train_classifier_results_121.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['3gruA00', '3fzpA00', '1ffuB00', '3cx8A00', '1qzrA01']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['3pcoC01', '5udrD00', '1w5fB00', '1r0yB00', '3lq3A01']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b8ce6048610>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0172962007175976, acc: 0.3954658458624364; test loss: 1.8014017725858642, acc: 0.4386669818009927
epoch: 2, train loss: 1.7438813118563752, acc: 0.46347815792589087; test loss: 1.724029188149366, acc: 0.4920822500590877
epoch: 3, train loss: 1.6466794608400548, acc: 0.49254173079199715; test loss: 1.5717603674004812, acc: 0.5270621602458048
epoch: 4, train loss: 1.58707774599953, acc: 0.515508464543625; test loss: 1.5055761818512763, acc: 0.5277712124793194
epoch: 5, train loss: 1.5231059988927769, acc: 0.5344501006274417; test loss: 1.4846111477703328, acc: 0.5521153391633183
epoch: 6, train loss: 1.4897207412491666, acc: 0.5458150822777318; test loss: 1.3798348114750285, acc: 0.5729142046797447
epoch: 7, train loss: 1.4534987255258787, acc: 0.5615011246596425; test loss: 1.5102989796458393, acc: 0.5367525407705034
epoch: 8, train loss: 1.4293897337669084, acc: 0.5694921273825027; test loss: 1.3797916401872339, acc: 0.5752777121247932
epoch: 9, train loss: 1.4041142899622872, acc: 0.5695513199952645; test loss: 1.3631757859838851, acc: 0.5835499881824627
epoch: 10, train loss: 1.392744760746446, acc: 0.574049958565171; test loss: 1.350114128376062, acc: 0.5714961002127157
epoch: 11, train loss: 1.3621582342094538, acc: 0.584704628862318; test loss: 1.26970852703328, acc: 0.6161663909241314
epoch: 12, train loss: 1.3385379721148276, acc: 0.5897951935598438; test loss: 1.3194647474904286, acc: 0.6104939730560152
epoch: 13, train loss: 1.3319614284261805, acc: 0.5910382384278442; test loss: 1.2979653339187633, acc: 0.5941857716851808
epoch: 14, train loss: 1.3081069375490089, acc: 0.5974902332188943; test loss: 1.331414383597702, acc: 0.5821318837154337
epoch: 15, train loss: 1.29241892678139, acc: 0.605007695039659; test loss: 1.2398959161316185, acc: 0.61073032380052
epoch: 16, train loss: 1.2993376405334789, acc: 0.6040606132354682; test loss: 1.2901718968112146, acc: 0.5993854880642874
epoch: 17, train loss: 1.284333140607048, acc: 0.6070202438735646; test loss: 1.2651868945062936, acc: 0.6095485700779958
epoch: 18, train loss: 1.259336988559288, acc: 0.6152480170474725; test loss: 1.2682485324613482, acc: 0.613802883479083
epoch: 19, train loss: 1.247045214802496, acc: 0.6185036107493784; test loss: 1.1918435571774668, acc: 0.6324745922949657
epoch: 20, train loss: 1.2232730356041208, acc: 0.6283887770806204; test loss: 1.2399622859247073, acc: 0.6159300401796266
epoch: 21, train loss: 1.228834260705379, acc: 0.6237125606724281; test loss: 1.2478926856871109, acc: 0.6223115102812574
epoch: 22, train loss: 1.2248303503542521, acc: 0.6243044868000474; test loss: 1.157490635786009, acc: 0.642637674308674
epoch: 23, train loss: 1.1972417217945366, acc: 0.6350183497099562; test loss: 1.1813696509302438, acc: 0.6317655400614512
epoch: 24, train loss: 1.1867998160877862, acc: 0.6386882917011957; test loss: 1.2048551941616714, acc: 0.6312928385724415
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9461931731978351, acc: 0.6428909672072926; test loss: 0.935507511083918, acc: 0.642637674308674
epoch: 26, train loss: 0.9375262793188365, acc: 0.6431277376583402; test loss: 1.1094017649790098, acc: 0.569368943512172
epoch: 27, train loss: 0.9439222211455034, acc: 0.638392328637386; test loss: 1.0213845499129308, acc: 0.616875443157646
epoch: 28, train loss: 0.9394525984440864, acc: 0.6430093524328164; test loss: 0.9651983440189997, acc: 0.6308201370834318
epoch: 29, train loss: 0.916286422668074, acc: 0.650645199479105; test loss: 0.9130622691449981, acc: 0.6525644055778775
epoch: 30, train loss: 0.8992588431705056, acc: 0.6543151414703445; test loss: 0.9453656293066138, acc: 0.6438194280311983
epoch: 31, train loss: 0.898896661936016, acc: 0.6537824079554871; test loss: 0.9762489661007338, acc: 0.6284566296383833
epoch: 32, train loss: 0.879619019609013, acc: 0.6605303658103469; test loss: 0.9983669472371848, acc: 0.6260931221933349
epoch: 33, train loss: 0.8788232246460795, acc: 0.6581626612998698; test loss: 0.8980975027654616, acc: 0.6499645473883243
epoch: 34, train loss: 0.8664769842396887, acc: 0.6637859595122528; test loss: 0.9198203964792394, acc: 0.6398014653746159
epoch: 35, train loss: 0.8693350788840489, acc: 0.662069373742157; test loss: 0.9894246395190426, acc: 0.6171117939021508
epoch: 36, train loss: 0.8513696511017298, acc: 0.669409257724636; test loss: 0.9056619510276468, acc: 0.6492554951548097
epoch: 37, train loss: 0.8497687237875495, acc: 0.6659168935716823; test loss: 0.9652535257201983, acc: 0.6301110848499173
epoch: 38, train loss: 0.8476371667830214, acc: 0.6674559015034923; test loss: 0.8971345480439005, acc: 0.6537461593004018
epoch: 39, train loss: 0.8484240813242326, acc: 0.6717177696223511; test loss: 0.8496933558172606, acc: 0.67170881588277
epoch: 40, train loss: 0.8275527582304625, acc: 0.6786433053154967; test loss: 0.9831529679633122, acc: 0.616875443157646
epoch: 41, train loss: 0.8276364427565738, acc: 0.6752693263880668; test loss: 0.8828741889900419, acc: 0.6511463011108485
epoch: 42, train loss: 0.816923045182039, acc: 0.6826684029833077; test loss: 0.8234754269693907, acc: 0.6733632710943039
epoch: 43, train loss: 0.8262340754845643, acc: 0.6749141707114952; test loss: 0.8421857010470133, acc: 0.6700543606712361
epoch: 44, train loss: 0.8059715358790694, acc: 0.6833195217236889; test loss: 0.7930508488826621, acc: 0.6891987709761286
epoch: 45, train loss: 0.7916991205148842, acc: 0.6874038120042618; test loss: 0.8530141496399113, acc: 0.6629638383360907
epoch: 46, train loss: 0.7886369044828432, acc: 0.6868118858766425; test loss: 0.882068350646998, acc: 0.6719451666272749
epoch: 47, train loss: 0.7840875538972986, acc: 0.6914289096720729; test loss: 0.8792546227648308, acc: 0.650437248877334
epoch: 48, train loss: 0.7812644170136992, acc: 0.6936782289570261; test loss: 0.7973440144817702, acc: 0.691562278421177
epoch: 49, train loss: 0.7794527363689663, acc: 0.6949804664377885; test loss: 0.8709659934241111, acc: 0.658000472701489
epoch: 50, train loss: 0.7579980568023804, acc: 0.6984136379779804; test loss: 0.780667572657117, acc: 0.6986528007563224
epoch: 51, train loss: 0.7539556297350455, acc: 0.7026755060968392; test loss: 0.9739039169082515, acc: 0.6388560623965965
epoch: 52, train loss: 0.762298381863451, acc: 0.6968746300461702; test loss: 0.836143078518883, acc: 0.6828173008744978
epoch: 53, train loss: 0.7490673172088633, acc: 0.7051615958328401; test loss: 0.8534803493060673, acc: 0.6674545024816828
epoch: 54, train loss: 0.7452997573525352, acc: 0.7065230259263644; test loss: 0.854788843719313, acc: 0.6733632710943039
epoch: 55, train loss: 0.7420849097301455, acc: 0.7074701077305552; test loss: 0.8575581355851132, acc: 0.6752540770503427
epoch: 56, train loss: 0.7320092149185573, acc: 0.7118503610749378; test loss: 0.8241247315407927, acc: 0.6802174426849444
epoch: 57, train loss: 0.7360854672160645, acc: 0.7115543980111282; test loss: 0.8933755960399111, acc: 0.6646182935476247
epoch: 58, train loss: 0.7545464832613111, acc: 0.7000118385225523; test loss: 0.7911025514221732, acc: 0.6894351217206334
epoch: 59, train loss: 0.7220586380112365, acc: 0.7140404877471291; test loss: 0.8209128595643843, acc: 0.6818718978964784
epoch: 60, train loss: 0.7230958908821008, acc: 0.7150467621640819; test loss: 1.0451402367217917, acc: 0.5925313164736469
epoch: 61, train loss: 0.7103027787394279, acc: 0.7159938439682728; test loss: 0.9208917564828467, acc: 0.6478373906877807
Epoch    61: reducing learning rate of group 0 to 1.5000e-03.
epoch: 62, train loss: 0.6614949404805232, acc: 0.7338108204096129; test loss: 0.7262018542288606, acc: 0.7199243677617585
epoch: 63, train loss: 0.625092587683262, acc: 0.7465372321534273; test loss: 0.7041131217720322, acc: 0.7279602930749232
epoch: 64, train loss: 0.6195234003201934, acc: 0.7515094116254292; test loss: 0.7474382632000174, acc: 0.7040888678799339
epoch: 65, train loss: 0.6124655874901387, acc: 0.7515094116254292; test loss: 0.7218571753965093, acc: 0.722051524462302
epoch: 66, train loss: 0.6107496724786936, acc: 0.7549425831656209; test loss: 0.7050843369563514, acc: 0.7293783975419522
epoch: 67, train loss: 0.5953658017390181, acc: 0.759737184799337; test loss: 0.7560840624496182, acc: 0.7040888678799339
epoch: 68, train loss: 0.599041017206573, acc: 0.7580205990292411; test loss: 0.6960329059392559, acc: 0.7291420467974474
epoch: 69, train loss: 0.5817337405460042, acc: 0.7666035278797206; test loss: 0.7314357984097002, acc: 0.722051524462302
epoch: 70, train loss: 0.5937614340506687, acc: 0.7587901029951462; test loss: 0.7446777584478946, acc: 0.7033798156464193
epoch: 71, train loss: 0.5891851493419522, acc: 0.7600331478631467; test loss: 0.7093340327795502, acc: 0.722051524462302
epoch: 72, train loss: 0.5632266427788407, acc: 0.7720492482538179; test loss: 0.699722035166164, acc: 0.7296147482864571
epoch: 73, train loss: 0.5789011594768501, acc: 0.7654196756244821; test loss: 0.7114200647264416, acc: 0.7255967856298747
epoch: 74, train loss: 0.5633230214184158, acc: 0.7681425358115307; test loss: 0.7183550076968167, acc: 0.7263058378633893
epoch: 75, train loss: 0.5624998547458727, acc: 0.7727003669941991; test loss: 0.7561722171416494, acc: 0.7121247931930985
epoch: 76, train loss: 0.5603259708862467, acc: 0.7725227891559133; test loss: 0.7351619075359506, acc: 0.7289056960529425
epoch: 77, train loss: 0.5550267404108283, acc: 0.7718716704155322; test loss: 0.7315352000234483, acc: 0.7352871661545733
epoch: 78, train loss: 0.5430572522142392, acc: 0.7770214277258198; test loss: 0.7097087154374904, acc: 0.7303238005199716
epoch: 79, train loss: 0.5438723741331876, acc: 0.7756599976322955; test loss: 0.749696970831218, acc: 0.7293783975419522
epoch: 80, train loss: 0.5444538544744828, acc: 0.7779685095300106; test loss: 0.7128401301876925, acc: 0.7248877333963601
epoch: 81, train loss: 0.5334288822386665, acc: 0.7828223037764886; test loss: 0.7398572775579969, acc: 0.7168518080831955
epoch: 82, train loss: 0.5349057875357536, acc: 0.779270747010773; test loss: 0.735453794763827, acc: 0.7241786811628457
epoch: 83, train loss: 0.5313649372587659, acc: 0.7797442879128684; test loss: 0.7258039846738105, acc: 0.7348144646655637
epoch: 84, train loss: 0.5314266991795937, acc: 0.7867882088315378; test loss: 0.7311082710697983, acc: 0.7258331363743796
epoch: 85, train loss: 0.5223693285409524, acc: 0.7822303776488694; test loss: 0.7077478379736666, acc: 0.7305601512644765
epoch: 86, train loss: 0.5155265244405143, acc: 0.790754113886587; test loss: 0.7264189049583946, acc: 0.7255967856298747
epoch: 87, train loss: 0.5111971084671048, acc: 0.7895702616313484; test loss: 0.8229482337335863, acc: 0.6979437485228078
epoch: 88, train loss: 0.5142761880340725, acc: 0.7879720610867764; test loss: 0.7516611051232616, acc: 0.725124084140865
epoch: 89, train loss: 0.5133989401310736, acc: 0.7902213803717296; test loss: 0.7336121273221092, acc: 0.711415740959584
epoch: 90, train loss: 0.5058796674355369, acc: 0.7935953592991595; test loss: 0.7340612156795177, acc: 0.7239423304183408
epoch: 91, train loss: 0.5023854855737757, acc: 0.7940689002012549; test loss: 0.9381236849707989, acc: 0.6487827936658
epoch: 92, train loss: 0.5072668573200682, acc: 0.7910500769503966; test loss: 0.7810996263986839, acc: 0.7149610021271567
epoch: 93, train loss: 0.4898140264203792, acc: 0.7948975967799219; test loss: 0.7306286019091064, acc: 0.7319782557315055
epoch: 94, train loss: 0.4788776561609031, acc: 0.8020007103113531; test loss: 0.7395668942661777, acc: 0.7281966438194281
epoch: 95, train loss: 0.49878344254139767, acc: 0.7935361666863976; test loss: 0.7602905226328558, acc: 0.7298510990309619
epoch: 96, train loss: 0.4883062255704306, acc: 0.8001065467029714; test loss: 0.7619314823723154, acc: 0.7232332781848263
epoch: 97, train loss: 0.4896494697413812, acc: 0.7964366047117319; test loss: 0.7387549596259961, acc: 0.7201607185062633
epoch: 98, train loss: 0.4757010618638637, acc: 0.8018823250858292; test loss: 0.7742876189449835, acc: 0.723705979673836
epoch: 99, train loss: 0.4828442359261362, acc: 0.8002249319284953; test loss: 0.7613895699026116, acc: 0.7300874497754668
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3735136594365331, acc: 0.8044276074345922; test loss: 0.5939105152661173, acc: 0.7244150319073505
epoch: 101, train loss: 0.3732874105706841, acc: 0.8018823250858292; test loss: 0.592782964719945, acc: 0.7367052706216024
epoch: 102, train loss: 0.3594327693692859, acc: 0.8018823250858292; test loss: 0.5994856370535057, acc: 0.7350508154100686
epoch: 103, train loss: 0.3671304639611877, acc: 0.802947792115544; test loss: 0.6165351083278092, acc: 0.7333963601985346
epoch: 104, train loss: 0.3609212750765901, acc: 0.8055522670770687; test loss: 0.6588900579794223, acc: 0.71756086031671
epoch: 105, train loss: 0.35688619101641655, acc: 0.8091038238427845; test loss: 0.6199874569129448, acc: 0.7329236587095249
epoch: 106, train loss: 0.3574085748659953, acc: 0.8051971114004972; test loss: 0.6564518321183127, acc: 0.7116520917040888
epoch: 107, train loss: 0.3535339273799026, acc: 0.8076832011364982; test loss: 0.6739310502048025, acc: 0.7078704797920113
epoch: 108, train loss: 0.3565526166349752, acc: 0.8083343198768793; test loss: 0.6125400956993942, acc: 0.7298510990309619
epoch: 109, train loss: 0.34905049186832765, acc: 0.8074464306854504; test loss: 0.6388186053219937, acc: 0.7234696289293311
epoch: 110, train loss: 0.3596500549088346, acc: 0.8088670533917367; test loss: 0.6352506870075029, acc: 0.7244150319073505
epoch: 111, train loss: 0.35714147582948313, acc: 0.8060258079791642; test loss: 0.6409820423315558, acc: 0.7090522335145356
epoch: 112, train loss: 0.35958278739086846, acc: 0.8031253699538298; test loss: 0.6585551799259353, acc: 0.6995982037343418
Epoch   112: reducing learning rate of group 0 to 7.5000e-04.
epoch: 113, train loss: 0.312530231483592, acc: 0.8269799928968865; test loss: 0.5987906387246154, acc: 0.7414322855116994
epoch: 114, train loss: 0.2832973565316643, acc: 0.8391144785130815; test loss: 0.6362896320583746, acc: 0.7322146064760104
epoch: 115, train loss: 0.27315187718482414, acc: 0.8423700722149876; test loss: 0.6468477325218559, acc: 0.7381233750886316
epoch: 116, train loss: 0.2641589960251614, acc: 0.8465135551083225; test loss: 0.6473984351763346, acc: 0.7393051288111557
epoch: 117, train loss: 0.2668097585287971, acc: 0.846868710784894; test loss: 0.6899054173289447, acc: 0.7284329945639328
epoch: 118, train loss: 0.2732177613697998, acc: 0.8424292648277495; test loss: 0.621064924145669, acc: 0.7428503899787284
epoch: 119, train loss: 0.25865192803614545, acc: 0.8478157925890849; test loss: 0.6401042684092079, acc: 0.7478137556133302
epoch: 120, train loss: 0.2651389376840832, acc: 0.8465727477210844; test loss: 0.7041353853113458, acc: 0.7310328527534862
epoch: 121, train loss: 0.26306740487131325, acc: 0.8440866579850834; test loss: 0.6596316463754014, acc: 0.7371779721106122
epoch: 122, train loss: 0.2618788820802213, acc: 0.8464543624955606; test loss: 0.6480055738639335, acc: 0.748050106357835
epoch: 123, train loss: 0.2550223711324582, acc: 0.8506570380016574; test loss: 0.6819636822028579, acc: 0.7315055542424959
epoch: 124, train loss: 0.25574339223610826, acc: 0.8508346158399431; test loss: 0.6602794788205519, acc: 0.7407232332781848
epoch: 125, train loss: 0.24634802279012843, acc: 0.8532615129631822; test loss: 0.6788273197177228, acc: 0.7414322855116994
epoch: 126, train loss: 0.2506618941339696, acc: 0.8530839351248964; test loss: 0.687227171269243, acc: 0.735759867643583
epoch: 127, train loss: 0.24246125599400056, acc: 0.8556884100864213; test loss: 0.6437652348852192, acc: 0.7440321437012527
epoch: 128, train loss: 0.24419272862038727, acc: 0.855865987924707; test loss: 0.6857020663922065, acc: 0.7381233750886316
epoch: 129, train loss: 0.24171277595946558, acc: 0.851012193678229; test loss: 0.7015676258923849, acc: 0.7331600094540298
epoch: 130, train loss: 0.2386668907745048, acc: 0.8532023203504203; test loss: 0.6940428740136929, acc: 0.7456865989127865
epoch: 131, train loss: 0.23762836565107118, acc: 0.8562803362140405; test loss: 0.7192805818112796, acc: 0.7312692034979911
epoch: 132, train loss: 0.2455223297665666, acc: 0.8523736237717533; test loss: 0.6762069107595275, acc: 0.7428503899787284
epoch: 133, train loss: 0.24408413994604417, acc: 0.8513081567420386; test loss: 0.693758680194412, acc: 0.7286693453084377
epoch: 134, train loss: 0.23684353242121434, acc: 0.8556292174736593; test loss: 0.688133297420737, acc: 0.7364689198770976
epoch: 135, train loss: 0.23579305311949617, acc: 0.8543269799928969; test loss: 0.7218928826782219, acc: 0.7272512408414087
epoch: 136, train loss: 0.23570779885210708, acc: 0.8541494021546111; test loss: 0.7070678516753992, acc: 0.7385960765776413
epoch: 137, train loss: 0.22802935538559224, acc: 0.8585296554989937; test loss: 0.7223089983265497, acc: 0.7199243677617585
epoch: 138, train loss: 0.23580090946257573, acc: 0.8559843731502309; test loss: 0.6939887611670461, acc: 0.7222878752068069
epoch: 139, train loss: 0.2347393149018951, acc: 0.8550964839588019; test loss: 0.7199840361388461, acc: 0.7400141810446703
epoch: 140, train loss: 0.22788206882429954, acc: 0.8614300935243282; test loss: 0.7127507626658155, acc: 0.7284329945639328
epoch: 141, train loss: 0.22927487258679968, acc: 0.8584112702734699; test loss: 0.6673405877920054, acc: 0.7447411959347672
epoch: 142, train loss: 0.2192948668528354, acc: 0.8640345684858529; test loss: 0.760580058611755, acc: 0.7255967856298747
epoch: 143, train loss: 0.22654300724188506, acc: 0.8578193441458506; test loss: 0.7148539535939004, acc: 0.7289056960529425
epoch: 144, train loss: 0.22080393418329233, acc: 0.8644489167751864; test loss: 0.7338580916717357, acc: 0.7303238005199716
epoch: 145, train loss: 0.2204521355688099, acc: 0.865691961643187; test loss: 0.702578170506336, acc: 0.7315055542424959
epoch: 146, train loss: 0.2277158217393704, acc: 0.8579377293713745; test loss: 0.6627848161306541, acc: 0.7478137556133302
epoch: 147, train loss: 0.2032029420557256, acc: 0.8707825263407126; test loss: 0.7169621856858955, acc: 0.7456865989127865
epoch: 148, train loss: 0.20267276453191435, acc: 0.8708417189534746; test loss: 0.7244348436403827, acc: 0.7461593004017962
epoch: 149, train loss: 0.2164720421544727, acc: 0.8621404048774713; test loss: 0.6832073008132246, acc: 0.7428503899787284
epoch: 150, train loss: 0.21742405461124714, acc: 0.8621404048774713; test loss: 0.7157909983975531, acc: 0.7487591585913496
epoch: 151, train loss: 0.20635788966152419, acc: 0.8666390434473777; test loss: 0.7271899239612905, acc: 0.7322146064760104
epoch: 152, train loss: 0.2082404362897104, acc: 0.8637386054220433; test loss: 0.7257475288616966, acc: 0.7352871661545733
epoch: 153, train loss: 0.2081077428429003, acc: 0.8643305315496626; test loss: 0.7097452455478706, acc: 0.7440321437012527
epoch: 154, train loss: 0.21016719527352953, acc: 0.8634426423582336; test loss: 0.7160109947319996, acc: 0.7362325691325927
epoch: 155, train loss: 0.20352202636949746, acc: 0.868000473540902; test loss: 0.77748956288099, acc: 0.735759867643583
epoch: 156, train loss: 0.21110619486394375, acc: 0.8640345684858529; test loss: 0.7155669702477163, acc: 0.7478137556133302
epoch: 157, train loss: 0.19850286539206072, acc: 0.868888362732331; test loss: 0.7315991144601799, acc: 0.7449775466792721
epoch: 158, train loss: 0.1955222257290477, acc: 0.8734461939149994; test loss: 0.719215342153812, acc: 0.7367052706216024
epoch: 159, train loss: 0.20329371586175618, acc: 0.8674677400260448; test loss: 0.7051294718586618, acc: 0.737414322855117
epoch: 160, train loss: 0.1978706974887673, acc: 0.8714928376938558; test loss: 0.6906209486760982, acc: 0.7473410541243205
epoch: 161, train loss: 0.18714149275291203, acc: 0.8770569432934769; test loss: 0.7319137206513728, acc: 0.748050106357835
epoch: 162, train loss: 0.18659921844456906, acc: 0.8782999881614775; test loss: 0.7437394616508619, acc: 0.7421413377452138
epoch: 163, train loss: 0.19171548865244234, acc: 0.87131525985557; test loss: 0.8090562928063851, acc: 0.7194516662727488
Epoch   163: reducing learning rate of group 0 to 3.7500e-04.
epoch: 164, train loss: 0.1647796524644279, acc: 0.8858766425950041; test loss: 0.7220139748039778, acc: 0.7575041361380288
epoch: 165, train loss: 0.14163613921917584, acc: 0.8976559725346277; test loss: 0.7526997830731963, acc: 0.7515953675254077
epoch: 166, train loss: 0.1446996934917448, acc: 0.8999052918195809; test loss: 0.7515402207614625, acc: 0.7489955093358545
epoch: 167, train loss: 0.13847130356744378, acc: 0.9035752338108204; test loss: 0.7794275958327869, acc: 0.7518317182699126
epoch: 168, train loss: 0.13866633310873466, acc: 0.9013259145258672; test loss: 0.7776898903870408, acc: 0.748050106357835
epoch: 169, train loss: 0.13803814533699668, acc: 0.9003196401089144; test loss: 0.8016502158463185, acc: 0.7459229496572914
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.10065435980150823, acc: 0.9041671599384397; test loss: 0.7099318763263623, acc: 0.7421413377452138
epoch: 171, train loss: 0.09915193170573679, acc: 0.9015626849769148; test loss: 0.692409242729407, acc: 0.752777121247932
epoch: 172, train loss: 0.09442863745761085, acc: 0.9065348644489167; test loss: 0.7054790426050058, acc: 0.7466320018908059
epoch: 173, train loss: 0.09206537399769563, acc: 0.9074227536403456; test loss: 0.7098808369155303, acc: 0.7492318600803592
epoch: 174, train loss: 0.09308139258508105, acc: 0.9049958565171067; test loss: 0.7230229733317661, acc: 0.7487591585913496
epoch: 175, train loss: 0.10022097469131185, acc: 0.9008523736237718; test loss: 0.7138393613300941, acc: 0.7449775466792721
epoch: 176, train loss: 0.09255532263153557, acc: 0.9057653604830117; test loss: 0.7121863050174555, acc: 0.7456865989127865
epoch: 177, train loss: 0.09351217604814598, acc: 0.9084882206700604; test loss: 0.7204536252820838, acc: 0.7452138974237769
epoch: 178, train loss: 0.0973579268671005, acc: 0.9026873446193915; test loss: 0.727260740956807, acc: 0.7466320018908059
epoch: 179, train loss: 0.09440564538838525, acc: 0.905587782644726; test loss: 0.7171559085295968, acc: 0.7440321437012527
epoch: 180, train loss: 0.0939119836584314, acc: 0.9062980939978691; test loss: 0.7358300901704634, acc: 0.7454502481682818
epoch: 181, train loss: 0.09459477920771858, acc: 0.9065348644489167; test loss: 0.7177712840973077, acc: 0.7442684944457575
epoch: 182, train loss: 0.09249530302196815, acc: 0.9045223156150113; test loss: 0.745095273445921, acc: 0.7471047033798156
epoch: 183, train loss: 0.09867920215660911, acc: 0.899017402628152; test loss: 0.7282600766855671, acc: 0.7376506735996219
epoch: 184, train loss: 0.09282414603246744, acc: 0.9062389013851071; test loss: 0.7553801893258145, acc: 0.7471047033798156
epoch: 185, train loss: 0.09091927737308082, acc: 0.9029241150704392; test loss: 0.7286168550154583, acc: 0.738832427322146
epoch: 186, train loss: 0.10205683396072716, acc: 0.8980111282111992; test loss: 0.7526795758166457, acc: 0.7463956511463011
epoch: 187, train loss: 0.09480794796939995, acc: 0.9017402628152007; test loss: 0.7471563619407631, acc: 0.7463956511463011
epoch: 188, train loss: 0.08485418838168003, acc: 0.9097312655380608; test loss: 0.7422797017783821, acc: 0.7508863152918932
epoch: 189, train loss: 0.08770819119928264, acc: 0.9089025689593939; test loss: 0.7578245159920945, acc: 0.7452138974237769
epoch: 190, train loss: 0.09284725091002799, acc: 0.9054102048064402; test loss: 0.7508889632539472, acc: 0.7423776884897187
epoch: 191, train loss: 0.09752132496273389, acc: 0.8984846691132946; test loss: 0.7558241135063704, acc: 0.7430867407232333
epoch: 192, train loss: 0.09797678998098919, acc: 0.8999644844323429; test loss: 0.7415925087699989, acc: 0.738832427322146
epoch: 193, train loss: 0.08852031677379205, acc: 0.9098496507635847; test loss: 0.7482091351001715, acc: 0.7400141810446703
epoch: 194, train loss: 0.08776230126622867, acc: 0.9056469752574878; test loss: 0.7518200606100668, acc: 0.7433230914677381
epoch: 195, train loss: 0.09502270310760828, acc: 0.9024505741683438; test loss: 0.7066018545326119, acc: 0.7504136138028835
epoch: 196, train loss: 0.08970365840097115, acc: 0.9059429383212975; test loss: 0.7593350480617684, acc: 0.7411959347671945
epoch: 197, train loss: 0.08975729507953016, acc: 0.9067124422872026; test loss: 0.7532167608251642, acc: 0.7463956511463011
epoch: 198, train loss: 0.0978168342388197, acc: 0.8995501361430094; test loss: 0.7404107996769984, acc: 0.7487591585913496
epoch: 199, train loss: 0.08703785241126479, acc: 0.9065940570616787; test loss: 0.758050152269894, acc: 0.7364689198770976
epoch: 200, train loss: 0.0882275112117058, acc: 0.9076003314786315; test loss: 0.7698745692038305, acc: 0.7310328527534862
best test acc 0.7575041361380288 at epoch 164.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9598    0.9754    0.9676      6100
           1     0.9530    0.9417    0.9473       926
           2     0.8830    0.9621    0.9208      2400
           3     0.9358    0.9336    0.9347       843
           4     0.9395    0.9432    0.9413       774
           5     0.9364    0.9735    0.9546      1512
           6     0.8621    0.8699    0.8660      1330
           7     0.9607    0.9148    0.9372       481
           8     0.8730    0.9454    0.9078       458
           9     0.9289    0.9823    0.9548       452
          10     0.9549    0.8856    0.9190       717
          11     0.9431    0.8468    0.8924       333
          12     0.9500    0.0635    0.1191       299
          13     0.9171    0.7398    0.8189       269

    accuracy                         0.9310     16894
   macro avg     0.9284    0.8555    0.8630     16894
weighted avg     0.9321    0.9310    0.9241     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8143    0.8656    0.8392      1525
           1     0.7944    0.8491    0.8208       232
           2     0.7224    0.7837    0.7518       601
           3     0.7938    0.7299    0.7605       211
           4     0.8772    0.7732    0.8219       194
           5     0.7575    0.8016    0.7789       378
           6     0.5432    0.5285    0.5358       333
           7     0.7054    0.6529    0.6781       121
           8     0.5242    0.5652    0.5439       115
           9     0.7895    0.7895    0.7895       114
          10     0.7673    0.6778    0.7198       180
          11     0.6721    0.4881    0.5655        84
          12     0.4286    0.0400    0.0732        75
          13     0.7727    0.5000    0.6071        68

    accuracy                         0.7575      4231
   macro avg     0.7116    0.6461    0.6633      4231
weighted avg     0.7516    0.7575    0.7500      4231

---------------------------------------
program finished.
