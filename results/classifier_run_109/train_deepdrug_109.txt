seed:  9
save trained model at:  ../trained_models/trained_classifier_model_109.pt
save loss at:  ./results/train_classifier_results_109.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['2j9dL01', '3cw8A00', '1h6vE01', '5x8aB00', '4uctA01']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['4b4lA00', '1zm4D00', '1zunB00', '2o7pA00', '2d3yA00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2ba6069fad60>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0524101413692035, acc: 0.3905528590031964; test loss: 1.7479084144385817, acc: 0.45733869061687543
epoch: 2, train loss: 1.73311895345871, acc: 0.47259382029122765; test loss: 1.7046163243848476, acc: 0.4835736232569133
epoch: 3, train loss: 1.6246277204953639, acc: 0.5020125488339056; test loss: 1.5177454842486524, acc: 0.5303710706688726
epoch: 4, train loss: 1.5695385985562431, acc: 0.5259855570024861; test loss: 1.4618486634554015, acc: 0.5516426376743087
epoch: 5, train loss: 1.5272121858466965, acc: 0.5341541375636321; test loss: 1.505633163948032, acc: 0.5350980855589695
epoch: 6, train loss: 1.4803215400331808, acc: 0.5500769503965905; test loss: 1.4155868126466409, acc: 0.5618057196880171
epoch: 7, train loss: 1.4282549165389489, acc: 0.5655854149402154; test loss: 1.3814131942547063, acc: 0.5717324509572205
epoch: 8, train loss: 1.4070908177092298, acc: 0.5700840535101219; test loss: 1.3732571399006204, acc: 0.5759867643583078
epoch: 9, train loss: 1.3973504122833085, acc: 0.575766544335267; test loss: 1.3876383176117015, acc: 0.5655873316000946
epoch: 10, train loss: 1.3679297027241284, acc: 0.5828104652539363; test loss: 1.4380876525472281, acc: 0.5608603167099976
epoch: 11, train loss: 1.3430922651087671, acc: 0.5868947555345093; test loss: 1.317674771236545, acc: 0.5904041597731032
epoch: 12, train loss: 1.338401337819818, acc: 0.5945306025807979; test loss: 1.3123127191913688, acc: 0.5939494209406759
epoch: 13, train loss: 1.3064866472704673, acc: 0.6057180063928022; test loss: 1.3642824920630856, acc: 0.5802410777593949
epoch: 14, train loss: 1.3003710493669434, acc: 0.6093287557712798; test loss: 1.2852044786242722, acc: 0.5948948239186953
epoch: 15, train loss: 1.288398205041462, acc: 0.6105718006392802; test loss: 1.3341830348720565, acc: 0.5934767194516662
epoch: 16, train loss: 1.2798818495339355, acc: 0.6125843494731857; test loss: 1.258356212865261, acc: 0.6112030252895296
epoch: 17, train loss: 1.2563160998279053, acc: 0.6209305078726175; test loss: 1.342300145684109, acc: 0.5934767194516662
epoch: 18, train loss: 1.2454358049577632, acc: 0.6229430567065231; test loss: 1.2725659231011452, acc: 0.6048215551878988
epoch: 19, train loss: 1.2503273804201605, acc: 0.6210488930981414; test loss: 1.471320735847547, acc: 0.5568423540534153
epoch: 20, train loss: 1.2364280423963367, acc: 0.6259026873446194; test loss: 1.2611425244703949, acc: 0.6114393760340345
epoch: 21, train loss: 1.2388787578235254, acc: 0.6261394577956672; test loss: 1.2065430406531215, acc: 0.6265658236823446
epoch: 22, train loss: 1.1939864883454463, acc: 0.6375044394459571; test loss: 1.2456064188178904, acc: 0.6282202788938785
epoch: 23, train loss: 1.2090302113881546, acc: 0.6369125133183379; test loss: 1.238609054076759, acc: 0.6185298983691798
epoch: 24, train loss: 1.1837606216670917, acc: 0.6433053154966261; test loss: 1.2477591404591404, acc: 0.6161663909241314
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9714717541246763, acc: 0.6379779803480525; test loss: 1.0062342697608784, acc: 0.6149846372016072
epoch: 26, train loss: 0.9435347855634431, acc: 0.6454362495560554; test loss: 0.9799376304367704, acc: 0.6258567714488301
epoch: 27, train loss: 0.9504520248009126, acc: 0.6427725819817687; test loss: 0.9484445748388894, acc: 0.6322382415504609
epoch: 28, train loss: 0.9199809964710515, acc: 0.6521250147981532; test loss: 1.082855777688636, acc: 0.5868588986055306
epoch: 29, train loss: 0.9308338094014479, acc: 0.648987806321771; test loss: 0.9382250801493051, acc: 0.6341290475064997
epoch: 30, train loss: 0.9239198680712625, acc: 0.6513555108322482; test loss: 0.9902107942135332, acc: 0.6291656818718979
epoch: 31, train loss: 0.9158318717832239, acc: 0.6554398011128211; test loss: 0.9713384786189744, acc: 0.6239659654927913
epoch: 32, train loss: 0.9128085884509375, acc: 0.653900793181011; test loss: 1.3065783656311216, acc: 0.5606239659654928
epoch: 33, train loss: 0.9017652538071952, acc: 0.6569788090446312; test loss: 1.0575595418771822, acc: 0.6055306074214134
epoch: 34, train loss: 0.8836864726394115, acc: 0.6605303658103469; test loss: 1.0108734825434513, acc: 0.619948002836209
epoch: 35, train loss: 0.8784594833462763, acc: 0.6640819225760625; test loss: 0.9596150092722479, acc: 0.6357835027180335
epoch: 36, train loss: 0.8762938694317721, acc: 0.6625429146442524; test loss: 1.2590144976068405, acc: 0.5759867643583078
epoch: 37, train loss: 0.8614287804510095, acc: 0.6743814371966379; test loss: 0.8964109275955817, acc: 0.6539825100449066
epoch: 38, train loss: 0.861041352941367, acc: 0.6672783236652066; test loss: 0.9886565832234082, acc: 0.6164027416686363
epoch: 39, train loss: 0.864193648526806, acc: 0.6741446667455901; test loss: 1.013158638464365, acc: 0.616875443157646
epoch: 40, train loss: 0.8571681342238404, acc: 0.6715401917840653; test loss: 1.1882559339139682, acc: 0.5603876152209879
epoch: 41, train loss: 0.8265593013382951, acc: 0.67751864567302; test loss: 0.953093829940493, acc: 0.6230205625147719
epoch: 42, train loss: 0.8304412423244831, acc: 0.6799455427962591; test loss: 0.9970365079461866, acc: 0.612148428267549
epoch: 43, train loss: 0.8124732683717365, acc: 0.683201136498165; test loss: 0.8879958333430407, acc: 0.6584731741904987
epoch: 44, train loss: 0.8114445669503978, acc: 0.684029833076832; test loss: 0.8823776618720397, acc: 0.658000472701489
epoch: 45, train loss: 0.7962018129062663, acc: 0.6888836273233101; test loss: 0.9626716029641195, acc: 0.6324745922949657
epoch: 46, train loss: 0.8012171405807139, acc: 0.6895939386764531; test loss: 0.9568392892335781, acc: 0.632001890805956
epoch: 47, train loss: 0.7870389306292642, acc: 0.6935598437315023; test loss: 0.8336340443402198, acc: 0.6728905696052943
epoch: 48, train loss: 0.7846759225512957, acc: 0.6938558067953119; test loss: 1.1156405538736125, acc: 0.5920586149846372
epoch: 49, train loss: 0.7783545242395149, acc: 0.6989463714928377; test loss: 0.8973537044755057, acc: 0.6587095249350036
epoch: 50, train loss: 0.7745161882741812, acc: 0.6987687936545519; test loss: 0.9242726832353542, acc: 0.6433467265421886
epoch: 51, train loss: 0.7925034117540608, acc: 0.693027110216645; test loss: 0.9405537607314259, acc: 0.642637674308674
epoch: 52, train loss: 0.7634849627370508, acc: 0.7031490469989345; test loss: 0.9239565988038906, acc: 0.6513826518553534
epoch: 53, train loss: 0.7542305496284577, acc: 0.7082396116964603; test loss: 0.9157600211804146, acc: 0.6532734578113921
epoch: 54, train loss: 0.7400393326973738, acc: 0.7113176275600805; test loss: 0.8396282165615702, acc: 0.6724178681162846
epoch: 55, train loss: 0.7485549484073192, acc: 0.7062270628625548; test loss: 0.8622872144216286, acc: 0.6695816591822265
epoch: 56, train loss: 0.732506455719732, acc: 0.7114360127856043; test loss: 0.8104649761049383, acc: 0.6873079650200898
epoch: 57, train loss: 0.7242105238406887, acc: 0.7105481235941754; test loss: 1.6420681587039818, acc: 0.4112502954384306
epoch: 58, train loss: 0.7390356014448608, acc: 0.7106073162069374; test loss: 0.893649051443563, acc: 0.6539825100449066
epoch: 59, train loss: 0.7346698726452751, acc: 0.7105481235941754; test loss: 0.813907854722144, acc: 0.6816355471519735
epoch: 60, train loss: 0.7091792096636684, acc: 0.7211436012785605; test loss: 0.8594068755102733, acc: 0.6726542188607895
epoch: 61, train loss: 0.7064794450804258, acc: 0.7192494376701788; test loss: 0.8339028177011156, acc: 0.6849444575750414
epoch: 62, train loss: 0.6983331350641193, acc: 0.7246951580442761; test loss: 0.8460475226379517, acc: 0.6719451666272749
epoch: 63, train loss: 0.6992054438681324, acc: 0.7198413637977981; test loss: 0.7868040632452139, acc: 0.6948711888442448
epoch: 64, train loss: 0.7121059892515438, acc: 0.7165857700958921; test loss: 0.8063482770511823, acc: 0.6858898605530608
epoch: 65, train loss: 0.6844190717576114, acc: 0.7304960340949449; test loss: 0.8940525595895564, acc: 0.6570550697234696
epoch: 66, train loss: 0.6939992432172615, acc: 0.7270036699419913; test loss: 0.8087187326706715, acc: 0.6863625620420705
epoch: 67, train loss: 0.6969530040785675, acc: 0.723096957499704; test loss: 0.7767545207683371, acc: 0.6967619948002837
epoch: 68, train loss: 0.6944741308653836, acc: 0.7262933585888481; test loss: 0.8005731828216682, acc: 0.6849444575750414
epoch: 69, train loss: 0.6690912251572955, acc: 0.7365928732094235; test loss: 0.7443544126464188, acc: 0.7085795320255259
epoch: 70, train loss: 0.6636411026411259, acc: 0.7387829998816148; test loss: 0.8024217580909341, acc: 0.6953438903332545
epoch: 71, train loss: 0.6733892889534833, acc: 0.7295489522907541; test loss: 0.8759789964432019, acc: 0.6686362562042071
epoch: 72, train loss: 0.6677468112054994, acc: 0.7341659760861844; test loss: 0.7943726008565565, acc: 0.6863625620420705
epoch: 73, train loss: 0.6763719670501872, acc: 0.7333964721202794; test loss: 0.9848278211211797, acc: 0.6353108012290238
epoch: 74, train loss: 0.6563496566868765, acc: 0.7374807624008524; test loss: 0.7552357054128627, acc: 0.7066887260694871
epoch: 75, train loss: 0.6471393578953188, acc: 0.7431632532259974; test loss: 0.8811932526928571, acc: 0.6837627038525171
epoch: 76, train loss: 0.6467507401556465, acc: 0.7397300816858056; test loss: 0.9366983016087805, acc: 0.6402741668636256
epoch: 77, train loss: 0.6380532949340828, acc: 0.747484313957618; test loss: 0.8495984549106533, acc: 0.6776175844953911
epoch: 78, train loss: 0.6436247439501313, acc: 0.7466556173789511; test loss: 0.7646559762830741, acc: 0.7073977783030017
epoch: 79, train loss: 0.6415355315473188, acc: 0.7440511424174263; test loss: 0.7986265635327001, acc: 0.6969983455447885
epoch: 80, train loss: 0.6437331865818046, acc: 0.7454717651237126; test loss: 0.7842094968098384, acc: 0.7026707634129048
Epoch    80: reducing learning rate of group 0 to 1.5000e-03.
epoch: 81, train loss: 0.567541954156646, acc: 0.771161359062389; test loss: 0.6901337472689797, acc: 0.7362325691325927
epoch: 82, train loss: 0.5423830628070491, acc: 0.7799218657511543; test loss: 0.6926440512477521, acc: 0.7395414795556606
epoch: 83, train loss: 0.5267673907963584, acc: 0.789096720729253; test loss: 0.7464366131599731, acc: 0.71756086031671
epoch: 84, train loss: 0.5263703282068252, acc: 0.7855451639635374; test loss: 0.7655775297958781, acc: 0.7121247931930985
epoch: 85, train loss: 0.5230769446097394, acc: 0.7886231798271576; test loss: 0.7824631307007319, acc: 0.7123611439376034
epoch: 86, train loss: 0.5234420612830163, acc: 0.7879128684740144; test loss: 0.74706344163832, acc: 0.7227605766958166
epoch: 87, train loss: 0.5191610457700456, acc: 0.7918195809163017; test loss: 0.7452824152944444, acc: 0.7277239423304184
epoch: 88, train loss: 0.5112410377959351, acc: 0.7920563513673493; test loss: 0.8155847428059696, acc: 0.7047979201134483
epoch: 89, train loss: 0.5195935326693989, acc: 0.7885639872143957; test loss: 0.8255943755003555, acc: 0.703852517135429
epoch: 90, train loss: 0.5005251486033, acc: 0.795548715520303; test loss: 0.7159532204611987, acc: 0.7331600094540298
epoch: 91, train loss: 0.49916764236814637, acc: 0.7937137445246834; test loss: 0.7219316241476897, acc: 0.7338690616875443
epoch: 92, train loss: 0.5056424409563871, acc: 0.7931218183970641; test loss: 0.7505723986606512, acc: 0.7341054124320492
epoch: 93, train loss: 0.49968601074023433, acc: 0.7978572274180182; test loss: 0.7359219556496065, acc: 0.725124084140865
epoch: 94, train loss: 0.49539256682265537, acc: 0.7953119450692554; test loss: 0.7479544802214682, acc: 0.7225242259513117
epoch: 95, train loss: 0.4938812726550488, acc: 0.7962590268734462; test loss: 0.7151451094498226, acc: 0.7355235168990782
epoch: 96, train loss: 0.4908620714576792, acc: 0.8015271694092577; test loss: 0.7140953468109975, acc: 0.7364689198770976
epoch: 97, train loss: 0.4935414466587512, acc: 0.7967917603883036; test loss: 0.7118350902197632, acc: 0.737414322855117
epoch: 98, train loss: 0.4794175360129968, acc: 0.8019415176985912; test loss: 0.7766924880408024, acc: 0.7232332781848263
epoch: 99, train loss: 0.49730408827101563, acc: 0.7942464780395406; test loss: 0.7254167119258123, acc: 0.7336327109430395
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.37105626892066923, acc: 0.805433881851545; test loss: 0.6166793150531009, acc: 0.734341763176554
epoch: 101, train loss: 0.3753208685509761, acc: 0.8027110216644963; test loss: 0.7365389661962499, acc: 0.6920349799101867
epoch: 102, train loss: 0.37050202665306964, acc: 0.8030661773410679; test loss: 0.6149244430752067, acc: 0.7194516662727488
epoch: 103, train loss: 0.36716886454112885, acc: 0.8028294068900201; test loss: 0.5809883102968325, acc: 0.7407232332781848
epoch: 104, train loss: 0.3573898714372247, acc: 0.8098141351959276; test loss: 0.6541702133125517, acc: 0.7170881588277003
epoch: 105, train loss: 0.35907141791796193, acc: 0.8089854386172606; test loss: 0.6003656544546742, acc: 0.7385960765776413
epoch: 106, train loss: 0.34939982850297296, acc: 0.8120634544808808; test loss: 0.6330173537399266, acc: 0.7291420467974474
epoch: 107, train loss: 0.35772529062370134, acc: 0.8035989108559252; test loss: 0.6001032823311525, acc: 0.7423776884897187
epoch: 108, train loss: 0.34916726661120784, acc: 0.8125369953829762; test loss: 0.6519477975985537, acc: 0.720633419995273
epoch: 109, train loss: 0.35510151734772105, acc: 0.8091630164555463; test loss: 0.6026430499663564, acc: 0.7445048451902624
epoch: 110, train loss: 0.3588043392847681, acc: 0.8066769267195454; test loss: 0.632452269587723, acc: 0.7289056960529425
epoch: 111, train loss: 0.3522977790522606, acc: 0.8100509056469752; test loss: 0.6386757314810937, acc: 0.7225242259513117
epoch: 112, train loss: 0.34157275768995593, acc: 0.8117082988043092; test loss: 0.6149199815315999, acc: 0.7293783975419522
epoch: 113, train loss: 0.346913382271641, acc: 0.8133656919616432; test loss: 0.6017871554989365, acc: 0.7393051288111557
epoch: 114, train loss: 0.3445604088723596, acc: 0.8114123357404995; test loss: 0.6839581087559172, acc: 0.7040888678799339
epoch: 115, train loss: 0.3461675352036664, acc: 0.8088670533917367; test loss: 0.6305848929482525, acc: 0.7329236587095249
epoch: 116, train loss: 0.3452686373935136, acc: 0.8104060613235469; test loss: 0.6612584840544176, acc: 0.7095249350035453
epoch: 117, train loss: 0.3422768847172086, acc: 0.8107020243873565; test loss: 0.6122764641724138, acc: 0.734341763176554
epoch: 118, train loss: 0.34577528215222375, acc: 0.8101692908724991; test loss: 0.6563443439955183, acc: 0.7123611439376034
epoch: 119, train loss: 0.3382443623972092, acc: 0.8125961879957382; test loss: 0.664803938785644, acc: 0.7201607185062633
epoch: 120, train loss: 0.3274604826900992, acc: 0.8197584941399313; test loss: 0.6651060401449471, acc: 0.7123611439376034
epoch: 121, train loss: 0.33469001986408314, acc: 0.8147863146679294; test loss: 0.6222399226657269, acc: 0.7341054124320492
epoch: 122, train loss: 0.32363667090969056, acc: 0.8182786788208831; test loss: 0.8136877887385925, acc: 0.6606003308910423
epoch: 123, train loss: 0.32562966005991545, acc: 0.8185746418846928; test loss: 0.6819377516888749, acc: 0.7118884424485937
epoch: 124, train loss: 0.36193571048572076, acc: 0.8038356813069729; test loss: 0.6312260006765078, acc: 0.720633419995273
epoch: 125, train loss: 0.3393119551506891, acc: 0.8119450692553569; test loss: 0.6129087548367494, acc: 0.7393051288111557
epoch: 126, train loss: 0.33879733413045077, acc: 0.8131289215105955; test loss: 0.6346344577029253, acc: 0.7329236587095249
epoch: 127, train loss: 0.33524521203440183, acc: 0.812714573221262; test loss: 0.6318769350144294, acc: 0.7362325691325927
epoch: 128, train loss: 0.3164789787591442, acc: 0.8185154492719309; test loss: 0.6787851278282063, acc: 0.7270148900969038
epoch: 129, train loss: 0.31336019959861294, acc: 0.8218302355865988; test loss: 0.6374080137732011, acc: 0.7421413377452138
epoch: 130, train loss: 0.31949135310426613, acc: 0.8204688054930744; test loss: 0.6705800898107224, acc: 0.7225242259513117
epoch: 131, train loss: 0.3137720538460038, acc: 0.8198768793654552; test loss: 0.6358571788487607, acc: 0.7447411959347672
epoch: 132, train loss: 0.3088795573764741, acc: 0.8228365100035515; test loss: 0.6346474187477236, acc: 0.7371779721106122
epoch: 133, train loss: 0.32388637776316676, acc: 0.8153190481827868; test loss: 0.6779168077012209, acc: 0.7274875915859135
epoch: 134, train loss: 0.3167380056353565, acc: 0.8199360719782172; test loss: 0.734314649444865, acc: 0.7024344126683999
epoch: 135, train loss: 0.32097380034201267, acc: 0.8166804782763111; test loss: 0.6062267135993382, acc: 0.7506499645473883
epoch: 136, train loss: 0.30884293699047194, acc: 0.8270391855096484; test loss: 0.6294916131991749, acc: 0.7364689198770976
epoch: 137, train loss: 0.2968748198065572, acc: 0.8275127264117438; test loss: 0.6485741383019026, acc: 0.7381233750886316
epoch: 138, train loss: 0.3226637791335491, acc: 0.8124186101574523; test loss: 0.6493271413747652, acc: 0.7385960765776413
epoch: 139, train loss: 0.2973861038804802, acc: 0.8272759559606961; test loss: 0.6258805384452223, acc: 0.7492318600803592
epoch: 140, train loss: 0.2985907220566669, acc: 0.8289333491180301; test loss: 0.6307261559000198, acc: 0.7416686362562042
epoch: 141, train loss: 0.30476439995879717, acc: 0.8230732804545993; test loss: 0.6505012578137009, acc: 0.7265421886078941
epoch: 142, train loss: 0.3005600371056793, acc: 0.8252042145140287; test loss: 0.755192246061779, acc: 0.6925076813991964
epoch: 143, train loss: 0.30270407016607265, acc: 0.824967444062981; test loss: 0.6691964156124446, acc: 0.7227605766958166
epoch: 144, train loss: 0.29518221506596004, acc: 0.8307683201136499; test loss: 0.6145746955576306, acc: 0.752540770503427
epoch: 145, train loss: 0.29947219526357055, acc: 0.8241387474843139; test loss: 0.6773325378690254, acc: 0.7338690616875443
epoch: 146, train loss: 0.2913233159550749, acc: 0.8295844678584112; test loss: 0.6879975752356035, acc: 0.7263058378633893
epoch: 147, train loss: 0.28028381093092836, acc: 0.8327808689475553; test loss: 0.7377038700759228, acc: 0.6967619948002837
epoch: 148, train loss: 0.2999516064822639, acc: 0.8262104889309814; test loss: 0.6631819048380914, acc: 0.7381233750886316
epoch: 149, train loss: 0.2862122136999393, acc: 0.8327216763347934; test loss: 0.6553404390375205, acc: 0.7416686362562042
epoch: 150, train loss: 0.3023135697360404, acc: 0.823310050905647; test loss: 0.6657660390659585, acc: 0.7296147482864571
epoch: 151, train loss: 0.2954687755094049, acc: 0.8272759559606961; test loss: 0.6581314244582506, acc: 0.7326873079650201
epoch: 152, train loss: 0.2884985518001079, acc: 0.8321297502071742; test loss: 0.7608269693101447, acc: 0.7003072559678563
epoch: 153, train loss: 0.29763396692738897, acc: 0.8277494968627915; test loss: 0.6592925101469325, acc: 0.7383597258331364
epoch: 154, train loss: 0.29166459079337453, acc: 0.8245530957736474; test loss: 0.6572060701667318, acc: 0.7348144646655637
epoch: 155, train loss: 0.28631764250075703, acc: 0.8298804309222209; test loss: 0.680823467888468, acc: 0.723705979673836
Epoch   155: reducing learning rate of group 0 to 7.5000e-04.
epoch: 156, train loss: 0.2430153116310392, acc: 0.8505386527761335; test loss: 0.6545467343462243, acc: 0.754195225714961
epoch: 157, train loss: 0.20961266985211385, acc: 0.8677045104770924; test loss: 0.6531775988351366, acc: 0.7518317182699126
epoch: 158, train loss: 0.20677309512838013, acc: 0.863975375873091; test loss: 0.6734650194208213, acc: 0.7544315764594659
epoch: 159, train loss: 0.19798115447800413, acc: 0.8709009115662365; test loss: 0.6849429090094605, acc: 0.7549042779484756
epoch: 160, train loss: 0.1920832803725801, acc: 0.8736829643660471; test loss: 0.6921999339543458, acc: 0.7475774048688253
epoch: 161, train loss: 0.19289771658761468, acc: 0.8758138984254765; test loss: 0.7417694362053435, acc: 0.7426140392342235
epoch: 162, train loss: 0.1901970701209466, acc: 0.8734461939149994; test loss: 0.7064428873812721, acc: 0.7494682108248641
epoch: 163, train loss: 0.19355396615043396, acc: 0.8752219722978573; test loss: 0.7218704234902894, acc: 0.748050106357835
epoch: 164, train loss: 0.19571856109329944, acc: 0.8714928376938558; test loss: 0.7620599864363755, acc: 0.7329236587095249
epoch: 165, train loss: 0.19808189295884124, acc: 0.8701314076003315; test loss: 0.7061601543899857, acc: 0.7504136138028835
epoch: 166, train loss: 0.19865488127531972, acc: 0.8694210962471883; test loss: 0.7168942915015513, acc: 0.7563223824155046
epoch: 167, train loss: 0.20841815735696265, acc: 0.8655143838049012; test loss: 0.6991116400447982, acc: 0.7445048451902624
epoch: 168, train loss: 0.20375504231712613, acc: 0.8694210962471883; test loss: 0.7120586147096472, acc: 0.7456865989127865
epoch: 169, train loss: 0.19281686603056267, acc: 0.8724991121108086; test loss: 0.7158389216026102, acc: 0.7499409123138738
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.14848079335778455, acc: 0.8717888007576654; test loss: 0.6139459178904725, acc: 0.7532498227369416
epoch: 171, train loss: 0.13830651651053114, acc: 0.8790694921273825; test loss: 0.6339118012128725, acc: 0.7359962183880879
epoch: 172, train loss: 0.13969395260027884, acc: 0.8751627796850953; test loss: 0.6294859606615056, acc: 0.7430867407232333
epoch: 173, train loss: 0.1386726439912968, acc: 0.8753403575233811; test loss: 0.6290257811630852, acc: 0.7437957929567478
epoch: 174, train loss: 0.1317790189725528, acc: 0.879483840416716; test loss: 0.6323193014442878, acc: 0.7411959347671945
epoch: 175, train loss: 0.13828805738177344, acc: 0.8772937137445247; test loss: 0.6186272591401769, acc: 0.7482864571023399
epoch: 176, train loss: 0.13776072342007403, acc: 0.874570853557476; test loss: 0.6498752610673861, acc: 0.7364689198770976
epoch: 177, train loss: 0.13835061116780673, acc: 0.8739197348170948; test loss: 0.6320762248840548, acc: 0.7478137556133302
epoch: 178, train loss: 0.1341689677863371, acc: 0.8804309222209068; test loss: 0.6454067508032124, acc: 0.7492318600803592
epoch: 179, train loss: 0.1351240030019625, acc: 0.8763466319403338; test loss: 0.6514113320157027, acc: 0.7333963601985346
epoch: 180, train loss: 0.13435735954124997, acc: 0.8793062625784303; test loss: 0.6579140129734005, acc: 0.737414322855117
epoch: 181, train loss: 0.13548376741170234, acc: 0.8772937137445247; test loss: 0.6482692437655423, acc: 0.7442684944457575
epoch: 182, train loss: 0.15365215674760682, acc: 0.8700130223748076; test loss: 0.6463479845835453, acc: 0.7421413377452138
epoch: 183, train loss: 0.13327619974092716, acc: 0.880253344382621; test loss: 0.6511173430866339, acc: 0.7411959347671945
epoch: 184, train loss: 0.13771234574975016, acc: 0.8736829643660471; test loss: 0.6092586636796851, acc: 0.7520680690144174
epoch: 185, train loss: 0.13459187286329422, acc: 0.8752219722978573; test loss: 0.633604837264104, acc: 0.7530134719924367
epoch: 186, train loss: 0.16866569731367315, acc: 0.8544453652184207; test loss: 0.6089324220307074, acc: 0.7390687780666509
epoch: 187, train loss: 0.14558340609250048, acc: 0.8715520303066178; test loss: 0.6105119822521746, acc: 0.7478137556133302
epoch: 188, train loss: 0.13343027901427906, acc: 0.8767609802296673; test loss: 0.6388943704750712, acc: 0.7482864571023399
epoch: 189, train loss: 0.12992602536999234, acc: 0.8771753285190008; test loss: 0.6320632771379529, acc: 0.7513590167809029
epoch: 190, train loss: 0.13115888583324245, acc: 0.8703089854386172; test loss: 0.6432318819862425, acc: 0.7454502481682818
epoch: 191, train loss: 0.1251082364410229, acc: 0.8788919142890967; test loss: 0.6865307669131306, acc: 0.7407232332781848
epoch: 192, train loss: 0.14016869260042084, acc: 0.8671717769622351; test loss: 0.6511172686477441, acc: 0.7463956511463011
epoch: 193, train loss: 0.14287043366648677, acc: 0.8690659405706168; test loss: 0.6516641508572154, acc: 0.7447411959347672
epoch: 194, train loss: 0.1316197719537423, acc: 0.8742156978809045; test loss: 0.683737296298502, acc: 0.743559442212243
epoch: 195, train loss: 0.129276514173302, acc: 0.8743932757191902; test loss: 0.6831094220129206, acc: 0.7348144646655637
epoch: 196, train loss: 0.13590738309076075, acc: 0.8722623416597609; test loss: 0.6505759735531177, acc: 0.7440321437012527
epoch: 197, train loss: 0.14119211299166504, acc: 0.8695986740854741; test loss: 0.6469369100865053, acc: 0.7369416213661073
epoch: 198, train loss: 0.1289437452492819, acc: 0.879483840416716; test loss: 0.6749651677725136, acc: 0.734341763176554
epoch: 199, train loss: 0.12721187469379794, acc: 0.8727358825618563; test loss: 0.6775168596331714, acc: 0.7345781139210589
epoch: 200, train loss: 0.13289073464911944, acc: 0.8767609802296673; test loss: 0.6778154519680007, acc: 0.7341054124320492
best test acc 0.7563223824155046 at epoch 166.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9001    0.9700    0.9337      6100
           1     0.9634    0.8812    0.9205       926
           2     0.8261    0.9383    0.8787      2400
           3     0.9682    0.8671    0.9149       843
           4     0.8821    0.9767    0.9270       774
           5     0.9110    0.9405    0.9255      1512
           6     0.7302    0.7286    0.7294      1330
           7     0.8852    0.7692    0.8231       481
           8     0.9149    0.6572    0.7649       458
           9     0.9170    0.9292    0.9231       452
          10     0.9544    0.7880    0.8633       717
          11     0.9153    0.6817    0.7814       333
          12     0.2000    0.0033    0.0066       299
          13     0.8832    0.6468    0.7468       269

    accuracy                         0.8832     16894
   macro avg     0.8465    0.7699    0.7956     16894
weighted avg     0.8736    0.8832    0.8737     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8017    0.8721    0.8354      1525
           1     0.8710    0.8147    0.8419       232
           2     0.6644    0.8003    0.7260       601
           3     0.8177    0.7014    0.7551       211
           4     0.8600    0.8866    0.8731       194
           5     0.7985    0.8492    0.8231       378
           6     0.5097    0.5526    0.5303       333
           7     0.7263    0.5702    0.6389       121
           8     0.7662    0.5130    0.6146       115
           9     0.7723    0.6842    0.7256       114
          10     0.8974    0.5833    0.7071       180
          11     0.5932    0.4167    0.4895        84
          12     0.5000    0.0133    0.0260        75
          13     0.7778    0.4118    0.5385        68

    accuracy                         0.7563      4231
   macro avg     0.7397    0.6192    0.6518      4231
weighted avg     0.7565    0.7563    0.7468      4231

---------------------------------------
program finished.
