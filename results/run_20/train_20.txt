seed:  666
number of classes (from original clusters): 10
positive training pair sampling threshold:  2800
negative training pair sampling threshold:  250
positive validation pair sampling threshold:  1000
negative validation pair sampling threshold:  70
number of epochs to train: 60
batch size: 256
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['charge', 'hydrophobicity', 'binding_probability', 'distance_to_center', 'sequence_entropy']
number of pockets in training set:  10520
number of pockets in validation set:  2245
number of pockets in test set:  2279
number of train positive pairs: 60450
number of train negative pairs: 63250
number of validation positive pairs: 16423
number of validation negative pairs: 17706
model architecture:
SiameseNet(
  (embedding_net): EmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=5, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=5, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=5, out_features=5, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.9101358244394042, validation loss: 0.8895679592244453.
epoch: 2, train loss: 0.8453647996537031, validation loss: 0.8547265614090965.
epoch: 3, train loss: 0.8160910384502734, validation loss: 0.8494214302118204.
epoch: 4, train loss: 0.7905748048749969, validation loss: 0.8219408911969053.
epoch: 5, train loss: 0.7623484519767298, validation loss: 0.8245892999614343.
epoch: 6, train loss: 0.7424968684326764, validation loss: 0.8163734737151346.
epoch: 7, train loss: 0.7236180445332632, validation loss: 0.7976970699991782.
epoch: 8, train loss: 0.7053287535091515, validation loss: 0.8095201148440246.
epoch: 9, train loss: 0.691058838704756, validation loss: 0.7680856651675674.
epoch: 10, train loss: 0.678944843387835, validation loss: 0.7867866392492455.
epoch: 11, train loss: 0.6650246742143685, validation loss: 0.785769956090741.
epoch: 12, train loss: 0.6551832063716558, validation loss: 0.7694855710987804.
epoch: 13, train loss: 0.6431338900640884, validation loss: 0.7730154446396736.
epoch: 14, train loss: 0.6332784136168496, validation loss: 0.7710334013081858.
epoch: 15, train loss: 0.6250894492541771, validation loss: 0.765443954729322.
epoch: 16, train loss: 0.6142551892711544, validation loss: 0.7610901222272596.
epoch: 17, train loss: 0.6073371393794289, validation loss: 0.7464718522326652.
epoch: 18, train loss: 0.6007502908174704, validation loss: 0.7825542752743955.
epoch: 19, train loss: 0.5936995522476727, validation loss: 0.7620116544582286.
epoch: 20, train loss: 0.5875365353016094, validation loss: 0.7720320273313316.
epoch: 21, train loss: 0.5830862687033034, validation loss: 0.7527972567339577.
epoch: 22, train loss: 0.5767115992775056, validation loss: 0.7680225201860782.
epoch: 23, train loss: 0.5724876805939979, validation loss: 0.761416593875386.
epoch: 24, train loss: 0.5700317152758809, validation loss: 0.7644644874752261.
epoch: 25, train loss: 0.5663209259847334, validation loss: 0.7357818134963633.
epoch: 26, train loss: 0.5646092603432139, validation loss: 0.7609971768025646.
epoch: 27, train loss: 0.5565631929365202, validation loss: 0.7472567152692667.
epoch: 28, train loss: 0.5561389265006458, validation loss: 0.7592545572797442.
epoch: 29, train loss: 0.5527726781994659, validation loss: 0.7587815813163327.
epoch: 30, train loss: 0.5501560877288844, validation loss: 0.7679188490340074.
epoch: 31, train loss: 0.5487753412467198, validation loss: 0.7631648150088062.
epoch: 32, train loss: 0.5475748060591874, validation loss: 0.7789910947509129.
epoch: 33, train loss: 0.547414555468671, validation loss: 0.7407331941989057.
epoch: 34, train loss: 0.5462753368676817, validation loss: 0.7491472380569746.
epoch: 35, train loss: 0.5423308836459728, validation loss: 0.7417567487081413.
epoch: 36, train loss: 0.5426431054228626, validation loss: 0.7494487950788506.
epoch: 37, train loss: 0.5429141052788847, validation loss: 0.7705762000775378.
epoch: 38, train loss: 0.5418823271175499, validation loss: 0.7615134169254886.
epoch: 39, train loss: 0.539197264773863, validation loss: 0.7560383220698204.
epoch: 40, train loss: 0.5388937869492132, validation loss: 0.7696125124742131.
epoch: 41, train loss: 0.5365213673251622, validation loss: 0.7547745067964504.
epoch: 42, train loss: 0.5331039258483541, validation loss: 0.7523087161708323.
epoch: 43, train loss: 0.5328680818352665, validation loss: 0.744891215336166.
epoch: 44, train loss: 0.5351512471648504, validation loss: 0.758115936147852.
epoch: 45, train loss: 0.5341309489294128, validation loss: 0.7750071746576694.
epoch: 46, train loss: 0.5324158567214262, validation loss: 0.8105202033037794.
epoch: 47, train loss: 0.5292046961865313, validation loss: 0.7687543143444782.
epoch: 48, train loss: 0.5291758726793812, validation loss: 0.7731392027229467.
epoch: 49, train loss: 0.5298429307498176, validation loss: 0.736116620720196.
epoch: 50, train loss: 0.5265754812575196, validation loss: 0.7533324302917676.
epoch: 51, train loss: 0.5253957152308817, validation loss: 0.7534046168065923.
epoch: 52, train loss: 0.5252493798665299, validation loss: 0.7539991833532219.
epoch: 53, train loss: 0.5267424592840739, validation loss: 0.7591741344435474.
epoch: 54, train loss: 0.5234893065493437, validation loss: 0.7543134748850123.
epoch: 55, train loss: 0.5252086199033424, validation loss: 0.7731834665792666.
epoch: 56, train loss: 0.5222605569110151, validation loss: 0.7620389395641182.
epoch: 57, train loss: 0.5215461528041041, validation loss: 0.7771059263321527.
epoch: 58, train loss: 0.5224656946772804, validation loss: 0.7612723147349716.
epoch: 59, train loss: 0.5236814677455104, validation loss: 0.752327876281157.
epoch: 60, train loss: 0.5215608130835099, validation loss: 0.7598490786701226.
best validation loss 0.7357818134963633 at epoch 25.
