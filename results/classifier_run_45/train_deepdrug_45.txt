seed:  5
save trained model at:  ../trained_models/trained_classifier_model_45.pt
save loss at:  ./results/train_classifier_results_45.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['6brxA00', '6ab1A01', '5i7vB01', '5aagA00', '3zf6A00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['3u9cB00', '4rzqA01', '6cp3L00', '4k4yI00', '5n1kA01']
model architecture:
DeepDruG(
  (embedding_net): JKMCNMMEmbeddingNet(
    (conv0): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=33, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b4c9a394f10>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 1.998188768672819, acc: 0.4022138037172961; test loss: 1.7483021285454452, acc: 0.4578113921058851
epoch: 2, train loss: 1.71068135935893, acc: 0.4738368651592281; test loss: 1.6491891008492932, acc: 0.4816828173008745
epoch: 3, train loss: 1.5949139977060325, acc: 0.5033739789274299; test loss: 1.5242030363617087, acc: 0.5242259513117467
epoch: 4, train loss: 1.5048908336043738, acc: 0.5419675624482064; test loss: 1.4474948932306897, acc: 0.5514062869298039
epoch: 5, train loss: 1.415776648295057, acc: 0.5662365336805967; test loss: 1.5318219637481971, acc: 0.5459702198061924
epoch: 6, train loss: 1.3677847628143533, acc: 0.5832248135432698; test loss: 1.334555708187124, acc: 0.5866225478610257
epoch: 7, train loss: 1.3195880328295033, acc: 0.5990292411507044; test loss: 1.345497420312552, acc: 0.5913495627511227
epoch: 8, train loss: 1.2573028145223677, acc: 0.6141233574049959; test loss: 1.2045750428643585, acc: 0.6343653982510045
epoch: 9, train loss: 1.2109420530694723, acc: 0.6340120752930034; test loss: 1.1734328858820267, acc: 0.6388560623965965
epoch: 10, train loss: 1.182628302316179, acc: 0.6431869302711022; test loss: 1.2015163435831162, acc: 0.6390924131411014
epoch: 11, train loss: 1.1289897436388319, acc: 0.6599384396827276; test loss: 1.2804014015580936, acc: 0.6029307492318601
epoch: 12, train loss: 1.0978145945477207, acc: 0.6688173315970167; test loss: 1.2727364495808677, acc: 0.6175844953911604
epoch: 13, train loss: 1.1028052201719274, acc: 0.6695276429501599; test loss: 1.1511138154947005, acc: 0.6461829354762467
epoch: 14, train loss: 1.059370483126572, acc: 0.6824908251450219; test loss: 1.1165960248100986, acc: 0.6546915622784212
epoch: 15, train loss: 1.0525030908576691, acc: 0.688114123357405; test loss: 1.174308012462175, acc: 0.6518553533443631
epoch: 16, train loss: 1.0113366508647592, acc: 0.6951580442760743; test loss: 1.156543421987607, acc: 0.6440557787757032
epoch: 17, train loss: 0.9942618101196655, acc: 0.7021427725819818; test loss: 1.190171300928183, acc: 0.647364689198771
epoch: 18, train loss: 0.9927942810135366, acc: 0.7008997277139813; test loss: 1.0542392700847174, acc: 0.6792720397069251
epoch: 19, train loss: 0.969245651438065, acc: 0.7104297383686516; test loss: 1.079819066787662, acc: 0.6743086740723233
epoch: 20, train loss: 0.9323013304704404, acc: 0.7213211791168462; test loss: 1.2639560528157647, acc: 0.6109666745450248
epoch: 21, train loss: 0.9200130097609491, acc: 0.722268260921037; test loss: 1.0389721644908028, acc: 0.693216733632711
epoch: 22, train loss: 0.9000452362151771, acc: 0.7305552267077069; test loss: 1.0953512695545113, acc: 0.6591822264240133
epoch: 23, train loss: 0.8932445880589689, acc: 0.729667337516278; test loss: 1.252596921206253, acc: 0.6603639801465374
epoch: 24, train loss: 0.8757693029457653, acc: 0.7387829998816148; test loss: 0.9440999770384256, acc: 0.7137792484046325
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.6692736335191357, acc: 0.7432224458387593; test loss: 0.821132768781099, acc: 0.6920349799101867
epoch: 26, train loss: 0.6568172667115993, acc: 0.745116609447141; test loss: 0.7507117780718333, acc: 0.7151973528716615
epoch: 27, train loss: 0.6377475765155902, acc: 0.7582573694802889; test loss: 1.050435349102196, acc: 0.6400378161191208
epoch: 28, train loss: 0.6303229356737861, acc: 0.7557120871315259; test loss: 0.8123887351601153, acc: 0.6922713306546916
epoch: 29, train loss: 0.6226940885145622, acc: 0.7609802296673375; test loss: 0.7941572834092431, acc: 0.6894351217206334
epoch: 30, train loss: 0.6264125665673366, acc: 0.7584941399313365; test loss: 0.946305293446313, acc: 0.6724178681162846
epoch: 31, train loss: 0.6473099583985045, acc: 0.7515094116254292; test loss: 0.9131705277469304, acc: 0.6509099503663437
epoch: 32, train loss: 0.6183803859470554, acc: 0.7591452586717178; test loss: 0.8536914415997384, acc: 0.6887260694871189
epoch: 33, train loss: 0.6263635409037799, acc: 0.7563040132591453; test loss: 0.6912695403584801, acc: 0.7369416213661073
epoch: 34, train loss: 0.5987445871209314, acc: 0.7694447732922931; test loss: 0.9131162829276265, acc: 0.6842354053415268
epoch: 35, train loss: 0.596043913021756, acc: 0.7692080028412455; test loss: 0.8306341791119144, acc: 0.6925076813991964
epoch: 36, train loss: 0.5855163628495881, acc: 0.7734698709601042; test loss: 0.7889943167380592, acc: 0.7107066887260695
epoch: 37, train loss: 0.5738655046004296, acc: 0.7767846572747721; test loss: 0.8489211433972346, acc: 0.6861262112975656
epoch: 38, train loss: 0.5597620461768084, acc: 0.7857227418018231; test loss: 0.6674496463725145, acc: 0.7482864571023399
epoch: 39, train loss: 0.5459827792225356, acc: 0.7851308156742038; test loss: 0.800337738473487, acc: 0.6953438903332545
epoch: 40, train loss: 0.5530026684967995, acc: 0.7857227418018231; test loss: 0.8552512561872028, acc: 0.691562278421177
epoch: 41, train loss: 0.591185374289303, acc: 0.7699183141943885; test loss: 0.7067322961491238, acc: 0.7310328527534862
epoch: 42, train loss: 0.5542434655633554, acc: 0.7841837338700131; test loss: 0.6902741874766051, acc: 0.7397778303001654
epoch: 43, train loss: 0.5280173265702544, acc: 0.7942464780395406; test loss: 0.7037891761768065, acc: 0.7329236587095249
epoch: 44, train loss: 0.5097071046171293, acc: 0.7987451166094471; test loss: 0.6829874454111502, acc: 0.7402505317891751
epoch: 45, train loss: 0.5081488796207373, acc: 0.8015863620220196; test loss: 0.7779384444887489, acc: 0.7159064051051761
epoch: 46, train loss: 0.5092784050153927, acc: 0.8015863620220196; test loss: 0.6869746490844568, acc: 0.7402505317891751
epoch: 47, train loss: 0.5100435020444632, acc: 0.8020599029241151; test loss: 0.6954797633966073, acc: 0.7315055542424959
epoch: 48, train loss: 0.5181270012619003, acc: 0.7964957973244939; test loss: 0.8588951218638176, acc: 0.6856535098085559
epoch: 49, train loss: 0.4927382671199382, acc: 0.8038356813069729; test loss: 0.7021478832937307, acc: 0.7239423304183408
Epoch    49: reducing learning rate of group 0 to 1.5000e-03.
epoch: 50, train loss: 0.4099900003465743, acc: 0.8356221143601279; test loss: 0.6236711910390257, acc: 0.7731032852753487
epoch: 51, train loss: 0.3728075420890346, acc: 0.8504794601633716; test loss: 0.6266116405373008, acc: 0.7780666509099504
epoch: 52, train loss: 0.360915153357804, acc: 0.8553332544098496; test loss: 0.6072823361735118, acc: 0.7887024344126684
epoch: 53, train loss: 0.35158190860809824, acc: 0.8562211436012785; test loss: 0.6468227136166094, acc: 0.7631765540061451
epoch: 54, train loss: 0.3435486228321803, acc: 0.8604238191073754; test loss: 0.732044426776703, acc: 0.755849680926495
epoch: 55, train loss: 0.35621201451168805, acc: 0.8559843731502309; test loss: 0.682161561875331, acc: 0.7539588749704561
epoch: 56, train loss: 0.3411097435982563, acc: 0.8581153072096602; test loss: 0.6373554496611864, acc: 0.7785393523989601
epoch: 57, train loss: 0.3274780267637214, acc: 0.8634426423582336; test loss: 0.6515245977458985, acc: 0.7738123375088631
epoch: 58, train loss: 0.3209523759933143, acc: 0.8675861252515686; test loss: 0.6866437286046965, acc: 0.7759394942094068
epoch: 59, train loss: 0.321582110260201, acc: 0.8666390434473777; test loss: 0.6960676840309048, acc: 0.7657764121956984
epoch: 60, train loss: 0.3122081661402184, acc: 0.8699538297620457; test loss: 0.7312225428801906, acc: 0.7631765540061451
epoch: 61, train loss: 0.3057479082098568, acc: 0.8735645791405232; test loss: 0.6973023657482078, acc: 0.7598676435830772
epoch: 62, train loss: 0.3100173756527847, acc: 0.8707233337279507; test loss: 0.7036043164071336, acc: 0.7714488300638147
epoch: 63, train loss: 0.29452570098680164, acc: 0.8788919142890967; test loss: 0.717939642861334, acc: 0.7591585913495628
epoch: 64, train loss: 0.3071739394890284, acc: 0.8695986740854741; test loss: 0.7488556844746918, acc: 0.7697943748522807
epoch: 65, train loss: 0.30404199591025355, acc: 0.8746892387829999; test loss: 0.6654759172400355, acc: 0.7790120538879698
epoch: 66, train loss: 0.28946453482847573, acc: 0.879424647803954; test loss: 0.7555132751289031, acc: 0.7518317182699126
epoch: 67, train loss: 0.3054339554382666, acc: 0.8720255712087132; test loss: 0.8602719244396847, acc: 0.7258331363743796
epoch: 68, train loss: 0.2820767534369052, acc: 0.8804901148336688; test loss: 0.6693034172621609, acc: 0.7773575986764358
epoch: 69, train loss: 0.2833314871871289, acc: 0.8805493074464307; test loss: 0.7381233249931182, acc: 0.7575041361380288
epoch: 70, train loss: 0.27615213642547126, acc: 0.882739434118622; test loss: 0.6997244280862684, acc: 0.7792484046324746
epoch: 71, train loss: 0.2961191610004601, acc: 0.8781816029359536; test loss: 0.6527219221006468, acc: 0.774048688253368
epoch: 72, train loss: 0.29648330975626014, acc: 0.8740973126553806; test loss: 0.6666127299901808, acc: 0.7768848971874261
epoch: 73, train loss: 0.2711321331981344, acc: 0.8823250858292885; test loss: 0.7033891043914348, acc: 0.7761758449539116
epoch: 74, train loss: 0.3039670837720047, acc: 0.86977625192376; test loss: 0.6726384300961333, acc: 0.7766485464429213
epoch: 75, train loss: 0.2472151868707741, acc: 0.8919142890967208; test loss: 0.768880694111423, acc: 0.7523044197589223
epoch: 76, train loss: 0.24553573666429526, acc: 0.8937492600923405; test loss: 0.743588324332457, acc: 0.7735759867643583
epoch: 77, train loss: 0.2632451106510713, acc: 0.8834497454717651; test loss: 0.6344477179145678, acc: 0.781611912077523
epoch: 78, train loss: 0.24259603924619752, acc: 0.8973600094708181; test loss: 0.8177038099545447, acc: 0.7376506735996219
epoch: 79, train loss: 0.2521726927263886, acc: 0.8903160885521487; test loss: 0.666559979301963, acc: 0.7842117702670763
epoch: 80, train loss: 0.25243567811274253, acc: 0.8932757191902451; test loss: 0.726648331449207, acc: 0.7721578822973293
epoch: 81, train loss: 0.2416167265496424, acc: 0.8971232390197703; test loss: 0.823075717122694, acc: 0.734341763176554
epoch: 82, train loss: 0.22363722949560344, acc: 0.9002012548833905; test loss: 0.7967994906777779, acc: 0.7754667927203971
epoch: 83, train loss: 0.21829771525947048, acc: 0.9068900201254884; test loss: 0.8279311174028902, acc: 0.7350508154100686
epoch: 84, train loss: 0.21759390306533463, acc: 0.9024505741683438; test loss: 0.8284385496210303, acc: 0.7563223824155046
epoch: 85, train loss: 0.2256743877498133, acc: 0.8995501361430094; test loss: 0.9927318531186043, acc: 0.722051524462302
epoch: 86, train loss: 0.2248657110973097, acc: 0.9020362258790103; test loss: 0.6935263561374103, acc: 0.7870479792011345
epoch: 87, train loss: 0.22161569959936417, acc: 0.9018586480407245; test loss: 0.7997475581596265, acc: 0.767903568896242
epoch: 88, train loss: 0.20813845074203602, acc: 0.9086657985083462; test loss: 0.7630243816997841, acc: 0.7688489718742614
epoch: 89, train loss: 0.20843243547779747, acc: 0.9084882206700604; test loss: 0.8141242803284193, acc: 0.7617584495391161
epoch: 90, train loss: 0.22109827904485593, acc: 0.9036344264235824; test loss: 0.7741477794890651, acc: 0.7799574568659892
epoch: 91, train loss: 0.19625121800839104, acc: 0.9147034450100627; test loss: 0.8450835704578065, acc: 0.7454502481682818
epoch: 92, train loss: 0.21366433218708966, acc: 0.9059429383212975; test loss: 0.7517575543136975, acc: 0.7702670763412904
epoch: 93, train loss: 0.20559505654521648, acc: 0.9090209541849177; test loss: 0.8223212594767322, acc: 0.7612857480501064
epoch: 94, train loss: 0.21515548797250428, acc: 0.9061205161595833; test loss: 0.6982270523437342, acc: 0.7806665090995036
epoch: 95, train loss: 0.18973718951393956, acc: 0.9158872972653013; test loss: 0.759304849624183, acc: 0.7825573150555424
epoch: 96, train loss: 0.1813548467096329, acc: 0.9187285426778738; test loss: 0.8021928490377492, acc: 0.7749940912313874
epoch: 97, train loss: 0.19669753367398157, acc: 0.9123949331123475; test loss: 0.7449472096478564, acc: 0.7849208225005909
epoch: 98, train loss: 0.20463563316783157, acc: 0.9105599621167279; test loss: 0.71404310772814, acc: 0.7792484046324746
epoch: 99, train loss: 0.2055063727456775, acc: 0.9086657985083462; test loss: 0.7537082299868341, acc: 0.7757031434649019
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.14892323775214952, acc: 0.9113294660826329; test loss: 0.5885275644321752, acc: 0.7809028598440085
Epoch   100: reducing learning rate of group 0 to 7.5000e-04.
epoch: 101, train loss: 0.09493163031252147, acc: 0.9395051497573103; test loss: 0.6087847970332753, acc: 0.796974710470338
epoch: 102, train loss: 0.06758284610859866, acc: 0.951994791050077; test loss: 0.635663517708757, acc: 0.8047742850389978
epoch: 103, train loss: 0.06047377356408446, acc: 0.9579140523262697; test loss: 0.630608294991223, acc: 0.8052469865280075
epoch: 104, train loss: 0.05290450228503206, acc: 0.9608144903516042; test loss: 0.652424913860331, acc: 0.8118648073741432
epoch: 105, train loss: 0.05043021912255541, acc: 0.9625310761217; test loss: 0.6646448547072062, acc: 0.8095012999290948
epoch: 106, train loss: 0.04531064680095564, acc: 0.9671480999171304; test loss: 0.6860753033240617, acc: 0.8073741432285512
epoch: 107, train loss: 0.048411041015709054, acc: 0.9662602107257015; test loss: 0.7245269639360739, acc: 0.7960293074923186
epoch: 108, train loss: 0.05718142233548171, acc: 0.9601633716112229; test loss: 0.6934632365795433, acc: 0.7993382179153864
epoch: 109, train loss: 0.08104146424536735, acc: 0.9466082632887416; test loss: 0.6617370998287448, acc: 0.7898841881351927
epoch: 110, train loss: 0.06976419266979694, acc: 0.9541849177222682; test loss: 0.7193024636329248, acc: 0.7965020089813283
epoch: 111, train loss: 0.09416597747119683, acc: 0.9399194980466438; test loss: 0.6650798762275716, acc: 0.7976837627038526
epoch: 112, train loss: 0.06390908627033347, acc: 0.9566710074582692; test loss: 0.654881996906725, acc: 0.7988655164263767
epoch: 113, train loss: 0.055205547590502425, acc: 0.9613472238664614; test loss: 0.7369262676266062, acc: 0.7896478373906878
epoch: 114, train loss: 0.054342669540800345, acc: 0.9594530602580797; test loss: 0.693166672943952, acc: 0.7993382179153864
epoch: 115, train loss: 0.05733347851070791, acc: 0.9594530602580797; test loss: 0.7052643916703262, acc: 0.7806665090995036
epoch: 116, train loss: 0.053011368933680976, acc: 0.9622943056706523; test loss: 0.6962478693933021, acc: 0.7811392105885133
epoch: 117, train loss: 0.05893731047038257, acc: 0.9582100153900793; test loss: 0.6872012933583667, acc: 0.7957929567478138
epoch: 118, train loss: 0.05956265240258488, acc: 0.9599266011601753; test loss: 0.6780423890386341, acc: 0.7986291656818719
epoch: 119, train loss: 0.06180800319889304, acc: 0.9566710074582692; test loss: 0.725518219974187, acc: 0.7887024344126684
epoch: 120, train loss: 0.06361437854864788, acc: 0.9554279625902687; test loss: 0.7112178745012817, acc: 0.7858662254786103
epoch: 121, train loss: 0.06128072565679679, acc: 0.9574997040369362; test loss: 0.7308009898118786, acc: 0.7903568896242024
epoch: 122, train loss: 0.06158404017769826, acc: 0.9589203267432225; test loss: 0.9455159512909556, acc: 0.7359962183880879
epoch: 123, train loss: 0.11495155006174337, acc: 0.9292056351367349; test loss: 0.6214356771515098, acc: 0.790829591113212
epoch: 124, train loss: 0.07592008444837273, acc: 0.9481472712205516; test loss: 0.6695181076421266, acc: 0.7991018671708816
epoch: 125, train loss: 0.055340479565002626, acc: 0.9604593346750325; test loss: 0.6972800589317804, acc: 0.8017017253604349
epoch: 126, train loss: 0.04895825241816401, acc: 0.9647212027938913; test loss: 0.7436697576604423, acc: 0.7922476955802411
epoch: 127, train loss: 0.049561036773386746, acc: 0.964188469279034; test loss: 0.6782961724582673, acc: 0.8061923895060269
epoch: 128, train loss: 0.07057364232334509, acc: 0.9526459097904582; test loss: 0.7680773646857997, acc: 0.7799574568659892
epoch: 129, train loss: 0.07473529206163942, acc: 0.9493903160885522; test loss: 0.697751694834562, acc: 0.793902150791775
epoch: 130, train loss: 0.06015931636119744, acc: 0.9573221261986504; test loss: 0.7029391832313366, acc: 0.7993382179153864
epoch: 131, train loss: 0.0808948955259493, acc: 0.9480880786077898; test loss: 0.7014796623972904, acc: 0.7924840463247459
epoch: 132, train loss: 0.06966131109520714, acc: 0.9506333609565526; test loss: 0.6769425313936511, acc: 0.7950839045142992
epoch: 133, train loss: 0.05233369667466271, acc: 0.9628862317982716; test loss: 0.7013765709587824, acc: 0.7955566060033089
epoch: 134, train loss: 0.04400656233872763, acc: 0.9677992186575115; test loss: 0.6823082970998102, acc: 0.8019380761049397
epoch: 135, train loss: 0.051206372918194396, acc: 0.9631821948620812; test loss: 0.6981424469948943, acc: 0.798392814937367
epoch: 136, train loss: 0.08722291709649023, acc: 0.9448324849058838; test loss: 0.6479579121247726, acc: 0.7920113448357362
epoch: 137, train loss: 0.06350205028307244, acc: 0.9572629335858884; test loss: 0.6691973756543572, acc: 0.7976837627038526
epoch: 138, train loss: 0.04875583020784777, acc: 0.9647803954066533; test loss: 0.7150620744741715, acc: 0.80146537461593
epoch: 139, train loss: 0.05783627944855883, acc: 0.9617023795430331; test loss: 0.7526505978443865, acc: 0.7728669345308438
epoch: 140, train loss: 0.07739475284248495, acc: 0.9470226115780751; test loss: 0.6709454282184945, acc: 0.7934294493027653
epoch: 141, train loss: 0.05710805220500991, acc: 0.9614064164792234; test loss: 0.6906488224620995, acc: 0.803119829827464
epoch: 142, train loss: 0.04469948978500351, acc: 0.9688646856872263; test loss: 0.7181064988444866, acc: 0.8035925313164737
epoch: 143, train loss: 0.0456126449563291, acc: 0.9675624482064639; test loss: 0.7095787205670184, acc: 0.7924840463247459
epoch: 144, train loss: 0.049114984744081235, acc: 0.9645436249556055; test loss: 0.7321934435728791, acc: 0.7957929567478138
epoch: 145, train loss: 0.08742810705504898, acc: 0.9473185746418847; test loss: 0.6689736225508652, acc: 0.7889387851571732
epoch: 146, train loss: 0.06930130255833397, acc: 0.9538889546584586; test loss: 0.6629701996773756, acc: 0.7986291656818719
epoch: 147, train loss: 0.05847280386959263, acc: 0.9566118148455073; test loss: 0.6955578541310508, acc: 0.798392814937367
epoch: 148, train loss: 0.06128603309865702, acc: 0.9566710074582692; test loss: 0.741486713413481, acc: 0.787757031434649
epoch: 149, train loss: 0.06240870639270786, acc: 0.9567302000710312; test loss: 0.709975166485344, acc: 0.7896478373906878
epoch: 150, train loss: 0.05002624937159515, acc: 0.964129276666272; test loss: 0.7009242668526022, acc: 0.7979201134483573
epoch: 151, train loss: 0.050202219266185566, acc: 0.9648395880194152; test loss: 0.6979158052567527, acc: 0.7986291656818719
Epoch   151: reducing learning rate of group 0 to 3.7500e-04.
epoch: 152, train loss: 0.033433458889001386, acc: 0.9760269918314194; test loss: 0.6872622295069486, acc: 0.8090285984400851
epoch: 153, train loss: 0.024559259034514187, acc: 0.982952527524565; test loss: 0.7143793013100928, acc: 0.8069014417395415
epoch: 154, train loss: 0.02062610550639692, acc: 0.9863265064519948; test loss: 0.7343404933764449, acc: 0.8090285984400851
epoch: 155, train loss: 0.01755160742675385, acc: 0.9876287439327572; test loss: 0.7386220006395023, acc: 0.8111557551406287
epoch: 156, train loss: 0.01627584908859249, acc: 0.9892861370900912; test loss: 0.754768763273722, acc: 0.8113921058851336
epoch: 157, train loss: 0.014642802576147565, acc: 0.991890612051616; test loss: 0.7628100987332731, acc: 0.8057196880170172
epoch: 158, train loss: 0.013824705244309347, acc: 0.9901740262815201; test loss: 0.7679325438255792, acc: 0.8083195462065705
epoch: 159, train loss: 0.0344715894381718, acc: 0.9783946963418966; test loss: 0.7701575016755636, acc: 0.7979201134483573
epoch: 160, train loss: 0.028766381389474578, acc: 0.9792825855333255; test loss: 0.7326288873550317, acc: 0.8059560387615221
epoch: 161, train loss: 0.018388519731515736, acc: 0.9878655143838049; test loss: 0.7294581783378528, acc: 0.8026471283384543
epoch: 162, train loss: 0.01656484366216518, acc: 0.9881022848348526; test loss: 0.7500337136606042, acc: 0.8061923895060269
epoch: 163, train loss: 0.017792056243643085, acc: 0.9876287439327572; test loss: 0.7393959406503575, acc: 0.8132829118411723
epoch: 164, train loss: 0.020266219858568357, acc: 0.9866816621285663; test loss: 0.7355710901680975, acc: 0.8154100685417159
epoch: 165, train loss: 0.017796773894363512, acc: 0.9882798626731384; test loss: 0.7356908867416075, acc: 0.8085558969510754
epoch: 166, train loss: 0.017838359348333926, acc: 0.9879247069965669; test loss: 0.7435329704244096, acc: 0.8092649491845899
epoch: 167, train loss: 0.022020053491569685, acc: 0.9868592399668521; test loss: 0.7490800866785412, acc: 0.8099740014181045
epoch: 168, train loss: 0.0225395131512208, acc: 0.985320232035042; test loss: 0.7649716578654662, acc: 0.8059560387615221
epoch: 169, train loss: 0.018949243760902252, acc: 0.9879838996093288; test loss: 0.7616038917509046, acc: 0.8059560387615221
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.012395314216469114, acc: 0.9872143956434237; test loss: 0.7135508176537164, acc: 0.8054833372725124
epoch: 171, train loss: 0.013372419727916574, acc: 0.9857937729371374; test loss: 0.6578569357853301, acc: 0.8085558969510754
epoch: 172, train loss: 0.015655810913092868, acc: 0.9857345803243756; test loss: 0.6536129682347499, acc: 0.8059560387615221
epoch: 173, train loss: 0.012458609803854834, acc: 0.9883390552859003; test loss: 0.636699280257598, acc: 0.8116284566296383
epoch: 174, train loss: 0.023585421770257162, acc: 0.9790458150822777; test loss: 0.6629898219603338, acc: 0.7986291656818719
epoch: 175, train loss: 0.037454146302630525, acc: 0.9696341896531313; test loss: 0.6322006745080355, acc: 0.7910659418577168
epoch: 176, train loss: 0.0291300459250118, acc: 0.9740736356102758; test loss: 0.6379555602412899, acc: 0.8012290238714252
epoch: 177, train loss: 0.018731238583758397, acc: 0.9823606013969457; test loss: 0.6306840232911332, acc: 0.8009926731269204
epoch: 178, train loss: 0.02708652947722941, acc: 0.9764413401207529; test loss: 0.6164154225110275, acc: 0.8057196880170172
epoch: 179, train loss: 0.013543766970548092, acc: 0.9866224695158045; test loss: 0.6255214484243409, acc: 0.8104467029071142
epoch: 180, train loss: 0.011212980590131814, acc: 0.9894637149283769; test loss: 0.6444096534817362, acc: 0.8057196880170172
epoch: 181, train loss: 0.011062319378678484, acc: 0.9889901740262815; test loss: 0.6449991603518626, acc: 0.8080831954620658
epoch: 182, train loss: 0.010740534481262264, acc: 0.9898780632177104; test loss: 0.6429743451785883, acc: 0.8066650909950366
epoch: 183, train loss: 0.009899946552641425, acc: 0.9914170711495205; test loss: 0.6486493482344033, acc: 0.8040652328054834
epoch: 184, train loss: 0.01036568483453748, acc: 0.9903516041198058; test loss: 0.6606860833369764, acc: 0.8083195462065705
epoch: 185, train loss: 0.014239268312372934, acc: 0.987036817805138; test loss: 0.6459486097118529, acc: 0.8002836208934058
epoch: 186, train loss: 0.01398938922364782, acc: 0.9864448916775187; test loss: 0.6636462267395792, acc: 0.8073741432285512
epoch: 187, train loss: 0.015325306088441916, acc: 0.9856753877116136; test loss: 0.665669213229391, acc: 0.798392814937367
epoch: 188, train loss: 0.02301611487786876, acc: 0.980466437788564; test loss: 0.6946649899637127, acc: 0.787757031434649
epoch: 189, train loss: 0.021834195920895656, acc: 0.9785130815674203; test loss: 0.6643716234091297, acc: 0.8047742850389978
epoch: 190, train loss: 0.020591283389070028, acc: 0.9806440156268498; test loss: 0.6691100097271715, acc: 0.7924840463247459
epoch: 191, train loss: 0.02060220053641524, acc: 0.9795785485971351; test loss: 0.6479577161098761, acc: 0.8019380761049397
epoch: 192, train loss: 0.017074789371964626, acc: 0.982893334911803; test loss: 0.6623443896199648, acc: 0.796974710470338
epoch: 193, train loss: 0.014296965595505444, acc: 0.9856161950988517; test loss: 0.675731629182531, acc: 0.8005199716379107
epoch: 194, train loss: 0.01562890283332533, acc: 0.9848466911329466; test loss: 0.6746734423266605, acc: 0.8009926731269204
epoch: 195, train loss: 0.022591617376025053, acc: 0.9798153190481828; test loss: 0.6509753571591234, acc: 0.7948475537697943
epoch: 196, train loss: 0.02377014458523469, acc: 0.9783946963418966; test loss: 0.6312197724912387, acc: 0.798392814937367
epoch: 197, train loss: 0.025494308750386264, acc: 0.9756718361548479; test loss: 0.6446496546057149, acc: 0.7962656582368235
epoch: 198, train loss: 0.03416578516071261, acc: 0.9708180419083698; test loss: 0.6286411304841282, acc: 0.7936658000472702
epoch: 199, train loss: 0.025648657223923087, acc: 0.9760861844441814; test loss: 0.6148973598854391, acc: 0.8033561805719688
epoch: 200, train loss: 0.015157380976498205, acc: 0.9859121581626613; test loss: 0.6247841566343224, acc: 0.812101158118648
best test acc 0.8154100685417159 at epoch 164.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9985    0.9995    0.9990      6100
           1     0.9989    0.9978    0.9984       926
           2     0.9958    0.9979    0.9969      2400
           3     0.9988    0.9976    0.9982       843
           4     0.9948    0.9974    0.9961       774
           5     0.9928    0.9993    0.9960      1512
           6     0.9962    0.9857    0.9909      1330
           7     0.9917    0.9958    0.9938       481
           8     1.0000    0.9978    0.9989       458
           9     1.0000    1.0000    1.0000       452
          10     1.0000    0.9944    0.9972       717
          11     1.0000    0.9970    0.9985       333
          12     0.9831    0.9699    0.9764       299
          13     0.9889    0.9926    0.9907       269

    accuracy                         0.9969     16894
   macro avg     0.9957    0.9945    0.9951     16894
weighted avg     0.9969    0.9969    0.9969     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8691    0.9272    0.8972      1525
           1     0.8542    0.8836    0.8686       232
           2     0.8377    0.7987    0.8177       601
           3     0.8610    0.7630    0.8090       211
           4     0.8929    0.9021    0.8974       194
           5     0.8889    0.8254    0.8560       378
           6     0.5544    0.6276    0.5887       333
           7     0.9318    0.6777    0.7847       121
           8     0.7525    0.6609    0.7037       115
           9     0.9091    0.7895    0.8451       114
          10     0.8280    0.7222    0.7715       180
          11     0.7273    0.6667    0.6957        84
          12     0.2019    0.2800    0.2346        75
          13     0.7222    0.5735    0.6393        68

    accuracy                         0.8154      4231
   macro avg     0.7736    0.7213    0.7435      4231
weighted avg     0.8224    0.8154    0.8169      4231

---------------------------------------
program finished.
