seed:  3
save trained model at:  ../trained_models/trained_classifier_model_103.pt
save loss at:  ./results/train_classifier_results_103.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['3nfmA00', '4jaiA00', '5j33B01', '5emkA00', '1lolA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['4ndgA00', '4jqcB01', '2oxxA00', '4nzmA00', '2iryA00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b77513ff730>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0333415538414816, acc: 0.3938084527051024; test loss: 1.7267137393126446, acc: 0.46939257858662253
epoch: 2, train loss: 1.7342292743560703, acc: 0.473955250384752; test loss: 1.6692928006197651, acc: 0.4750649964547388
epoch: 3, train loss: 1.6210974237122056, acc: 0.5064519947910501; test loss: 1.63095942357279, acc: 0.5093358544079414
epoch: 4, train loss: 1.5779237025190813, acc: 0.5226707706878182; test loss: 1.5067612762514284, acc: 0.5386433467265422
epoch: 5, train loss: 1.5105133147243637, acc: 0.5458150822777318; test loss: 1.5890539663391907, acc: 0.5296620184353581
epoch: 6, train loss: 1.472083590673359, acc: 0.5545163963537351; test loss: 1.4126330309516006, acc: 0.5670054360671236
epoch: 7, train loss: 1.4382892402090293, acc: 0.5669468450337398; test loss: 1.4500969227705405, acc: 0.5592058614984637
epoch: 8, train loss: 1.413263741690576, acc: 0.57002486089736; test loss: 1.3821865979223718, acc: 0.5653509808555897
epoch: 9, train loss: 1.379371804599439, acc: 0.5815674203859358; test loss: 1.3616619798145913, acc: 0.5766958165918222
epoch: 10, train loss: 1.3554761009243406, acc: 0.5905055049129868; test loss: 1.3678573042302231, acc: 0.569368943512172
epoch: 11, train loss: 1.3699962383503743, acc: 0.5844678584112702; test loss: 1.3869540711098376, acc: 0.569368943512172
epoch: 12, train loss: 1.3385289792719854, acc: 0.592044512844797; test loss: 1.448749124003595, acc: 0.5632238241550461
epoch: 13, train loss: 1.3046472112963408, acc: 0.6061915472948975; test loss: 1.2937059742822288, acc: 0.5977310328527535
epoch: 14, train loss: 1.3044992290164672, acc: 0.6041789984609921; test loss: 1.3039116260994694, acc: 0.5922949657291421
epoch: 15, train loss: 1.28454137448236, acc: 0.6090327927074701; test loss: 1.352863417649996, acc: 0.5835499881824627
epoch: 16, train loss: 1.2738869505774217, acc: 0.6154255948857582; test loss: 1.333104828406496, acc: 0.5814228314819192
epoch: 17, train loss: 1.2559977620681306, acc: 0.6192731147152836; test loss: 1.2581384214096052, acc: 0.6095485700779958
epoch: 18, train loss: 1.2622374142925317, acc: 0.6167278323665206; test loss: 1.2472104981728294, acc: 0.6154573386906169
epoch: 19, train loss: 1.2237457497749045, acc: 0.6299869776251924; test loss: 1.284072707263439, acc: 0.5960765776412196
epoch: 20, train loss: 1.224262569745758, acc: 0.629454244110335; test loss: 1.2387811419699553, acc: 0.6192389506026944
epoch: 21, train loss: 1.2193431972229538, acc: 0.6312300224931928; test loss: 1.233569268497331, acc: 0.6126211297565587
epoch: 22, train loss: 1.2096804325095405, acc: 0.6323546821356695; test loss: 1.17971201749931, acc: 0.6331836445284803
epoch: 23, train loss: 1.2030613661602685, acc: 0.6372084763821475; test loss: 1.2196536513260707, acc: 0.6133301819900733
epoch: 24, train loss: 1.1993557120746898, acc: 0.639280217828815; test loss: 1.2405905751746982, acc: 0.6166390924131411
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9576740432691614, acc: 0.6445483603646265; test loss: 0.9621225076768731, acc: 0.6424013235641692
epoch: 26, train loss: 0.9428254535060755, acc: 0.6492245767728188; test loss: 1.019049505510107, acc: 0.6192389506026944
epoch: 27, train loss: 0.9357894366911153, acc: 0.6486326506451995; test loss: 0.9303844426433913, acc: 0.6421649728196643
epoch: 28, train loss: 0.9313742499020137, acc: 0.6483958801941517; test loss: 0.9911163193597435, acc: 0.6294020326164027
epoch: 29, train loss: 0.9159243036349511, acc: 0.6527169409257725; test loss: 0.9262047755016669, acc: 0.6471283384542661
epoch: 30, train loss: 0.9175485509435736, acc: 0.6495797324493903; test loss: 0.9568169524950836, acc: 0.6324745922949657
epoch: 31, train loss: 0.8980100097297224, acc: 0.6565052681425358; test loss: 0.9942517819852025, acc: 0.6324745922949657
epoch: 32, train loss: 0.8956388418132827, acc: 0.6578666982360601; test loss: 0.9490725020375046, acc: 0.6343653982510045
epoch: 33, train loss: 0.8963955070572491, acc: 0.6617734106783474; test loss: 0.9522133474230513, acc: 0.6324745922949657
epoch: 34, train loss: 0.8955222478953849, acc: 0.6591689357168226; test loss: 0.9141765853468901, acc: 0.6509099503663437
epoch: 35, train loss: 0.8876304220021427, acc: 0.6651473896057772; test loss: 1.0551529459230895, acc: 0.5991491373197826
epoch: 36, train loss: 0.8798652893059971, acc: 0.6650881969930152; test loss: 0.9634384166768921, acc: 0.628929331127393
epoch: 37, train loss: 0.8747421180452555, acc: 0.666863975375873; test loss: 1.0118893000905436, acc: 0.6024580477428504
epoch: 38, train loss: 0.8783545515954149, acc: 0.6617734106783474; test loss: 0.9099637947587419, acc: 0.654927913022926
epoch: 39, train loss: 0.863398845109401, acc: 0.6702971469160649; test loss: 0.9833647017500357, acc: 0.6166390924131411
epoch: 40, train loss: 0.8571607847530996, acc: 0.6716585770095892; test loss: 0.9407579918556872, acc: 0.6284566296383833
epoch: 41, train loss: 0.8347619144228121, acc: 0.6755060968391144; test loss: 0.8877716892240628, acc: 0.6603639801465374
epoch: 42, train loss: 0.8531195250462961, acc: 0.6753877116135906; test loss: 0.8823008538982993, acc: 0.6596549279130229
epoch: 43, train loss: 0.8360813248364957, acc: 0.6816621285663549; test loss: 0.9100546156703595, acc: 0.6471283384542661
epoch: 44, train loss: 0.8447292901445905, acc: 0.675032555937019; test loss: 0.8544303037065953, acc: 0.662727487591586
epoch: 45, train loss: 0.8346696550555097, acc: 0.6810110098259737; test loss: 0.8598827492455392, acc: 0.6702907114157409
epoch: 46, train loss: 0.8126088820703922, acc: 0.6878773529063573; test loss: 1.080709765183393, acc: 0.6043488536988891
epoch: 47, train loss: 0.8120571300025746, acc: 0.6882325085829288; test loss: 0.8404275126785173, acc: 0.6733632710943039
epoch: 48, train loss: 0.8033011121567543, acc: 0.6894755534509294; test loss: 0.9147981731808797, acc: 0.6464192862207516
epoch: 49, train loss: 0.7993901892852625, acc: 0.6890020125488339; test loss: 0.9603512095548503, acc: 0.6379106594185772
epoch: 50, train loss: 0.8031490188216802, acc: 0.6932046880549307; test loss: 0.9261018457324813, acc: 0.6393287638856062
epoch: 51, train loss: 0.7916439137713392, acc: 0.6938558067953119; test loss: 1.061001925343347, acc: 0.6123847790120539
epoch: 52, train loss: 0.8303712302091433, acc: 0.6784657274772108; test loss: 0.8217022118064647, acc: 0.6688726069487119
epoch: 53, train loss: 0.7914376645394722, acc: 0.6988871788800758; test loss: 0.8388974289611225, acc: 0.6662727487591585
epoch: 54, train loss: 0.7721977676670129, acc: 0.701195690777791; test loss: 0.9634808218245302, acc: 0.635074450484519
epoch: 55, train loss: 0.7734628937449612, acc: 0.7008997277139813; test loss: 0.8015034201015002, acc: 0.67950839045143
epoch: 56, train loss: 0.7569347196561254, acc: 0.7042737066414112; test loss: 0.800070606374594, acc: 0.6787993382179154
epoch: 57, train loss: 0.739957400640962, acc: 0.7107848940452232; test loss: 0.8624436979850565, acc: 0.6587095249350036
epoch: 58, train loss: 0.7550252605588165, acc: 0.7063454480880786; test loss: 0.8609947401478172, acc: 0.6679272039706925
epoch: 59, train loss: 0.7584243689441986, acc: 0.7053983662838877; test loss: 0.8611999736894983, acc: 0.6698180099267312
epoch: 60, train loss: 0.7412387167361695, acc: 0.710903279270747; test loss: 0.8641466382152221, acc: 0.6577641219569842
epoch: 61, train loss: 0.7430198336404019, acc: 0.7098970048537943; test loss: 0.8706593476861004, acc: 0.6544552115339163
epoch: 62, train loss: 0.7269194505899718, acc: 0.7101929679176039; test loss: 1.0921996458572463, acc: 0.5974946821082486
epoch: 63, train loss: 0.7191713377895109, acc: 0.716644962708654; test loss: 0.8681170375768075, acc: 0.6728905696052943
epoch: 64, train loss: 0.7159241697301918, acc: 0.7154019178406534; test loss: 0.801619390031458, acc: 0.6875443157645946
epoch: 65, train loss: 0.710254027234657, acc: 0.719012667219131; test loss: 0.8088317444859259, acc: 0.6738359725833136
epoch: 66, train loss: 0.7086890707485095, acc: 0.721498756955132; test loss: 0.8482575481367911, acc: 0.6719451666272749
epoch: 67, train loss: 0.7066182856222744, acc: 0.7202557120871316; test loss: 0.9263098936953237, acc: 0.6509099503663437
epoch: 68, train loss: 0.7064071824540186, acc: 0.7237480762400852; test loss: 0.968913900460568, acc: 0.632001890805956
epoch: 69, train loss: 0.6995593579729675, acc: 0.7226826092103705; test loss: 0.8294854510115044, acc: 0.6802174426849444
epoch: 70, train loss: 0.6885705820989423, acc: 0.7279507517461821; test loss: 0.8366047595584682, acc: 0.6683999054597022
epoch: 71, train loss: 0.693637715573422, acc: 0.7277731739078963; test loss: 0.8527441411344077, acc: 0.6662727487591585
epoch: 72, train loss: 0.6776583641326482, acc: 0.7293121818397064; test loss: 0.8015820020988066, acc: 0.6832900023635075
epoch: 73, train loss: 0.678987481932224, acc: 0.7343435539244703; test loss: 0.8577922939042678, acc: 0.6650909950366344
epoch: 74, train loss: 0.6665026707396333, acc: 0.7328045459926601; test loss: 0.7733811475627108, acc: 0.6927440321437013
epoch: 75, train loss: 0.6631759288213063, acc: 0.7411507043920919; test loss: 0.7864853730018019, acc: 0.6920349799101867
epoch: 76, train loss: 0.6762220730044312, acc: 0.7334556647330414; test loss: 0.78154959416057, acc: 0.6974710470337981
epoch: 77, train loss: 0.6697896337797052, acc: 0.7330413164437078; test loss: 0.7984376517741907, acc: 0.6865989127865753
epoch: 78, train loss: 0.6459334157014777, acc: 0.7398484669113294; test loss: 0.7777405210038332, acc: 0.6972346962892934
epoch: 79, train loss: 0.6582598031957262, acc: 0.7399668521368533; test loss: 0.8039729534889389, acc: 0.6780902859844008
epoch: 80, train loss: 0.6563552634538498, acc: 0.7379543033029478; test loss: 0.9014440594314542, acc: 0.6568187189789648
epoch: 81, train loss: 0.6468403521297641, acc: 0.7397892742985676; test loss: 0.840718026872336, acc: 0.6832900023635075
epoch: 82, train loss: 0.6468735446700743, acc: 0.7453533798981887; test loss: 0.8941868293710815, acc: 0.6622547861025763
epoch: 83, train loss: 0.6235609596278295, acc: 0.7498520184680951; test loss: 0.7633793518577617, acc: 0.6981800992673127
epoch: 84, train loss: 0.6400201464879156, acc: 0.7468923878299988; test loss: 0.8719021833357138, acc: 0.6629638383360907
epoch: 85, train loss: 0.6345579077805995, acc: 0.7441103350301882; test loss: 0.7532518471253733, acc: 0.7156700543606712
epoch: 86, train loss: 0.6322679636717492, acc: 0.7535811530720966; test loss: 0.8003105982007555, acc: 0.6948711888442448
epoch: 87, train loss: 0.6322972612351627, acc: 0.7465964247661891; test loss: 0.8450758454929822, acc: 0.6847081068305365
epoch: 88, train loss: 0.6261939953400056, acc: 0.7481946253107612; test loss: 0.9754043497168068, acc: 0.6372016071850626
epoch: 89, train loss: 0.6214577838896689, acc: 0.7517461820764768; test loss: 0.8516867557503996, acc: 0.6780902859844008
epoch: 90, train loss: 0.6175792903410157, acc: 0.7532259973955251; test loss: 0.8083979734093042, acc: 0.6877806665090995
epoch: 91, train loss: 0.6085982110368524, acc: 0.7526932638806677; test loss: 0.7579603497792125, acc: 0.703852517135429
epoch: 92, train loss: 0.6066369789936874, acc: 0.7599147626376228; test loss: 0.939030662436092, acc: 0.645710233987237
epoch: 93, train loss: 0.6104948729103777, acc: 0.7526932638806677; test loss: 0.7526367100044166, acc: 0.7078704797920113
epoch: 94, train loss: 0.5949271520566867, acc: 0.7612170001183852; test loss: 0.7985815208478336, acc: 0.6951075395887497
epoch: 95, train loss: 0.616919346881418, acc: 0.7521013377530484; test loss: 0.7640827219414672, acc: 0.708343181281021
epoch: 96, train loss: 0.5980365848038851, acc: 0.7600331478631467; test loss: 0.7792378135168139, acc: 0.7012526589458756
Epoch    96: reducing learning rate of group 0 to 1.5000e-03.
epoch: 97, train loss: 0.5279689548289944, acc: 0.7860778974783947; test loss: 0.7325285441259548, acc: 0.7203970692507682
epoch: 98, train loss: 0.5047584349542902, acc: 0.7918787735290636; test loss: 0.7308316171042493, acc: 0.7286693453084377
epoch: 99, train loss: 0.4854624541508343, acc: 0.8015271694092577; test loss: 0.7110217407616958, acc: 0.7293783975419522
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3899559935402049, acc: 0.7969693382265893; test loss: 0.5907809078425725, acc: 0.7317419049870008
epoch: 101, train loss: 0.38037936496878566, acc: 0.8035989108559252; test loss: 0.6263933112451693, acc: 0.7225242259513117
epoch: 102, train loss: 0.37097161413791463, acc: 0.8028294068900201; test loss: 0.6024138943237831, acc: 0.725124084140865
epoch: 103, train loss: 0.38010411963485974, acc: 0.7965549899372558; test loss: 0.5778374917060313, acc: 0.7338690616875443
epoch: 104, train loss: 0.37345987684672804, acc: 0.8052563040132591; test loss: 0.6079748743844353, acc: 0.7203970692507682
epoch: 105, train loss: 0.3835110031981235, acc: 0.8020599029241151; test loss: 0.5972657367090501, acc: 0.7307965020089813
epoch: 106, train loss: 0.37232616725116985, acc: 0.8017047472475435; test loss: 0.6279191033717899, acc: 0.7111793902150791
epoch: 107, train loss: 0.3691651269827028, acc: 0.802119095536877; test loss: 0.601323347400475, acc: 0.737414322855117
epoch: 108, train loss: 0.3679946610578153, acc: 0.8031253699538298; test loss: 0.6446416653011234, acc: 0.7142519498936422
epoch: 109, train loss: 0.371414434585936, acc: 0.8006984728305907; test loss: 0.635219116409291, acc: 0.7196880170172536
epoch: 110, train loss: 0.3580519441028763, acc: 0.8050787261749733; test loss: 0.6232298096143108, acc: 0.7213424722287876
epoch: 111, train loss: 0.3612060474583111, acc: 0.8034805256304013; test loss: 0.694697105808479, acc: 0.7017253604348853
epoch: 112, train loss: 0.37205595455113794, acc: 0.8015271694092577; test loss: 0.6360347729657903, acc: 0.7159064051051761
epoch: 113, train loss: 0.36854139352121396, acc: 0.8032437551793536; test loss: 0.6308656285993035, acc: 0.7170881588277003
epoch: 114, train loss: 0.3675219606644757, acc: 0.7995146205753522; test loss: 0.6621149250644511, acc: 0.7092885842590404
epoch: 115, train loss: 0.3495798613230011, acc: 0.8084527051024032; test loss: 0.6636149331599763, acc: 0.703852517135429
epoch: 116, train loss: 0.3467866576034049, acc: 0.8067361193323074; test loss: 0.6345370503301402, acc: 0.7166154573386906
epoch: 117, train loss: 0.34931960098980813, acc: 0.8034213330176394; test loss: 0.6169078117339951, acc: 0.7270148900969038
epoch: 118, train loss: 0.3475229662029478, acc: 0.8111163726766899; test loss: 0.6190267392462575, acc: 0.7322146064760104
epoch: 119, train loss: 0.351659873801462, acc: 0.8079199715875459; test loss: 0.6590870375104673, acc: 0.7142519498936422
epoch: 120, train loss: 0.3545657295905247, acc: 0.8060258079791642; test loss: 0.6698184229188765, acc: 0.7000709052233515
epoch: 121, train loss: 0.36052700972802626, acc: 0.8008168580561146; test loss: 0.6337655452204438, acc: 0.7156700543606712
epoch: 122, train loss: 0.34396223614339194, acc: 0.8101692908724991; test loss: 0.6053087158818471, acc: 0.7281966438194281
epoch: 123, train loss: 0.3287708739726603, acc: 0.8156742038593584; test loss: 0.6386964995650867, acc: 0.7246513826518554
epoch: 124, train loss: 0.3383991704532433, acc: 0.8117082988043092; test loss: 0.6360678185922997, acc: 0.7159064051051761
epoch: 125, train loss: 0.3445461611994995, acc: 0.8092222090683083; test loss: 0.6187551538309175, acc: 0.7255967856298747
epoch: 126, train loss: 0.3300411390548375, acc: 0.8139576180892625; test loss: 0.6236548237810742, acc: 0.7289056960529425
epoch: 127, train loss: 0.3287085162735245, acc: 0.8134248845744051; test loss: 0.6530370263167516, acc: 0.7125974946821082
epoch: 128, train loss: 0.31907849561904217, acc: 0.8196401089144075; test loss: 0.6492931423556915, acc: 0.7218151737177972
epoch: 129, train loss: 0.3523615684553873, acc: 0.8030661773410679; test loss: 0.6508393724937863, acc: 0.7182699125502245
epoch: 130, train loss: 0.3393164554248551, acc: 0.8076832011364982; test loss: 0.6580497326509408, acc: 0.7121247931930985
epoch: 131, train loss: 0.33066220562655896, acc: 0.8120634544808808; test loss: 0.6693369992600353, acc: 0.7116520917040888
epoch: 132, train loss: 0.32513299743508506, acc: 0.8126553806085001; test loss: 0.6302554093757382, acc: 0.7277239423304184
epoch: 133, train loss: 0.3209235454376201, acc: 0.8131289215105955; test loss: 0.6637761456497001, acc: 0.7142519498936422
epoch: 134, train loss: 0.31699856048831576, acc: 0.8174499822422162; test loss: 0.6629979533806559, acc: 0.71756086031671
epoch: 135, train loss: 0.3232628669070678, acc: 0.8176867526932639; test loss: 0.6899164192390848, acc: 0.7050342708579532
epoch: 136, train loss: 0.3231726769047883, acc: 0.8157333964721203; test loss: 0.6464732468649446, acc: 0.7159064051051761
epoch: 137, train loss: 0.32015748805543637, acc: 0.819166568012312; test loss: 0.6565545113535813, acc: 0.7187426140392342
epoch: 138, train loss: 0.3169302249520689, acc: 0.8160293595359299; test loss: 0.6253514286474933, acc: 0.7281966438194281
epoch: 139, train loss: 0.3140796636905672, acc: 0.8206463833313602; test loss: 0.699406836450424, acc: 0.7142519498936422
epoch: 140, train loss: 0.3277115944618161, acc: 0.8120634544808808; test loss: 0.6267011426239537, acc: 0.7260694871188844
epoch: 141, train loss: 0.3026383877950716, acc: 0.8236652065822185; test loss: 0.6534871118228167, acc: 0.718978964783739
epoch: 142, train loss: 0.3091576217094991, acc: 0.820883153782408; test loss: 0.6613841641740973, acc: 0.7156700543606712
epoch: 143, train loss: 0.3044473405588276, acc: 0.8225997395525039; test loss: 0.6750883115039483, acc: 0.7144883006381471
epoch: 144, train loss: 0.31050795826514516, acc: 0.8169172487273588; test loss: 0.6447494838627144, acc: 0.7246513826518554
epoch: 145, train loss: 0.3174336730721181, acc: 0.8160293595359299; test loss: 0.623906035029953, acc: 0.7253604348853699
epoch: 146, train loss: 0.31016773201028625, acc: 0.8217710429738369; test loss: 0.6857767704107384, acc: 0.7057433230914677
epoch: 147, train loss: 0.29991695243564037, acc: 0.8227773173907896; test loss: 0.6626878140018331, acc: 0.7196880170172536
Epoch   147: reducing learning rate of group 0 to 7.5000e-04.
epoch: 148, train loss: 0.2613341650804627, acc: 0.8421333017639399; test loss: 0.6544105537786914, acc: 0.7293783975419522
epoch: 149, train loss: 0.23322196799621817, acc: 0.8545045578311826; test loss: 0.6483953565436598, acc: 0.7359962183880879
epoch: 150, train loss: 0.22712521206327793, acc: 0.8587664259500415; test loss: 0.6876328613030603, acc: 0.7317419049870008
epoch: 151, train loss: 0.22540975620058765, acc: 0.8610749378477566; test loss: 0.6494698856379004, acc: 0.737414322855117
epoch: 152, train loss: 0.2148551304721121, acc: 0.8640937610986149; test loss: 0.680022186659324, acc: 0.7312692034979911
epoch: 153, train loss: 0.21695037158978991, acc: 0.8601278560435658; test loss: 0.6652466426981, acc: 0.7307965020089813
epoch: 154, train loss: 0.21666888442810886, acc: 0.8633242571327099; test loss: 0.6605592078279023, acc: 0.7402505317891751
epoch: 155, train loss: 0.2148964602565912, acc: 0.8646264946134722; test loss: 0.6668789214317017, acc: 0.7437957929567478
epoch: 156, train loss: 0.21411091568901283, acc: 0.861607671362614; test loss: 0.6867295797483715, acc: 0.735759867643583
epoch: 157, train loss: 0.20880954351434486, acc: 0.8666982360601397; test loss: 0.6906117693635764, acc: 0.734341763176554
epoch: 158, train loss: 0.20930718481399554, acc: 0.8676453178643305; test loss: 0.6993908009836787, acc: 0.7348144646655637
epoch: 159, train loss: 0.20827411998795858, acc: 0.8663430803835681; test loss: 0.6719840391114428, acc: 0.7367052706216024
epoch: 160, train loss: 0.2185640561502333, acc: 0.8620220196519475; test loss: 0.7603223207176676, acc: 0.7180335618057196
epoch: 161, train loss: 0.20610808196103939, acc: 0.8702497928258554; test loss: 0.6798602029015567, acc: 0.734341763176554
epoch: 162, train loss: 0.21348912660237584, acc: 0.8632650645199479; test loss: 0.700103490748774, acc: 0.7255967856298747
epoch: 163, train loss: 0.21261915030021616, acc: 0.8626139457795667; test loss: 0.685613296804516, acc: 0.7400141810446703
epoch: 164, train loss: 0.20826606830683236, acc: 0.8668758138984255; test loss: 0.6920430708709154, acc: 0.7326873079650201
epoch: 165, train loss: 0.20494722658478326, acc: 0.8666390434473777; test loss: 0.689023864942644, acc: 0.7378870243441267
epoch: 166, train loss: 0.2116592719637601, acc: 0.8636794128092814; test loss: 0.6896737780984595, acc: 0.7336327109430395
epoch: 167, train loss: 0.19592245029841454, acc: 0.8723807268852847; test loss: 0.7142800843848092, acc: 0.7333963601985346
epoch: 168, train loss: 0.20617317695074058, acc: 0.8668166212856635; test loss: 0.7347174858376366, acc: 0.7194516662727488
epoch: 169, train loss: 0.19563608005249503, acc: 0.872084763821475; test loss: 0.7133009154811815, acc: 0.7300874497754668
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.15011700186571933, acc: 0.871374452468332; test loss: 0.6487886392036191, acc: 0.7350508154100686
epoch: 171, train loss: 0.1489426146403733, acc: 0.8696578666982361; test loss: 0.6215870280379636, acc: 0.7381233750886316
epoch: 172, train loss: 0.14827921897141175, acc: 0.8694802888599503; test loss: 0.6015929506901899, acc: 0.7383597258331364
epoch: 173, train loss: 0.14981199709765372, acc: 0.8700722149875696; test loss: 0.6053388624488815, acc: 0.7277239423304184
epoch: 174, train loss: 0.15276376704069175, acc: 0.8658695394814727; test loss: 0.6449490850683477, acc: 0.7194516662727488
epoch: 175, train loss: 0.15055661048813798, acc: 0.8695986740854741; test loss: 0.6592320445806134, acc: 0.7277239423304184
epoch: 176, train loss: 0.15217276669291613, acc: 0.8663430803835681; test loss: 0.5974488086211318, acc: 0.7381233750886316
epoch: 177, train loss: 0.15749180581448444, acc: 0.8624955605540429; test loss: 0.5881218693268212, acc: 0.7364689198770976
epoch: 178, train loss: 0.1466920983725022, acc: 0.8693619036344264; test loss: 0.5925949258010907, acc: 0.7407232332781848
epoch: 179, train loss: 0.13762367233357034, acc: 0.8760506688765242; test loss: 0.6093754665250334, acc: 0.7359962183880879
epoch: 180, train loss: 0.13636046905580804, acc: 0.8743932757191902; test loss: 0.641586086874565, acc: 0.7296147482864571
epoch: 181, train loss: 0.15052968801846092, acc: 0.864804072451758; test loss: 0.6203278230454335, acc: 0.735759867643583
epoch: 182, train loss: 0.148803826461632, acc: 0.8671125843494731; test loss: 0.595979699680524, acc: 0.7367052706216024
epoch: 183, train loss: 0.14392835682237504, acc: 0.8703089854386172; test loss: 0.6212306273403362, acc: 0.7355235168990782
epoch: 184, train loss: 0.13904620866507655, acc: 0.870486563276903; test loss: 0.5983402883998048, acc: 0.7367052706216024
epoch: 185, train loss: 0.14505532714154037, acc: 0.8704273706641411; test loss: 0.6191373669320712, acc: 0.7348144646655637
epoch: 186, train loss: 0.13600479554315706, acc: 0.8750443944595715; test loss: 0.6164639568926171, acc: 0.7341054124320492
epoch: 187, train loss: 0.13982860828498786, acc: 0.8716704155321416; test loss: 0.6056172138859661, acc: 0.7423776884897187
epoch: 188, train loss: 0.14470989818691263, acc: 0.8712560672428081; test loss: 0.6272741829748837, acc: 0.735759867643583
epoch: 189, train loss: 0.1351427952105708, acc: 0.8716704155321416; test loss: 0.5954029686877531, acc: 0.7454502481682818
epoch: 190, train loss: 0.14237282992227834, acc: 0.8700722149875696; test loss: 0.6368766085697499, acc: 0.7296147482864571
epoch: 191, train loss: 0.1526107118406902, acc: 0.862436367941281; test loss: 0.6127808619210467, acc: 0.7336327109430395
epoch: 192, train loss: 0.1411959924023434, acc: 0.8709009115662365; test loss: 0.6164023524845388, acc: 0.7350508154100686
epoch: 193, train loss: 0.14629328918500076, acc: 0.8686515922812833; test loss: 0.6094490577638698, acc: 0.7395414795556606
epoch: 194, train loss: 0.13619313001230482, acc: 0.8744524683319522; test loss: 0.6005991326525713, acc: 0.7416686362562042
epoch: 195, train loss: 0.1406691013898767, acc: 0.8712560672428081; test loss: 0.6115522298990933, acc: 0.7338690616875443
epoch: 196, train loss: 0.13747429454857263, acc: 0.8717296081449035; test loss: 0.6126978618826491, acc: 0.7400141810446703
epoch: 197, train loss: 0.14392211385502596, acc: 0.8670533917367113; test loss: 0.6149648686048128, acc: 0.7381233750886316
epoch: 198, train loss: 0.13653328610980836, acc: 0.8701906002130934; test loss: 0.6386680322965813, acc: 0.7305601512644765
epoch: 199, train loss: 0.1270190723292416, acc: 0.8744524683319522; test loss: 0.6454897054513559, acc: 0.7381233750886316
epoch: 200, train loss: 0.13495768848164935, acc: 0.8700130223748076; test loss: 0.6536939164702421, acc: 0.7227605766958166
Epoch   200: reducing learning rate of group 0 to 3.7500e-04.
best test acc 0.7454502481682818 at epoch 189.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9458    0.9466    0.9462      6100
           1     0.9634    0.8823    0.9211       926
           2     0.8575    0.9650    0.9081      2400
           3     0.9542    0.9134    0.9333       843
           4     0.8845    0.9302    0.9068       774
           5     0.9185    0.9696    0.9434      1512
           6     0.8131    0.8015    0.8073      1330
           7     0.8901    0.8254    0.8565       481
           8     0.7226    0.9214    0.8100       458
           9     0.9313    0.9602    0.9455       452
          10     0.9264    0.8605    0.8923       717
          11     0.8785    0.8468    0.8624       333
          12     0.7500    0.0602    0.1115       299
          13     0.9171    0.6989    0.7932       269

    accuracy                         0.9049     16894
   macro avg     0.8824    0.8273    0.8312     16894
weighted avg     0.9048    0.9049    0.8983     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8058    0.8544    0.8294      1525
           1     0.8164    0.7284    0.7699       232
           2     0.7173    0.7937    0.7536       601
           3     0.8448    0.6967    0.7636       211
           4     0.8010    0.8505    0.8250       194
           5     0.7610    0.8254    0.7919       378
           6     0.4835    0.5285    0.5050       333
           7     0.7021    0.5455    0.6140       121
           8     0.5349    0.6000    0.5656       115
           9     0.8273    0.7982    0.8125       114
          10     0.8077    0.5833    0.6774       180
          11     0.5625    0.5357    0.5488        84
          12     0.2000    0.0267    0.0471        75
          13     0.7714    0.3971    0.5243        68

    accuracy                         0.7455      4231
   macro avg     0.6883    0.6260    0.6449      4231
weighted avg     0.7404    0.7455    0.7384      4231

---------------------------------------
program finished.
