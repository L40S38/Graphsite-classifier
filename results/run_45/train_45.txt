seed:  666
number of classes (from original clusters): 14
how to merge clusters:  [[0, 9, 12], [1, 5, 11], 2, [3, 8, 13], 4, 6, 7, 10]
whether to further subcluster data according to chemical reaction: False
positive training pair sampling threshold:  15000
negative training pair sampling threshold:  4400
positive validation pair sampling threshold:  3400
negative validation pair sampling threshold:  1000
number of epochs to train: 60
learning rate decay to half at epoch 30.
batch size: 256
similar margin of contrastive loss: 0.0
dissimilar margin of contrastive loss: 2.0
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of classes after merging:  8
number of pockets in training set:  12475
number of pockets in validation set:  2670
number of pockets in test set:  2681
number of train positive pairs: 120000
number of train negative pairs: 123200
number of validation positive pairs: 27200
number of validation negative pairs: 28000
model architecture:
ResidualSiameseNet(
  (embedding_net): ResidualEmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=48, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=48, out_features=48, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (rb_2): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_3): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_4): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_5): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_6): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_7): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_8): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (bn_8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(48, 96)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.7583711614106831, validation loss: 0.6849954085419143.
epoch: 2, train loss: 0.6735608422756195, validation loss: 0.6725654480422752.
epoch: 3, train loss: 0.6330696962381664, validation loss: 0.6504767393029255.
epoch: 4, train loss: 0.5978334583106794, validation loss: 0.6211473835378454.
epoch: 5, train loss: 0.5684986525774002, validation loss: 0.6149838088215261.
epoch: 6, train loss: 0.5474346826578441, validation loss: 0.6271828768909842.
epoch: 7, train loss: 0.5241484836528175, validation loss: 0.6364283641870471.
epoch: 8, train loss: 0.5115074419034155, validation loss: 0.618292215457861.
epoch: 9, train loss: 0.5029003453568408, validation loss: 0.6343902004628942.
epoch: 10, train loss: 0.4930149463603371, validation loss: 0.6167666330199311.
epoch: 11, train loss: 0.48012131113755074, validation loss: 0.6064159080947655.
epoch: 12, train loss: 0.47139836317614503, validation loss: 0.6709481283547222.
epoch: 13, train loss: 0.46293859177514124, validation loss: 0.6357882422295169.
epoch: 14, train loss: 0.4522590963777743, validation loss: 0.6198196275683416.
epoch: 15, train loss: 0.4444717549336584, validation loss: 0.616437333424886.
epoch: 16, train loss: 0.4416060234998402, validation loss: 0.612505134637805.
epoch: 17, train loss: 0.43305811442826925, validation loss: 0.6341218821207683.
epoch: 18, train loss: 0.4277261850080992, validation loss: 0.6359584142159724.
epoch: 19, train loss: 0.42718561304242986, validation loss: 0.6257958674776382.
epoch: 20, train loss: 0.41866067936545925, validation loss: 0.6380480406940847.
epoch: 21, train loss: 0.411706233526531, validation loss: 0.6094259151514025.
epoch: 22, train loss: 0.40854786747380306, validation loss: 0.6451941957335541.
epoch: 23, train loss: 0.4033429174987893, validation loss: 0.6210119194915329.
epoch: 24, train loss: 0.4073777415564186, validation loss: 0.6172317706674769.
epoch: 25, train loss: 0.3991287110040062, validation loss: 0.6219379552205404.
epoch: 26, train loss: 0.3940633684396744, validation loss: 0.6404643990336985.
epoch: 27, train loss: 0.39272390011109803, validation loss: 0.6359397722327191.
epoch: 28, train loss: 0.3908099480051743, validation loss: 0.637654509337052.
epoch: 29, train loss: 0.3920489765468397, validation loss: 0.6146994905886443.
epoch: 30, train loss: 0.3335195727724778, validation loss: 0.6161697425013003.
epoch: 31, train loss: 0.3317280814992754, validation loss: 0.6255519692794136.
epoch: 32, train loss: 0.3283150116079732, validation loss: 0.6322775381889896.
epoch: 33, train loss: 0.323696722403953, validation loss: 0.6506626975018045.
epoch: 34, train loss: 0.3280967305992779, validation loss: 0.6419508538729902.
epoch: 35, train loss: 0.3137602780524053, validation loss: 0.6266158361020295.
epoch: 36, train loss: 0.328875648347955, validation loss: 0.6207139841715494.
epoch: 37, train loss: 0.3135057991899942, validation loss: 0.6293810453276704.
epoch: 38, train loss: 0.3193303170800209, validation loss: 0.6351449814395628.
epoch: 39, train loss: 0.3181047504512887, validation loss: 0.63361804464589.
epoch: 40, train loss: 0.31331627842627074, validation loss: 0.6368172913703366.
epoch: 41, train loss: 0.3160875877894853, validation loss: 0.6261899678603463.
epoch: 42, train loss: 0.31458995873990814, validation loss: 0.6273728401073511.
epoch: 43, train loss: 0.31310123758880715, validation loss: 0.6076444788946622.
epoch: 44, train loss: 0.32130243508439316, validation loss: 0.6396330504486526.
epoch: 45, train loss: 0.3084454933906856, validation loss: 0.6621253746143286.
epoch: 46, train loss: 0.31277439285265773, validation loss: 0.6242508448725161.
epoch: 47, train loss: 0.3324566306879646, validation loss: 0.6346964780835138.
epoch: 48, train loss: 0.3087524242934428, validation loss: 0.6150521883757218.
epoch: 49, train loss: 0.3128195263523805, validation loss: 0.6311767614060554.
epoch: 50, train loss: 0.308808071330974, validation loss: 0.6390814369312231.
epoch: 51, train loss: 0.3096510098952996, validation loss: 0.6167860086413397.
epoch: 52, train loss: 0.30987368848763014, validation loss: 0.6289562554290329.
epoch: 53, train loss: 0.3033974842805611, validation loss: 0.666672166188558.
epoch: 54, train loss: 0.3063909306808522, validation loss: 0.6486427553149238.
epoch: 55, train loss: 0.3104173566949995, validation loss: 0.6346753938647284.
epoch: 56, train loss: 0.3097258298804885, validation loss: 0.6145956274391948.
epoch: 57, train loss: 0.30066030492908075, validation loss: 0.6419353363479393.
epoch: 58, train loss: 0.3084278805475486, validation loss: 0.6227110778421595.
epoch: 59, train loss: 0.30590032833187203, validation loss: 0.6363020543084629.
epoch: 60, train loss: 0.30845444172620773, validation loss: 0.626982638594033.
best validation loss 0.6064159080947655 at epoch 11.
