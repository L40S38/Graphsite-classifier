seed:  7
save trained model at:  ../trained_models/trained_classifier_model_107.pt
save loss at:  ./results/train_classifier_results_107.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['2btoB00', '5oj7A00', '6d4vA01', '2qtzA01', '6ef3R01']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1vgvC00', '5kviA01', '5j5xA00', '4usjD00', '4cvlA00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2ada38b2e730>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 1.9973597780436303, acc: 0.3957026163134841; test loss: 1.748112388266877, acc: 0.4630111084849917
epoch: 2, train loss: 1.7101302365374957, acc: 0.472297857227418; test loss: 1.63040111009124, acc: 0.5057905932403687
epoch: 3, train loss: 1.6114306323299947, acc: 0.5085829288504795; test loss: 1.5582529927675, acc: 0.5249350035452611
epoch: 4, train loss: 1.534870892907792, acc: 0.5339173671125843; test loss: 1.4768415395037497, acc: 0.534389033325455
epoch: 5, train loss: 1.4974787448360551, acc: 0.5453415413756363; test loss: 1.4294198538726792, acc: 0.5691325927676673
epoch: 6, train loss: 1.4830424892446537, acc: 0.556232982123831; test loss: 1.4747800530623218, acc: 0.554242495863862
epoch: 7, train loss: 1.4424316956190524, acc: 0.5647567183615485; test loss: 1.4053172144983372, acc: 0.5802410777593949
epoch: 8, train loss: 1.4107909393801665, acc: 0.5755297738842192; test loss: 1.4145371665968114, acc: 0.5582604585204444
epoch: 9, train loss: 1.3907903605020016, acc: 0.5774831301053629; test loss: 1.3778185101723, acc: 0.5729142046797447
epoch: 10, train loss: 1.3765779435881584, acc: 0.5828696578666982; test loss: 1.40762497944292, acc: 0.5670054360671236
epoch: 11, train loss: 1.3499838168259497, acc: 0.5885521486918432; test loss: 1.3254904036430624, acc: 0.5951311746632002
epoch: 12, train loss: 1.3283407874427435, acc: 0.5944714099680359; test loss: 1.4916904951995835, acc: 0.574095958402269
epoch: 13, train loss: 1.306412511144543, acc: 0.6053036581034686; test loss: 1.3071080839287386, acc: 0.6097849208225006
epoch: 14, train loss: 1.2945226256884357, acc: 0.6064283177459453; test loss: 1.334282961615263, acc: 0.5875679508390451
epoch: 15, train loss: 1.302828793174574, acc: 0.6015745234994673; test loss: 1.3957344419537523, acc: 0.5811864807374143
epoch: 16, train loss: 1.281181272605728, acc: 0.609861489286137; test loss: 1.2658313309095397, acc: 0.613802883479083
epoch: 17, train loss: 1.2747019009574907, acc: 0.6145377056943293; test loss: 1.294371783944682, acc: 0.598440085086268
epoch: 18, train loss: 1.25118850841739, acc: 0.6230614419320469; test loss: 1.2898610468819136, acc: 0.6166390924131411
epoch: 19, train loss: 1.2621997597091392, acc: 0.6169054102048065; test loss: 1.4156411154505153, acc: 0.5660600330891042
epoch: 20, train loss: 1.2432107363168312, acc: 0.6201018112939505; test loss: 1.245606998607245, acc: 0.6126211297565587
epoch: 21, train loss: 1.2118745449098225, acc: 0.6325322599739552; test loss: 1.2834453715750636, acc: 0.6088395178444812
epoch: 22, train loss: 1.1902125862148, acc: 0.6398721439564342; test loss: 1.2303166382703283, acc: 0.6156936894351217
epoch: 23, train loss: 1.2416437299375465, acc: 0.6196874630046171; test loss: 1.2545293356278522, acc: 0.613802883479083
epoch: 24, train loss: 1.2046104169843097, acc: 0.6369125133183379; test loss: 1.2142416775522433, acc: 0.6201843535807138
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9501943992583359, acc: 0.6456138273943411; test loss: 0.997968375499692, acc: 0.6253840699598203
epoch: 26, train loss: 0.9355721524781527, acc: 0.6479223392920563; test loss: 0.9847082113492746, acc: 0.6246750177263058
epoch: 27, train loss: 0.9321587350053224, acc: 0.6427725819817687; test loss: 1.0104176122453077, acc: 0.6095485700779958
epoch: 28, train loss: 0.9269576125685755, acc: 0.6479223392920563; test loss: 0.9244650313544854, acc: 0.6431103757976837
epoch: 29, train loss: 0.8996141852536004, acc: 0.6560909198532023; test loss: 0.9799080362727923, acc: 0.6303474355944221
epoch: 30, train loss: 0.9042218725356997, acc: 0.6541375636320588; test loss: 0.999933624121067, acc: 0.6187662491136847
epoch: 31, train loss: 0.8975135915883691, acc: 0.6573931573339648; test loss: 1.0750289912822257, acc: 0.5887497045615694
epoch: 32, train loss: 0.8910393971189939, acc: 0.6559133420149166; test loss: 0.9290532097132373, acc: 0.6374379579295675
epoch: 33, train loss: 0.8824804857791324, acc: 0.6605895584231088; test loss: 0.9493830171326773, acc: 0.627511226660364
epoch: 34, train loss: 0.8718062425148025, acc: 0.665324967444063; test loss: 0.9692061682791047, acc: 0.6201843535807138
epoch: 35, train loss: 0.8689003091698443, acc: 0.6659168935716823; test loss: 1.1317267987610347, acc: 0.5752777121247932
epoch: 36, train loss: 0.8623223311804844, acc: 0.665265774831301; test loss: 0.9255533957137647, acc: 0.6360198534625384
epoch: 37, train loss: 0.8506707220341703, acc: 0.6725464662010181; test loss: 0.9689512683324739, acc: 0.63365634601749
epoch: 38, train loss: 0.8356907184242257, acc: 0.673493548005209; test loss: 0.9721668458554064, acc: 0.6145119357125975
epoch: 39, train loss: 0.8397265432275257, acc: 0.6692908724991121; test loss: 0.9003339588599858, acc: 0.6398014653746159
Epoch    39: reducing learning rate of group 0 to 1.5000e-03.
epoch: 40, train loss: 0.7809927745148799, acc: 0.6943293476974074; test loss: 0.8015886176931354, acc: 0.6863625620420705
epoch: 41, train loss: 0.7464711708938975, acc: 0.7049248253817924; test loss: 0.8006219399564234, acc: 0.6856535098085559
epoch: 42, train loss: 0.7430165226507204, acc: 0.7048656327690305; test loss: 0.798478766610171, acc: 0.6837627038525171
epoch: 43, train loss: 0.7211658348201139, acc: 0.7130934059429384; test loss: 0.7998369141296697, acc: 0.6899078232096431
epoch: 44, train loss: 0.7264405107221562, acc: 0.7122647093642713; test loss: 0.7965528176654121, acc: 0.6865989127865753
epoch: 45, train loss: 0.7117269109787709, acc: 0.7135077542322719; test loss: 0.8248411015510785, acc: 0.6769085322618766
epoch: 46, train loss: 0.7148779559392286, acc: 0.7169409257724636; test loss: 0.7930150046131059, acc: 0.6891987709761286
epoch: 47, train loss: 0.700954175311545, acc: 0.7208476382147508; test loss: 0.7732738945901718, acc: 0.7026707634129048
epoch: 48, train loss: 0.7097989501184834, acc: 0.7188350893808453; test loss: 0.8621770685318091, acc: 0.6757267785393524
epoch: 49, train loss: 0.6949121997217265, acc: 0.722327453533799; test loss: 0.793068268259094, acc: 0.6894351217206334
epoch: 50, train loss: 0.6907841627225518, acc: 0.7222090683082751; test loss: 0.7827099696650512, acc: 0.691562278421177
epoch: 51, train loss: 0.6791934425822092, acc: 0.7248727358825618; test loss: 0.7731874865354079, acc: 0.6995982037343418
epoch: 52, train loss: 0.6684368163703588, acc: 0.7307919971587545; test loss: 0.777966648488145, acc: 0.7026707634129048
epoch: 53, train loss: 0.6526070096315795, acc: 0.7368296436604712; test loss: 0.7792539842808903, acc: 0.7007799574568659
epoch: 54, train loss: 0.6605936542578501, acc: 0.732922931218184; test loss: 0.7738415677619019, acc: 0.697707397778303
epoch: 55, train loss: 0.6443615150307711, acc: 0.7420977861962827; test loss: 0.8137824200084941, acc: 0.6799810919404397
epoch: 56, train loss: 0.6674071681546053, acc: 0.7310879602225642; test loss: 0.8033268005395438, acc: 0.6939257858662254
epoch: 57, train loss: 0.6548294093364143, acc: 0.738605422043329; test loss: 0.8241944620159044, acc: 0.691562278421177
epoch: 58, train loss: 0.6322776554138037, acc: 0.7465964247661891; test loss: 0.8161062843211856, acc: 0.6849444575750414
epoch: 59, train loss: 0.6278552882019636, acc: 0.7453533798981887; test loss: 0.7881010470731803, acc: 0.6906168754431576
epoch: 60, train loss: 0.6318640877498569, acc: 0.744347105481236; test loss: 0.8129500418626735, acc: 0.6910895769321673
epoch: 61, train loss: 0.6149579582375804, acc: 0.7509766781105718; test loss: 0.7730651991047833, acc: 0.7130701961711179
epoch: 62, train loss: 0.60979594420269, acc: 0.7493192849532379; test loss: 0.7791642295753552, acc: 0.7088158827700307
epoch: 63, train loss: 0.6146930703202889, acc: 0.7526932638806677; test loss: 0.7859085071061526, acc: 0.696289293311274
epoch: 64, train loss: 0.6010773655181949, acc: 0.7558896649698118; test loss: 0.7400455477671036, acc: 0.7203970692507682
epoch: 65, train loss: 0.5918141454299469, acc: 0.7625192375991476; test loss: 0.8234099947344015, acc: 0.6877806665090995
epoch: 66, train loss: 0.5791117291810655, acc: 0.761453770569433; test loss: 0.7798853765224852, acc: 0.7059796738359726
epoch: 67, train loss: 0.5872527883788657, acc: 0.7592044512844797; test loss: 0.7786373387158438, acc: 0.708343181281021
epoch: 68, train loss: 0.587165749953656, acc: 0.7601515330886706; test loss: 0.7436812390562096, acc: 0.7109430394705744
epoch: 69, train loss: 0.5751341009924676, acc: 0.7613353853439091; test loss: 0.7844676071519521, acc: 0.693216733632711
epoch: 70, train loss: 0.5727422126682747, acc: 0.7667811057180064; test loss: 0.7658939043755961, acc: 0.720633419995273
epoch: 71, train loss: 0.5664426286897674, acc: 0.7673730318456257; test loss: 0.7254819400032709, acc: 0.7265421886078941
epoch: 72, train loss: 0.5553732378335765, acc: 0.7747129158281046; test loss: 0.843080768571681, acc: 0.7014890096903805
epoch: 73, train loss: 0.5644686340481286, acc: 0.7709245886113413; test loss: 0.7456014467959313, acc: 0.7225242259513117
epoch: 74, train loss: 0.5369561765692679, acc: 0.7796850953001065; test loss: 0.7537405006261729, acc: 0.7116520917040888
epoch: 75, train loss: 0.5484491796751509, acc: 0.7751864567302; test loss: 0.7590049901095386, acc: 0.718978964783739
epoch: 76, train loss: 0.5345296554908648, acc: 0.7777909316917249; test loss: 0.7935690236863614, acc: 0.7118884424485937
epoch: 77, train loss: 0.5387687840813982, acc: 0.7786788208831538; test loss: 0.7370185484646117, acc: 0.7182699125502245
epoch: 78, train loss: 0.5203577304882765, acc: 0.7831182668402983; test loss: 0.734695979704166, acc: 0.7265421886078941
epoch: 79, train loss: 0.5068814410632131, acc: 0.7889783355037291; test loss: 0.8068155797202377, acc: 0.6965256440557788
epoch: 80, train loss: 0.515043940264134, acc: 0.7882088315378241; test loss: 0.7339762341353494, acc: 0.7378870243441267
epoch: 81, train loss: 0.5088328741225348, acc: 0.7888007576654433; test loss: 0.7691429376433122, acc: 0.7085795320255259
epoch: 82, train loss: 0.50485243975968, acc: 0.7902805729844915; test loss: 0.8024148583778448, acc: 0.7081068305365162
epoch: 83, train loss: 0.5026475494467071, acc: 0.7889783355037291; test loss: 0.841693992734209, acc: 0.6974710470337981
epoch: 84, train loss: 0.4949444860724482, acc: 0.7942464780395406; test loss: 0.7989500736519677, acc: 0.708343181281021
epoch: 85, train loss: 0.48910549447086843, acc: 0.798093997869066; test loss: 0.7479912559069294, acc: 0.7310328527534862
epoch: 86, train loss: 0.49443284252543157, acc: 0.7970285308393512; test loss: 0.7942317672667282, acc: 0.7102339872370598
epoch: 87, train loss: 0.4953519975813067, acc: 0.7945424411033503; test loss: 0.7798746258788175, acc: 0.722051524462302
epoch: 88, train loss: 0.4977446313410719, acc: 0.7954895229075412; test loss: 0.7721715012894993, acc: 0.7293783975419522
epoch: 89, train loss: 0.4748050843143768, acc: 0.8033621404048775; test loss: 0.8306974233732356, acc: 0.7078704797920113
epoch: 90, train loss: 0.4737046006178311, acc: 0.8035989108559252; test loss: 0.7695113053927042, acc: 0.7010163082013708
epoch: 91, train loss: 0.48334710351303367, acc: 0.7948384041671599; test loss: 0.7071507552418972, acc: 0.7303238005199716
Epoch    91: reducing learning rate of group 0 to 7.5000e-04.
epoch: 92, train loss: 0.41313211194534927, acc: 0.8299988161477447; test loss: 0.7571147292873533, acc: 0.7263058378633893
epoch: 93, train loss: 0.38602616326673733, acc: 0.8362140404877472; test loss: 0.7359389050072797, acc: 0.7421413377452138
epoch: 94, train loss: 0.36729411406320583, acc: 0.8417781460873683; test loss: 0.7745902969943691, acc: 0.7471047033798156
epoch: 95, train loss: 0.3687217707385308, acc: 0.8431395761808926; test loss: 0.7659654507426273, acc: 0.7482864571023399
epoch: 96, train loss: 0.36700298363984346, acc: 0.844441813661655; test loss: 0.7717392888088312, acc: 0.7397778303001654
epoch: 97, train loss: 0.35078023546783826, acc: 0.8512489641292766; test loss: 0.7812603293680125, acc: 0.743559442212243
epoch: 98, train loss: 0.36556851883265934, acc: 0.8446785841127027; test loss: 0.7754628395701774, acc: 0.7348144646655637
epoch: 99, train loss: 0.35953201297628706, acc: 0.8458624363679412; test loss: 0.768554073224195, acc: 0.7440321437012527
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.26925751334028697, acc: 0.8496507635847046; test loss: 0.6421562865732184, acc: 0.748050106357835
epoch: 101, train loss: 0.26668687668603963, acc: 0.8532615129631822; test loss: 0.662030914381474, acc: 0.7445048451902624
epoch: 102, train loss: 0.2532727835629132, acc: 0.8550964839588019; test loss: 0.6579400870963998, acc: 0.7355235168990782
epoch: 103, train loss: 0.2543914094752527, acc: 0.8555700248608974; test loss: 0.6499940069593512, acc: 0.7482864571023399
epoch: 104, train loss: 0.2549522547830812, acc: 0.8529655498993726; test loss: 0.6488456868359, acc: 0.7331600094540298
epoch: 105, train loss: 0.24545583271290178, acc: 0.8560435657629928; test loss: 0.6500544916002836, acc: 0.7456865989127865
epoch: 106, train loss: 0.24239758440654235, acc: 0.8584704628862317; test loss: 0.6779306974571045, acc: 0.7433230914677381
epoch: 107, train loss: 0.24650773929606898, acc: 0.8587664259500415; test loss: 0.663369465764604, acc: 0.7414322855116994
epoch: 108, train loss: 0.2421959909063917, acc: 0.8569906475671836; test loss: 0.7001566343683916, acc: 0.7336327109430395
epoch: 109, train loss: 0.2463259964394572, acc: 0.8559843731502309; test loss: 0.6832169919565084, acc: 0.7430867407232333
epoch: 110, train loss: 0.24787834934458952, acc: 0.8564579140523263; test loss: 0.7123527788321365, acc: 0.7161427558496809
epoch: 111, train loss: 0.2472185424351616, acc: 0.8551556765715639; test loss: 0.6713901017467848, acc: 0.7461593004017962
epoch: 112, train loss: 0.2305750658247645, acc: 0.8607789747839469; test loss: 0.6722550921281216, acc: 0.7523044197589223
epoch: 113, train loss: 0.2296610754827811, acc: 0.8610749378477566; test loss: 0.6677468245656629, acc: 0.7463956511463011
epoch: 114, train loss: 0.23406759578687447, acc: 0.8587072333372795; test loss: 0.6879772593918153, acc: 0.7423776884897187
epoch: 115, train loss: 0.22963279180693544, acc: 0.8664022729963301; test loss: 0.7102224663960063, acc: 0.7416686362562042
epoch: 116, train loss: 0.23665009062537445, acc: 0.8578785367586125; test loss: 0.6942321336232751, acc: 0.74048688253368
epoch: 117, train loss: 0.23956500238478642, acc: 0.857523381082041; test loss: 0.7027010049303777, acc: 0.7263058378633893
epoch: 118, train loss: 0.23156762681967608, acc: 0.8579377293713745; test loss: 0.7034612393384847, acc: 0.735759867643583
epoch: 119, train loss: 0.218690983796735, acc: 0.8690659405706168; test loss: 0.7075063688651915, acc: 0.7421413377452138
epoch: 120, train loss: 0.22809077015187848, acc: 0.8642121463241388; test loss: 0.6946590893997197, acc: 0.743559442212243
epoch: 121, train loss: 0.2179172912867687, acc: 0.8676453178643305; test loss: 0.6928747901677578, acc: 0.7442684944457575
epoch: 122, train loss: 0.20711591931345957, acc: 0.8732094234639517; test loss: 0.7174791354880606, acc: 0.7414322855116994
epoch: 123, train loss: 0.21521860541740762, acc: 0.8671717769622351; test loss: 0.7066838332762028, acc: 0.7381233750886316
epoch: 124, train loss: 0.21763056001889575, acc: 0.8646264946134722; test loss: 0.7042993691141682, acc: 0.7461593004017962
epoch: 125, train loss: 0.210768809738534, acc: 0.86977625192376; test loss: 0.7227034494404082, acc: 0.7341054124320492
epoch: 126, train loss: 0.2167245520368559, acc: 0.8664022729963301; test loss: 0.7829556892848527, acc: 0.725124084140865
epoch: 127, train loss: 0.21623042227072561, acc: 0.8661063099325205; test loss: 0.705148753418874, acc: 0.7362325691325927
epoch: 128, train loss: 0.20943705586938585, acc: 0.8665798508346159; test loss: 0.7157755351128575, acc: 0.7350508154100686
epoch: 129, train loss: 0.21023278820933824, acc: 0.8704273706641411; test loss: 0.7484744865825007, acc: 0.7329236587095249
epoch: 130, train loss: 0.20769057446603198, acc: 0.8684740144429975; test loss: 0.7201279984724153, acc: 0.7362325691325927
epoch: 131, train loss: 0.19992471181232857, acc: 0.872143956434237; test loss: 0.7289953404807278, acc: 0.7411959347671945
epoch: 132, train loss: 0.19270752233817928, acc: 0.8758138984254765; test loss: 0.8286415241311781, acc: 0.7218151737177972
epoch: 133, train loss: 0.2016117901608438, acc: 0.8700722149875696; test loss: 0.7298850419250287, acc: 0.7317419049870008
epoch: 134, train loss: 0.21250971786239464, acc: 0.8674677400260448; test loss: 0.7295043570469582, acc: 0.7350508154100686
epoch: 135, train loss: 0.19072005996813332, acc: 0.8745116609447141; test loss: 0.728823618752487, acc: 0.7430867407232333
epoch: 136, train loss: 0.20221729669554553, acc: 0.8736237717532852; test loss: 0.7491582009674556, acc: 0.752540770503427
epoch: 137, train loss: 0.19241428996022952, acc: 0.8790102995146206; test loss: 0.7365749908434643, acc: 0.7331600094540298
epoch: 138, train loss: 0.20322847507635602, acc: 0.86977625192376; test loss: 0.7842047331716907, acc: 0.7123611439376034
epoch: 139, train loss: 0.20069665477214318, acc: 0.8695394814727122; test loss: 0.7442308976221411, acc: 0.7352871661545733
epoch: 140, train loss: 0.19030369209956738, acc: 0.8776488694210962; test loss: 0.7370056755518525, acc: 0.7421413377452138
epoch: 141, train loss: 0.18990345523458155, acc: 0.8792470699656683; test loss: 0.7382535871899288, acc: 0.7400141810446703
epoch: 142, train loss: 0.1823821563357021, acc: 0.8800757665443353; test loss: 0.7513850261009323, acc: 0.7395414795556606
epoch: 143, train loss: 0.1831189983287224, acc: 0.8803717296081449; test loss: 0.7279640623537594, acc: 0.752540770503427
epoch: 144, train loss: 0.17548190470777406, acc: 0.8818515449271931; test loss: 0.7647286099312407, acc: 0.7433230914677381
epoch: 145, train loss: 0.18361032063811153, acc: 0.8792470699656683; test loss: 0.729381908228763, acc: 0.7426140392342235
epoch: 146, train loss: 0.18868222816626645, acc: 0.8783591807742394; test loss: 0.7813608282932686, acc: 0.7336327109430395
epoch: 147, train loss: 0.1851572719349358, acc: 0.8800165739315733; test loss: 0.7286217233526653, acc: 0.7456865989127865
Epoch   147: reducing learning rate of group 0 to 3.7500e-04.
epoch: 148, train loss: 0.14914880603167296, acc: 0.8991949804664378; test loss: 0.7401986791574203, acc: 0.7454502481682818
epoch: 149, train loss: 0.13073861531404343, acc: 0.9074227536403456; test loss: 0.7750876169062709, acc: 0.7487591585913496
epoch: 150, train loss: 0.12255509395465013, acc: 0.9135195927548242; test loss: 0.7909288245773406, acc: 0.7523044197589223
epoch: 151, train loss: 0.1222001499585376, acc: 0.9123949331123475; test loss: 0.7893228311117195, acc: 0.7494682108248641
epoch: 152, train loss: 0.11932445081032224, acc: 0.9119805848230141; test loss: 0.8319837991240683, acc: 0.7520680690144174
epoch: 153, train loss: 0.1152966503837595, acc: 0.9171895347460637; test loss: 0.8267978143815987, acc: 0.7489955093358545
epoch: 154, train loss: 0.11482122207813056, acc: 0.9133420149165384; test loss: 0.8072921134314901, acc: 0.7560860316709997
epoch: 155, train loss: 0.11773736659727167, acc: 0.9139339410441577; test loss: 0.8175971756531425, acc: 0.7471047033798156
epoch: 156, train loss: 0.11834295208325425, acc: 0.91369717059311; test loss: 0.813820947628161, acc: 0.7523044197589223
epoch: 157, train loss: 0.1217374151352737, acc: 0.9125133183378714; test loss: 0.8520881760010508, acc: 0.7362325691325927
epoch: 158, train loss: 0.1199576210394207, acc: 0.9111518882443471; test loss: 0.837096911630741, acc: 0.7523044197589223
epoch: 159, train loss: 0.11587383820715777, acc: 0.9125133183378714; test loss: 0.8267574115330321, acc: 0.7487591585913496
epoch: 160, train loss: 0.1058321463000821, acc: 0.920208357996922; test loss: 0.8326285958318276, acc: 0.7482864571023399
epoch: 161, train loss: 0.10948510578878394, acc: 0.9187285426778738; test loss: 0.84809589340906, acc: 0.7534861734814464
epoch: 162, train loss: 0.1102936133800547, acc: 0.9175446904226352; test loss: 0.8610007331828309, acc: 0.7475774048688253
epoch: 163, train loss: 0.10803467504538149, acc: 0.9206818988990174; test loss: 0.8418405533851221, acc: 0.7494682108248641
epoch: 164, train loss: 0.11377163069430535, acc: 0.9171895347460637; test loss: 0.8360303334664634, acc: 0.743559442212243
epoch: 165, train loss: 0.1155164902393629, acc: 0.9157689120397774; test loss: 0.8402997309703462, acc: 0.7553769794374853
epoch: 166, train loss: 0.11239673443711043, acc: 0.9167159938439683; test loss: 0.8542150036715582, acc: 0.7473410541243205
epoch: 167, train loss: 0.1090338798589335, acc: 0.9174854978098733; test loss: 0.8558085733371772, acc: 0.7449775466792721
epoch: 168, train loss: 0.10480987588703329, acc: 0.9199715875458743; test loss: 0.8658221501657174, acc: 0.7499409123138738
epoch: 169, train loss: 0.11119338366580854, acc: 0.9183141943885403; test loss: 0.8964153900407951, acc: 0.7430867407232333
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.07846338541723362, acc: 0.9200899727713981; test loss: 0.7684258950908989, acc: 0.7497045615693689
epoch: 171, train loss: 0.07781732765505889, acc: 0.9190245057416835; test loss: 0.734306913491711, acc: 0.7546679272039707
epoch: 172, train loss: 0.07420951436454902, acc: 0.92014916538416; test loss: 0.7513150418799303, acc: 0.7452138974237769
epoch: 173, train loss: 0.07795144701174792, acc: 0.9206227062862554; test loss: 0.7690923373993901, acc: 0.7456865989127865
epoch: 174, train loss: 0.07837134953206766, acc: 0.917840653486445; test loss: 0.7531376645740283, acc: 0.7414322855116994
epoch: 175, train loss: 0.06877762398184073, acc: 0.9257132709837812; test loss: 0.7348798361548124, acc: 0.7501772630583786
epoch: 176, train loss: 0.07048224362643835, acc: 0.9240558778264473; test loss: 0.7435592423373434, acc: 0.7463956511463011
epoch: 177, train loss: 0.07511096411719045, acc: 0.9216881733159702; test loss: 0.7672369298121237, acc: 0.7400141810446703
epoch: 178, train loss: 0.08632228978360153, acc: 0.912039777435776; test loss: 0.7691986750158907, acc: 0.7468683526353108
epoch: 179, train loss: 0.0679285667675253, acc: 0.924292648277495; test loss: 0.7479133440286829, acc: 0.748050106357835
epoch: 180, train loss: 0.06908817528642831, acc: 0.9221617142180656; test loss: 0.7810638736985418, acc: 0.7456865989127865
epoch: 181, train loss: 0.07298986650002247, acc: 0.9209186693500652; test loss: 0.7686270285204944, acc: 0.7463956511463011
epoch: 182, train loss: 0.06961148395398756, acc: 0.9248253817923523; test loss: 0.7519110882149833, acc: 0.7492318600803592
epoch: 183, train loss: 0.07524961671071236, acc: 0.9193204688054931; test loss: 0.7607288199997039, acc: 0.7407232332781848
epoch: 184, train loss: 0.07707230993011259, acc: 0.9194980466437789; test loss: 0.7510181420014502, acc: 0.738832427322146
epoch: 185, train loss: 0.07548708459767023, acc: 0.917781460873683; test loss: 0.7218558544302291, acc: 0.7504136138028835
epoch: 186, train loss: 0.07611720576464276, acc: 0.9187285426778738; test loss: 0.7682250986367472, acc: 0.7402505317891751
epoch: 187, train loss: 0.07896778517056177, acc: 0.9171303421333018; test loss: 0.7515097435302301, acc: 0.7397778303001654
epoch: 188, train loss: 0.07451010568986295, acc: 0.9170711495205398; test loss: 0.765067620301072, acc: 0.7463956511463011
epoch: 189, train loss: 0.07278818135870072, acc: 0.9203859358352078; test loss: 0.7331313698096242, acc: 0.7449775466792721
epoch: 190, train loss: 0.06758623516134811, acc: 0.9248845744051143; test loss: 0.7600213812474409, acc: 0.7532498227369416
epoch: 191, train loss: 0.06666759790362473, acc: 0.9262460044986386; test loss: 0.7633760390172357, acc: 0.7449775466792721
epoch: 192, train loss: 0.06786922655882349, acc: 0.9257132709837812; test loss: 0.7635009482633248, acc: 0.7487591585913496
epoch: 193, train loss: 0.07379545006785038, acc: 0.920208357996922; test loss: 0.7935680874806718, acc: 0.7319782557315055
epoch: 194, train loss: 0.06369827083323332, acc: 0.927489049366639; test loss: 0.7576431538359377, acc: 0.751122666036398
epoch: 195, train loss: 0.07054355263893486, acc: 0.920208357996922; test loss: 0.7583179561783591, acc: 0.7428503899787284
epoch: 196, train loss: 0.07656039060709235, acc: 0.9178998460992068; test loss: 0.7571861634607209, acc: 0.7400141810446703
epoch: 197, train loss: 0.07003410724164241, acc: 0.9197940097075885; test loss: 0.7657172913755319, acc: 0.7430867407232333
epoch: 198, train loss: 0.07149324238801322, acc: 0.9203267432224458; test loss: 0.7534417742618965, acc: 0.748050106357835
Epoch   198: reducing learning rate of group 0 to 1.8750e-04.
epoch: 199, train loss: 0.05920884445551317, acc: 0.9277850124304486; test loss: 0.7472018887883409, acc: 0.7494682108248641
epoch: 200, train loss: 0.049431136992422735, acc: 0.9365455191192139; test loss: 0.7697503151439431, acc: 0.7530134719924367
best test acc 0.7560860316709997 at epoch 154.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9738    0.9803    0.9770      6100
           1     0.9875    0.9363    0.9612       926
           2     0.8863    0.9808    0.9312      2400
           3     0.9740    0.9336    0.9534       843
           4     0.9424    0.9935    0.9673       774
           5     0.9681    0.9848    0.9764      1512
           6     0.8857    0.8797    0.8827      1330
           7     0.9190    0.9439    0.9313       481
           8     0.9136    0.9236    0.9186       458
           9     0.9313    0.9889    0.9592       452
          10     0.9547    0.9107    0.9322       717
          11     0.9294    0.9489    0.9391       333
          12     0.7000    0.0702    0.1277       299
          13     0.8449    0.7695    0.8054       269

    accuracy                         0.9434     16894
   macro avg     0.9150    0.8746    0.8759     16894
weighted avg     0.9403    0.9434    0.9368     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8147    0.8675    0.8403      1525
           1     0.8486    0.7974    0.8222       232
           2     0.6897    0.7804    0.7322       601
           3     0.8444    0.7204    0.7775       211
           4     0.8265    0.8351    0.8308       194
           5     0.8057    0.8228    0.8141       378
           6     0.5562    0.5646    0.5604       333
           7     0.5833    0.6364    0.6087       121
           8     0.5657    0.4870    0.5234       115
           9     0.8333    0.7895    0.8108       114
          10     0.8370    0.6278    0.7175       180
          11     0.5616    0.4881    0.5223        84
          12     0.0667    0.0133    0.0222        75
          13     0.6596    0.4559    0.5391        68

    accuracy                         0.7561      4231
   macro avg     0.6781    0.6347    0.6515      4231
weighted avg     0.7470    0.7561    0.7491      4231

---------------------------------------
program finished.
