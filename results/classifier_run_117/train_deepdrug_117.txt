seed:  17
save trained model at:  ../trained_models/trained_classifier_model_117.pt
save loss at:  ./results/train_classifier_results_117.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['5wc2A00', '4i2eA00', '3ffuA00', '3ngaA00', '3mwlB00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['6c8zA00', '4c38A00', '5e13A00', '2pyuA00', '5l3qB02']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b5716f0ed60>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.009273642786328, acc: 0.39582100153900796; test loss: 1.7641155360804976, acc: 0.46513826518553536
epoch: 2, train loss: 1.733660538845462, acc: 0.4670297146916065; test loss: 1.6283124072476782, acc: 0.4942094067596313
epoch: 3, train loss: 1.6494371264421461, acc: 0.4921865751154256; test loss: 1.549493165253973, acc: 0.5133538170645238
epoch: 4, train loss: 1.5866309634267726, acc: 0.5123712560672428; test loss: 1.6455640660986723, acc: 0.4956275112266604
epoch: 5, train loss: 1.544283276001095, acc: 0.5304250029596307; test loss: 1.4596020153188107, acc: 0.5518789884188136
epoch: 6, train loss: 1.496384077020209, acc: 0.5465845862436368; test loss: 1.412074889057045, acc: 0.5722051524462302
epoch: 7, train loss: 1.4675013624705886, acc: 0.5574168343790695; test loss: 1.4832674997181399, acc: 0.5476246750177263
epoch: 8, train loss: 1.4476255692150857, acc: 0.5665324967444063; test loss: 1.3761442322957547, acc: 0.5809501299929095
epoch: 9, train loss: 1.4122010731084595, acc: 0.5726293358588848; test loss: 1.413120057897854, acc: 0.5698416450011817
epoch: 10, train loss: 1.3821463853886413, acc: 0.5787853675861252; test loss: 1.5012506761327755, acc: 0.555660600330891
epoch: 11, train loss: 1.372430246187072, acc: 0.5818041908369835; test loss: 1.351524820510334, acc: 0.5875679508390451
epoch: 12, train loss: 1.3563915856762074, acc: 0.585474132828223; test loss: 1.4599698594602055, acc: 0.5497518317182699
epoch: 13, train loss: 1.3584183435602641, acc: 0.5852373623771753; test loss: 1.2933589145382705, acc: 0.5979673835972583
epoch: 14, train loss: 1.3180731543002078, acc: 0.5972534627678465; test loss: 1.3848277344767914, acc: 0.5826045852044434
epoch: 15, train loss: 1.3090366706405572, acc: 0.6032319166568012; test loss: 1.2684617358667296, acc: 0.6008035925313164
epoch: 16, train loss: 1.293796721952151, acc: 0.6061323546821357; test loss: 1.3959290895640104, acc: 0.5627511226660364
epoch: 17, train loss: 1.2780931526152244, acc: 0.612347579022138; test loss: 1.247906987646234, acc: 0.6133301819900733
epoch: 18, train loss: 1.255381874407595, acc: 0.6177932993962354; test loss: 1.227822691688186, acc: 0.621366107303238
epoch: 19, train loss: 1.227502355931764, acc: 0.6250147981531905; test loss: 1.2233644460454052, acc: 0.6310564878279367
epoch: 20, train loss: 1.2247540957266951, acc: 0.6273825026636676; test loss: 1.240274878152294, acc: 0.6220751595367525
epoch: 21, train loss: 1.2200960600123514, acc: 0.6281520066295726; test loss: 1.2422104354052361, acc: 0.6227842117702671
epoch: 22, train loss: 1.2082862063132758, acc: 0.6289215105954777; test loss: 1.2505524537716033, acc: 0.615220987946112
epoch: 23, train loss: 1.1949397258904186, acc: 0.6369125133183379; test loss: 1.2351752718242734, acc: 0.6220751595367525
epoch: 24, train loss: 1.1918898200062982, acc: 0.6356102758375755; test loss: 1.1915084085574925, acc: 0.6348380997400142
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.946954106676292, acc: 0.6437196637859595; test loss: 1.0203363907249845, acc: 0.6260931221933349
epoch: 26, train loss: 0.932886931124814, acc: 0.6454954421688174; test loss: 0.9674335151551549, acc: 0.6341290475064997
epoch: 27, train loss: 0.9252106271242269, acc: 0.6463833313602463; test loss: 0.92203157485109, acc: 0.6480737414322855
epoch: 28, train loss: 0.9171123514799757, acc: 0.6491061915472949; test loss: 0.9825076480420085, acc: 0.618293547624675
epoch: 29, train loss: 0.9166407285815402, acc: 0.6517698591215816; test loss: 0.9128402066101168, acc: 0.6414559205861499
epoch: 30, train loss: 0.8878542247489739, acc: 0.6632532259973956; test loss: 0.9427388478497528, acc: 0.6386197116520917
epoch: 31, train loss: 0.8853349485975391, acc: 0.6635491890612052; test loss: 1.0291406569596977, acc: 0.6109666745450248
epoch: 32, train loss: 0.8860156323707384, acc: 0.662069373742157; test loss: 0.9455053170672322, acc: 0.6315291893169463
epoch: 33, train loss: 0.8617479098722386, acc: 0.6679294424055878; test loss: 1.0400176692927483, acc: 0.6083668163554715
epoch: 34, train loss: 0.8470071777022208, acc: 0.6733159701669231; test loss: 0.9544038618746392, acc: 0.6206570550697235
epoch: 35, train loss: 0.8527966258694353, acc: 0.6722505031372085; test loss: 0.930447540957154, acc: 0.645710233987237
epoch: 36, train loss: 0.8463668929016264, acc: 0.6737303184562566; test loss: 0.8909517635664628, acc: 0.6570550697234696
epoch: 37, train loss: 0.8307092120636987, acc: 0.6779329939623535; test loss: 0.9534922888644676, acc: 0.6346017489955094
epoch: 38, train loss: 0.840299218706059, acc: 0.6756244820646383; test loss: 0.8941621487936887, acc: 0.6532734578113921
epoch: 39, train loss: 0.8174677733219459, acc: 0.684917722268261; test loss: 0.9279373860815631, acc: 0.6379106594185772
epoch: 40, train loss: 0.8181961259362655, acc: 0.6826092103705458; test loss: 0.9245937790777125, acc: 0.6400378161191208
epoch: 41, train loss: 0.8108949809578505, acc: 0.6866343080383568; test loss: 0.8250300970462952, acc: 0.6821082486409832
epoch: 42, train loss: 0.8020582065450181, acc: 0.6872262341659761; test loss: 0.8750641166783484, acc: 0.6617820846135666
epoch: 43, train loss: 0.811977976974798, acc: 0.6857464188469279; test loss: 0.8634136402699942, acc: 0.6546915622784212
epoch: 44, train loss: 0.787903895014854, acc: 0.6920208357996922; test loss: 0.8298539168106075, acc: 0.6669818009926731
epoch: 45, train loss: 0.7705186503983763, acc: 0.7008997277139813; test loss: 0.9487369294934057, acc: 0.6357835027180335
epoch: 46, train loss: 0.7774163220596833, acc: 0.6981176749141708; test loss: 1.1072188300518413, acc: 0.5894587567950839
epoch: 47, train loss: 0.7721642976232262, acc: 0.6956907777909317; test loss: 0.897053194113641, acc: 0.658000472701489
epoch: 48, train loss: 0.7594258638335443, acc: 0.7030898543861726; test loss: 0.9213234434396036, acc: 0.6301110848499173
epoch: 49, train loss: 0.7512596571065836, acc: 0.7035633952882681; test loss: 0.8938012870877833, acc: 0.6565823682344599
epoch: 50, train loss: 0.7567007507362831, acc: 0.7048656327690305; test loss: 0.9526007675273681, acc: 0.6376743086740724
epoch: 51, train loss: 0.755016496679099, acc: 0.6997750680715047; test loss: 0.8327244417911386, acc: 0.6714724651382652
epoch: 52, train loss: 0.7269892818944418, acc: 0.7127382502663667; test loss: 0.8874893436418361, acc: 0.6584731741904987
Epoch    52: reducing learning rate of group 0 to 1.5000e-03.
epoch: 53, train loss: 0.6799064333147871, acc: 0.7304960340949449; test loss: 0.7541098037923489, acc: 0.6979437485228078
epoch: 54, train loss: 0.648980407068768, acc: 0.7408547413282822; test loss: 0.7618522473864735, acc: 0.6948711888442448
epoch: 55, train loss: 0.6391996150964023, acc: 0.7427489049366639; test loss: 0.7610187455571534, acc: 0.7000709052233515
epoch: 56, train loss: 0.631551699143296, acc: 0.7463596543151415; test loss: 0.7633150492794152, acc: 0.7005436067123612
epoch: 57, train loss: 0.6254351383499158, acc: 0.7497928258553332; test loss: 0.7508492856914906, acc: 0.7033798156464193
epoch: 58, train loss: 0.6188681515431396, acc: 0.7498520184680951; test loss: 0.8108881707733558, acc: 0.6967619948002837
epoch: 59, train loss: 0.6197095682060166, acc: 0.7529300343317153; test loss: 0.7227450320974813, acc: 0.7137792484046325
epoch: 60, train loss: 0.6197143967679172, acc: 0.7493784775659997; test loss: 0.7634090120080683, acc: 0.7000709052233515
epoch: 61, train loss: 0.6143531649369833, acc: 0.7510358707233338; test loss: 0.7927495998796406, acc: 0.6991255022453321
epoch: 62, train loss: 0.6144167776430685, acc: 0.7498520184680951; test loss: 0.7760616750250582, acc: 0.6991255022453321
epoch: 63, train loss: 0.5968227828552388, acc: 0.7615721557949567; test loss: 0.7554452329683856, acc: 0.7102339872370598
epoch: 64, train loss: 0.594950364103709, acc: 0.7579614064164792; test loss: 0.7297166985164887, acc: 0.720633419995273
epoch: 65, train loss: 0.5910444006815992, acc: 0.7602107257014324; test loss: 0.7847668521653899, acc: 0.6896714724651383
epoch: 66, train loss: 0.5884210482086248, acc: 0.7663667574286729; test loss: 0.7394703623533418, acc: 0.7180335618057196
epoch: 67, train loss: 0.5735234125515435, acc: 0.7709837812241033; test loss: 0.7537146468332715, acc: 0.7095249350035453
epoch: 68, train loss: 0.5636468938190979, acc: 0.7698591215816266; test loss: 0.73606844327039, acc: 0.7130701961711179
epoch: 69, train loss: 0.572437562384492, acc: 0.766307564815911; test loss: 0.7192992251749384, acc: 0.7253604348853699
epoch: 70, train loss: 0.5621801404977277, acc: 0.7699775068071505; test loss: 0.7413462989016419, acc: 0.7123611439376034
epoch: 71, train loss: 0.5595230236975868, acc: 0.7721676334793418; test loss: 0.7532095960961028, acc: 0.7128338454266131
epoch: 72, train loss: 0.5596779010995007, acc: 0.7743577601515331; test loss: 0.8163644287552966, acc: 0.6939257858662254
epoch: 73, train loss: 0.5434244189288358, acc: 0.7795075174618208; test loss: 0.7731427097117696, acc: 0.7066887260694871
epoch: 74, train loss: 0.5413204175566193, acc: 0.7761927311471528; test loss: 0.7259254312999872, acc: 0.7199243677617585
epoch: 75, train loss: 0.534289933724334, acc: 0.7821119924233456; test loss: 0.8350932066166156, acc: 0.6927440321437013
epoch: 76, train loss: 0.544743584919021, acc: 0.7748904936663904; test loss: 0.7402479781013548, acc: 0.7196880170172536
epoch: 77, train loss: 0.5230410372143993, acc: 0.7860187048656327; test loss: 0.7328809573841388, acc: 0.7244150319073505
epoch: 78, train loss: 0.5272317158409557, acc: 0.7810465253936308; test loss: 0.7500717142912491, acc: 0.7147246513826518
epoch: 79, train loss: 0.5396887179491886, acc: 0.7782052799810584; test loss: 0.7599667792792468, acc: 0.7118884424485937
epoch: 80, train loss: 0.5141539883229723, acc: 0.7885047946016337; test loss: 0.8211371057339522, acc: 0.6967619948002837
epoch: 81, train loss: 0.5214454747169304, acc: 0.7860778974783947; test loss: 0.7607675676677166, acc: 0.7161427558496809
epoch: 82, train loss: 0.5157113194225723, acc: 0.7864330531549663; test loss: 0.7800980270740073, acc: 0.7095249350035453
epoch: 83, train loss: 0.5110849975177744, acc: 0.7905765360483011; test loss: 0.7504118747729214, acc: 0.7192153155282439
epoch: 84, train loss: 0.5087346109650157, acc: 0.7915828104652539; test loss: 0.7175338399576482, acc: 0.7291420467974474
epoch: 85, train loss: 0.4942288395143958, acc: 0.7963182194862081; test loss: 0.7310550328378446, acc: 0.7218151737177972
epoch: 86, train loss: 0.4930811081678328, acc: 0.7960814490351604; test loss: 0.7766595547206575, acc: 0.7163791065941858
epoch: 87, train loss: 0.48644167529883237, acc: 0.797265301290399; test loss: 0.7406665524870644, acc: 0.7215788229732923
epoch: 88, train loss: 0.4894542454812348, acc: 0.7952527524564934; test loss: 0.7428345260426272, acc: 0.7218151737177972
epoch: 89, train loss: 0.47467892396742173, acc: 0.8000473540902096; test loss: 0.7599961904170974, acc: 0.7213424722287876
epoch: 90, train loss: 0.47710161246787136, acc: 0.8027702142772581; test loss: 0.7708135532504474, acc: 0.709997636492555
epoch: 91, train loss: 0.47931971178614236, acc: 0.7983307683201136; test loss: 0.7700082985004109, acc: 0.7161427558496809
epoch: 92, train loss: 0.4687283681640544, acc: 0.8038356813069729; test loss: 0.7915812613862087, acc: 0.723705979673836
epoch: 93, train loss: 0.47248135002404257, acc: 0.8032437551793536; test loss: 0.7694708326132803, acc: 0.7201607185062633
epoch: 94, train loss: 0.46591719552809296, acc: 0.8051379187877353; test loss: 0.7677710422582588, acc: 0.7144883006381471
epoch: 95, train loss: 0.452897586017526, acc: 0.8092814016810702; test loss: 0.7959966948905247, acc: 0.7170881588277003
epoch: 96, train loss: 0.46083173609505296, acc: 0.8082159346513555; test loss: 0.7680544307738493, acc: 0.7232332781848263
epoch: 97, train loss: 0.4497830858523569, acc: 0.8098733278086895; test loss: 0.7510634879078659, acc: 0.7258331363743796
epoch: 98, train loss: 0.45154530888155847, acc: 0.8099917130342134; test loss: 0.7277393317679034, acc: 0.7291420467974474
epoch: 99, train loss: 0.4551744889130575, acc: 0.8075056232982124; test loss: 0.7939225851404726, acc: 0.7286693453084377
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.35114814154605295, acc: 0.8114715283532615; test loss: 0.642524059008718, acc: 0.7192153155282439
epoch: 101, train loss: 0.342587058443536, acc: 0.8141943885403101; test loss: 0.6590938754353335, acc: 0.7149610021271567
epoch: 102, train loss: 0.3422826700511452, acc: 0.8129513436723097; test loss: 0.7348793593176542, acc: 0.6929803828882061
epoch: 103, train loss: 0.3371655648419656, acc: 0.8125961879957382; test loss: 0.6100722961858552, acc: 0.7378870243441267
epoch: 104, train loss: 0.3357430408801696, acc: 0.8160885521486918; test loss: 0.6366019875297195, acc: 0.7057433230914677
epoch: 105, train loss: 0.3486844120299984, acc: 0.8093997869065941; test loss: 0.6432764680073971, acc: 0.7293783975419522
epoch: 106, train loss: 0.3311471104678338, acc: 0.8175091748549781; test loss: 0.6547820058113518, acc: 0.7168518080831955
epoch: 107, train loss: 0.33544910107192955, acc: 0.8136024624126909; test loss: 0.663686721439312, acc: 0.709997636492555
epoch: 108, train loss: 0.32454863272504125, acc: 0.8177459453060258; test loss: 0.655670969347051, acc: 0.7156700543606712
epoch: 109, train loss: 0.3498160736799607, acc: 0.8028294068900201; test loss: 0.6322941133187641, acc: 0.7177972110612149
epoch: 110, train loss: 0.343590177039049, acc: 0.8076832011364982; test loss: 0.6397797818218837, acc: 0.7125974946821082
epoch: 111, train loss: 0.3279956666283231, acc: 0.8180419083698355; test loss: 0.6294435676292054, acc: 0.7211061214842827
epoch: 112, train loss: 0.34393867515481774, acc: 0.8075056232982124; test loss: 0.599205407062847, acc: 0.7331600094540298
epoch: 113, train loss: 0.3190714287182079, acc: 0.8201728424292648; test loss: 0.6424217741136093, acc: 0.726778539352399
epoch: 114, train loss: 0.30670568695558914, acc: 0.826565644607553; test loss: 0.650595414951264, acc: 0.7244150319073505
Epoch   114: reducing learning rate of group 0 to 7.5000e-04.
epoch: 115, train loss: 0.2743018135962944, acc: 0.8397655972534628; test loss: 0.6230182879868762, acc: 0.7433230914677381
epoch: 116, train loss: 0.25081138516920365, acc: 0.8529063572866107; test loss: 0.627734113010045, acc: 0.7423776884897187
epoch: 117, train loss: 0.2470021705571273, acc: 0.8527879720610868; test loss: 0.6431201871645645, acc: 0.7336327109430395
epoch: 118, train loss: 0.24237708331593907, acc: 0.8532615129631822; test loss: 0.6610225707581353, acc: 0.7258331363743796
epoch: 119, train loss: 0.23736832121801021, acc: 0.8527879720610868; test loss: 0.6565828617795143, acc: 0.7423776884897187
epoch: 120, train loss: 0.23692114188838487, acc: 0.8559251805374689; test loss: 0.659180055101891, acc: 0.7371779721106122
epoch: 121, train loss: 0.22948523123680353, acc: 0.858352077660708; test loss: 0.6559680029168756, acc: 0.7359962183880879
epoch: 122, train loss: 0.2285442051378611, acc: 0.8573458032437552; test loss: 0.6637761316747988, acc: 0.738832427322146
epoch: 123, train loss: 0.22657485855067427, acc: 0.85995027820528; test loss: 0.6453442823461426, acc: 0.7400141810446703
epoch: 124, train loss: 0.22473671319669022, acc: 0.8587072333372795; test loss: 0.710757006372692, acc: 0.7319782557315055
epoch: 125, train loss: 0.22707228809753874, acc: 0.8579377293713745; test loss: 0.7188705254942214, acc: 0.7234696289293311
epoch: 126, train loss: 0.22691007091319673, acc: 0.8590031964010891; test loss: 0.6577255692895606, acc: 0.7383597258331364
epoch: 127, train loss: 0.22378188716359035, acc: 0.8584112702734699; test loss: 0.6797825971469054, acc: 0.7324509572205152
epoch: 128, train loss: 0.23115563738751133, acc: 0.8556884100864213; test loss: 0.6375743338076169, acc: 0.7454502481682818
epoch: 129, train loss: 0.22882886733178404, acc: 0.8585296554989937; test loss: 0.6721279668458611, acc: 0.7367052706216024
epoch: 130, train loss: 0.2357150419683898, acc: 0.8540310169290872; test loss: 0.6768404633067399, acc: 0.7317419049870008
epoch: 131, train loss: 0.22369490736572648, acc: 0.8597727003669942; test loss: 0.666064647122552, acc: 0.7430867407232333
epoch: 132, train loss: 0.22213990516220591, acc: 0.8595359299159465; test loss: 0.708168045478745, acc: 0.7336327109430395
epoch: 133, train loss: 0.20181715528478394, acc: 0.8688291701195691; test loss: 0.7138975492974202, acc: 0.7433230914677381
epoch: 134, train loss: 0.21054960204797962, acc: 0.8617852492008997; test loss: 0.7454064529687624, acc: 0.7265421886078941
epoch: 135, train loss: 0.20835504134801827, acc: 0.8694210962471883; test loss: 0.7110687648396322, acc: 0.7331600094540298
epoch: 136, train loss: 0.22232728341147365, acc: 0.858352077660708; test loss: 0.6786429477453626, acc: 0.7350508154100686
epoch: 137, train loss: 0.22943510296846603, acc: 0.8549189061205161; test loss: 0.6807075193153153, acc: 0.7407232332781848
epoch: 138, train loss: 0.2125547004724037, acc: 0.8629691014561383; test loss: 0.6965250912019193, acc: 0.7367052706216024
epoch: 139, train loss: 0.2097472724963098, acc: 0.8681780513791879; test loss: 0.6876378660364315, acc: 0.7395414795556606
epoch: 140, train loss: 0.20426171866693313, acc: 0.8668166212856635; test loss: 0.6894182337285554, acc: 0.7411959347671945
epoch: 141, train loss: 0.2029879110281716, acc: 0.8674085474132828; test loss: 0.7023787785015949, acc: 0.7376506735996219
epoch: 142, train loss: 0.19713521377354723, acc: 0.8706049485024269; test loss: 0.7028789385022691, acc: 0.7369416213661073
epoch: 143, train loss: 0.2972617688772593, acc: 0.8291701195690778; test loss: 0.7162765039954729, acc: 0.7033798156464193
epoch: 144, train loss: 0.25168471753322064, acc: 0.8435539244702261; test loss: 0.7001412876726916, acc: 0.7159064051051761
epoch: 145, train loss: 0.21853463882008176, acc: 0.8595359299159465; test loss: 0.6717038933646451, acc: 0.7449775466792721
epoch: 146, train loss: 0.190856514591867, acc: 0.8719071859831893; test loss: 0.6998043228508705, acc: 0.7433230914677381
epoch: 147, train loss: 0.20321434205088149, acc: 0.8690067479578548; test loss: 0.7395195805189655, acc: 0.7248877333963601
epoch: 148, train loss: 0.204454142122674, acc: 0.8659879247069966; test loss: 0.686022266263067, acc: 0.7341054124320492
epoch: 149, train loss: 0.1954098991547814, acc: 0.8712560672428081; test loss: 0.7038337377137085, acc: 0.7411959347671945
epoch: 150, train loss: 0.18984773857742848, acc: 0.8741565052681425; test loss: 0.6925197213683898, acc: 0.7456865989127865
epoch: 151, train loss: 0.19514740108949724, acc: 0.8720255712087132; test loss: 0.7442463109521994, acc: 0.7185062632947293
epoch: 152, train loss: 0.19620096627404537, acc: 0.8681780513791879; test loss: 0.7129706794616602, acc: 0.7423776884897187
epoch: 153, train loss: 0.192935642893602, acc: 0.8715520303066178; test loss: 0.7370306877232251, acc: 0.7265421886078941
epoch: 154, train loss: 0.19255766563441212, acc: 0.8711968746300461; test loss: 0.7336665711710566, acc: 0.7390687780666509
epoch: 155, train loss: 0.17822127181269706, acc: 0.8771161359062389; test loss: 0.7458498909016418, acc: 0.7359962183880879
epoch: 156, train loss: 0.18417309732483422, acc: 0.8729726530129039; test loss: 0.7254432359504744, acc: 0.7445048451902624
epoch: 157, train loss: 0.17800036875386202, acc: 0.8804309222209068; test loss: 0.8272172509849114, acc: 0.7090522335145356
epoch: 158, train loss: 0.1922240221505336, acc: 0.8718479933704274; test loss: 0.7468985104160765, acc: 0.7341054124320492
epoch: 159, train loss: 0.18393140406958292, acc: 0.8724399194980467; test loss: 0.7389090553972067, acc: 0.7359962183880879
epoch: 160, train loss: 0.18173764047678284, acc: 0.8764650171658577; test loss: 0.713947767823674, acc: 0.7409595840226897
epoch: 161, train loss: 0.19047720747769084, acc: 0.8743340831064284; test loss: 0.7917951905285036, acc: 0.7182699125502245
epoch: 162, train loss: 0.1896109274608702, acc: 0.8726174973363324; test loss: 0.7116184643723108, acc: 0.7442684944457575
epoch: 163, train loss: 0.1767639539147095, acc: 0.879424647803954; test loss: 0.734195926177589, acc: 0.737414322855117
epoch: 164, train loss: 0.1752779820486965, acc: 0.8776488694210962; test loss: 0.7608439701439161, acc: 0.7263058378633893
epoch: 165, train loss: 0.1710823829032986, acc: 0.8796614182550018; test loss: 0.7631084260803057, acc: 0.7253604348853699
Epoch   165: reducing learning rate of group 0 to 3.7500e-04.
epoch: 166, train loss: 0.1453227332862747, acc: 0.8927429856753877; test loss: 0.7088724045966484, acc: 0.7515953675254077
epoch: 167, train loss: 0.1266044754611256, acc: 0.9090801467976797; test loss: 0.7425403761486161, acc: 0.7478137556133302
epoch: 168, train loss: 0.12393440093968174, acc: 0.9050550491298686; test loss: 0.7647775895092341, acc: 0.7499409123138738
epoch: 169, train loss: 0.12138690146677901, acc: 0.9081330649934888; test loss: 0.7809773183438097, acc: 0.751122666036398
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.08973749636601763, acc: 0.9067124422872026; test loss: 0.7099076205183501, acc: 0.738832427322146
epoch: 171, train loss: 0.08224926996727844, acc: 0.9099088433763466; test loss: 0.6977015868114597, acc: 0.7426140392342235
epoch: 172, train loss: 0.08531456219648291, acc: 0.9075411388658695; test loss: 0.6949256465835679, acc: 0.7466320018908059
epoch: 173, train loss: 0.08538456457058777, acc: 0.9076595240913934; test loss: 0.6999889217181511, acc: 0.7466320018908059
epoch: 174, train loss: 0.07999604784911012, acc: 0.9105007695039659; test loss: 0.7023832102640107, acc: 0.7471047033798156
epoch: 175, train loss: 0.07997908996515363, acc: 0.9111518882443471; test loss: 0.6966884197621896, acc: 0.7428503899787284
epoch: 176, train loss: 0.08473796722567038, acc: 0.9070084053510122; test loss: 0.6987043037901418, acc: 0.7454502481682818
epoch: 177, train loss: 0.08467810855287765, acc: 0.9090209541849177; test loss: 0.6883539877949919, acc: 0.7414322855116994
epoch: 178, train loss: 0.08312423656018399, acc: 0.9106783473422517; test loss: 0.6960220760105632, acc: 0.7445048451902624
epoch: 179, train loss: 0.08139756311851802, acc: 0.9086657985083462; test loss: 0.7205485895265504, acc: 0.7411959347671945
epoch: 180, train loss: 0.08527660298692527, acc: 0.9067124422872026; test loss: 0.7210150129601792, acc: 0.734341763176554
epoch: 181, train loss: 0.08732392429966779, acc: 0.9042263525512017; test loss: 0.7051188487359689, acc: 0.7414322855116994
epoch: 182, train loss: 0.08380748407578516, acc: 0.9094353024742512; test loss: 0.7102147095932011, acc: 0.7468683526353108
epoch: 183, train loss: 0.08485632291877568, acc: 0.9052326269681543; test loss: 0.7257736540945167, acc: 0.7326873079650201
epoch: 184, train loss: 0.08948185213983616, acc: 0.9054693974192021; test loss: 0.6918142102509068, acc: 0.7504136138028835
epoch: 185, train loss: 0.08359787255663187, acc: 0.9086066058955843; test loss: 0.7224046142071422, acc: 0.7329236587095249
epoch: 186, train loss: 0.087918046862328, acc: 0.9054102048064402; test loss: 0.7254585418766538, acc: 0.7298510990309619
epoch: 187, train loss: 0.08302386481161864, acc: 0.9062980939978691; test loss: 0.7444943266821031, acc: 0.7317419049870008
epoch: 188, train loss: 0.08265355376325388, acc: 0.9042855451639635; test loss: 0.7340919936810563, acc: 0.7383597258331364
epoch: 189, train loss: 0.08554480467937649, acc: 0.9062980939978691; test loss: 0.7022834065055712, acc: 0.7419049870007091
epoch: 190, train loss: 0.07953148018522448, acc: 0.909672072925299; test loss: 0.7197856175592396, acc: 0.7423776884897187
epoch: 191, train loss: 0.07946045082219044, acc: 0.9084882206700604; test loss: 0.724076149734699, acc: 0.7421413377452138
epoch: 192, train loss: 0.08484502443744965, acc: 0.9055285900319641; test loss: 0.7519265950021944, acc: 0.737414322855117
epoch: 193, train loss: 0.07920569048960054, acc: 0.9074819462531076; test loss: 0.7203005435646525, acc: 0.7433230914677381
epoch: 194, train loss: 0.07731166649823698, acc: 0.9100864212146325; test loss: 0.7310377352346458, acc: 0.7397778303001654
epoch: 195, train loss: 0.08421579413658432, acc: 0.9075411388658695; test loss: 0.7185877708643781, acc: 0.7381233750886316
epoch: 196, train loss: 0.08470001845768645, acc: 0.9069492127382502; test loss: 0.7494381068023883, acc: 0.7381233750886316
epoch: 197, train loss: 0.08377803596777648, acc: 0.9053510121936782; test loss: 0.7090924910353983, acc: 0.7383597258331364
epoch: 198, train loss: 0.08640209195831759, acc: 0.9044039303894874; test loss: 0.7092392512512838, acc: 0.7423776884897187
epoch: 199, train loss: 0.08255075508971405, acc: 0.9074819462531076; test loss: 0.7271491894060431, acc: 0.7333963601985346
epoch: 200, train loss: 0.08391762867956257, acc: 0.9036344264235824; test loss: 0.716361708884486, acc: 0.7428503899787284
best test acc 0.7515953675254077 at epoch 166.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9721    0.9700    0.9710      6100
           1     0.9740    0.9309    0.9520       926
           2     0.8802    0.9796    0.9272      2400
           3     0.9585    0.9597    0.9591       843
           4     0.9301    0.9625    0.9460       774
           5     0.9448    0.9735    0.9590      1512
           6     0.8684    0.8782    0.8733      1330
           7     0.8602    0.9210    0.8896       481
           8     0.8943    0.9236    0.9087       458
           9     0.9229    0.9801    0.9506       452
          10     0.9562    0.9135    0.9344       717
          11     0.9284    0.9339    0.9311       333
          12     0.7188    0.0769    0.1390       299
          13     0.9399    0.6394    0.7611       269

    accuracy                         0.9349     16894
   macro avg     0.9106    0.8602    0.8644     16894
weighted avg     0.9328    0.9349    0.9283     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8227    0.8400    0.8313      1525
           1     0.8310    0.7629    0.7955       232
           2     0.7127    0.7554    0.7334       601
           3     0.7833    0.7536    0.7681       211
           4     0.8290    0.8247    0.8269       194
           5     0.7809    0.8201    0.8000       378
           6     0.5478    0.5856    0.5660       333
           7     0.5778    0.6446    0.6094       121
           8     0.5238    0.5739    0.5477       115
           9     0.8065    0.8772    0.8403       114
          10     0.7578    0.6778    0.7155       180
          11     0.5325    0.4881    0.5093        84
          12     0.1818    0.0267    0.0465        75
          13     0.8537    0.5147    0.6422        68

    accuracy                         0.7516      4231
   macro avg     0.6815    0.6532    0.6594      4231
weighted avg     0.7455    0.7516    0.7463      4231

---------------------------------------
program finished.
