seed:  666
save trained model at:  ../trained_models/trained_classifier_model_24.pt
save loss at:  ./results/train_classifier_results_24.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  14780
number of pockets in validation set:  3162
number of pockets in test set:  3183
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=80, out_features=80, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=80, out_features=80, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(80, 160)
  )
  (fc1): Linear(in_features=160, out_features=80, bias=True)
  (fc2): Linear(in_features=80, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2afc961359a0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0685520547007354, acc: 0.38626522327469553, val loss: 1.8433546695733356, acc: 0.45098039215686275, test loss: 1.830483488492759, acc: 0.450832547910776
epoch: 2, train loss: 1.774608961687359, acc: 0.4597428958051421, val loss: 1.7386341063302202, acc: 0.478494623655914, test loss: 1.7488797943883447, acc: 0.47093936537857367
epoch: 3, train loss: 1.6798019134627304, acc: 0.4901217861975643, val loss: 1.6114502651340068, acc: 0.49873497786211257, test loss: 1.6038812892900636, acc: 0.5108388312912346
epoch: 4, train loss: 1.616435482698784, acc: 0.5090663058186738, val loss: 1.577855476015056, acc: 0.5205566097406704, test loss: 1.5803827556318881, acc: 0.5108388312912346
epoch: 5, train loss: 1.5709093198079378, acc: 0.5209066305818674, val loss: 1.5569708133181162, acc: 0.5205566097406704, test loss: 1.5680927810824745, acc: 0.5237197612315426
epoch: 6, train loss: 1.5234043302652156, acc: 0.5338294993234101, val loss: 1.5060497345161317, acc: 0.5189753320683111, test loss: 1.4846744462449784, acc: 0.5271756204838203
epoch: 7, train loss: 1.4801980888407995, acc: 0.5495940460081191, val loss: 1.452021996783426, acc: 0.5509171410499684, test loss: 1.4487517339195277, acc: 0.5485391140433553
epoch: 8, train loss: 1.4734962942151804, acc: 0.5557510148849797, val loss: 1.4151299411476903, acc: 0.5667299177735611, test loss: 1.416058186499008, acc: 0.5639333961671379
epoch: 9, train loss: 1.4078557072214892, acc: 0.5756427604871448, val loss: 1.550341957057299, acc: 0.5230866540164453, test loss: 1.554831808781122, acc: 0.5337731699654414
epoch: 10, train loss: 1.403992689930215, acc: 0.5764546684709067, val loss: 1.3629949539240065, acc: 0.5676786843769766, test loss: 1.361956713673756, acc: 0.5717876217404964
epoch: 11, train loss: 1.3755462440005497, acc: 0.5832882273342355, val loss: 1.387256563199916, acc: 0.5667299177735611, test loss: 1.3844815597120712, acc: 0.5790135092679862
epoch: 12, train loss: 1.3481900639075872, acc: 0.5867388362652233, val loss: 1.4662396209591328, acc: 0.5493358633776091, test loss: 1.4609496445165953, acc: 0.5557650015708451
epoch: 13, train loss: 1.3429993621712608, acc: 0.5906630581867388, val loss: 1.3062202429786502, acc: 0.5964579380139152, test loss: 1.3160985970025239, acc: 0.5937794533459001
epoch: 14, train loss: 1.3063794390919727, acc: 0.604127198917456, val loss: 1.357350213251711, acc: 0.5822264389626819, test loss: 1.352779444156111, acc: 0.5799560163367892
epoch: 15, train loss: 1.283638042784189, acc: 0.6121109607577808, val loss: 1.4402486195223456, acc: 0.562618595825427, test loss: 1.4463551477407683, acc: 0.5739868049010367
epoch: 16, train loss: 1.2911788079348243, acc: 0.6121109607577808, val loss: 1.3003606910874466, acc: 0.5825426944971537, test loss: 1.3005333232160563, acc: 0.5878102419101476
epoch: 17, train loss: 1.2627939503796206, acc: 0.6213125845737483, val loss: 1.326647557313498, acc: 0.6005692599620494, test loss: 1.3223429038544852, acc: 0.6063462142632736
epoch: 18, train loss: 1.2165583544396903, acc: 0.6335588633288227, val loss: 1.361240779652315, acc: 0.592662871600253, test loss: 1.3752113859720136, acc: 0.5953502984605717
epoch: 19, train loss: 1.1999703754752835, acc: 0.6359269282814615, val loss: 1.366506257440228, acc: 0.5872865275142315, test loss: 1.3844361738929425, acc: 0.5871819038642789
epoch: 20, train loss: 1.2090724846345646, acc: 0.6394451962110961, val loss: 1.2219336400825265, acc: 0.622707147375079, test loss: 1.2223026419899354, acc: 0.626138862708137
epoch: 21, train loss: 1.1731965280837393, acc: 0.6494587280108255, val loss: 1.2220616099353383, acc: 0.6195445920303605, test loss: 1.2416573735553815, acc: 0.6151429469054351
epoch: 22, train loss: 1.1707898949092714, acc: 0.6501353179972936, val loss: 1.2259042588764169, acc: 0.6166982922201139, test loss: 1.272232114463492, acc: 0.6116870876531574
epoch: 23, train loss: 1.1647170338159647, acc: 0.6519621109607577, val loss: 1.290391409570548, acc: 0.6220746363061354, test loss: 1.297375758488973, acc: 0.6182846371347785
epoch: 24, train loss: 1.1240589217984789, acc: 0.6661028416779432, val loss: 1.2662033135946758, acc: 0.6290322580645161, test loss: 1.284057936427206, acc: 0.625196355639334
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9116639335197428, acc: 0.6602165087956698, val loss: 0.8960885924677695, acc: 0.6562302340290955, test loss: 0.8998530153729352, acc: 0.6550424128180962
epoch: 26, train loss: 0.8714036701335314, acc: 0.6748308525033829, val loss: 0.9920136749103505, acc: 0.6157495256166983, test loss: 1.0211068824368978, acc: 0.6163996229971724
epoch: 27, train loss: 0.8443603242201798, acc: 0.6820027063599459, val loss: 0.8964193808285968, acc: 0.6647691334598356, test loss: 0.8912859988444787, acc: 0.6638391454602576
epoch: 28, train loss: 0.8512294978830908, acc: 0.6807171853856563, val loss: 0.8929601088119111, acc: 0.6559139784946236, test loss: 0.891229982462987, acc: 0.6685516808042727
epoch: 29, train loss: 0.8391388265625227, acc: 0.6828822733423545, val loss: 0.9009899848778741, acc: 0.6543327008222644, test loss: 0.8939804360434352, acc: 0.6638391454602576
epoch: 30, train loss: 0.8222251493972758, acc: 0.6880920162381596, val loss: 1.0212284725425667, acc: 0.6135357368753953, test loss: 1.0160324647671541, acc: 0.6214263273641218
epoch: 31, train loss: 0.8409607139588693, acc: 0.6850473612990527, val loss: 0.8387139535719349, acc: 0.67235926628716, test loss: 0.8567523339092076, acc: 0.6833176248821866
epoch: 32, train loss: 0.8134311108692411, acc: 0.6924221921515562, val loss: 0.8810417520932048, acc: 0.6571790006325111, test loss: 0.9092133245938535, acc: 0.65315739868049
epoch: 33, train loss: 0.7959701264662736, acc: 0.6960757780784844, val loss: 0.9284611888357086, acc: 0.644212523719165, test loss: 0.9333643147752901, acc: 0.6380772855796418
epoch: 34, train loss: 0.7844224525078708, acc: 0.6989851150202977, val loss: 0.8952455992641365, acc: 0.6587602783048704, test loss: 0.9106096712029134, acc: 0.6641533144831919
epoch: 35, train loss: 0.7902402579865049, acc: 0.70148849797023, val loss: 0.8973215185789124, acc: 0.653067678684377, test loss: 0.9047738853356316, acc: 0.644674835061263
epoch: 36, train loss: 0.7793209863289767, acc: 0.7045331529093369, val loss: 1.0812007890782125, acc: 0.6135357368753953, test loss: 1.0750875599609768, acc: 0.6101162425384857
epoch: 37, train loss: 0.7877412933296699, acc: 0.6980378890392422, val loss: 0.8390044425575889, acc: 0.6736242884250474, test loss: 0.8854580994413366, acc: 0.6676091737354697
epoch: 38, train loss: 0.7503678789319463, acc: 0.7074424898511502, val loss: 0.8210629522611943, acc: 0.6752055660974067, test loss: 0.8243436491268444, acc: 0.6811184417216463
epoch: 39, train loss: 0.7389748317785612, acc: 0.7148849797023004, val loss: 0.9162117252615266, acc: 0.66382036685642, test loss: 0.9347708599019867, acc: 0.6625824693685203
epoch: 40, train loss: 0.7278627353527549, acc: 0.7207713125845737, val loss: 0.994453270928457, acc: 0.6230234029095509, test loss: 1.0259217916327454, acc: 0.626138862708137
epoch: 41, train loss: 0.7393885967373364, acc: 0.7161705006765899, val loss: 0.8415571661556468, acc: 0.6796331435800127, test loss: 0.8698126322212663, acc: 0.6713792020106818
epoch: 42, train loss: 0.7212922391775335, acc: 0.7198917456021651, val loss: 0.9048391391324665, acc: 0.6641366223908919, test loss: 0.9095168234303505, acc: 0.6710650329877474
epoch: 43, train loss: 0.7107248737299071, acc: 0.7240189445196211, val loss: 0.8205842440977343, acc: 0.6846932321315623, test loss: 0.8570047661526339, acc: 0.6842601319509897
epoch: 44, train loss: 0.7012883198116081, acc: 0.7301759133964817, val loss: 0.9592650450912177, acc: 0.6584440227703985, test loss: 0.9694488936911729, acc: 0.6399622997172478
epoch: 45, train loss: 0.7364983713997878, acc: 0.7228687415426251, val loss: 0.9038003855758947, acc: 0.6603415559772297, test loss: 0.9170099391601687, acc: 0.6569274269557022
epoch: 46, train loss: 0.732170856402272, acc: 0.7213125845737484, val loss: 0.8459966359147537, acc: 0.6780518659076534, test loss: 0.8565150162651597, acc: 0.671693371033616
epoch: 47, train loss: 0.7001118535279259, acc: 0.7309201623815967, val loss: 0.8571512491019898, acc: 0.6720430107526881, test loss: 0.9018921122549466, acc: 0.6597549481621112
epoch: 48, train loss: 0.6798055749139541, acc: 0.7380920162381597, val loss: 0.7679152653083101, acc: 0.7024035420619861, test loss: 0.7968750270402473, acc: 0.700282752120641
epoch: 49, train loss: 0.7451560381623502, acc: 0.7131935047361299, val loss: 0.911019319224252, acc: 0.6555977229601518, test loss: 0.9317736544026318, acc: 0.6550424128180962
epoch: 50, train loss: 0.7464097109795907, acc: 0.7164411366711773, val loss: 0.7928429025400295, acc: 0.7017710309930424, test loss: 0.8117129159280799, acc: 0.6939993716619541
epoch: 51, train loss: 0.6808623716698932, acc: 0.7364005412719892, val loss: 1.0160465220874204, acc: 0.6344086021505376, test loss: 1.044339690911032, acc: 0.6308513980521521
epoch: 52, train loss: 0.6654142309267563, acc: 0.7396481732070366, val loss: 0.8265573064561587, acc: 0.6796331435800127, test loss: 0.8638252874159266, acc: 0.6770342444234998
epoch: 53, train loss: 0.6487708805701085, acc: 0.7516238159675237, val loss: 0.7874051559431352, acc: 0.6973434535104365, test loss: 0.8176841282672265, acc: 0.685516808042727
epoch: 54, train loss: 0.6364914681979219, acc: 0.7521650879566982, val loss: 0.7663343981200573, acc: 0.715370018975332, test loss: 0.7973578605418156, acc: 0.7062519635563933
epoch: 55, train loss: 0.6503292476211414, acc: 0.7461434370771313, val loss: 0.8070187894397713, acc: 0.6989247311827957, test loss: 0.8682959620547377, acc: 0.6839459629280553
epoch: 56, train loss: 0.6471989983957417, acc: 0.7483761840324763, val loss: 0.7979846840640701, acc: 0.6976597090449083, test loss: 0.8348829304314319, acc: 0.685516808042727
epoch: 57, train loss: 0.6296802219583153, acc: 0.7546684709066306, val loss: 0.7442180412167013, acc: 0.7122074636306135, test loss: 0.7684532106952625, acc: 0.7125353440150801
epoch: 58, train loss: 0.6020031982571572, acc: 0.7656968876860623, val loss: 0.7864487419997039, acc: 0.7033523086654017, test loss: 0.8003034968440577, acc: 0.7071944706251964
epoch: 59, train loss: 0.6185535848060061, acc: 0.7598782138024357, val loss: 0.7424712552065189, acc: 0.7229601518026565, test loss: 0.7752500179613158, acc: 0.7172478793590952
epoch: 60, train loss: 0.6087711328421297, acc: 0.7606901217861975, val loss: 0.7502106526921928, acc: 0.704617330803289, test loss: 0.7733027735035112, acc: 0.6968268928683632
epoch: 61, train loss: 0.6166838092313568, acc: 0.756765899864682, val loss: 0.8017236394719336, acc: 0.7033523086654017, test loss: 0.8138016524121628, acc: 0.7056236255105247
epoch: 62, train loss: 0.5946856191258307, acc: 0.7641407307171854, val loss: 0.78784378037884, acc: 0.7144212523719166, test loss: 0.8048121788349685, acc: 0.7103361608545398
epoch: 63, train loss: 0.5867032178522608, acc: 0.7682679296346414, val loss: 0.7355246275456022, acc: 0.713472485768501, test loss: 0.7662569098303312, acc: 0.7012252591894439
epoch: 64, train loss: 0.5882886635273171, acc: 0.7699594046008119, val loss: 0.9137981045027461, acc: 0.6793168880455408, test loss: 0.922360931744637, acc: 0.6814326107445806
epoch: 65, train loss: 0.5871015280606137, acc: 0.7688768606224627, val loss: 0.8096682271348156, acc: 0.7039848197343453, test loss: 0.8381315834148477, acc: 0.6933710336160854
epoch: 66, train loss: 0.5713144688870814, acc: 0.7744925575101489, val loss: 0.830000567481459, acc: 0.7080961416824795, test loss: 0.8499618837977969, acc: 0.6990260760289035
epoch: 67, train loss: 0.5658629358702003, acc: 0.78234100135318, val loss: 0.7803900731959575, acc: 0.7024035420619861, test loss: 0.82551612795579, acc: 0.6971410618912975
epoch: 68, train loss: 0.5647704367789267, acc: 0.7742219215155616, val loss: 0.7364259690592969, acc: 0.7210626185958254, test loss: 0.7544243002402269, acc: 0.7207037386113729
epoch: 69, train loss: 0.5686511871133992, acc: 0.775169147496617, val loss: 0.7788791555456539, acc: 0.7049335863377609, test loss: 0.8222053547606617, acc: 0.6936852026390198
epoch: 70, train loss: 0.5608726633901686, acc: 0.7809878213802436, val loss: 0.7179633097736086, acc: 0.7400379506641366, test loss: 0.7541281461640794, acc: 0.7247879359095193
epoch: 71, train loss: 0.5867043190944501, acc: 0.771041948579161, val loss: 0.801911435045825, acc: 0.6998734977862112, test loss: 0.7844534522514092, acc: 0.7043669494187873
epoch: 72, train loss: 0.5691865831331245, acc: 0.7748985115020298, val loss: 0.9650658362278224, acc: 0.6815306767868438, test loss: 0.9721392529811493, acc: 0.6748350612629594
epoch: 73, train loss: 0.5646829407334489, acc: 0.777063599458728, val loss: 0.8979975620754755, acc: 0.6888045540796964, test loss: 0.9221811761205915, acc: 0.667923342758404
epoch: 74, train loss: 0.5510175697697032, acc: 0.7855209742895806, val loss: 0.7376754864494232, acc: 0.7156862745098039, test loss: 0.7642055359270226, acc: 0.7147345271756205
epoch: 75, train loss: 0.5541045280851434, acc: 0.7802435723951285, val loss: 0.7047106563705647, acc: 0.7352941176470589, test loss: 0.7413178698431573, acc: 0.7219604147031102
epoch: 76, train loss: 0.5224828627177278, acc: 0.7888362652232747, val loss: 0.7344791085413933, acc: 0.7299177735610373, test loss: 0.7616528789839054, acc: 0.7178762174049639
epoch: 77, train loss: 0.5386857065199515, acc: 0.7848443843031123, val loss: 0.7631780938360225, acc: 0.7122074636306135, test loss: 0.8034044874564604, acc: 0.7100219918316054
epoch: 78, train loss: 0.5174942327610372, acc: 0.7945196211096076, val loss: 0.7381407204193077, acc: 0.7371916508538899, test loss: 0.7347911826627194, acc: 0.7354696826892868
epoch: 79, train loss: 0.5255670617010662, acc: 0.7930311231393775, val loss: 0.7967622319329472, acc: 0.7043010752688172, test loss: 0.8172087404213347, acc: 0.7093936537857367
epoch: 80, train loss: 0.5175390111093431, acc: 0.796617050067659, val loss: 0.7609702296682281, acc: 0.7239089184060721, test loss: 0.8003511505983851, acc: 0.7065661325793277
epoch: 81, train loss: 0.5180300374153664, acc: 0.7966847090663058, val loss: 0.7079676530694449, acc: 0.736875395319418, test loss: 0.7320560360344334, acc: 0.7316996544140748
Epoch    81: reducing learning rate of group 0 to 1.5000e-03.
epoch: 82, train loss: 0.45239753394875376, acc: 0.8170500676589987, val loss: 0.7060467486589638, acc: 0.7539531941808981, test loss: 0.7391920261251226, acc: 0.7505497957901351
epoch: 83, train loss: 0.41134123768309616, acc: 0.8351826792963464, val loss: 0.6775598995750423, acc: 0.7552182163187856, test loss: 0.6990449695215636, acc: 0.7499214577442664
epoch: 84, train loss: 0.3820575299137177, acc: 0.8447225981055481, val loss: 0.7134445627455015, acc: 0.7571157495256167, test loss: 0.7517109830312821, acc: 0.7499214577442664
epoch: 85, train loss: 0.3724329861478328, acc: 0.8494587280108254, val loss: 0.7524811349889283, acc: 0.7425679949399114, test loss: 0.7868915159223666, acc: 0.7392397109644989
epoch: 86, train loss: 0.3884586043141692, acc: 0.8418132611637348, val loss: 0.71870477360913, acc: 0.7501581277672359, test loss: 0.7362021458362432, acc: 0.7404963870562362
epoch: 87, train loss: 0.40264775763506494, acc: 0.8339648173207036, val loss: 0.6936240015265159, acc: 0.7558507273877293, test loss: 0.707252675543931, acc: 0.7530631479736098
epoch: 88, train loss: 0.3717810549900561, acc: 0.85, val loss: 0.7771086088092474, acc: 0.7324478178368121, test loss: 0.7840151913391507, acc: 0.7304429783223374
epoch: 89, train loss: 0.3759781318039952, acc: 0.8472259810554804, val loss: 0.8011820720465104, acc: 0.7305502846299811, test loss: 0.8202282535254563, acc: 0.7351555136663525
epoch: 90, train loss: 0.38317262018968995, acc: 0.8455345060893099, val loss: 0.7678173140285137, acc: 0.7441492726122707, test loss: 0.7727177340842777, acc: 0.7408105560791706
epoch: 91, train loss: 0.41198293065831204, acc: 0.8354533152909337, val loss: 0.8251875327554856, acc: 0.7229601518026565, test loss: 0.8159176522817321, acc: 0.7216462456801759
epoch: 92, train loss: 0.3773562961486099, acc: 0.8452638700947226, val loss: 0.7043344442185384, acc: 0.7609108159392789, test loss: 0.7355734037596439, acc: 0.7602890355010996
epoch: 93, train loss: 0.3440788451728382, acc: 0.859404600811908, val loss: 0.7581791770676884, acc: 0.7571157495256167, test loss: 0.7760432925270745, acc: 0.7477222745837261
epoch: 94, train loss: 0.34948643931516615, acc: 0.8565629228687416, val loss: 0.764803805960499, acc: 0.7523719165085389, test loss: 0.7799455443156653, acc: 0.7470939365378574
epoch: 95, train loss: 0.33889912297503066, acc: 0.8587280108254398, val loss: 0.7548374908020193, acc: 0.7530044275774826, test loss: 0.7602940677585296, acc: 0.7492931196983977
epoch: 96, train loss: 0.33454966399117964, acc: 0.8606901217861975, val loss: 0.7902680574377302, acc: 0.7251739405439596, test loss: 0.8527232958691472, acc: 0.7093936537857367
epoch: 97, train loss: 0.33929571784397583, acc: 0.8616373477672531, val loss: 0.7425904166917119, acc: 0.7549019607843137, test loss: 0.7639217923506229, acc: 0.7511781338360037
epoch: 98, train loss: 0.3464647241471101, acc: 0.8598782138024357, val loss: 0.8089523929195416, acc: 0.7378241619228336, test loss: 0.8163861773727901, acc: 0.7320138234370092
epoch: 99, train loss: 0.3344072977406085, acc: 0.8652909336941813, val loss: 0.7415721761810863, acc: 0.7507906388361796, test loss: 0.7510015420706962, acc: 0.7562048382029531
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.2642515882022971, acc: 0.8647496617050068, val loss: 0.6421551063496579, acc: 0.7536369386464263, test loss: 0.6415618247967863, acc: 0.7499214577442664
epoch: 101, train loss: 0.2301331565288149, acc: 0.8750338294993234, val loss: 0.6459579654768401, acc: 0.7599620493358634, test loss: 0.67324452590463, acc: 0.7489789506754634
epoch: 102, train loss: 0.2268888838316978, acc: 0.8811231393775372, val loss: 0.6241809621330758, acc: 0.7621758380771664, test loss: 0.6206328270385147, acc: 0.7640590637763116
epoch: 103, train loss: 0.2237338717396753, acc: 0.8771989174560216, val loss: 0.729431483807404, acc: 0.7441492726122707, test loss: 0.7330002315082604, acc: 0.7442664153314483
epoch: 104, train loss: 0.23897183914145856, acc: 0.8719891745602165, val loss: 0.6655683344787318, acc: 0.7504743833017078, test loss: 0.6709010152370346, acc: 0.744894753377317
epoch: 105, train loss: 0.23753774896042273, acc: 0.8706359945872801, val loss: 0.6595238363494005, acc: 0.7567994939911449, test loss: 0.6650585994906221, acc: 0.7596606974552309
epoch: 106, train loss: 0.23135473423300965, acc: 0.8736129905277402, val loss: 0.6430653767220813, acc: 0.7488931056293485, test loss: 0.6506845437989758, acc: 0.7533773169965441
epoch: 107, train loss: 0.2231068011549716, acc: 0.8753044654939107, val loss: 0.6566051264340028, acc: 0.7444655281467426, test loss: 0.6810200395653018, acc: 0.7474081055607917
epoch: 108, train loss: 0.23629352137829518, acc: 0.8692151556156968, val loss: 0.6078615582946884, acc: 0.7634408602150538, test loss: 0.6234802081080828, acc: 0.7483506126295947
epoch: 109, train loss: 0.22898102209113125, acc: 0.8717185385656292, val loss: 0.6973911313449636, acc: 0.7460468058191019, test loss: 0.6922803544938246, acc: 0.7430097392397109
epoch: 110, train loss: 0.2152258073202812, acc: 0.8767929634641407, val loss: 0.6617266916158002, acc: 0.7450980392156863, test loss: 0.6683136169317943, acc: 0.7367263587810242
epoch: 111, train loss: 0.22894566474489977, acc: 0.8740189445196211, val loss: 0.6573310362093815, acc: 0.7460468058191019, test loss: 0.7241537047376282, acc: 0.7320138234370092
epoch: 112, train loss: 0.24241598192829242, acc: 0.8663734776725305, val loss: 0.6711100087295523, acc: 0.75426944971537, test loss: 0.7026254457383271, acc: 0.7423814011938423
epoch: 113, train loss: 0.22246273258380864, acc: 0.8765899864682003, val loss: 0.6479217495667942, acc: 0.7609108159392789, test loss: 0.6656197313239205, acc: 0.7511781338360037
epoch: 114, train loss: 0.232694871499832, acc: 0.8668470906630582, val loss: 0.8025569200968155, acc: 0.7172675521821632, test loss: 0.832359094459457, acc: 0.7093936537857367
epoch: 115, train loss: 0.22872917724720357, acc: 0.8726657645466848, val loss: 0.6657362263237352, acc: 0.7571157495256167, test loss: 0.6823836163159178, acc: 0.7436380772855796
epoch: 116, train loss: 0.2146510721462828, acc: 0.8778078484438431, val loss: 0.665515402387321, acc: 0.7482605945604048, test loss: 0.6734393257559975, acc: 0.7433239082626453
epoch: 117, train loss: 0.22471063296349672, acc: 0.8738836265223274, val loss: 0.7023211244840097, acc: 0.7362428842504743, test loss: 0.6897156129196459, acc: 0.7335846685516808
epoch: 118, train loss: 0.21573406178428614, acc: 0.878213802435724, val loss: 0.6660096328670205, acc: 0.756483238456673, test loss: 0.6830333715109865, acc: 0.7508639648130694
epoch: 119, train loss: 0.22125869138476328, acc: 0.8752368064952639, val loss: 0.6421310865449272, acc: 0.7447817836812144, test loss: 0.6917931930920705, acc: 0.7338988375746152
epoch: 120, train loss: 0.20723642847212145, acc: 0.88085250338295, val loss: 0.7262184141859988, acc: 0.7413029728020241, test loss: 0.7569356800586294, acc: 0.7276154571159283
epoch: 121, train loss: 0.21936476204811478, acc: 0.8741542625169147, val loss: 0.708416347937973, acc: 0.7226438962681847, test loss: 0.7096375371164172, acc: 0.7194470625196355
epoch: 122, train loss: 0.20401171293407075, acc: 0.8818673883626522, val loss: 0.6939063486911163, acc: 0.7432005060088551, test loss: 0.6946578298841974, acc: 0.7477222745837261
epoch: 123, train loss: 0.20396482494429097, acc: 0.8807171853856562, val loss: 0.797752448835379, acc: 0.7147375079063883, test loss: 0.7645413241145224, acc: 0.6987119070059692
epoch: 124, train loss: 0.21168963516758646, acc: 0.8747631935047361, val loss: 0.7347646477401898, acc: 0.7362428842504743, test loss: 0.7850900348012566, acc: 0.7244737668865849
epoch: 125, train loss: 0.230192351659838, acc: 0.8654939106901218, val loss: 0.6802301390272089, acc: 0.7533206831119544, test loss: 0.7259908716820187, acc: 0.7414388941250393
epoch: 126, train loss: 0.20481577872907358, acc: 0.8805818673883626, val loss: 0.6478257446934497, acc: 0.7599620493358634, test loss: 0.6632373995500805, acc: 0.7474081055607917
epoch: 127, train loss: 0.1941342632930237, acc: 0.8816644113667118, val loss: 0.7408558273526553, acc: 0.7466793168880456, test loss: 0.8080865052598024, acc: 0.7360980207351555
epoch: 128, train loss: 0.19075648289814048, acc: 0.8886332882273342, val loss: 0.6901185439403565, acc: 0.7473118279569892, test loss: 0.723422502257934, acc: 0.7433239082626453
epoch: 129, train loss: 0.17450754367853533, acc: 0.8924221921515562, val loss: 0.7384455154245221, acc: 0.7545857052498419, test loss: 0.7599922944793078, acc: 0.7395538799874333
epoch: 130, train loss: 0.21199895604456875, acc: 0.8820027063599458, val loss: 0.7197735339761309, acc: 0.7359266287160026, test loss: 0.7385124698810296, acc: 0.7338988375746152
epoch: 131, train loss: 0.2201533113829338, acc: 0.8690121786197564, val loss: 0.6600087783820715, acc: 0.7561669829222012, test loss: 0.7057033832291182, acc: 0.7445805843543827
epoch: 132, train loss: 0.21514723302950234, acc: 0.873680649526387, val loss: 0.6880260970606071, acc: 0.7438330170777988, test loss: 0.7006802547288847, acc: 0.7357838517122212
Epoch   132: reducing learning rate of group 0 to 7.5000e-04.
epoch: 133, train loss: 0.1643871641247779, acc: 0.8984438430311231, val loss: 0.6837046309259404, acc: 0.7590132827324478, test loss: 0.6656933323302585, acc: 0.7725416273955388
epoch: 134, train loss: 0.12915935001168424, acc: 0.9146820027063599, val loss: 0.686515499865383, acc: 0.7694497153700189, test loss: 0.6904815367621668, acc: 0.7558906691800189
epoch: 135, train loss: 0.12165551233194839, acc: 0.9194181326116373, val loss: 0.7535177431552943, acc: 0.7647058823529411, test loss: 0.7384004902247921, acc: 0.7631165567075087
epoch: 136, train loss: 0.11815708180403033, acc: 0.924424898511502, val loss: 0.7201648251607703, acc: 0.7722960151802657, test loss: 0.7426341322657075, acc: 0.763430725730443
epoch: 137, train loss: 0.11982153489963934, acc: 0.9227334235453315, val loss: 0.7552181245555311, acc: 0.7640733712839974, test loss: 0.7745969457668289, acc: 0.7621740496387056
epoch: 138, train loss: 0.10781856667205025, acc: 0.9265223274695534, val loss: 0.7730758660658487, acc: 0.7643896268184693, test loss: 0.7806370390161868, acc: 0.7540056550424128
epoch: 139, train loss: 0.10703133898352415, acc: 0.9276725304465494, val loss: 0.7938489152388358, acc: 0.7729285262492094, test loss: 0.7706175642945352, acc: 0.7659440779139177
epoch: 140, train loss: 0.11191916223791842, acc: 0.9269282814614344, val loss: 0.773218729615136, acc: 0.7599620493358634, test loss: 0.8019196219388847, acc: 0.7496072887213321
epoch: 141, train loss: 0.11414610358794423, acc: 0.9256427604871448, val loss: 0.7672499578863212, acc: 0.7593295382669196, test loss: 0.7571007151518309, acc: 0.7558906691800189
epoch: 142, train loss: 0.11332631032303964, acc: 0.9227334235453315, val loss: 0.7726335478763954, acc: 0.7567994939911449, test loss: 0.7566938741455054, acc: 0.7609173735469683
epoch: 143, train loss: 0.10997580325530895, acc: 0.9277401894451962, val loss: 0.7727429504111023, acc: 0.7647058823529411, test loss: 0.8066342186336056, acc: 0.763430725730443
epoch: 144, train loss: 0.11926925409388639, acc: 0.9211096075778078, val loss: 0.7925122741805384, acc: 0.7555344718532574, test loss: 0.7925921219757732, acc: 0.7590323594093622
epoch: 145, train loss: 0.11176569670317783, acc: 0.9274018944519621, val loss: 0.7868596555913724, acc: 0.7669196710942442, test loss: 0.802238998756145, acc: 0.7552623311341502
epoch: 146, train loss: 0.10609561449297387, acc: 0.9272665764546685, val loss: 0.8171899820867028, acc: 0.7530044275774826, test loss: 0.7741745572175539, acc: 0.7596606974552309
epoch: 147, train loss: 0.10585563662329611, acc: 0.9321380243572395, val loss: 0.8093060176633416, acc: 0.767235926628716, test loss: 0.8463128515657599, acc: 0.7574615142946906
epoch: 148, train loss: 0.1049618671243665, acc: 0.930040595399188, val loss: 0.8635639854807555, acc: 0.7507906388361796, test loss: 0.9017923264768638, acc: 0.7367263587810242
epoch: 149, train loss: 0.12202459471716126, acc: 0.9234776725304465, val loss: 0.7437665142792829, acc: 0.7662871600253004, test loss: 0.7636221520675266, acc: 0.7574615142946906
epoch: 150, train loss: 0.10917631342543317, acc: 0.9264546684709066, val loss: 0.8008163601141199, acc: 0.7624920936116382, test loss: 0.8136135099670477, acc: 0.7543198240653471
epoch: 151, train loss: 0.10834237240728732, acc: 0.9295669824086603, val loss: 0.8184497172435878, acc: 0.7580645161290323, test loss: 0.8510137452968366, acc: 0.7505497957901351
epoch: 152, train loss: 0.12805206265517274, acc: 0.9193504736129905, val loss: 0.7365996876824589, acc: 0.767235926628716, test loss: 0.7661618357591572, acc: 0.7540056550424128
epoch: 153, train loss: 0.102348001129895, acc: 0.9312584573748308, val loss: 0.7888597844900171, acc: 0.7666034155597723, test loss: 0.8129666020500933, acc: 0.7499214577442664
epoch: 154, train loss: 0.08724956901413823, acc: 0.9399864682002707, val loss: 0.804755780004686, acc: 0.7647058823529411, test loss: 0.7872450304151218, acc: 0.7621740496387056
epoch: 155, train loss: 0.09070349312078646, acc: 0.9374830852503383, val loss: 0.8236424849049944, acc: 0.7685009487666035, test loss: 0.8227773778962145, acc: 0.7568331762488218
epoch: 156, train loss: 0.1065891704112493, acc: 0.9283491204330175, val loss: 0.7775329689976537, acc: 0.7609108159392789, test loss: 0.7795689071531832, acc: 0.7549481621112158
epoch: 157, train loss: 0.10249727846643761, acc: 0.9296346414073072, val loss: 0.7803960899097266, acc: 0.7574320050600886, test loss: 0.8007918157706349, acc: 0.7565190072258875
epoch: 158, train loss: 0.0941267376904236, acc: 0.934573748308525, val loss: 0.8009945926298302, acc: 0.7634408602150538, test loss: 0.8540933637022785, acc: 0.7524348099277411
epoch: 159, train loss: 0.09256778297106533, acc: 0.9369418132611638, val loss: 0.8299429204025124, acc: 0.7612270714737508, test loss: 0.8270531014155712, acc: 0.760603204524034
epoch: 160, train loss: 0.09525542906882475, acc: 0.9360622462787551, val loss: 0.8459958248994985, acc: 0.7697659709044908, test loss: 0.8552819597545076, acc: 0.746779767514923
epoch: 161, train loss: 0.12231298227352122, acc: 0.9192151556156969, val loss: 0.818087574652974, acc: 0.7318153067678684, test loss: 0.8318264256698172, acc: 0.7260446120012567
epoch: 162, train loss: 0.1163766950489865, acc: 0.925575101488498, val loss: 0.8197415534036964, acc: 0.7691334598355472, test loss: 0.864289826991758, acc: 0.7540056550424128
epoch: 163, train loss: 0.11262610455488481, acc: 0.9262516914749662, val loss: 0.7605119185836763, acc: 0.7691334598355472, test loss: 0.8041901104561833, acc: 0.7593465284322966
epoch: 164, train loss: 0.08583099148229914, acc: 0.9399188092016239, val loss: 0.8602088771237368, acc: 0.765022137887413, test loss: 0.8740244749541407, acc: 0.7536914860194784
epoch: 165, train loss: 0.08843389312170839, acc: 0.9408660351826793, val loss: 0.8032083767716602, acc: 0.7643896268184693, test loss: 0.8330141450862183, acc: 0.7593465284322966
epoch: 166, train loss: 0.09400571484285053, acc: 0.937212449255751, val loss: 0.8205056941486627, acc: 0.7634408602150538, test loss: 0.8562118726970638, acc: 0.7558906691800189
epoch: 167, train loss: 0.14191003759594506, acc: 0.9152909336941814, val loss: 0.7875156049710297, acc: 0.7469955724225174, test loss: 0.7995180189703157, acc: 0.7401822180333019
epoch: 168, train loss: 0.1169868390558537, acc: 0.9263870094722598, val loss: 0.7998322172303656, acc: 0.7577482605945604, test loss: 0.8122618820844789, acc: 0.7511781338360037
epoch: 169, train loss: 0.08556563176103793, acc: 0.9401894451962111, val loss: 0.8397307300929553, acc: 0.7666034155597723, test loss: 0.8497888997332914, acc: 0.7546339930882815
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.06546524476980808, acc: 0.935723951285521, val loss: 0.7401318298269871, acc: 0.7675521821631879, test loss: 0.7530913115895996, acc: 0.7618598806157713
epoch: 171, train loss: 0.07615230869053181, acc: 0.9353179972936401, val loss: 0.7325387700006677, acc: 0.7378241619228336, test loss: 0.7343734372834383, acc: 0.7338988375746152
epoch: 172, train loss: 0.07155138038814632, acc: 0.9347767253044655, val loss: 0.7515908091374397, acc: 0.7612270714737508, test loss: 0.7289178826469616, acc: 0.7571473452717562
epoch: 173, train loss: 0.06305376557035923, acc: 0.9376860622462787, val loss: 0.7185020159498791, acc: 0.7577482605945604, test loss: 0.7249790384377693, acc: 0.7530631479736098
epoch: 174, train loss: 0.06189297248328329, acc: 0.9390392422192152, val loss: 0.7278396740961045, acc: 0.7685009487666035, test loss: 0.7150871415502277, acc: 0.7546339930882815
epoch: 175, train loss: 0.055606604906880325, acc: 0.9420162381596753, val loss: 0.7711129481094234, acc: 0.7536369386464263, test loss: 0.8004680597815842, acc: 0.7496072887213321
epoch: 176, train loss: 0.051378441947803444, acc: 0.9453315290933694, val loss: 0.7464930870953728, acc: 0.7678684376976597, test loss: 0.759812987041743, acc: 0.7646874018221803
epoch: 177, train loss: 0.046375643075802976, acc: 0.9514208389715832, val loss: 0.7755839961906689, acc: 0.7647058823529411, test loss: 0.7585282757564644, acc: 0.7540056550424128
epoch: 178, train loss: 0.061374211558348754, acc: 0.938362652232747, val loss: 0.7033523477232207, acc: 0.7666034155597723, test loss: 0.7356544191118984, acc: 0.7555765001570846
epoch: 179, train loss: 0.06057346509177243, acc: 0.9393775372124492, val loss: 0.7672808369377758, acc: 0.7498418722327641, test loss: 0.775654624626766, acc: 0.7477222745837261
epoch: 180, train loss: 0.08562967949452355, acc: 0.9232070365358592, val loss: 0.77152928342403, acc: 0.7447817836812144, test loss: 0.7491814943742647, acc: 0.7376688658498272
epoch: 181, train loss: 0.08973644710522705, acc: 0.9222598105548038, val loss: 0.6753813392523343, acc: 0.7602783048703352, test loss: 0.6881139810602881, acc: 0.7496072887213321
epoch: 182, train loss: 0.06504251425417899, acc: 0.9359269282814614, val loss: 0.7219125330259648, acc: 0.7599620493358634, test loss: 0.7312440692048907, acc: 0.7533773169965441
epoch: 183, train loss: 0.055089705613856385, acc: 0.9426251691474966, val loss: 0.7412381893015602, acc: 0.7593295382669196, test loss: 0.7553597910015455, acc: 0.7452089224002514
Epoch   183: reducing learning rate of group 0 to 3.7500e-04.
epoch: 184, train loss: 0.0499069709839937, acc: 0.9453991880920163, val loss: 0.7083128578371823, acc: 0.7675521821631879, test loss: 0.7311866387907454, acc: 0.7577756833176249
epoch: 185, train loss: 0.03868341367737124, acc: 0.9571718538565629, val loss: 0.7322509021867611, acc: 0.7760910815939279, test loss: 0.7457309579309936, acc: 0.76248821866164
epoch: 186, train loss: 0.03516807762429582, acc: 0.9585926928281462, val loss: 0.7586345260162884, acc: 0.7691334598355472, test loss: 0.7545193376460242, acc: 0.7587181903864278
epoch: 187, train loss: 0.0332164313094703, acc: 0.9619079837618403, val loss: 0.7611124883491279, acc: 0.775774826059456, test loss: 0.7859198576693186, acc: 0.7640590637763116
epoch: 188, train loss: 0.034448641731185746, acc: 0.9582543978349121, val loss: 0.7849900434471097, acc: 0.7710309930423782, test loss: 0.7874456384142278, acc: 0.7590323594093622
epoch: 189, train loss: 0.032285760647046385, acc: 0.962043301759134, val loss: 0.7777554032773929, acc: 0.7710309930423782, test loss: 0.7714559818789751, acc: 0.7628023876845743
epoch: 190, train loss: 0.0337511106841619, acc: 0.9611637347767253, val loss: 0.7863401896134726, acc: 0.7691334598355472, test loss: 0.7800038612903831, acc: 0.7665724159597863
epoch: 191, train loss: 0.03193158466212643, acc: 0.9623139377537212, val loss: 0.8023087195698632, acc: 0.7685009487666035, test loss: 0.8061309425552948, acc: 0.7628023876845743
epoch: 192, train loss: 0.03223632434345227, acc: 0.9636671177266577, val loss: 0.7799920786188645, acc: 0.7707147375079064, test loss: 0.7841287821746644, acc: 0.7687715991203268
epoch: 193, train loss: 0.0298703018903571, acc: 0.9632611637347768, val loss: 0.8218629178482094, acc: 0.7647058823529411, test loss: 0.8203325019929906, acc: 0.7628023876845743
epoch: 194, train loss: 0.031190642973607224, acc: 0.963125845737483, val loss: 0.8011358701903786, acc: 0.7653383934218849, test loss: 0.8032879622297919, acc: 0.7628023876845743
epoch: 195, train loss: 0.03412717043224304, acc: 0.9605548037889039, val loss: 0.7821654403307707, acc: 0.7729285262492094, test loss: 0.7893447040050913, acc: 0.765315739868049
epoch: 196, train loss: 0.03715750491082749, acc: 0.9577807848443843, val loss: 0.8177383636689352, acc: 0.7691334598355472, test loss: 0.8029723100380594, acc: 0.7725416273955388
epoch: 197, train loss: 0.03068572492798707, acc: 0.9626522327469553, val loss: 0.8166879310704424, acc: 0.7707147375079064, test loss: 0.8018989834139791, acc: 0.7637448947533774
epoch: 198, train loss: 0.030413940147634128, acc: 0.9633964817320704, val loss: 0.8265173798644943, acc: 0.7722960151802657, test loss: 0.8369752134574198, acc: 0.7640590637763116
epoch: 199, train loss: 0.03037677588538646, acc: 0.9630581867388363, val loss: 0.8289682978998627, acc: 0.7707147375079064, test loss: 0.8209425212825352, acc: 0.7646874018221803
epoch: 200, train loss: 0.02693698425274418, acc: 0.9661705006765899, val loss: 0.8431302183116259, acc: 0.7662871600253004, test loss: 0.8244640656023927, acc: 0.7690857681432611
best val acc 0.7760910815939279 at epoch 185.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9957    0.9978    0.9967      5337
           1     0.9947    0.9333    0.9631       810
           2     0.9540    0.9971    0.9751      2100
           3     0.9747    0.9919    0.9832       737
           4     0.9384    0.9897    0.9633       677
           5     0.9799    0.9962    0.9880      1323
           6     0.9628    0.9553    0.9590      1164
           7     0.9881    0.9881    0.9881       421
           8     0.9851    0.9900    0.9876       401
           9     0.9610    0.9949    0.9777       396
          10     0.9886    0.9649    0.9766       627
          11     0.9896    0.9828    0.9862       291
          12     0.9341    0.5977    0.7290       261
          13     0.9621    0.8638    0.9103       235

    accuracy                         0.9786     14780
   macro avg     0.9721    0.9460    0.9560     14780
weighted avg     0.9786    0.9786    0.9777     14780

train confusion matrix:
[[9.97751546e-01 0.00000000e+00 1.87371182e-04 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.74742365e-04 9.36855912e-04
  3.74742365e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.74742365e-04 0.00000000e+00]
 [0.00000000e+00 9.33333333e-01 0.00000000e+00 0.00000000e+00
  5.43209877e-02 6.17283951e-03 0.00000000e+00 0.00000000e+00
  3.70370370e-03 0.00000000e+00 2.46913580e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 9.97142857e-01 0.00000000e+00
  0.00000000e+00 9.52380952e-04 0.00000000e+00 0.00000000e+00
  0.00000000e+00 4.76190476e-04 0.00000000e+00 0.00000000e+00
  1.42857143e-03 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 9.91858887e-01
  0.00000000e+00 0.00000000e+00 2.71370421e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 5.42740841e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 4.43131462e-03 0.00000000e+00 0.00000000e+00
  9.89660266e-01 5.90841950e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.51171580e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.96220711e-01 0.00000000e+00 0.00000000e+00
  7.55857899e-04 0.00000000e+00 0.00000000e+00 1.51171580e-03
  0.00000000e+00 0.00000000e+00]
 [6.01374570e-03 0.00000000e+00 0.00000000e+00 1.71821306e-03
  0.00000000e+00 1.20274914e-02 9.55326460e-01 0.00000000e+00
  0.00000000e+00 1.28865979e-02 0.00000000e+00 0.00000000e+00
  5.15463918e-03 6.87285223e-03]
 [9.50118765e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.88123515e-01
  0.00000000e+00 0.00000000e+00 2.37529691e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [7.48129676e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.90024938e-01 0.00000000e+00 0.00000000e+00 2.49376559e-03
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 5.05050505e-03 0.00000000e+00
  0.00000000e+00 9.94949495e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.59489633e-03 0.00000000e+00 2.71132376e-02
  0.00000000e+00 0.00000000e+00 6.37958533e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.64912281e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.71821306e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.82817869e-01
  0.00000000e+00 0.00000000e+00]
 [7.66283525e-03 0.00000000e+00 3.83141762e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.14942529e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  5.97701149e-01 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 8.51063830e-03 1.27659574e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 8.63829787e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8575    0.8898    0.8733      1143
           1     0.8854    0.8035    0.8424       173
           2     0.7733    0.7733    0.7733       450
           3     0.8165    0.8165    0.8165       158
           4     0.8358    0.7724    0.8029       145
           5     0.7600    0.8057    0.7822       283
           6     0.5351    0.5823    0.5577       249
           7     0.8732    0.6889    0.7702        90
           8     0.5862    0.6000    0.5930        85
           9     0.7952    0.7857    0.7904        84
          10     0.8305    0.7313    0.7778       134
          11     0.6250    0.4839    0.5455        62
          12     0.0571    0.0714    0.0635        56
          13     0.8621    0.5000    0.6329        50

    accuracy                         0.7761      3162
   macro avg     0.7209    0.6646    0.6873      3162
weighted avg     0.7816    0.7761    0.7770      3162

validation confusion matrix:
[[8.89763780e-01 1.74978128e-03 3.23709536e-02 1.74978128e-03
  0.00000000e+00 7.87401575e-03 2.97462817e-02 6.12423447e-03
  1.22484689e-02 3.49956255e-03 8.74890639e-04 1.74978128e-03
  1.04986877e-02 1.74978128e-03]
 [2.31213873e-02 8.03468208e-01 1.15606936e-02 5.78034682e-03
  6.35838150e-02 4.04624277e-02 5.78034682e-03 0.00000000e+00
  4.04624277e-02 0.00000000e+00 5.78034682e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.44444444e-02 2.22222222e-03 7.73333333e-01 1.77777778e-02
  0.00000000e+00 2.22222222e-02 5.33333333e-02 0.00000000e+00
  6.66666667e-03 0.00000000e+00 4.44444444e-03 8.88888889e-03
  4.66666667e-02 0.00000000e+00]
 [2.53164557e-02 0.00000000e+00 2.53164557e-02 8.16455696e-01
  0.00000000e+00 3.16455696e-02 2.53164557e-02 0.00000000e+00
  0.00000000e+00 6.32911392e-03 5.69620253e-02 0.00000000e+00
  1.26582278e-02 0.00000000e+00]
 [6.89655172e-03 5.51724138e-02 1.37931034e-02 0.00000000e+00
  7.72413793e-01 1.24137931e-01 1.37931034e-02 0.00000000e+00
  1.37931034e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.88692580e-02 7.06713781e-03 1.76678445e-02 3.53356890e-03
  3.53356890e-02 8.05653710e-01 2.47349823e-02 0.00000000e+00
  1.76678445e-02 0.00000000e+00 7.06713781e-03 1.76678445e-02
  2.47349823e-02 0.00000000e+00]
 [1.36546185e-01 4.01606426e-03 8.03212851e-02 2.40963855e-02
  4.01606426e-03 4.81927711e-02 5.82329317e-01 4.01606426e-03
  0.00000000e+00 3.21285141e-02 8.03212851e-03 0.00000000e+00
  6.82730924e-02 8.03212851e-03]
 [2.11111111e-01 0.00000000e+00 3.33333333e-02 1.11111111e-02
  0.00000000e+00 0.00000000e+00 3.33333333e-02 6.88888889e-01
  1.11111111e-02 0.00000000e+00 0.00000000e+00 1.11111111e-02
  0.00000000e+00 0.00000000e+00]
 [2.35294118e-01 2.35294118e-02 1.17647059e-02 0.00000000e+00
  0.00000000e+00 4.70588235e-02 1.17647059e-02 0.00000000e+00
  6.00000000e-01 0.00000000e+00 1.17647059e-02 3.52941176e-02
  2.35294118e-02 0.00000000e+00]
 [3.57142857e-02 0.00000000e+00 3.57142857e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.42857143e-01 0.00000000e+00
  0.00000000e+00 7.85714286e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.47761194e-02 7.46268657e-03 2.98507463e-02 3.73134328e-02
  0.00000000e+00 2.98507463e-02 6.71641791e-02 7.46268657e-03
  1.49253731e-02 0.00000000e+00 7.31343284e-01 1.49253731e-02
  1.49253731e-02 0.00000000e+00]
 [2.74193548e-01 1.61290323e-02 3.22580645e-02 1.61290323e-02
  0.00000000e+00 3.22580645e-02 4.83870968e-02 0.00000000e+00
  0.00000000e+00 6.45161290e-02 0.00000000e+00 4.83870968e-01
  3.22580645e-02 0.00000000e+00]
 [2.50000000e-01 0.00000000e+00 3.39285714e-01 5.35714286e-02
  0.00000000e+00 0.00000000e+00 1.96428571e-01 0.00000000e+00
  3.57142857e-02 0.00000000e+00 3.57142857e-02 1.78571429e-02
  7.14285714e-02 0.00000000e+00]
 [1.40000000e-01 0.00000000e+00 0.00000000e+00 2.00000000e-02
  0.00000000e+00 2.00000000e-02 3.00000000e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  2.00000000e-02 5.00000000e-01]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8456    0.8559    0.8507      1145
           1     0.8693    0.7600    0.8110       175
           2     0.7825    0.8137    0.7978       451
           3     0.7834    0.7736    0.7785       159
           4     0.8296    0.7671    0.7972       146
           5     0.7841    0.8310    0.8068       284
           6     0.5159    0.5840    0.5478       250
           7     0.7386    0.7143    0.7263        91
           8     0.5625    0.5172    0.5389        87
           9     0.7439    0.7093    0.7262        86
          10     0.7768    0.6397    0.7016       136
          11     0.6667    0.5000    0.5714        64
          12     0.1818    0.2456    0.2090        57
          13     0.6667    0.5000    0.5714        52

    accuracy                         0.7625      3183
   macro avg     0.6962    0.6580    0.6739      3183
weighted avg     0.7678    0.7625    0.7638      3183

test confusion matrix:
[[0.8558952  0.00174672 0.0279476  0.00349345 0.         0.00873362
  0.03056769 0.01659389 0.01746725 0.00873362 0.00174672 0.00611354
  0.01659389 0.00436681]
 [0.02857143 0.76       0.01714286 0.00571429 0.09714286 0.05142857
  0.00571429 0.         0.00571429 0.         0.01714286 0.00571429
  0.00571429 0.        ]
 [0.07095344 0.00221729 0.81374723 0.01330377 0.         0.00665188
  0.05321508 0.         0.         0.         0.00443459 0.
  0.03547672 0.        ]
 [0.02515723 0.00628931 0.01886792 0.77358491 0.00628931 0.03773585
  0.05031447 0.00628931 0.         0.         0.05031447 0.
  0.02515723 0.        ]
 [0.         0.06164384 0.01369863 0.00684932 0.76712329 0.12328767
  0.00684932 0.         0.02054795 0.         0.         0.
  0.         0.        ]
 [0.03169014 0.00352113 0.01760563 0.01760563 0.01408451 0.83098592
  0.02816901 0.         0.01408451 0.         0.00704225 0.01056338
  0.02112676 0.00352113]
 [0.104      0.012      0.124      0.024      0.004      0.032
  0.584      0.004      0.         0.02       0.024      0.004
  0.044      0.02      ]
 [0.24175824 0.         0.01098901 0.         0.         0.
  0.01098901 0.71428571 0.01098901 0.         0.         0.
  0.01098901 0.        ]
 [0.36781609 0.         0.01149425 0.         0.         0.02298851
  0.02298851 0.01149425 0.51724138 0.01149425 0.02298851 0.01149425
  0.         0.        ]
 [0.08139535 0.         0.01162791 0.         0.         0.
  0.1744186  0.         0.         0.70930233 0.         0.
  0.         0.02325581]
 [0.05882353 0.00735294 0.02941176 0.05147059 0.         0.02205882
  0.11029412 0.00735294 0.02205882 0.00735294 0.63970588 0.01470588
  0.02941176 0.        ]
 [0.265625   0.03125    0.03125    0.         0.         0.0625
  0.046875   0.         0.03125    0.03125    0.         0.5
  0.         0.        ]
 [0.22807018 0.         0.26315789 0.07017544 0.         0.03508772
  0.10526316 0.         0.01754386 0.01754386 0.         0.01754386
  0.24561404 0.        ]
 [0.07692308 0.         0.03846154 0.         0.         0.
  0.34615385 0.         0.         0.01923077 0.         0.
  0.01923077 0.5       ]]
---------------------------------------
program finished.
