seed:  666
number of classes (from original clusters): 14
how to merge clusters:  [[0, 9, 12], [1, 5, 11], 2, [3, 8, 13], 4, 6, 7, 10]
whether to further subcluster data according to chemical reaction: False
positive training pair sampling threshold:  16000
negative training pair sampling threshold:  4600
number of epochs to train: 55
learning rate decay to half at epoch 25.
batch size: 256
similar margin of contrastive loss: 0.0
dissimilar margin of contrastive loss: 2.0
number of workers to load data:  36
device:  cuda
number of gpus:  2
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of classes after merging:  8
number of pockets in training set:  12475
number of pockets in validation set:  2670
number of pockets in test set:  2681
number of train positive pairs: 128000
number of train negative pairs: 128800
model architecture:
ResidualSiameseNet(
  (embedding_net): ResidualEmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=48, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=48, out_features=48, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (rb_2): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_3): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_4): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_5): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_6): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_7): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (rb_8): ResidualBlock(
      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): GINMolecularConv(nn=Sequential(
        (0): Linear(in_features=48, out_features=48, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=48, out_features=48, bias=True)
      ))(edge_transformer=Sequential(
        (0): Linear(in_features=1, out_features=8, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=8, out_features=48, bias=True)
        (3): ELU(alpha=1.0)
      ))
    )
    (bn_8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(48, 96)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(similar_margin=0.0, dissimilar_margin=2.0, normalize=True, mean=True)
epoch: 1, train loss: 0.7450153912933445, train acc: 0.8010597302504817, validation acc: 0.7210960960960962.
epoch: 2, train loss: 0.6541151456743757, train acc: 0.8080443159922929, validation acc: 0.7188438438438438.
epoch: 3, train loss: 0.6097087188078978, train acc: 0.7979287090558767, validation acc: 0.7218468468468469.
epoch: 4, train loss: 0.5775561061752177, train acc: 0.8118978805394991, validation acc: 0.7293543543543544.
epoch: 5, train loss: 0.5467110516200556, train acc: 0.8213712267180475, validation acc: 0.7267267267267268.
epoch: 6, train loss: 0.5190528022611623, train acc: 0.8255459216441875, validation acc: 0.7342342342342343.
epoch: 7, train loss: 0.49301110457779834, train acc: 0.8261078998073218, validation acc: 0.7237237237237237.
epoch: 8, train loss: 0.47405057437694703, train acc: 0.8384714193962749, validation acc: 0.7282282282282282.
epoch: 9, train loss: 0.45243072854395594, train acc: 0.8461785484906872, validation acc: 0.7383633633633634.
epoch: 10, train loss: 0.43748528932113884, train acc: 0.8380700064226075, validation acc: 0.7304804804804805.
epoch: 11, train loss: 0.42478247520708223, train acc: 0.857096981374438, validation acc: 0.7417417417417418.
epoch: 12, train loss: 0.41893828620791806, train acc: 0.8464996788696211, validation acc: 0.7334834834834835.
epoch: 13, train loss: 0.40262666556694054, train acc: 0.8574983943481054, validation acc: 0.7492492492492493.
epoch: 14, train loss: 0.3915723836310556, train acc: 0.8596660244059088, validation acc: 0.7481231231231231.
epoch: 15, train loss: 0.39052670160929365, train acc: 0.8510757867694284, validation acc: 0.7368618618618619.
epoch: 16, train loss: 0.37807627716539804, train acc: 0.8537251123956326, validation acc: 0.737987987987988.
epoch: 17, train loss: 0.377193684741344, train acc: 0.8630378933847141, validation acc: 0.7304804804804805.
epoch: 18, train loss: 0.37000620868718515, train acc: 0.8673731535003212, validation acc: 0.753003003003003.
epoch: 19, train loss: 0.3668222395876115, train acc: 0.8635998715478485, validation acc: 0.7432432432432432.
epoch: 20, train loss: 0.37047283673212167, train acc: 0.857257546563905, validation acc: 0.7364864864864865.
epoch: 21, train loss: 0.3552843379083081, train acc: 0.8747591522157996, validation acc: 0.7443693693693694.
epoch: 22, train loss: 0.3589602487629448, train acc: 0.8589434810533076, validation acc: 0.7256006006006006.
epoch: 23, train loss: 0.35770157199039637, train acc: 0.8483461785484907, validation acc: 0.7323573573573574.
epoch: 24, train loss: 0.35656734303150595, train acc: 0.8700224791265254, validation acc: 0.7391141141141141.
epoch: 25, train loss: 0.29545753100208033, train acc: 0.8919396274887604, validation acc: 0.7653903903903904.
epoch: 26, train loss: 0.30053753009094997, train acc: 0.8872832369942196, validation acc: 0.7578828828828829.
epoch: 27, train loss: 0.2978537873984126, train acc: 0.892742453436095, validation acc: 0.7515015015015015.
epoch: 28, train loss: 0.2945541970529289, train acc: 0.8888086062941555, validation acc: 0.7515015015015015.
epoch: 29, train loss: 0.29331564618048267, train acc: 0.8827071290944123, validation acc: 0.7417417417417418.
epoch: 30, train loss: 0.2995866205833411, train acc: 0.8893705844572897, validation acc: 0.7458708708708709.
epoch: 31, train loss: 0.29465077398722045, train acc: 0.8883269107257546, validation acc: 0.7454954954954955.
epoch: 32, train loss: 0.29632720249092837, train acc: 0.8964354527938343, validation acc: 0.7507507507507507.
epoch: 33, train loss: 0.29028752673080777, train acc: 0.8938664097623635, validation acc: 0.7515015015015015.
epoch: 34, train loss: 0.2909543188487258, train acc: 0.8891297366730893, validation acc: 0.7515015015015015.
epoch: 35, train loss: 0.2880898573985352, train acc: 0.8912170841361593, validation acc: 0.7526276276276276.
epoch: 36, train loss: 0.290128134329371, train acc: 0.893946692357097, validation acc: 0.7518768768768769.
epoch: 37, train loss: 0.290136738298838, train acc: 0.8958734746307001, validation acc: 0.7597597597597597.
epoch: 38, train loss: 0.28539920729640117, train acc: 0.8905748233782916, validation acc: 0.7466216216216216.
epoch: 39, train loss: 0.2875990026390812, train acc: 0.8942678227360308, validation acc: 0.7473723723723724.
epoch: 40, train loss: 0.2853857678119267, train acc: 0.8988439306358381, validation acc: 0.7511261261261262.
epoch: 41, train loss: 0.29470490715585396, train acc: 0.8986030828516378, validation acc: 0.7567567567567568.
epoch: 42, train loss: 0.27826698387894677, train acc: 0.8966763005780347, validation acc: 0.753003003003003.
epoch: 43, train loss: 0.2945809025482225, train acc: 0.8958734746307001, validation acc: 0.7507507507507507.
epoch: 44, train loss: 0.28491407343903064, train acc: 0.898121387283237, validation acc: 0.759009009009009.
epoch: 45, train loss: 0.2760941296574483, train acc: 0.8961143224149004, validation acc: 0.746996996996997.
epoch: 46, train loss: 0.2928627304273231, train acc: 0.8942678227360308, validation acc: 0.7503753753753754.
epoch: 47, train loss: 0.30887192963811094, train acc: 0.8884071933204881, validation acc: 0.7394894894894894.
epoch: 48, train loss: 0.28063106829503615, train acc: 0.9037411689145793, validation acc: 0.7601351351351351.
epoch: 49, train loss: 0.28217637039790644, train acc: 0.8983622350674374, validation acc: 0.762012012012012.
epoch: 50, train loss: 0.28429879062272306, train acc: 0.8886480411046885, validation acc: 0.753003003003003.
epoch: 51, train loss: 0.4104092287497357, train acc: 0.8537251123956326, validation acc: 0.7413663663663663.
epoch: 52, train loss: 0.30596317228869857, train acc: 0.8809409120102761, validation acc: 0.7406156156156156.
epoch: 53, train loss: 0.2761242414040729, train acc: 0.9076750160565189, validation acc: 0.7488738738738738.
epoch: 54, train loss: 0.28679265818491906, train acc: 0.8881663455362877, validation acc: 0.7387387387387387.
epoch: 55, train loss: 0.27706026092125247, train acc: 0.9039017341040463, validation acc: 0.756006006006006.
best validation acc 0.7653903903903904 at epoch 25.
