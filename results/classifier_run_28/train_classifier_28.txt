seed:  666
save trained model at:  ../trained_models/trained_classifier_model_28.pt
save loss at:  ./results/train_classifier_results_28.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  14780
number of pockets in validation set:  3162
number of pockets in test set:  3183
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKMCNMMEmbeddingNet(
    (conv0): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=22, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0006
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2ba9014eb3d0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.041906035994967, acc: 0.3968876860622463, val loss: 1.7621579174751123, acc: 0.44370651486401014, test loss: 1.7561052052284838, acc: 0.4596292805529375
epoch: 2, train loss: 1.7325957112770443, acc: 0.46650879566982406, val loss: 1.7975075195138777, acc: 0.4414927261227071, test loss: 1.788748726393867, acc: 0.4467483506126296
epoch: 3, train loss: 1.6125949650236653, acc: 0.5037212449255751, val loss: 1.65846634680225, acc: 0.48671726755218214, test loss: 1.6628834251484106, acc: 0.5004712535344015
epoch: 4, train loss: 1.542163006539919, acc: 0.5258457374830853, val loss: 1.6059710950809216, acc: 0.5063251106894371, test loss: 1.6025506982704472, acc: 0.507382972038957
epoch: 5, train loss: 1.4904833776218482, acc: 0.5497293640054127, val loss: 1.4486679475869957, acc: 0.549652118912081, test loss: 1.4111292991404485, acc: 0.5645617342130066
epoch: 6, train loss: 1.4404961188526695, acc: 0.5653585926928282, val loss: 1.5810275364344366, acc: 0.4898798228969007, test loss: 1.5659400756127648, acc: 0.4963870562362551
epoch: 7, train loss: 1.3830659499510376, acc: 0.5874154262516915, val loss: 1.380396868442147, acc: 0.5825426944971537, test loss: 1.3626358174244038, acc: 0.5868677348413447
epoch: 8, train loss: 1.351284370403006, acc: 0.5936400541271989, val loss: 1.3177399728812424, acc: 0.5939278937381404, test loss: 1.3080080548734963, acc: 0.5903235940936223
epoch: 9, train loss: 1.2928304559645698, acc: 0.6054127198917456, val loss: 1.2604566525236705, acc: 0.6125869702719797, test loss: 1.2487694707038153, acc: 0.6170279610430411
epoch: 10, train loss: 1.2479357853310034, acc: 0.6259810554803789, val loss: 1.5247621681025179, acc: 0.5585072738772928, test loss: 1.4867562104649712, acc: 0.5717876217404964
epoch: 11, train loss: 1.2200304955365049, acc: 0.6329499323410014, val loss: 1.2861634385050436, acc: 0.6081593927893738, test loss: 1.257433949646394, acc: 0.6145146088595664
epoch: 12, train loss: 1.1786295691749562, acc: 0.6464817320703654, val loss: 1.1739590463571952, acc: 0.6416824794433903, test loss: 1.1418724235333326, acc: 0.647502356267672
epoch: 13, train loss: 1.1579304417996026, acc: 0.652232746955345, val loss: 1.532624243785127, acc: 0.5259329538266919, test loss: 1.5302843189149617, acc: 0.5513666352497644
epoch: 14, train loss: 1.12685170589829, acc: 0.6621786197564276, val loss: 1.219103906367264, acc: 0.6321948134092347, test loss: 1.1790311305057766, acc: 0.6440464970153943
epoch: 15, train loss: 1.0864310668189083, acc: 0.6734776725304465, val loss: 1.5709868054990148, acc: 0.47185325743200507, test loss: 1.5818798819166513, acc: 0.4696826892868363
epoch: 16, train loss: 1.0758129557027545, acc: 0.6747631935047361, val loss: 1.2341336062405397, acc: 0.633459835547122, test loss: 1.2088407397532366, acc: 0.6427898209236569
epoch: 17, train loss: 1.040580941633862, acc: 0.6845060893098782, val loss: 1.2089167728520587, acc: 0.6084756483238457, test loss: 1.2155337744900687, acc: 0.6057178762174049
epoch: 18, train loss: 1.0315811971205013, acc: 0.6916779431664412, val loss: 1.1864441920503042, acc: 0.616382036685642, test loss: 1.1663098142014485, acc: 0.6336789192585611
epoch: 19, train loss: 1.0108459378450263, acc: 0.6949255751014884, val loss: 1.479415607482553, acc: 0.5566097406704618, test loss: 1.4733602489048379, acc: 0.5585925227772541
epoch: 20, train loss: 0.9960446319657508, acc: 0.6985791610284168, val loss: 1.183912691475846, acc: 0.6429475015812777, test loss: 1.1405903081117919, acc: 0.6603832862079799
epoch: 21, train loss: 0.9725362959669472, acc: 0.7016914749661705, val loss: 1.0500243240937939, acc: 0.6739405439595193, test loss: 1.0443298194230894, acc: 0.6760917373546969
epoch: 22, train loss: 0.9333774699571168, acc: 0.7182002706359946, val loss: 1.0531920414043332, acc: 0.683111954459203, test loss: 1.0552197226255597, acc: 0.6738925541941565
epoch: 23, train loss: 0.9146640042329512, acc: 0.7246955345060893, val loss: 1.2082532456522874, acc: 0.6246046805819102, test loss: 1.192550279417766, acc: 0.6264530317310714
epoch: 24, train loss: 0.9099639889997139, acc: 0.7235453315290934, val loss: 1.0345602055126772, acc: 0.6878557874762808, test loss: 1.0439543625187133, acc: 0.6764059063776312
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.726074348638119, acc: 0.724424898511502, val loss: 0.8523783983873612, acc: 0.6704617330803289, test loss: 0.8472042585595239, acc: 0.6760917373546969
epoch: 26, train loss: 0.6810460569894039, acc: 0.7357239512855209, val loss: 0.8581834717086415, acc: 0.6878557874762808, test loss: 0.886108742545245, acc: 0.6804901036757776
epoch: 27, train loss: 0.6743042823587606, acc: 0.7391069012178619, val loss: 0.843672936660892, acc: 0.6913345983554712, test loss: 0.8661257468639137, acc: 0.6971410618912975
epoch: 28, train loss: 0.6682932426868176, acc: 0.7434370771312585, val loss: 0.9152863806821665, acc: 0.6559139784946236, test loss: 0.9092197410722518, acc: 0.6682375117813384
epoch: 29, train loss: 0.6348217308117992, acc: 0.7542625169147497, val loss: 0.8745909913441262, acc: 0.674573055028463, test loss: 0.8698742635363196, acc: 0.6757775683317625
epoch: 30, train loss: 0.6383333815289447, acc: 0.7538565629228687, val loss: 0.7607784459743222, acc: 0.7084123972169513, test loss: 0.8038672274326776, acc: 0.6996544140747722
epoch: 31, train loss: 0.6393889921443872, acc: 0.7500676589986468, val loss: 0.731095050574103, acc: 0.7141049968374447, test loss: 0.7310177019246898, acc: 0.7241595978636507
epoch: 32, train loss: 0.6350436838134539, acc: 0.7540595399188093, val loss: 1.3262399111572207, acc: 0.5101201771030993, test loss: 1.3663010439931167, acc: 0.509267986176563
epoch: 33, train loss: 0.6253793511403591, acc: 0.7536535859269283, val loss: 0.8103032893277964, acc: 0.693864642631246, test loss: 0.8193407472933458, acc: 0.6968268928683632
epoch: 34, train loss: 0.6214471474552026, acc: 0.7609607577807849, val loss: 0.7662746491573944, acc: 0.706831119544592, test loss: 0.7751096098669438, acc: 0.7141061891297518
epoch: 35, train loss: 0.5919716340601847, acc: 0.7685385656292287, val loss: 0.7728762726177527, acc: 0.7001897533206831, test loss: 0.8096496832659137, acc: 0.6974552309142319
epoch: 36, train loss: 0.5788047640830157, acc: 0.7709742895805142, val loss: 0.8343945537316203, acc: 0.6771030993042378, test loss: 0.8316378875831894, acc: 0.6889726672950047
epoch: 37, train loss: 0.5817031382707846, acc: 0.7718538565629228, val loss: 0.8358224671826492, acc: 0.681214421252372, test loss: 0.8808019291556859, acc: 0.6707508639648131
epoch: 38, train loss: 0.5858106100510841, acc: 0.770703653585927, val loss: 0.81272434781127, acc: 0.687539531941809, test loss: 0.8226733073624957, acc: 0.6811184417216463
epoch: 39, train loss: 0.5523282430975298, acc: 0.7812584573748309, val loss: 0.7359088994821817, acc: 0.7185325743200506, test loss: 0.7366845853441809, acc: 0.7251021049324536
epoch: 40, train loss: 0.5341021365661259, acc: 0.7853856562922868, val loss: 0.7638332596464598, acc: 0.7179000632511069, test loss: 0.7641350353359614, acc: 0.7175620483820295
epoch: 41, train loss: 0.5696331324048229, acc: 0.7758457374830853, val loss: 0.7927855303738405, acc: 0.700506008855155, test loss: 0.8377506553920754, acc: 0.6952560477536914
epoch: 42, train loss: 0.555384247067494, acc: 0.7838971583220569, val loss: 0.8000644467285658, acc: 0.6891208096141682, test loss: 0.8099807654766403, acc: 0.688344329249136
epoch: 43, train loss: 0.5506668354403504, acc: 0.7799052774018944, val loss: 0.8039304766000486, acc: 0.6944971537001897, test loss: 0.8156961338946253, acc: 0.7024819352811813
epoch: 44, train loss: 0.5242345655722611, acc: 0.7928958051420839, val loss: 0.8780401415043734, acc: 0.6796331435800127, test loss: 0.8749767709295218, acc: 0.6748350612629594
epoch: 45, train loss: 0.5207844146856274, acc: 0.7952638700947225, val loss: 0.6865114732907589, acc: 0.7416192283364959, test loss: 0.725564026660302, acc: 0.7323279924599434
epoch: 46, train loss: 0.5118822881510842, acc: 0.7942489851150203, val loss: 0.8256713787563837, acc: 0.6913345983554712, test loss: 0.8377902533998288, acc: 0.6946277097078228
epoch: 47, train loss: 0.5230651312977761, acc: 0.7912719891745602, val loss: 0.8493713771595675, acc: 0.67235926628716, test loss: 0.8592091154235735, acc: 0.6770342444234998
epoch: 48, train loss: 0.5038188117605423, acc: 0.7978349120433018, val loss: 0.7033051581868335, acc: 0.7381404174573055, test loss: 0.704930756101809, acc: 0.7386113729186302
epoch: 49, train loss: 0.5069439025144615, acc: 0.7962110960757781, val loss: 0.7457225013578489, acc: 0.7283364958886781, test loss: 0.7648387430150891, acc: 0.728243795161797
epoch: 50, train loss: 0.4974145602147537, acc: 0.8029093369418132, val loss: 0.7927921674886435, acc: 0.6929158760278304, test loss: 0.8193837231597997, acc: 0.6971410618912975
epoch: 51, train loss: 0.4833656490171714, acc: 0.8079837618403247, val loss: 0.9214985887888185, acc: 0.6777356103731815, test loss: 0.9336182441196088, acc: 0.6606974552309143
epoch: 52, train loss: 0.49081112825499496, acc: 0.8029093369418132, val loss: 0.7843749123536508, acc: 0.7011385199240987, test loss: 0.7926662682812805, acc: 0.7078228086710651
epoch: 53, train loss: 0.48337486069321794, acc: 0.8062922868741542, val loss: 0.803359286231681, acc: 0.698292220113852, test loss: 0.8145329274361215, acc: 0.7012252591894439
epoch: 54, train loss: 0.5067289666410066, acc: 0.8012178619756427, val loss: 0.6823417597522772, acc: 0.7406704617330804, test loss: 0.6998618080898384, acc: 0.7326421614828778
epoch: 55, train loss: 0.4711704169703756, acc: 0.8094722598105548, val loss: 0.7821909811284103, acc: 0.7109424414927261, test loss: 0.8000105195849937, acc: 0.7071944706251964
epoch: 56, train loss: 0.4661443868089916, acc: 0.8119756427604872, val loss: 0.7187516710412871, acc: 0.7308665401644528, test loss: 0.7186053090600372, acc: 0.7332704995287465
Epoch    56: reducing learning rate of group 0 to 1.5000e-03.
epoch: 57, train loss: 0.39400787130099024, acc: 0.8381596752368065, val loss: 0.6505602411949356, acc: 0.7659709044908286, test loss: 0.673826703252262, acc: 0.7628023876845743
epoch: 58, train loss: 0.34107361972251343, acc: 0.8576454668470906, val loss: 0.6227914278149831, acc: 0.7722960151802657, test loss: 0.6393708271909876, acc: 0.7703424442349984
epoch: 59, train loss: 0.3295670930926306, acc: 0.8633964817320704, val loss: 0.7631276139422808, acc: 0.7466793168880456, test loss: 0.7579651247432612, acc: 0.7477222745837261
epoch: 60, train loss: 0.32944419001694136, acc: 0.8648173207036536, val loss: 0.6750978146226703, acc: 0.7659709044908286, test loss: 0.6891823651616094, acc: 0.7687715991203268
epoch: 61, train loss: 0.3067471838449109, acc: 0.8702300405953992, val loss: 0.7395448343878981, acc: 0.7482605945604048, test loss: 0.7495807226146874, acc: 0.7442664153314483
epoch: 62, train loss: 0.311004007814379, acc: 0.8692828146143438, val loss: 0.661398153829846, acc: 0.765022137887413, test loss: 0.6973075789548375, acc: 0.7678290920515237
epoch: 63, train loss: 0.30071841259770526, acc: 0.8731393775372125, val loss: 0.6941722917224997, acc: 0.7549019607843137, test loss: 0.7125489612888548, acc: 0.760603204524034
epoch: 64, train loss: 0.2830211061747374, acc: 0.8777401894451962, val loss: 0.671729886479049, acc: 0.7653383934218849, test loss: 0.7494581862511667, acc: 0.7609173735469683
epoch: 65, train loss: 0.2926146696723685, acc: 0.8748985115020298, val loss: 0.8188002308586743, acc: 0.7299177735610373, test loss: 0.8638226169480717, acc: 0.7207037386113729
epoch: 66, train loss: 0.2971466395867855, acc: 0.8741542625169147, val loss: 0.7962055856257131, acc: 0.7296015180265655, test loss: 0.8156010871633134, acc: 0.7316996544140748
epoch: 67, train loss: 0.2942140380890992, acc: 0.8733423545331529, val loss: 0.7421612139369475, acc: 0.7621758380771664, test loss: 0.7815841930376222, acc: 0.7646874018221803
epoch: 68, train loss: 0.28041376657834394, acc: 0.8796346414073072, val loss: 0.7294576033846175, acc: 0.7669196710942442, test loss: 0.7810718355559344, acc: 0.7577756833176249
epoch: 69, train loss: 0.2797804088408634, acc: 0.8833558863328823, val loss: 0.7308514878238628, acc: 0.7416192283364959, test loss: 0.7720096929246362, acc: 0.742067232170908
epoch: 70, train loss: 0.2680155196031795, acc: 0.8859945872801083, val loss: 0.7192651768563142, acc: 0.7624920936116382, test loss: 0.7609951473653148, acc: 0.7558906691800189
epoch: 71, train loss: 0.26276447427772864, acc: 0.8855886332882273, val loss: 0.7589022517581291, acc: 0.7666034155597723, test loss: 0.8079680452697351, acc: 0.760603204524034
epoch: 72, train loss: 0.2853037194970173, acc: 0.8784844384303112, val loss: 0.7134174555634336, acc: 0.7666034155597723, test loss: 0.741426232173463, acc: 0.7628023876845743
epoch: 73, train loss: 0.26352128480864795, acc: 0.8852503382949932, val loss: 0.7580556299473801, acc: 0.75426944971537, test loss: 0.7906956623981031, acc: 0.7571473452717562
epoch: 74, train loss: 0.25626119862389984, acc: 0.8886332882273342, val loss: 0.8267835015967102, acc: 0.7473118279569892, test loss: 0.8666923680546671, acc: 0.7373546968268929
epoch: 75, train loss: 0.2562520732616701, acc: 0.8908660351826793, val loss: 0.7354916205216178, acc: 0.7599620493358634, test loss: 0.7614322998521466, acc: 0.7593465284322966
epoch: 76, train loss: 0.22443019758219326, acc: 0.9010148849797023, val loss: 0.7550677037706562, acc: 0.7726122707147375, test loss: 0.7963732144685051, acc: 0.7659440779139177
epoch: 77, train loss: 0.2455586931903888, acc: 0.8960757780784845, val loss: 0.7482120016569732, acc: 0.7526881720430108, test loss: 0.7754697611674249, acc: 0.7549481621112158
epoch: 78, train loss: 0.23706629519856187, acc: 0.8957374830852504, val loss: 0.7445403950490355, acc: 0.7707147375079064, test loss: 0.78411580495328, acc: 0.7668865849827207
epoch: 79, train loss: 0.22337560148781144, acc: 0.9029093369418133, val loss: 0.7883879713134476, acc: 0.7659709044908286, test loss: 0.8187627334322077, acc: 0.7602890355010996
epoch: 80, train loss: 0.23682179353395238, acc: 0.8984438430311231, val loss: 0.7780004779723986, acc: 0.7561669829222012, test loss: 0.7889597613669925, acc: 0.7511781338360037
epoch: 81, train loss: 0.223660117823313, acc: 0.9041948579161029, val loss: 0.8803081421969737, acc: 0.7245414294750158, test loss: 0.8677629040278544, acc: 0.7354696826892868
epoch: 82, train loss: 0.23859034266136334, acc: 0.8965493910690122, val loss: 0.7709883043541326, acc: 0.7681846932321316, test loss: 0.7903589581379604, acc: 0.7599748664781653
epoch: 83, train loss: 0.25129354109380175, acc: 0.8896481732070365, val loss: 0.8059492188115274, acc: 0.7438330170777988, test loss: 0.810014930566901, acc: 0.7492931196983977
epoch: 84, train loss: 0.21313993270245554, acc: 0.9049391069012178, val loss: 0.7550617084708265, acc: 0.7656546489563567, test loss: 0.7666218851558675, acc: 0.7697141061891297
epoch: 85, train loss: 0.21940299325770068, acc: 0.9027063599458728, val loss: 0.760816940094081, acc: 0.7688172043010753, test loss: 0.7721288663352993, acc: 0.7681432610744581
epoch: 86, train loss: 0.21420404987664282, acc: 0.9041271989174561, val loss: 0.7728747376002216, acc: 0.7593295382669196, test loss: 0.7714580890707651, acc: 0.7637448947533774
epoch: 87, train loss: 0.19955770210419682, acc: 0.9125845737483085, val loss: 0.7752338603355702, acc: 0.7700822264389627, test loss: 0.7595716056080686, acc: 0.7656299088909834
epoch: 88, train loss: 0.20298478175887236, acc: 0.9075778078484439, val loss: 0.911041043816602, acc: 0.7384566729917773, test loss: 0.9291764083913838, acc: 0.7436380772855796
epoch: 89, train loss: 0.22838098909277393, acc: 0.8994587280108255, val loss: 0.8363788527525504, acc: 0.7511068943706515, test loss: 0.8123819340410152, acc: 0.7596606974552309
epoch: 90, train loss: 0.19440349779609736, acc: 0.9169824086603519, val loss: 0.7901004015230665, acc: 0.7647058823529411, test loss: 0.8219786388410399, acc: 0.7700282752120641
epoch: 91, train loss: 0.23697400074527777, acc: 0.8959404600811908, val loss: 0.8856423258555229, acc: 0.7321315623023403, test loss: 0.8668902236786872, acc: 0.7382972038956959
epoch: 92, train loss: 0.1994171962847729, acc: 0.9123815967523681, val loss: 0.8685374881254578, acc: 0.7549019607843137, test loss: 0.8639826351391082, acc: 0.7533773169965441
epoch: 93, train loss: 0.19673267711402598, acc: 0.9149526387009472, val loss: 0.8343905616907437, acc: 0.7530044275774826, test loss: 0.8412487085158443, acc: 0.7508639648130694
epoch: 94, train loss: 0.19353027133238332, acc: 0.9141407307171854, val loss: 0.8130229017087586, acc: 0.7539531941808981, test loss: 0.8567329767624462, acc: 0.742067232170908
epoch: 95, train loss: 0.17786043346008848, acc: 0.9209742895805142, val loss: 0.8144740453632024, acc: 0.7716635041113219, test loss: 0.8554170445155468, acc: 0.7621740496387056
epoch: 96, train loss: 0.18245186549239617, acc: 0.91765899864682, val loss: 0.9403923503361797, acc: 0.7432005060088551, test loss: 0.990260170047627, acc: 0.7360980207351555
epoch: 97, train loss: 0.23635911373308127, acc: 0.8991880920162382, val loss: 0.778797420046889, acc: 0.7719797596457938, test loss: 0.8325797622397828, acc: 0.7621740496387056
epoch: 98, train loss: 0.19723790916683548, acc: 0.9106901217861976, val loss: 0.837435551491966, acc: 0.7624920936116382, test loss: 0.8520829864841117, acc: 0.7650015708451147
epoch: 99, train loss: 0.1985700634561469, acc: 0.9122462787550745, val loss: 0.8368187116566224, acc: 0.7628083491461101, test loss: 0.8630501253965176, acc: 0.7571473452717562
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.13526731677887727, acc: 0.9196887686062246, val loss: 0.6364057252559384, acc: 0.7697659709044908, test loss: 0.6622754033915921, acc: 0.7744266415331448
epoch: 101, train loss: 0.10312572660804278, acc: 0.9337618403247632, val loss: 0.6730846569253099, acc: 0.7789373814041746, test loss: 0.6885938753768629, acc: 0.7822808671065034
epoch: 102, train loss: 0.10326957090862388, acc: 0.9347090663058186, val loss: 0.7152276375412866, acc: 0.775774826059456, test loss: 0.7586229168466154, acc: 0.7675149230285894
epoch: 103, train loss: 0.11146740141514996, acc: 0.9277401894451962, val loss: 0.6874181026902703, acc: 0.7713472485768501, test loss: 0.7306739684736208, acc: 0.7734841344643418
epoch: 104, train loss: 0.1001452161994774, acc: 0.9352503382949933, val loss: 0.7013467178549818, acc: 0.7836812144212524, test loss: 0.721504071992484, acc: 0.7778825007854225
epoch: 105, train loss: 0.09813706368328916, acc: 0.9378890392422192, val loss: 0.6778864620758868, acc: 0.7741935483870968, test loss: 0.7107674372184962, acc: 0.7737983034872762
epoch: 106, train loss: 0.09919788244814286, acc: 0.9355209742895805, val loss: 0.7314312349460609, acc: 0.7722960151802657, test loss: 0.7452225063418653, acc: 0.7734841344643418
epoch: 107, train loss: 0.12204734570925226, acc: 0.9257104194857916, val loss: 0.6470978057211069, acc: 0.7776723592662872, test loss: 0.6681331578493492, acc: 0.7612315425699026
epoch: 108, train loss: 0.11675020341177916, acc: 0.9247631935047361, val loss: 0.6971870243813251, acc: 0.7567994939911449, test loss: 0.7190601478909832, acc: 0.7555765001570846
epoch: 109, train loss: 0.12795180177898305, acc: 0.9213125845737483, val loss: 0.7498191516660272, acc: 0.7558507273877293, test loss: 0.7367170410563726, acc: 0.7527489789506755
epoch: 110, train loss: 0.1216351094321727, acc: 0.9215155615696887, val loss: 0.7878223486447319, acc: 0.7482605945604048, test loss: 0.770228353192362, acc: 0.7543198240653471
epoch: 111, train loss: 0.12367921756310779, acc: 0.9215832205683356, val loss: 0.7044285792930454, acc: 0.7615433270082227, test loss: 0.7179846144531494, acc: 0.7599748664781653
epoch: 112, train loss: 0.11269494480628606, acc: 0.9267929634641408, val loss: 0.7279996882504409, acc: 0.7586970271979759, test loss: 0.7116103773030177, acc: 0.7593465284322966
epoch: 113, train loss: 0.11728395013509164, acc: 0.9232746955345061, val loss: 0.6753352098567511, acc: 0.7643896268184693, test loss: 0.7003593183035676, acc: 0.7700282752120641
epoch: 114, train loss: 0.11435433171276149, acc: 0.926657645466847, val loss: 0.7556417929379718, acc: 0.7615433270082227, test loss: 0.830473067810653, acc: 0.7445805843543827
epoch: 115, train loss: 0.10929878406498848, acc: 0.9275372124492558, val loss: 0.7340755890925272, acc: 0.7520556609740671, test loss: 0.7455256571756231, acc: 0.7505497957901351
Epoch   115: reducing learning rate of group 0 to 7.5000e-04.
epoch: 116, train loss: 0.07039666579625442, acc: 0.9495263870094722, val loss: 0.7356172675652115, acc: 0.7881087919038583, test loss: 0.7595174295363695, acc: 0.7835375431982406
epoch: 117, train loss: 0.0565756357419878, acc: 0.9583220568335589, val loss: 0.7172414963943607, acc: 0.7871600253004427, test loss: 0.752153639631604, acc: 0.7860508953817154
epoch: 118, train loss: 0.04346319767917445, acc: 0.9670500676589987, val loss: 0.7528987745180076, acc: 0.7868437697659709, test loss: 0.7777245577273787, acc: 0.785736726358781
epoch: 119, train loss: 0.035872253460030756, acc: 0.972192151556157, val loss: 0.8156449159891224, acc: 0.7789373814041746, test loss: 0.8377211426475158, acc: 0.781024191014766
epoch: 120, train loss: 0.0367604800465949, acc: 0.9712449255751014, val loss: 0.8206981481591936, acc: 0.7754585705249842, test loss: 0.8281688646366864, acc: 0.7841658812441094
epoch: 121, train loss: 0.04366844706590185, acc: 0.9667794316644114, val loss: 0.8300075512909573, acc: 0.7754585705249842, test loss: 0.8489134027790881, acc: 0.7807100219918316
epoch: 122, train loss: 0.04786795024862148, acc: 0.9654939106901218, val loss: 0.7843617661552743, acc: 0.7811511701454775, test loss: 0.8056992800251478, acc: 0.7772541627395538
epoch: 123, train loss: 0.03927996495841966, acc: 0.9703653585926928, val loss: 0.79820872273044, acc: 0.7789373814041746, test loss: 0.8329612876947219, acc: 0.7719132893496701
epoch: 124, train loss: 0.049990821255721324, acc: 0.9646143437077132, val loss: 0.8012412974232741, acc: 0.7789373814041746, test loss: 0.8300298496196002, acc: 0.7794533459000943
epoch: 125, train loss: 0.04253783147172128, acc: 0.9700270635994588, val loss: 0.846601328315952, acc: 0.773877292852625, test loss: 0.862591813429324, acc: 0.7763116556707509
epoch: 126, train loss: 0.0636142078953441, acc: 0.9555480378890392, val loss: 0.7576941205760032, acc: 0.7767235926628716, test loss: 0.7654149291128098, acc: 0.7750549795790135
epoch: 127, train loss: 0.05067181683502114, acc: 0.9639377537212449, val loss: 0.7919864456989885, acc: 0.7735610373181531, test loss: 0.7986161474383106, acc: 0.7722274583726044
epoch: 128, train loss: 0.04080562483978046, acc: 0.9696887686062247, val loss: 0.7689554127313758, acc: 0.7855787476280834, test loss: 0.7818985283093898, acc: 0.7873075714734528
epoch: 129, train loss: 0.045827776070618664, acc: 0.9672530446549391, val loss: 0.8029475656964219, acc: 0.7792536369386465, test loss: 0.830871944040987, acc: 0.7785108388312912
epoch: 130, train loss: 0.051730623576129724, acc: 0.9619756427604872, val loss: 0.7931331172161356, acc: 0.7729285262492094, test loss: 0.8172397780186195, acc: 0.7781966698083569
epoch: 131, train loss: 0.049195145237913636, acc: 0.9640054127198917, val loss: 0.818056489907059, acc: 0.7707147375079064, test loss: 0.8332442353066131, acc: 0.7722274583726044
epoch: 132, train loss: 0.04508167281041126, acc: 0.9669824086603518, val loss: 0.847757076327368, acc: 0.7643896268184693, test loss: 0.8630254278383725, acc: 0.7684574300973924
epoch: 133, train loss: 0.05218068291716711, acc: 0.9636671177266577, val loss: 0.8125039161872744, acc: 0.775774826059456, test loss: 0.7887480512865302, acc: 0.7744266415331448
epoch: 134, train loss: 0.05207604310136283, acc: 0.9640054127198917, val loss: 0.8549685537023382, acc: 0.7605945604048071, test loss: 0.8512670805422046, acc: 0.7709707822808671
epoch: 135, train loss: 0.0485618466191266, acc: 0.9633964817320704, val loss: 0.8352509422591786, acc: 0.7681846932321316, test loss: 0.820991099948146, acc: 0.7706566132579328
epoch: 136, train loss: 0.04301659616569866, acc: 0.9671177266576455, val loss: 0.8210435436919549, acc: 0.7703984819734345, test loss: 0.8224355223940635, acc: 0.7747408105560791
epoch: 137, train loss: 0.04673318750551411, acc: 0.968809201623816, val loss: 0.8354221082502796, acc: 0.7764073371283997, test loss: 0.8439997102792556, acc: 0.7772541627395538
epoch: 138, train loss: 0.046137083614187085, acc: 0.9654939106901218, val loss: 0.8839140633553512, acc: 0.7685009487666035, test loss: 0.8891609325522789, acc: 0.7781966698083569
epoch: 139, train loss: 0.07625071238398713, acc: 0.9500676589986469, val loss: 0.7346838894409141, acc: 0.7703984819734345, test loss: 0.7719524627656185, acc: 0.7759974866478165
epoch: 140, train loss: 0.05844302032047099, acc: 0.9599458728010826, val loss: 0.7925619611553117, acc: 0.7647058823529411, test loss: 0.7971744346723517, acc: 0.7646874018221803
epoch: 141, train loss: 0.04582035170215231, acc: 0.9688768606224628, val loss: 0.8055904621869835, acc: 0.7811511701454775, test loss: 0.8460564969135013, acc: 0.7637448947533774
epoch: 142, train loss: 0.051529558950654544, acc: 0.9635994587280108, val loss: 0.7626200159616972, acc: 0.782416192283365, test loss: 0.7845904951083597, acc: 0.7797675149230285
epoch: 143, train loss: 0.041676929743307695, acc: 0.9702300405953992, val loss: 0.78074901395022, acc: 0.7767235926628716, test loss: 0.8200485074890884, acc: 0.7832233741753063
epoch: 144, train loss: 0.05285718268607724, acc: 0.9663734776725305, val loss: 0.8052498861478146, acc: 0.7760910815939279, test loss: 0.8478171010081812, acc: 0.7753691486019478
epoch: 145, train loss: 0.06543579509075988, acc: 0.9548714479025711, val loss: 0.7635592400914578, acc: 0.7798861480075902, test loss: 0.7801215130691816, acc: 0.7769399937166196
epoch: 146, train loss: 0.05436630859386292, acc: 0.9612313937753721, val loss: 0.8416258993215157, acc: 0.7669196710942442, test loss: 0.8767073338712644, acc: 0.7659440779139177
epoch: 147, train loss: 0.0571653926989545, acc: 0.958660351826793, val loss: 0.7459634677433349, acc: 0.7770398481973435, test loss: 0.7888206320665858, acc: 0.7800816839459629
epoch: 148, train loss: 0.04499406866396554, acc: 0.9664411366711773, val loss: 0.8300376984984116, acc: 0.7703984819734345, test loss: 0.8433589172782742, acc: 0.7703424442349984
epoch: 149, train loss: 0.044714905511139214, acc: 0.9703653585926928, val loss: 0.8223869910653372, acc: 0.7700822264389627, test loss: 0.8384462241814865, acc: 0.7703424442349984
epoch: 150, train loss: 0.033805701742515834, acc: 0.9742219215155615, val loss: 0.8420274420526493, acc: 0.7710309930423782, test loss: 0.8623401097744994, acc: 0.7741124725102105
epoch: 151, train loss: 0.056356853107557565, acc: 0.9607577807848444, val loss: 0.8464901860494692, acc: 0.7618595825426945, test loss: 0.8008204398947694, acc: 0.7665724159597863
epoch: 152, train loss: 0.04606654846660178, acc: 0.9674560216508795, val loss: 0.88765057757412, acc: 0.7726122707147375, test loss: 0.9226430592355659, acc: 0.7675149230285894
epoch: 153, train loss: 0.04351230342349438, acc: 0.9693504736129905, val loss: 0.8200170717836557, acc: 0.7726122707147375, test loss: 0.8236369821510711, acc: 0.7678290920515237
epoch: 154, train loss: 0.0528650805091987, acc: 0.9625169147496617, val loss: 0.8080197820778062, acc: 0.7700822264389627, test loss: 0.8339556068359888, acc: 0.765315739868049
epoch: 155, train loss: 0.059329966680526085, acc: 0.9609607577807848, val loss: 0.8114044103797368, acc: 0.7681846932321316, test loss: 0.8158504427359459, acc: 0.7731699654414075
epoch: 156, train loss: 0.05101765237133622, acc: 0.9643437077131258, val loss: 0.7788077171357051, acc: 0.7719797596457938, test loss: 0.8178445069699253, acc: 0.7778825007854225
epoch: 157, train loss: 0.05408812704646055, acc: 0.963531799729364, val loss: 0.7932017041036858, acc: 0.767235926628716, test loss: 0.8474599831215885, acc: 0.7690857681432611
epoch: 158, train loss: 0.0518378196473212, acc: 0.9623139377537212, val loss: 0.7994552371925677, acc: 0.7814674256799494, test loss: 0.8296309699436947, acc: 0.7769399937166196
epoch: 159, train loss: 0.03958864247272641, acc: 0.9723274695534506, val loss: 0.7904100878038111, acc: 0.7789373814041746, test loss: 0.8449602836413538, acc: 0.781024191014766
epoch: 160, train loss: 0.02825345469484955, acc: 0.9772665764546685, val loss: 0.8325868464209927, acc: 0.775774826059456, test loss: 0.8774457736619043, acc: 0.7775683317624882
epoch: 161, train loss: 0.036147751057260896, acc: 0.9757104194857916, val loss: 0.8284381183923459, acc: 0.7722960151802657, test loss: 0.8512778934577033, acc: 0.7753691486019478
epoch: 162, train loss: 0.06041605900645417, acc: 0.9591339648173207, val loss: 0.796710280660283, acc: 0.7675521821631879, test loss: 0.8093653651778892, acc: 0.7684574300973924
epoch: 163, train loss: 0.05754925639384977, acc: 0.9610960757780784, val loss: 0.7599473531905192, acc: 0.7795698924731183, test loss: 0.8174058173087045, acc: 0.7741124725102105
epoch: 164, train loss: 0.043142136463615664, acc: 0.9705006765899865, val loss: 0.8169222846203104, acc: 0.7719797596457938, test loss: 0.8619302948278487, acc: 0.7712849513038015
epoch: 165, train loss: 0.05622772120534164, acc: 0.9623139377537212, val loss: 0.8264606575058051, acc: 0.7552182163187856, test loss: 0.8356185608077941, acc: 0.7587181903864278
epoch: 166, train loss: 0.04932818607967826, acc: 0.9627875507442489, val loss: 0.8216237311571327, acc: 0.7643896268184693, test loss: 0.8539460330424428, acc: 0.7659440779139177
Epoch   166: reducing learning rate of group 0 to 3.7500e-04.
epoch: 167, train loss: 0.030365004628412778, acc: 0.9777401894451963, val loss: 0.784005787612365, acc: 0.7855787476280834, test loss: 0.8340370277545005, acc: 0.7844800502670437
epoch: 168, train loss: 0.018721990908353674, acc: 0.9847090663058187, val loss: 0.7853197945890964, acc: 0.7877925363693865, test loss: 0.8282025818775332, acc: 0.7835375431982406
epoch: 169, train loss: 0.01381523532513151, acc: 0.9899188092016238, val loss: 0.8135013164107668, acc: 0.7874762808349146, test loss: 0.8664348670756933, acc: 0.7847942192899781
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.008760309154331604, acc: 0.9904600811907984, val loss: 0.7810389390857366, acc: 0.7868437697659709, test loss: 0.8172574106567579, acc: 0.7863650644046497
epoch: 171, train loss: 0.008101304290823749, acc: 0.9916102841677943, val loss: 0.7537642279280506, acc: 0.7877925363693865, test loss: 0.780609538710196, acc: 0.7816525290606346
epoch: 172, train loss: 0.0072746356805542004, acc: 0.9921515561569689, val loss: 0.7562133091903049, acc: 0.7881087919038583, test loss: 0.776548896650829, acc: 0.783851712221175
epoch: 173, train loss: 0.008558304330865891, acc: 0.9910690121786198, val loss: 0.7370741672714928, acc: 0.7843137254901961, test loss: 0.7671185115430103, acc: 0.7851083883129123
epoch: 174, train loss: 0.00708452503792992, acc: 0.9920162381596752, val loss: 0.7296766565993645, acc: 0.7868437697659709, test loss: 0.7612093653350815, acc: 0.7851083883129123
epoch: 175, train loss: 0.00796412693132446, acc: 0.9910690121786198, val loss: 0.7366065102842019, acc: 0.786527514231499, test loss: 0.7585473695042521, acc: 0.7879359095193214
epoch: 176, train loss: 0.0066309326301872, acc: 0.9928958051420839, val loss: 0.7311672610168891, acc: 0.7858950031625553, test loss: 0.7650956853620293, acc: 0.7854225573358466
epoch: 177, train loss: 0.007416537264551015, acc: 0.9925575101488497, val loss: 0.7244984970901074, acc: 0.7893738140417458, test loss: 0.7715565343716592, acc: 0.7835375431982406
epoch: 178, train loss: 0.009134086150378595, acc: 0.9910690121786198, val loss: 0.7499891915402783, acc: 0.7849462365591398, test loss: 0.7956887192970321, acc: 0.7788250078542256
epoch: 179, train loss: 0.014975627934678644, acc: 0.9834912043301759, val loss: 0.7000357168222967, acc: 0.7811511701454775, test loss: 0.7585906959201718, acc: 0.781024191014766
epoch: 180, train loss: 0.015320292629896384, acc: 0.9852503382949932, val loss: 0.7186012192665211, acc: 0.7792536369386465, test loss: 0.7514217850400186, acc: 0.7794533459000943
epoch: 181, train loss: 0.010758154776769013, acc: 0.9884979702300406, val loss: 0.7243289542605348, acc: 0.7868437697659709, test loss: 0.7546046197021153, acc: 0.7863650644046497
epoch: 182, train loss: 0.009581079672270924, acc: 0.9893775372124493, val loss: 0.7352762178557346, acc: 0.7881087919038583, test loss: 0.7755697002734473, acc: 0.7844800502670437
epoch: 183, train loss: 0.011057077579739654, acc: 0.9890392422192151, val loss: 0.7013242042645256, acc: 0.7830487033523087, test loss: 0.7420300498834167, acc: 0.7781966698083569
epoch: 184, train loss: 0.011411309341303954, acc: 0.9875507442489851, val loss: 0.7188246493698448, acc: 0.790955091714105, test loss: 0.7527682268503436, acc: 0.7832233741753063
epoch: 185, train loss: 0.009585986924212784, acc: 0.990595399188092, val loss: 0.7247828137339255, acc: 0.7881087919038583, test loss: 0.7547695064110236, acc: 0.7885642475651901
epoch: 186, train loss: 0.008030195797116814, acc: 0.9916779431664411, val loss: 0.7178176148340417, acc: 0.7906388361796332, test loss: 0.7577200838128538, acc: 0.7863650644046497
epoch: 187, train loss: 0.007434366466238067, acc: 0.9931664411366712, val loss: 0.7150914537084774, acc: 0.7906388361796332, test loss: 0.7700213433210856, acc: 0.7888784165881244
epoch: 188, train loss: 0.006389830362288652, acc: 0.9935047361299053, val loss: 0.7480271385109628, acc: 0.788741302972802, test loss: 0.7864832782610535, acc: 0.7885642475651901
epoch: 189, train loss: 0.008757858474944317, acc: 0.9918132611637348, val loss: 0.7495625991748904, acc: 0.7858950031625553, test loss: 0.7677779204434207, acc: 0.7882500785422557
epoch: 190, train loss: 0.00889176427813925, acc: 0.9911366711772666, val loss: 0.7583911518444373, acc: 0.7843137254901961, test loss: 0.7851775580519144, acc: 0.7854225573358466
epoch: 191, train loss: 0.009533339750944197, acc: 0.9895805142083897, val loss: 0.7370650438776807, acc: 0.777988614800759, test loss: 0.7602667472963995, acc: 0.7879359095193214
epoch: 192, train loss: 0.011801798770730969, acc: 0.9879566982408661, val loss: 0.7215599897615069, acc: 0.7852624920936117, test loss: 0.7537232532914689, acc: 0.7825950361294376
epoch: 193, train loss: 0.014735899035502553, acc: 0.984979702300406, val loss: 0.7190862442405068, acc: 0.7783048703352309, test loss: 0.7443802111410248, acc: 0.7775683317624882
epoch: 194, train loss: 0.013201010124871309, acc: 0.9870094722598105, val loss: 0.712855382707585, acc: 0.7811511701454775, test loss: 0.7631812539446702, acc: 0.7813383600377003
epoch: 195, train loss: 0.010322059717039415, acc: 0.9884979702300406, val loss: 0.7268954197264109, acc: 0.7748260594560404, test loss: 0.7759577078224539, acc: 0.7753691486019478
epoch: 196, train loss: 0.017785042394028224, acc: 0.9824086603518268, val loss: 0.7471341634384822, acc: 0.7767235926628716, test loss: 0.7813798550573715, acc: 0.7769399937166196
epoch: 197, train loss: 0.01715757346463784, acc: 0.9818673883626522, val loss: 0.7663656121036209, acc: 0.7811511701454775, test loss: 0.7728122741292941, acc: 0.7825950361294376
epoch: 198, train loss: 0.029910888580330655, acc: 0.972192151556157, val loss: 0.7198568461138866, acc: 0.7751423149905123, test loss: 0.7624164333966705, acc: 0.7753691486019478
epoch: 199, train loss: 0.027186122418700116, acc: 0.974424898511502, val loss: 0.7203357183503472, acc: 0.7691334598355472, test loss: 0.7298942720444368, acc: 0.7807100219918316
epoch: 200, train loss: 0.017723462851629852, acc: 0.9834235453315291, val loss: 0.7105517881267965, acc: 0.7732447817836812, test loss: 0.7379559055275038, acc: 0.7775683317624882
best val acc 0.790955091714105 at epoch 184.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9998    0.9998    0.9998      5337
           1     0.9975    1.0000    0.9988       810
           2     0.9962    0.9995    0.9979      2100
           3     1.0000    1.0000    1.0000       737
           4     1.0000    0.9985    0.9993       677
           5     0.9970    0.9977    0.9974      1323
           6     0.9957    0.9931    0.9944      1164
           7     0.9976    1.0000    0.9988       421
           8     1.0000    0.9975    0.9988       401
           9     0.9875    0.9975    0.9925       396
          10     1.0000    1.0000    1.0000       627
          11     1.0000    1.0000    1.0000       291
          12     1.0000    0.9693    0.9844       261
          13     1.0000    0.9915    0.9957       235

    accuracy                         0.9982     14780
   macro avg     0.9980    0.9960    0.9970     14780
weighted avg     0.9982    0.9982    0.9982     14780

train confusion matrix:
[[9.99812629e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.87371182e-04
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 9.99523810e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 4.76190476e-04 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.47710487e-03 0.00000000e+00 0.00000000e+00
  9.98522895e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.97732426e-01 2.26757370e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.57731959e-03 9.93127148e-01 0.00000000e+00
  0.00000000e+00 4.29553265e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.49376559e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.97506234e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.52525253e-03 0.00000000e+00
  0.00000000e+00 9.97474747e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 3.06513410e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.69348659e-01 0.00000000e+00]
 [0.00000000e+00 4.25531915e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 4.25531915e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.91489362e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8670    0.8836    0.8752      1143
           1     0.8696    0.8092    0.8383       173
           2     0.8163    0.7800    0.7977       450
           3     0.8000    0.7848    0.7923       158
           4     0.8639    0.8759    0.8699       145
           5     0.8427    0.7951    0.8182       283
           6     0.5418    0.6506    0.5912       249
           7     0.7711    0.7111    0.7399        90
           8     0.6892    0.6000    0.6415        85
           9     0.8493    0.7381    0.7898        84
          10     0.8696    0.7463    0.8032       134
          11     0.6842    0.6290    0.6555        62
          12     0.1556    0.2500    0.1918        56
          13     0.6957    0.6400    0.6667        50

    accuracy                         0.7910      3162
   macro avg     0.7368    0.7067    0.7194      3162
weighted avg     0.8019    0.7910    0.7952      3162

validation confusion matrix:
[[8.83639545e-01 6.99912511e-03 2.79965004e-02 8.74890639e-04
  0.00000000e+00 4.37445319e-03 3.32458443e-02 8.74890639e-03
  6.12423447e-03 3.49956255e-03 8.74890639e-04 4.37445319e-03
  1.66229221e-02 2.62467192e-03]
 [2.89017341e-02 8.09248555e-01 5.78034682e-03 1.15606936e-02
  6.93641618e-02 3.46820809e-02 1.15606936e-02 5.78034682e-03
  1.73410405e-02 0.00000000e+00 5.78034682e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [8.00000000e-02 0.00000000e+00 7.80000000e-01 6.66666667e-03
  2.22222222e-03 8.88888889e-03 3.77777778e-02 2.22222222e-03
  4.44444444e-03 0.00000000e+00 8.88888889e-03 2.22222222e-03
  6.22222222e-02 4.44444444e-03]
 [2.53164557e-02 6.32911392e-03 3.79746835e-02 7.84810127e-01
  0.00000000e+00 1.89873418e-02 5.69620253e-02 0.00000000e+00
  6.32911392e-03 6.32911392e-03 1.26582278e-02 6.32911392e-03
  3.79746835e-02 0.00000000e+00]
 [1.37931034e-02 2.06896552e-02 1.37931034e-02 0.00000000e+00
  8.75862069e-01 6.89655172e-02 0.00000000e+00 0.00000000e+00
  6.89655172e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.59363958e-02 1.06007067e-02 7.06713781e-03 2.12014134e-02
  2.47349823e-02 7.95053004e-01 6.00706714e-02 0.00000000e+00
  1.06007067e-02 0.00000000e+00 0.00000000e+00 3.53356890e-03
  2.12014134e-02 0.00000000e+00]
 [8.83534137e-02 0.00000000e+00 6.02409639e-02 3.61445783e-02
  0.00000000e+00 2.40963855e-02 6.50602410e-01 1.20481928e-02
  0.00000000e+00 2.40963855e-02 1.20481928e-02 1.20481928e-02
  4.81927711e-02 3.21285141e-02]
 [2.00000000e-01 0.00000000e+00 1.11111111e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 4.44444444e-02 7.11111111e-01
  0.00000000e+00 0.00000000e+00 1.11111111e-02 0.00000000e+00
  2.22222222e-02 0.00000000e+00]
 [2.35294118e-01 1.17647059e-02 1.17647059e-02 1.17647059e-02
  0.00000000e+00 5.88235294e-02 2.35294118e-02 0.00000000e+00
  6.00000000e-01 0.00000000e+00 0.00000000e+00 4.70588235e-02
  0.00000000e+00 0.00000000e+00]
 [4.76190476e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.90476190e-01 1.19047619e-02
  0.00000000e+00 7.38095238e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.19047619e-02]
 [2.23880597e-02 3.73134328e-02 2.98507463e-02 2.98507463e-02
  0.00000000e+00 0.00000000e+00 1.11940299e-01 7.46268657e-03
  7.46268657e-03 0.00000000e+00 7.46268657e-01 0.00000000e+00
  7.46268657e-03 0.00000000e+00]
 [2.09677419e-01 0.00000000e+00 1.61290323e-02 1.61290323e-02
  0.00000000e+00 1.61290323e-02 1.61290323e-02 1.61290323e-02
  8.06451613e-02 0.00000000e+00 0.00000000e+00 6.29032258e-01
  0.00000000e+00 0.00000000e+00]
 [2.14285714e-01 0.00000000e+00 2.14285714e-01 5.35714286e-02
  0.00000000e+00 3.57142857e-02 1.25000000e-01 1.78571429e-02
  0.00000000e+00 0.00000000e+00 3.57142857e-02 5.35714286e-02
  2.50000000e-01 0.00000000e+00]
 [6.00000000e-02 0.00000000e+00 4.00000000e-02 2.00000000e-02
  0.00000000e+00 0.00000000e+00 1.80000000e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.00000000e-02 0.00000000e+00
  4.00000000e-02 6.40000000e-01]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8620    0.8838    0.8728      1145
           1     0.8634    0.7943    0.8274       175
           2     0.7982    0.7982    0.7982       451
           3     0.7922    0.7673    0.7796       159
           4     0.8855    0.7945    0.8375       146
           5     0.8566    0.7993    0.8270       284
           6     0.5161    0.5760    0.5444       250
           7     0.7604    0.8022    0.7807        91
           8     0.7467    0.6437    0.6914        87
           9     0.8072    0.7791    0.7929        86
          10     0.8763    0.6250    0.7296       136
          11     0.5513    0.6719    0.6056        64
          12     0.1750    0.2456    0.2044        57
          13     0.5932    0.6731    0.6306        52

    accuracy                         0.7832      3183
   macro avg     0.7203    0.7039    0.7087      3183
weighted avg     0.7931    0.7832    0.7864      3183

test confusion matrix:
[[8.83842795e-01 2.62008734e-03 2.27074236e-02 8.73362445e-04
  0.00000000e+00 4.36681223e-03 3.05676856e-02 1.31004367e-02
  7.86026201e-03 3.49344978e-03 0.00000000e+00 8.73362445e-03
  1.65938865e-02 5.24017467e-03]
 [4.00000000e-02 7.94285714e-01 1.71428571e-02 5.71428571e-03
  6.85714286e-02 4.00000000e-02 1.71428571e-02 5.71428571e-03
  0.00000000e+00 0.00000000e+00 5.71428571e-03 5.71428571e-03
  0.00000000e+00 0.00000000e+00]
 [7.09534368e-02 0.00000000e+00 7.98226164e-01 8.86917960e-03
  0.00000000e+00 2.21729490e-03 4.43458980e-02 0.00000000e+00
  2.21729490e-03 2.21729490e-03 4.43458980e-03 2.21729490e-03
  5.76496674e-02 6.65188470e-03]
 [2.51572327e-02 0.00000000e+00 1.88679245e-02 7.67295597e-01
  0.00000000e+00 3.14465409e-02 8.80503145e-02 6.28930818e-03
  0.00000000e+00 1.25786164e-02 1.88679245e-02 1.25786164e-02
  1.88679245e-02 0.00000000e+00]
 [6.84931507e-03 8.21917808e-02 6.84931507e-03 0.00000000e+00
  7.94520548e-01 7.53424658e-02 0.00000000e+00 0.00000000e+00
  1.36986301e-02 0.00000000e+00 0.00000000e+00 1.36986301e-02
  0.00000000e+00 6.84931507e-03]
 [5.98591549e-02 1.40845070e-02 7.04225352e-03 2.46478873e-02
  7.04225352e-03 7.99295775e-01 3.87323944e-02 0.00000000e+00
  1.76056338e-02 0.00000000e+00 3.52112676e-03 1.05633803e-02
  1.05633803e-02 7.04225352e-03]
 [1.44000000e-01 8.00000000e-03 1.00000000e-01 3.60000000e-02
  0.00000000e+00 1.20000000e-02 5.76000000e-01 8.00000000e-03
  0.00000000e+00 2.40000000e-02 1.20000000e-02 1.60000000e-02
  4.00000000e-02 2.40000000e-02]
 [1.20879121e-01 0.00000000e+00 3.29670330e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.09890110e-02 8.02197802e-01
  0.00000000e+00 0.00000000e+00 1.09890110e-02 1.09890110e-02
  0.00000000e+00 1.09890110e-02]
 [1.83908046e-01 0.00000000e+00 1.14942529e-02 1.14942529e-02
  0.00000000e+00 1.14942529e-02 1.14942529e-02 0.00000000e+00
  6.43678161e-01 0.00000000e+00 0.00000000e+00 1.14942529e-01
  0.00000000e+00 1.14942529e-02]
 [3.48837209e-02 0.00000000e+00 1.16279070e-02 1.16279070e-02
  0.00000000e+00 0.00000000e+00 1.27906977e-01 1.16279070e-02
  0.00000000e+00 7.79069767e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.32558140e-02]
 [6.61764706e-02 7.35294118e-03 6.61764706e-02 2.20588235e-02
  0.00000000e+00 3.67647059e-02 1.17647059e-01 1.47058824e-02
  0.00000000e+00 0.00000000e+00 6.25000000e-01 0.00000000e+00
  2.94117647e-02 1.47058824e-02]
 [1.09375000e-01 0.00000000e+00 6.25000000e-02 3.12500000e-02
  1.56250000e-02 0.00000000e+00 4.68750000e-02 1.56250000e-02
  3.12500000e-02 0.00000000e+00 0.00000000e+00 6.71875000e-01
  1.56250000e-02 0.00000000e+00]
 [2.45614035e-01 0.00000000e+00 2.10526316e-01 5.26315789e-02
  0.00000000e+00 0.00000000e+00 1.92982456e-01 0.00000000e+00
  0.00000000e+00 1.75438596e-02 1.75438596e-02 1.75438596e-02
  2.45614035e-01 0.00000000e+00]
 [9.61538462e-02 0.00000000e+00 1.92307692e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.73076923e-01 0.00000000e+00
  0.00000000e+00 3.84615385e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 6.73076923e-01]]
---------------------------------------
program finished.
