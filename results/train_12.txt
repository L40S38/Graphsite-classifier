number of classes: 10
number of epochs to train: 60
batch size: 256
number of workers to load data:  36
device:  cuda
number of gpus:  2
number of pockets in training set:  10526
number of pockets in validation set:  2252
number of pockets in test set:  2266
number of train positive pairs: 90000
number of train negative pairs: 90000
number of validation positive pairs: 27000
number of validation negative pairs: 27000
model architecture:
SiameseNet(
  (embedding_net): EmbeddingNet(
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=5, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=5, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=5, out_features=5, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
ContrastiveLoss(margin=2.0, normalize=True, mean=True)
train loss: 0.8552206175062391, validation loss: 0.8057942612259477.
train loss: 0.7804381606207954, validation loss: 0.7987108027140299.
train loss: 0.7318092666625976, validation loss: 0.8313419158370406.
train loss: 0.6998447402954101, validation loss: 0.7665303762930411.
train loss: 0.6759204416910807, validation loss: 0.7334499582361292.
train loss: 0.6504511702643501, validation loss: 0.7535602196587456.
train loss: 0.6301900182935927, validation loss: 0.7200368273699725.
train loss: 0.6112896350860596, validation loss: 0.7161648084852431.
train loss: 0.5962565007951525, validation loss: 0.7424667256673178.
train loss: 0.581456498252021, validation loss: 0.7251286776507342.
train loss: 0.5674253279791938, validation loss: 0.7171522781937211.
train loss: 0.5578121740976969, validation loss: 0.7002819714016385.
train loss: 0.5469954580518934, validation loss: 0.7080131030612522.
train loss: 0.5381997678120931, validation loss: 0.7455476096824364.
train loss: 0.530058800760905, validation loss: 0.7273173565334744.
train loss: 0.5230676617516412, validation loss: 0.7325382715861003.
train loss: 0.5187049865298801, validation loss: 0.7201782220911097.
train loss: 0.5123824545966255, validation loss: 0.7082578568635164.
train loss: 0.5045751282162136, validation loss: 0.6905199757328739.
train loss: 0.4979475419362386, validation loss: 0.7099580338089554.
train loss: 0.49479473872714574, validation loss: 0.6997316284179688.
train loss: 0.48835887383355037, validation loss: 0.6958222359551324.
train loss: 0.4861240152994792, validation loss: 0.7134076077496564.
train loss: 0.47975915722317164, validation loss: 0.7018028550324616.
train loss: 0.47943139021131725, validation loss: 0.7499145211113823.
train loss: 0.476981458791097, validation loss: 0.7190426983303494.
train loss: 0.4734580289628771, validation loss: 0.7400320680406358.
train loss: 0.4716340460459391, validation loss: 0.7221202061971028.
train loss: 0.46921587630377876, validation loss: 0.7167496662846318.
train loss: 0.4667176597595215, validation loss: 0.7168064114605939.
train loss: 0.463835476981269, validation loss: 0.7139339198359737.
train loss: 0.46024861874050566, validation loss: 0.7315390020299841.
train loss: 0.45911448474460176, validation loss: 0.7096663835313585.
train loss: 0.4577851274702284, validation loss: 0.7277407754968713.
train loss: 0.4594444147745768, validation loss: 0.7105511206167715.
train loss: 0.45620796326531304, validation loss: 0.7132595373083044.
train loss: 0.4528571554819743, validation loss: 0.7599888333921079.
train loss: 0.4513061959584554, validation loss: 0.73428069446705.
train loss: 0.4541787773132324, validation loss: 0.7245214493362991.
train loss: 0.44948277854919433, validation loss: 0.7118921929818612.
train loss: 0.4479556098514133, validation loss: 0.7385016372115524.
train loss: 0.4465192521413167, validation loss: 0.7153309193363896.
train loss: 0.4476431168874105, validation loss: 0.72025454598886.
train loss: 0.4450434693654378, validation loss: 0.7122100558810764.
train loss: 0.44381722649468314, validation loss: 0.7101408117788809.
train loss: 0.4409297014448378, validation loss: 0.7206328387790256.
train loss: 0.4419113826751709, validation loss: 0.7393211876198098.
train loss: 0.4396746818542481, validation loss: 0.6995958116319444.
train loss: 0.4372811525556776, validation loss: 0.700245785465947.
train loss: 0.4377106232961019, validation loss: 0.7674257874665437.
train loss: 0.43732990684509276, validation loss: 0.747040618331344.
train loss: 0.436336595026652, validation loss: 0.7106184717814128.
train loss: 0.43280327000088165, validation loss: 0.7160694221214012.
train loss: 0.43482092530992295, validation loss: 0.7364043991654008.
train loss: 0.43236582209269203, validation loss: 0.7239239041363752.
train loss: 0.42705802625020345, validation loss: 0.7705980764318395.
train loss: 0.43441226183573406, validation loss: 0.7407674128214519.
train loss: 0.43073611687554253, validation loss: 0.7264406464188187.
train loss: 0.4293603248596191, validation loss: 0.7487095311482748.
train loss: 0.42835721168518065, validation loss: 0.7071507147329825.
best validation loss 0.6905199757328739 at epoch 19.
