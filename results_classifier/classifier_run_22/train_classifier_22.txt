seed:  666
save trained model at:  ../trained_models/trained_classifier_model_22.pt
save loss at:  ./results/train_classifier_results_22.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  14780
number of pockets in validation set:  3162
number of pockets in test set:  3183
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2ad2a5919970>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 1.9468772667831917, acc: 0.4141407307171854, val loss: 1.789108341815425, acc: 0.4405439595192916, test loss: 1.7747866830996586, acc: 0.44234998429154887
epoch: 2, train loss: 1.6965231290205567, acc: 0.47719891745602167, val loss: 1.617463882477656, acc: 0.49968374446552816, test loss: 1.617293927566907, acc: 0.49764373232799247
epoch: 3, train loss: 1.6351765955575264, acc: 0.49742895805142084, val loss: 1.553899630046509, acc: 0.5088551549652119, test loss: 1.5450126814684926, acc: 0.5086396481306943
epoch: 4, train loss: 1.5701588453233484, acc: 0.5203653585926928, val loss: 1.5339759222545781, acc: 0.5309930423782416, test loss: 1.537325506647765, acc: 0.5249764373232799
epoch: 5, train loss: 1.5085273085492228, acc: 0.5447902571041948, val loss: 1.4707091745886902, acc: 0.5373181530676787, test loss: 1.4629467710398518, acc: 0.5403707194470625
epoch: 6, train loss: 1.479846325585258, acc: 0.5552097428958052, val loss: 1.4114926916975374, acc: 0.5683111954459203, test loss: 1.4150191884425838, acc: 0.5636192271442035
epoch: 7, train loss: 1.4205362520617948, acc: 0.5728687415426251, val loss: 1.4210165157716232, acc: 0.5547122074636306, test loss: 1.4222547880628744, acc: 0.5620483820295319
epoch: 8, train loss: 1.3960199480934299, acc: 0.5819350473612991, val loss: 1.3723186864497012, acc: 0.5765338393421885, test loss: 1.3660201537245817, acc: 0.5761859880615772
epoch: 9, train loss: 1.3206472342972504, acc: 0.6004059539918809, val loss: 1.2637839698851825, acc: 0.6087919038583175, test loss: 1.2758428569563598, acc: 0.6041470311027333
epoch: 10, train loss: 1.2863902420410445, acc: 0.607916102841678, val loss: 1.311136699341129, acc: 0.5942441492726123, test loss: 1.3100059824172634, acc: 0.5912661011624254
epoch: 11, train loss: 1.2547675334390993, acc: 0.6182002706359946, val loss: 1.2417363149316005, acc: 0.6129032258064516, test loss: 1.2369628144329237, acc: 0.6173421300659755
epoch: 12, train loss: 1.2126266620478545, acc: 0.6287550744248985, val loss: 1.1751896852484842, acc: 0.633459835547122, test loss: 1.166918289575268, acc: 0.6311655670750864
epoch: 13, train loss: 1.1759634981619973, acc: 0.6428958051420839, val loss: 1.2820162105077735, acc: 0.5882352941176471, test loss: 1.2652064319530298, acc: 0.6016336789192586
epoch: 14, train loss: 1.1622579880109176, acc: 0.6455345060893098, val loss: 1.1754323231276644, acc: 0.6397849462365591, test loss: 1.1639892463222674, acc: 0.6405906377631165
epoch: 15, train loss: 1.1157848347185106, acc: 0.663531799729364, val loss: 1.1770792723757644, acc: 0.6337760910815939, test loss: 1.159160236873381, acc: 0.6346214263273641
epoch: 16, train loss: 1.0944928453320257, acc: 0.6666441136671177, val loss: 1.0888296206641392, acc: 0.6672991777356104, test loss: 1.0797427106570268, acc: 0.6641533144831919
epoch: 17, train loss: 1.069050073397821, acc: 0.6740189445196211, val loss: 1.1572405382623256, acc: 0.6369386464263125, test loss: 1.1662599983508954, acc: 0.6440464970153943
epoch: 18, train loss: 1.0442898788697341, acc: 0.6828146143437077, val loss: 1.0848739687058837, acc: 0.6628716002530044, test loss: 1.0763858050772726, acc: 0.6707508639648131
epoch: 19, train loss: 1.0363346386342958, acc: 0.6848443843031123, val loss: 1.0761774853013273, acc: 0.6669829222011385, test loss: 1.0911390293030254, acc: 0.663210807414389
epoch: 20, train loss: 1.0313161156335606, acc: 0.6900541271989175, val loss: 1.135888705443612, acc: 0.6407337128399747, test loss: 1.1606145008457482, acc: 0.6343072573044298
epoch: 21, train loss: 1.0097655103558294, acc: 0.6944519621109607, val loss: 1.022951475809376, acc: 0.6790006325110689, test loss: 1.0238335960285116, acc: 0.6889726672950047
epoch: 22, train loss: 0.9739418803435701, acc: 0.7051420838971583, val loss: 1.0384672267161013, acc: 0.6856419987349779, test loss: 1.0454637387096226, acc: 0.6811184417216463
epoch: 23, train loss: 1.0179831483689954, acc: 0.6987821380243573, val loss: 1.0209148580103566, acc: 0.6881720430107527, test loss: 1.0197452842683639, acc: 0.6908576814326107
epoch: 24, train loss: 0.9412105858890549, acc: 0.7141407307171854, val loss: 1.0174584189673754, acc: 0.687539531941809, test loss: 1.0267076273590672, acc: 0.684574300973924
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.7344730293960468, acc: 0.7158998646820027, val loss: 0.8465822512103666, acc: 0.6739405439595193, test loss: 0.8596362117098594, acc: 0.6732642161482878
epoch: 26, train loss: 0.7194495714888359, acc: 0.7239512855209743, val loss: 0.7973938329094652, acc: 0.6894370651486401, test loss: 0.8139738925702535, acc: 0.6918001885014138
epoch: 27, train loss: 0.7043208989948639, acc: 0.7284844384303112, val loss: 0.7998068239174637, acc: 0.6951296647691335, test loss: 0.8319528675888306, acc: 0.6886584982720704
epoch: 28, train loss: 0.7114916232990477, acc: 0.7221244925575101, val loss: 0.8870405901993927, acc: 0.6543327008222644, test loss: 0.9140412309354782, acc: 0.6478165252906063
epoch: 29, train loss: 0.685604783114303, acc: 0.7343707713125845, val loss: 0.8308217180144704, acc: 0.687539531941809, test loss: 0.8779289131452181, acc: 0.6710650329877474
epoch: 30, train loss: 0.7002302339983567, acc: 0.7285520974289581, val loss: 0.8986861200592021, acc: 0.681214421252372, test loss: 0.9026106153686504, acc: 0.6804901036757776
epoch: 31, train loss: 0.6787911639323254, acc: 0.737212449255751, val loss: 0.8239763864303675, acc: 0.685325743200506, test loss: 0.8274144357830483, acc: 0.6930568645931511
epoch: 32, train loss: 0.6663579900629304, acc: 0.7384979702300406, val loss: 0.7508497132597507, acc: 0.700506008855155, test loss: 0.7855473183402092, acc: 0.6965127238454288
epoch: 33, train loss: 0.6644199124208484, acc: 0.743234100135318, val loss: 0.7857184788609215, acc: 0.7077798861480076, test loss: 0.7875182815141885, acc: 0.7034244423499842
epoch: 34, train loss: 0.6436373532221669, acc: 0.752232746955345, val loss: 0.8007528486318184, acc: 0.6951296647691335, test loss: 0.8332149891818876, acc: 0.6918001885014138
epoch: 35, train loss: 0.6350118649022021, acc: 0.7504736129905277, val loss: 0.8613909669196884, acc: 0.6903858317520557, test loss: 0.8733307421675577, acc: 0.6782909205152372
epoch: 36, train loss: 0.6222919536540247, acc: 0.7527063599458728, val loss: 0.8437782511690008, acc: 0.687539531941809, test loss: 0.9101618541474242, acc: 0.6754633993088281
epoch: 37, train loss: 0.6289509405463893, acc: 0.7475642760487144, val loss: 0.8584362923684262, acc: 0.6761543327008223, test loss: 0.8769737901906342, acc: 0.6811184417216463
epoch: 38, train loss: 0.620640787091081, acc: 0.7552774018944519, val loss: 0.8703068273871251, acc: 0.6562302340290955, test loss: 0.9179566062324421, acc: 0.6569274269557022
epoch: 39, train loss: 0.624513446542665, acc: 0.7500676589986468, val loss: 0.786126179607664, acc: 0.7043010752688172, test loss: 0.8385165905450599, acc: 0.6943135406848885
epoch: 40, train loss: 0.6015660715038624, acc: 0.7589309878213802, val loss: 0.7820734694213675, acc: 0.7033523086654017, test loss: 0.8366955339328096, acc: 0.6820609487904492
epoch: 41, train loss: 0.610402983306064, acc: 0.7608930987821381, val loss: 0.8690980175942428, acc: 0.6717267552182163, test loss: 0.8765161984977279, acc: 0.6581841030474396
epoch: 42, train loss: 0.6069554204547195, acc: 0.7596752368064953, val loss: 0.7867336844734418, acc: 0.7030360531309298, test loss: 0.8272924714593292, acc: 0.6946277097078228
epoch: 43, train loss: 0.5945466966364478, acc: 0.7606224627875507, val loss: 0.7336216241328646, acc: 0.7103099304237824, test loss: 0.7637302913719702, acc: 0.7071944706251964
epoch: 44, train loss: 0.5689554465155802, acc: 0.771786197564276, val loss: 0.7500855419018991, acc: 0.7194813409234662, test loss: 0.7697871293131751, acc: 0.7191328934967012
epoch: 45, train loss: 0.581448023938681, acc: 0.7648173207036536, val loss: 0.7381424420547968, acc: 0.7080961416824795, test loss: 0.8024041189025342, acc: 0.6933710336160854
epoch: 46, train loss: 0.5666916188109712, acc: 0.7698917456021651, val loss: 0.7495924352770135, acc: 0.7141049968374447, test loss: 0.753357306764143, acc: 0.7197612315425699
epoch: 47, train loss: 0.555610889752921, acc: 0.7807171853856563, val loss: 0.7801909615614986, acc: 0.7099936748893105, test loss: 0.8059725283527763, acc: 0.7018535972353126
epoch: 48, train loss: 0.5587848931268684, acc: 0.7767253044654939, val loss: 0.8565761545351379, acc: 0.6707779886148008, test loss: 0.9109083729047952, acc: 0.6566132579327678
epoch: 49, train loss: 0.5545783684605352, acc: 0.7801082543978349, val loss: 0.733860724187968, acc: 0.7239089184060721, test loss: 0.7613335674452475, acc: 0.7147345271756205
epoch: 50, train loss: 0.5349431962218433, acc: 0.7861975642760487, val loss: 0.7575170087482566, acc: 0.7141049968374447, test loss: 0.7740287012249428, acc: 0.7053094564875904
epoch: 51, train loss: 0.5434915061813246, acc: 0.7780108254397835, val loss: 0.8289804586800196, acc: 0.6903858317520557, test loss: 0.8296629521144924, acc: 0.6990260760289035
epoch: 52, train loss: 0.5232858197937477, acc: 0.7880243572395128, val loss: 0.8534252067520074, acc: 0.6758380771663504, test loss: 0.8765592433505188, acc: 0.6742067232170907
epoch: 53, train loss: 0.5306719642208135, acc: 0.7874830852503383, val loss: 0.7341034228405117, acc: 0.7289690069576218, test loss: 0.7558106194866778, acc: 0.7288721332076658
epoch: 54, train loss: 0.5293483284555366, acc: 0.786874154262517, val loss: 0.775107897813376, acc: 0.7270714737507906, test loss: 0.8076343227024539, acc: 0.704052780395853
epoch: 55, train loss: 0.518059626661231, acc: 0.7903924221921516, val loss: 0.8032975322004363, acc: 0.7074636306135358, test loss: 0.8039977700314056, acc: 0.7178762174049639
epoch: 56, train loss: 0.5195768544251284, acc: 0.7899188092016238, val loss: 0.8752188239197125, acc: 0.6913345983554712, test loss: 0.9721539138886227, acc: 0.6622683003455859
epoch: 57, train loss: 0.5067364316509283, acc: 0.793640054127199, val loss: 0.7802513184085969, acc: 0.7096774193548387, test loss: 0.8050550071465831, acc: 0.708765315739868
epoch: 58, train loss: 0.49683285687869877, acc: 0.7964140730717185, val loss: 0.7076171809250158, acc: 0.7318153067678684, test loss: 0.7330848883054484, acc: 0.7335846685516808
epoch: 59, train loss: 0.5055555776185048, acc: 0.7930987821380243, val loss: 0.7424332254073199, acc: 0.7378241619228336, test loss: 0.7782977367398726, acc: 0.7376688658498272
epoch: 60, train loss: 0.49375089122252147, acc: 0.7981732070365358, val loss: 0.712318976267767, acc: 0.7346616065781151, test loss: 0.7730334498692787, acc: 0.7178762174049639
epoch: 61, train loss: 0.4745408497901989, acc: 0.809404600811908, val loss: 0.7711464380026618, acc: 0.7131562302340291, test loss: 0.7798464677261175, acc: 0.7194470625196355
epoch: 62, train loss: 0.47379711938162455, acc: 0.8058186738836265, val loss: 0.7260654796006784, acc: 0.7314990512333965, test loss: 0.7505734556319764, acc: 0.728243795161797
epoch: 63, train loss: 0.4863775271685423, acc: 0.802638700947226, val loss: 0.7207558420472021, acc: 0.7318153067678684, test loss: 0.7698404439919556, acc: 0.7276154571159283
epoch: 64, train loss: 0.47755891896068486, acc: 0.8039918809201624, val loss: 0.7477048781007095, acc: 0.7362428842504743, test loss: 0.7829144375525741, acc: 0.7251021049324536
epoch: 65, train loss: 0.4596218786155742, acc: 0.8127198917456022, val loss: 0.7288856167042278, acc: 0.7296015180265655, test loss: 0.7513950408872029, acc: 0.7291863022306001
epoch: 66, train loss: 0.4468773416801783, acc: 0.8152232746955345, val loss: 0.7367210498269339, acc: 0.7305502846299811, test loss: 0.7709832319552667, acc: 0.7188187244737669
epoch: 67, train loss: 0.4657894183560218, acc: 0.8102165087956699, val loss: 0.6859389796127329, acc: 0.7400379506641366, test loss: 0.7113910580520318, acc: 0.7320138234370092
epoch: 68, train loss: 0.46853176489250586, acc: 0.8096075778078484, val loss: 0.7537469842779267, acc: 0.7280202403542062, test loss: 0.8241880337771776, acc: 0.7131636820609488
epoch: 69, train loss: 0.4734887207957824, acc: 0.8072395128552098, val loss: 0.7031125684541513, acc: 0.736875395319418, test loss: 0.7238354428249675, acc: 0.7370405278039586
epoch: 70, train loss: 0.4556504234086838, acc: 0.8129905277401894, val loss: 0.7441840391985455, acc: 0.7283364958886781, test loss: 0.756475529804044, acc: 0.7320138234370092
epoch: 71, train loss: 0.4343304397611399, acc: 0.819959404600812, val loss: 0.7420453922722929, acc: 0.7318153067678684, test loss: 0.7695402440656254, acc: 0.7260446120012567
epoch: 72, train loss: 0.43561552665391695, acc: 0.8203653585926928, val loss: 0.77675812037816, acc: 0.7213788741302973, test loss: 0.812661555215618, acc: 0.7163053722902922
epoch: 73, train loss: 0.4253637484994406, acc: 0.8231393775372124, val loss: 0.711437793716006, acc: 0.7378241619228336, test loss: 0.7459117811204801, acc: 0.7298146402764687
epoch: 74, train loss: 0.4244755177762415, acc: 0.818809201623816, val loss: 0.702176238674668, acc: 0.7333965844402277, test loss: 0.7330551648192386, acc: 0.7342130065975495
epoch: 75, train loss: 0.4363091029640141, acc: 0.8192828146143437, val loss: 0.7461354215261229, acc: 0.7397216951296648, test loss: 0.7954422253840306, acc: 0.723531259817782
epoch: 76, train loss: 0.45109440619793895, acc: 0.8154939106901218, val loss: 0.8271024647881003, acc: 0.7017710309930424, test loss: 0.832645263531056, acc: 0.6996544140747722
epoch: 77, train loss: 0.4227487400355617, acc: 0.825575101488498, val loss: 0.794336917202809, acc: 0.7201138519924098, test loss: 0.8230239127815575, acc: 0.7131636820609488
epoch: 78, train loss: 0.45171693542490793, acc: 0.8137347767253045, val loss: 0.8154050922333478, acc: 0.7011385199240987, test loss: 0.8604650565598919, acc: 0.6924285265472825
Epoch    78: reducing learning rate of group 0 to 1.5000e-03.
epoch: 79, train loss: 0.3873566244064713, acc: 0.8382949932341002, val loss: 0.6638858108713535, acc: 0.7666034155597723, test loss: 0.7153594272899957, acc: 0.7574615142946906
epoch: 80, train loss: 0.32603960803461657, acc: 0.8619756427604871, val loss: 0.6613220072184689, acc: 0.7675521821631879, test loss: 0.7048612773474604, acc: 0.7536914860194784
epoch: 81, train loss: 0.311671538806252, acc: 0.8671853856562923, val loss: 0.6694920170088496, acc: 0.7767235926628716, test loss: 0.7126429804533186, acc: 0.7693999371661954
epoch: 82, train loss: 0.304004949476464, acc: 0.8724627875507442, val loss: 0.6863546594647498, acc: 0.7640733712839974, test loss: 0.7284375739629412, acc: 0.7502356267672008
epoch: 83, train loss: 0.2896645511035506, acc: 0.8732746955345061, val loss: 0.7161064366612685, acc: 0.7640733712839974, test loss: 0.742448551340969, acc: 0.7596606974552309
epoch: 84, train loss: 0.2965883506812327, acc: 0.8738159675236806, val loss: 0.7443576071701195, acc: 0.7574320050600886, test loss: 0.7698487500668021, acc: 0.7558906691800189
epoch: 85, train loss: 0.3180347606277595, acc: 0.8656292286874154, val loss: 0.7162997873428123, acc: 0.7520556609740671, test loss: 0.7489156338017279, acc: 0.7552623311341502
epoch: 86, train loss: 0.29140787998942785, acc: 0.8778078484438431, val loss: 0.7473109267619047, acc: 0.7501581277672359, test loss: 0.7840457693645874, acc: 0.7489789506754634
epoch: 87, train loss: 0.28435018878021806, acc: 0.8767929634641407, val loss: 0.817520042524392, acc: 0.7530044275774826, test loss: 0.8681097635112766, acc: 0.7404963870562362
epoch: 88, train loss: 0.2841379637082633, acc: 0.8772665764546684, val loss: 0.744190941574753, acc: 0.767235926628716, test loss: 0.7710735782901333, acc: 0.7618598806157713
epoch: 89, train loss: 0.2890665154334495, acc: 0.8758457374830853, val loss: 0.8186231896818476, acc: 0.7387729285262492, test loss: 0.8790259910685965, acc: 0.7320138234370092
epoch: 90, train loss: 0.2799292584072108, acc: 0.8811231393775372, val loss: 0.7778852742959396, acc: 0.7583807716635041, test loss: 0.8361081261324725, acc: 0.746779767514923
epoch: 91, train loss: 0.27781684725710437, acc: 0.8768606224627875, val loss: 0.7561213630878948, acc: 0.7536369386464263, test loss: 0.7807633545726042, acc: 0.7480364436066603
epoch: 92, train loss: 0.27489325030730766, acc: 0.8776725304465494, val loss: 0.8144382427645062, acc: 0.7356103731815307, test loss: 0.826743177449295, acc: 0.7360980207351555
epoch: 93, train loss: 0.2535638889456634, acc: 0.8877537212449256, val loss: 0.8320256175307215, acc: 0.7447817836812144, test loss: 0.8752388934687627, acc: 0.7354696826892868
epoch: 94, train loss: 0.2548122574899129, acc: 0.8904600811907983, val loss: 0.7904272652516857, acc: 0.7640733712839974, test loss: 0.8368165426347153, acc: 0.7609173735469683
epoch: 95, train loss: 0.2690363551347601, acc: 0.8807171853856562, val loss: 0.8270300859744454, acc: 0.7223276407337128, test loss: 0.9035455303449808, acc: 0.7185045554508326
epoch: 96, train loss: 0.27114212472642063, acc: 0.8811231393775372, val loss: 0.7961688845463752, acc: 0.7552182163187856, test loss: 0.8003812472924399, acc: 0.7511781338360037
epoch: 97, train loss: 0.2514877515732839, acc: 0.8916102841677943, val loss: 0.7901187802627824, acc: 0.7571157495256167, test loss: 0.8142249675580551, acc: 0.7511781338360037
epoch: 98, train loss: 0.24580714580412646, acc: 0.8920162381596752, val loss: 0.8255969330451671, acc: 0.7523719165085389, test loss: 0.8586401419560189, acc: 0.7426955702167767
epoch: 99, train loss: 0.25432526845086734, acc: 0.8870094722598105, val loss: 0.7713831546259862, acc: 0.7561669829222012, test loss: 0.793346003020967, acc: 0.7571473452717562
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.19750645799872354, acc: 0.8849797023004059, val loss: 0.640422401512672, acc: 0.7700822264389627, test loss: 0.655536984558417, acc: 0.7615457115928369
epoch: 101, train loss: 0.1687995673153494, acc: 0.901150202976996, val loss: 0.6713658464927601, acc: 0.7602783048703352, test loss: 0.699550896854922, acc: 0.7470939365378574
epoch: 102, train loss: 0.177286616036308, acc: 0.8958051420838972, val loss: 0.6841812668383537, acc: 0.763124604680582, test loss: 0.7322064913034514, acc: 0.7464655984919887
epoch: 103, train loss: 0.16638769184381616, acc: 0.899255751014885, val loss: 0.7002003378388552, acc: 0.7596457938013915, test loss: 0.7402489451990538, acc: 0.7527489789506755
epoch: 104, train loss: 0.1580348242159948, acc: 0.9063599458728011, val loss: 0.6997955729130474, acc: 0.7602783048703352, test loss: 0.7442607129852314, acc: 0.7511781338360037
epoch: 105, train loss: 0.17107130453212335, acc: 0.8979025710419486, val loss: 0.6618110769387667, acc: 0.7621758380771664, test loss: 0.7420199026373846, acc: 0.7426955702167767
epoch: 106, train loss: 0.1767160298859799, acc: 0.8968876860622462, val loss: 0.6838015973907268, acc: 0.7514231499051234, test loss: 0.7365967249143734, acc: 0.7511781338360037
epoch: 107, train loss: 0.1649461707299391, acc: 0.8985115020297699, val loss: 0.7151402540405968, acc: 0.7586970271979759, test loss: 0.7725075735373201, acc: 0.7540056550424128
epoch: 108, train loss: 0.16869599807488256, acc: 0.8963464140730717, val loss: 0.7476478757773827, acc: 0.7498418722327641, test loss: 0.7790362400114256, acc: 0.7426955702167767
epoch: 109, train loss: 0.16471912605594718, acc: 0.8968876860622462, val loss: 0.6858265112654308, acc: 0.75426944971537, test loss: 0.7210266307731339, acc: 0.7461514294690543
epoch: 110, train loss: 0.16599841924820283, acc: 0.8976995940460081, val loss: 0.6407623691395972, acc: 0.7729285262492094, test loss: 0.6909352969044678, acc: 0.7596606974552309
epoch: 111, train loss: 0.16357221118652127, acc: 0.898105548037889, val loss: 0.6973096586193637, acc: 0.7586970271979759, test loss: 0.7316834861438978, acc: 0.7530631479736098
epoch: 112, train loss: 0.17336365632581777, acc: 0.8916779431664411, val loss: 0.7409482216397695, acc: 0.7517394054395952, test loss: 0.7919612923321617, acc: 0.7370405278039586
epoch: 113, train loss: 0.17806947200643516, acc: 0.8916102841677943, val loss: 0.6720625108589785, acc: 0.763124604680582, test loss: 0.6930375874435756, acc: 0.7555765001570846
epoch: 114, train loss: 0.17000982195582537, acc: 0.8952638700947226, val loss: 0.7001372351818338, acc: 0.7416192283364959, test loss: 0.7395448627765546, acc: 0.7348413446434181
epoch: 115, train loss: 0.17670715051187713, acc: 0.8876860622462788, val loss: 0.6868516221821572, acc: 0.7602783048703352, test loss: 0.7441546161183014, acc: 0.7552623311341502
epoch: 116, train loss: 0.15937043394948858, acc: 0.9009472259810555, val loss: 0.6757929794853206, acc: 0.7729285262492094, test loss: 0.7332997514585111, acc: 0.7571473452717562
epoch: 117, train loss: 0.1487213775172769, acc: 0.906765899864682, val loss: 0.6660359292600367, acc: 0.7751423149905123, test loss: 0.7333423200733737, acc: 0.7562048382029531
epoch: 118, train loss: 0.16018356051149807, acc: 0.8983085250338295, val loss: 0.6889913760733559, acc: 0.7558507273877293, test loss: 0.7489535291352962, acc: 0.7530631479736098
epoch: 119, train loss: 0.16251969026131946, acc: 0.9002029769959404, val loss: 0.6849809828474128, acc: 0.7523719165085389, test loss: 0.7747363012765063, acc: 0.74583726044612
epoch: 120, train loss: 0.151267234914519, acc: 0.9052774018944519, val loss: 0.7000936853365383, acc: 0.7669196710942442, test loss: 0.7682046650566846, acc: 0.7546339930882815
epoch: 121, train loss: 0.17330837332428067, acc: 0.8928281461434371, val loss: 0.7173561745244894, acc: 0.7514231499051234, test loss: 0.7696676929155086, acc: 0.7379830348727615
epoch: 122, train loss: 0.17812895988982488, acc: 0.8901217861975643, val loss: 0.656999291535196, acc: 0.7583807716635041, test loss: 0.7113462896643664, acc: 0.7511781338360037
epoch: 123, train loss: 0.15967358739514795, acc: 0.9002029769959404, val loss: 0.6859528469481398, acc: 0.7748260594560404, test loss: 0.739646877165377, acc: 0.7590323594093622
epoch: 124, train loss: 0.14561081407679918, acc: 0.9064952638700947, val loss: 0.6974697713984634, acc: 0.7675521821631879, test loss: 0.768378871849712, acc: 0.7502356267672008
epoch: 125, train loss: 0.1463486326765143, acc: 0.9077131258457375, val loss: 0.7445242587408637, acc: 0.7441492726122707, test loss: 0.775352283127833, acc: 0.7398680490103676
epoch: 126, train loss: 0.15568175567099787, acc: 0.9002706359945872, val loss: 0.745698552204038, acc: 0.7425679949399114, test loss: 0.7963164675509147, acc: 0.7360980207351555
epoch: 127, train loss: 0.1595581364244505, acc: 0.8995940460081191, val loss: 0.6624450940711826, acc: 0.7552182163187856, test loss: 0.7354583206919802, acc: 0.7452089224002514
epoch: 128, train loss: 0.16095062286992518, acc: 0.8991880920162382, val loss: 0.7489380160892107, acc: 0.7356103731815307, test loss: 0.7895097519817347, acc: 0.7342130065975495
epoch: 129, train loss: 0.17971428020155317, acc: 0.8880243572395129, val loss: 0.6593441458007793, acc: 0.7643896268184693, test loss: 0.7071721982251837, acc: 0.7580898523405593
Epoch   129: reducing learning rate of group 0 to 7.5000e-04.
epoch: 130, train loss: 0.12397589537948975, acc: 0.9152232746955345, val loss: 0.6533241526828696, acc: 0.7808349146110057, test loss: 0.7031056384713703, acc: 0.7690857681432611
epoch: 131, train loss: 0.09366801523871932, acc: 0.9336941813261164, val loss: 0.6863400423398582, acc: 0.7871600253004427, test loss: 0.7569842098870069, acc: 0.7728557964184731
epoch: 132, train loss: 0.08065218130209771, acc: 0.939851150202977, val loss: 0.7364936279089371, acc: 0.7852624920936117, test loss: 0.7880373986457228, acc: 0.7697141061891297
epoch: 133, train loss: 0.08141374279504215, acc: 0.9409336941813261, val loss: 0.7572288953526574, acc: 0.7751423149905123, test loss: 0.8083530633358921, acc: 0.7665724159597863
epoch: 134, train loss: 0.07903593718118371, acc: 0.9434370771312585, val loss: 0.7557405389463653, acc: 0.7748260594560404, test loss: 0.8085649544961266, acc: 0.7599748664781653
epoch: 135, train loss: 0.07690089589567081, acc: 0.9430311231393775, val loss: 0.794703928297792, acc: 0.7685009487666035, test loss: 0.8128988451452851, acc: 0.7596606974552309
epoch: 136, train loss: 0.08247122842218621, acc: 0.9408660351826793, val loss: 0.7653525787995331, acc: 0.777988614800759, test loss: 0.8207007348069296, acc: 0.7665724159597863
epoch: 137, train loss: 0.0823229317816733, acc: 0.940595399188092, val loss: 0.7941775584356608, acc: 0.773877292852625, test loss: 0.8257998799588596, acc: 0.760603204524034
epoch: 138, train loss: 0.07725606050155805, acc: 0.9441136671177267, val loss: 0.7870326378464624, acc: 0.7805186590765338, test loss: 0.8438098437374881, acc: 0.7609173735469683
epoch: 139, train loss: 0.07393806704165003, acc: 0.944722598105548, val loss: 0.8349253706054398, acc: 0.7754585705249842, test loss: 0.834390667673555, acc: 0.7631165567075087
epoch: 140, train loss: 0.08348638965005965, acc: 0.9403924221921516, val loss: 0.7890446694269729, acc: 0.773877292852625, test loss: 0.8542660771770519, acc: 0.764373232799246
epoch: 141, train loss: 0.07901918378851411, acc: 0.9440460081190798, val loss: 0.7803058711732964, acc: 0.7795698924731183, test loss: 0.8200105751156245, acc: 0.7690857681432611
epoch: 142, train loss: 0.07398418782528424, acc: 0.9460757780784844, val loss: 0.8023091434753823, acc: 0.7820999367488931, test loss: 0.872402724326874, acc: 0.7646874018221803
epoch: 143, train loss: 0.08211009570968328, acc: 0.939106901217862, val loss: 0.8156809800791635, acc: 0.7697659709044908, test loss: 0.9025188583717981, acc: 0.7555765001570846
epoch: 144, train loss: 0.08082974437972044, acc: 0.9430987821380243, val loss: 0.8040422903141743, acc: 0.7624920936116382, test loss: 0.8503484630824408, acc: 0.7590323594093622
epoch: 145, train loss: 0.08522140346013805, acc: 0.9400541271989175, val loss: 0.7835213899763229, acc: 0.7767235926628716, test loss: 0.8156272998741503, acc: 0.767200754005655
epoch: 146, train loss: 0.08192512152038504, acc: 0.9435723951285521, val loss: 0.7962185178354976, acc: 0.773877292852625, test loss: 0.8612767866561895, acc: 0.7599748664781653
epoch: 147, train loss: 0.08493561788848998, acc: 0.939106901217862, val loss: 0.775393218197904, acc: 0.7681846932321316, test loss: 0.8280109900032017, acc: 0.7678290920515237
epoch: 148, train loss: 0.07529050815895866, acc: 0.9441136671177267, val loss: 0.7899472013747368, acc: 0.7659709044908286, test loss: 0.8530014343992779, acc: 0.7584040213634936
epoch: 149, train loss: 0.06797511858288426, acc: 0.9486468200270636, val loss: 0.8253900578020495, acc: 0.7776723592662872, test loss: 0.8833904935996786, acc: 0.7612315425699026
epoch: 150, train loss: 0.07158762107481008, acc: 0.9460757780784844, val loss: 0.8099745835479795, acc: 0.7716635041113219, test loss: 0.8658199145514524, acc: 0.7668865849827207
epoch: 151, train loss: 0.07828430776183111, acc: 0.9424221921515562, val loss: 0.8358824903040578, acc: 0.7605945604048071, test loss: 0.8878045750233575, acc: 0.7514923028589381
epoch: 152, train loss: 0.09674879249402087, acc: 0.9332205683355886, val loss: 0.7749608795074328, acc: 0.7669196710942442, test loss: 0.8295579794776765, acc: 0.7524348099277411
epoch: 153, train loss: 0.07727016628756414, acc: 0.9433017591339649, val loss: 0.774018741875491, acc: 0.7751423149905123, test loss: 0.8228152680614255, acc: 0.7618598806157713
epoch: 154, train loss: 0.07336117880915434, acc: 0.9458051420838972, val loss: 0.8087689066747561, acc: 0.7741935483870968, test loss: 0.8556554692742063, acc: 0.760603204524034
epoch: 155, train loss: 0.06265211115025376, acc: 0.9535182679296347, val loss: 0.8173771610296202, acc: 0.7722960151802657, test loss: 0.8908612701902591, acc: 0.7615457115928369
epoch: 156, train loss: 0.07065643591450903, acc: 0.9495263870094722, val loss: 0.8253166925899595, acc: 0.7691334598355472, test loss: 0.8995631076688104, acc: 0.7659440779139177
epoch: 157, train loss: 0.07913870623736001, acc: 0.9416102841677944, val loss: 0.8043731266301618, acc: 0.7647058823529411, test loss: 0.864123384045461, acc: 0.7518064718818724
epoch: 158, train loss: 0.0703501444264581, acc: 0.9477672530446549, val loss: 0.8406943069991245, acc: 0.7596457938013915, test loss: 0.8865006784204787, acc: 0.7580898523405593
epoch: 159, train loss: 0.08368049409134623, acc: 0.9391745602165088, val loss: 0.8167947395778321, acc: 0.7751423149905123, test loss: 0.880628633229492, acc: 0.7508639648130694
epoch: 160, train loss: 0.0758255210575296, acc: 0.9416779431664412, val loss: 0.8213492441147208, acc: 0.7697659709044908, test loss: 0.8534856498747024, acc: 0.7612315425699026
epoch: 161, train loss: 0.07298645391072246, acc: 0.9461434370771312, val loss: 0.821037981044183, acc: 0.7770398481973435, test loss: 0.8999420127664914, acc: 0.7590323594093622
epoch: 162, train loss: 0.0765772212442913, acc: 0.9454668470906631, val loss: 0.8248972702750219, acc: 0.7647058823529411, test loss: 0.8847469142724462, acc: 0.7571473452717562
epoch: 163, train loss: 0.06416029996857431, acc: 0.9512178619756427, val loss: 0.8319575290450713, acc: 0.7783048703352309, test loss: 0.8736845533914473, acc: 0.7580898523405593
epoch: 164, train loss: 0.07000050652817558, acc: 0.9476319350473613, val loss: 0.9472624827305929, acc: 0.7444655281467426, test loss: 1.0145518863646819, acc: 0.7323279924599434
epoch: 165, train loss: 0.09727600773189, acc: 0.934979702300406, val loss: 0.8142316478630321, acc: 0.7691334598355472, test loss: 0.8628560743203075, acc: 0.7593465284322966
epoch: 166, train loss: 0.07414168298183985, acc: 0.9432341001353179, val loss: 0.8410054821518688, acc: 0.7729285262492094, test loss: 0.8971978213627534, acc: 0.7533773169965441
epoch: 167, train loss: 0.06330556621086936, acc: 0.9504059539918809, val loss: 0.8115863616069912, acc: 0.7754585705249842, test loss: 0.858342733691782, acc: 0.764373232799246
epoch: 168, train loss: 0.06510815712470808, acc: 0.9531123139377538, val loss: 0.8778852101446027, acc: 0.7694497153700189, test loss: 0.9111684823313337, acc: 0.7543198240653471
epoch: 169, train loss: 0.06309508876081087, acc: 0.9551420838971583, val loss: 0.860160762791691, acc: 0.7609108159392789, test loss: 0.9018889538641812, acc: 0.7499214577442664
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.06710060524158129, acc: 0.939106901217862, val loss: 0.7569744713618588, acc: 0.7691334598355472, test loss: 0.7906387342883849, acc: 0.7584040213634936
epoch: 171, train loss: 0.07822216443909037, acc: 0.9308525033829499, val loss: 0.6539157345504568, acc: 0.7688172043010753, test loss: 0.7445217517423734, acc: 0.7552623311341502
epoch: 172, train loss: 0.0493864335230626, acc: 0.9479025710419485, val loss: 0.6767225953915842, acc: 0.7745098039215687, test loss: 0.7495186722132533, acc: 0.7574615142946906
epoch: 173, train loss: 0.04658360489226968, acc: 0.9518267929634642, val loss: 0.707037831921128, acc: 0.767235926628716, test loss: 0.7308180346790065, acc: 0.7621740496387056
epoch: 174, train loss: 0.04478524133805009, acc: 0.953044654939107, val loss: 0.6829796933735721, acc: 0.7764073371283997, test loss: 0.7588517469615259, acc: 0.7668865849827207
epoch: 175, train loss: 0.044153364180147404, acc: 0.9535859269282815, val loss: 0.704051972641966, acc: 0.7773561037318153, test loss: 0.7748586515042529, acc: 0.7650015708451147
epoch: 176, train loss: 0.042178379838053684, acc: 0.95148849797023, val loss: 0.7024062762601251, acc: 0.7786211258697027, test loss: 0.7615609079496388, acc: 0.7596606974552309
epoch: 177, train loss: 0.040447243237527686, acc: 0.9542625169147496, val loss: 0.7588377504089375, acc: 0.7688172043010753, test loss: 0.814878553554692, acc: 0.7571473452717562
epoch: 178, train loss: 0.04525356629059344, acc: 0.9497970230040595, val loss: 0.7342495301492149, acc: 0.7675521821631879, test loss: 0.8077563794573784, acc: 0.7637448947533774
epoch: 179, train loss: 0.052403758280735374, acc: 0.9467523680649527, val loss: 0.7420546171561138, acc: 0.773877292852625, test loss: 0.7864800610334065, acc: 0.760603204524034
epoch: 180, train loss: 0.04773176811403609, acc: 0.94851150202977, val loss: 0.7513807818227596, acc: 0.7634408602150538, test loss: 0.8017143329059482, acc: 0.7555765001570846
Epoch   180: reducing learning rate of group 0 to 3.7500e-04.
epoch: 181, train loss: 0.036445108927151186, acc: 0.959742895805142, val loss: 0.720938437502268, acc: 0.7751423149905123, test loss: 0.7805550259462213, acc: 0.7706566132579328
epoch: 182, train loss: 0.02921622406303157, acc: 0.9652909336941813, val loss: 0.7210779030212943, acc: 0.7751423149905123, test loss: 0.787883110552736, acc: 0.765315739868049
epoch: 183, train loss: 0.028658685164276575, acc: 0.96468200270636, val loss: 0.7328454282448158, acc: 0.7811511701454775, test loss: 0.7978758489864636, acc: 0.767200754005655
epoch: 184, train loss: 0.02427359888977869, acc: 0.9677943166441136, val loss: 0.7482348734031065, acc: 0.7770398481973435, test loss: 0.8093109250705346, acc: 0.764373232799246
epoch: 185, train loss: 0.025784776410710506, acc: 0.9674560216508795, val loss: 0.7616372440224731, acc: 0.7783048703352309, test loss: 0.8312176658490751, acc: 0.7621740496387056
epoch: 186, train loss: 0.025200225638681885, acc: 0.968064952638701, val loss: 0.779239386565168, acc: 0.7814674256799494, test loss: 0.8355451247394741, acc: 0.7662582469368521
epoch: 187, train loss: 0.02487255905998092, acc: 0.9706359945872801, val loss: 0.7658060173080512, acc: 0.7820999367488931, test loss: 0.8286166766361737, acc: 0.7668865849827207
epoch: 188, train loss: 0.02405768875511557, acc: 0.9699594046008119, val loss: 0.7792307330716041, acc: 0.7764073371283997, test loss: 0.8352074339971803, acc: 0.7675149230285894
epoch: 189, train loss: 0.023938186595995226, acc: 0.9683355886332883, val loss: 0.7825707861473861, acc: 0.7767235926628716, test loss: 0.8311380858395858, acc: 0.7684574300973924
epoch: 190, train loss: 0.023667088001764838, acc: 0.9696887686062247, val loss: 0.7858691431162554, acc: 0.7773561037318153, test loss: 0.8525260479765571, acc: 0.7637448947533774
epoch: 191, train loss: 0.02291058259682178, acc: 0.9707713125845737, val loss: 0.780177701356515, acc: 0.773877292852625, test loss: 0.8512156132516193, acc: 0.765315739868049
epoch: 192, train loss: 0.023807110230618628, acc: 0.9702300405953992, val loss: 0.8191561199757662, acc: 0.7776723592662872, test loss: 0.8656081109162288, acc: 0.7687715991203268
epoch: 193, train loss: 0.02466880170091563, acc: 0.9694857916102841, val loss: 0.7917740581760974, acc: 0.7748260594560404, test loss: 0.854086294288048, acc: 0.765315739868049
epoch: 194, train loss: 0.027397513656759777, acc: 0.9667117726657646, val loss: 0.7920133637748588, acc: 0.7748260594560404, test loss: 0.8485657067707864, acc: 0.7678290920515237
epoch: 195, train loss: 0.02646930593195723, acc: 0.9677266576454668, val loss: 0.8496627678681143, acc: 0.7574320050600886, test loss: 0.9065908378600475, acc: 0.7508639648130694
epoch: 196, train loss: 0.024830289619580333, acc: 0.9686062246278755, val loss: 0.8109521045479723, acc: 0.7745098039215687, test loss: 0.8620883836485551, acc: 0.7656299088909834
epoch: 197, train loss: 0.02993535604756401, acc: 0.965764546684709, val loss: 0.7990647562691714, acc: 0.7760910815939279, test loss: 0.8336737707954812, acc: 0.7609173735469683
epoch: 198, train loss: 0.02808395099494712, acc: 0.9648849797023004, val loss: 0.806984227341538, acc: 0.7688172043010753, test loss: 0.8657083744303146, acc: 0.7621740496387056
epoch: 199, train loss: 0.028239348820122072, acc: 0.9653585926928282, val loss: 0.7877854335768022, acc: 0.7729285262492094, test loss: 0.8392228662986407, acc: 0.7577756833176249
epoch: 200, train loss: 0.027139940315476934, acc: 0.9670500676589987, val loss: 0.7900019385707597, acc: 0.7751423149905123, test loss: 0.8395086607392239, acc: 0.7612315425699026
best val acc 0.7871600253004427 at epoch 131.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9793    0.9910    0.9851      5337
           1     0.9858    0.9457    0.9653       810
           2     0.9529    0.9833    0.9679      2100
           3     0.9753    0.9634    0.9693       737
           4     0.9395    0.9867    0.9625       677
           5     0.9790    0.9887    0.9838      1323
           6     0.9365    0.9115    0.9238      1164
           7     0.9532    0.9667    0.9599       421
           8     0.9520    0.9900    0.9707       401
           9     0.9607    0.9874    0.9738       396
          10     0.9837    0.9601    0.9718       627
          11     0.9857    0.9485    0.9667       291
          12     0.8812    0.5402    0.6698       261
          13     0.9241    0.8809    0.9020       235

    accuracy                         0.9667     14780
   macro avg     0.9564    0.9317    0.9409     14780
weighted avg     0.9662    0.9667    0.9656     14780

train confusion matrix:
[[9.91006183e-01 0.00000000e+00 7.49484729e-04 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.31159828e-03 3.74742365e-03
  2.81056773e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.74742365e-04 0.00000000e+00]
 [0.00000000e+00 9.45679012e-01 0.00000000e+00 0.00000000e+00
  4.93827160e-02 2.46913580e-03 0.00000000e+00 0.00000000e+00
  1.23456790e-03 0.00000000e+00 1.23456790e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.42857143e-02 0.00000000e+00 9.83333333e-01 9.52380952e-04
  0.00000000e+00 4.76190476e-04 4.76190476e-04 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.76190476e-04 0.00000000e+00]
 [1.35685210e-03 0.00000000e+00 9.49796472e-03 9.63364993e-01
  0.00000000e+00 5.42740841e-03 6.78426052e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.08548168e-02 2.71370421e-03
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 8.86262925e-03 0.00000000e+00 0.00000000e+00
  9.86706056e-01 4.43131462e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.26757370e-03 2.26757370e-03 7.55857899e-04 2.26757370e-03
  2.26757370e-03 9.88662132e-01 0.00000000e+00 0.00000000e+00
  1.51171580e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.31958763e-02 0.00000000e+00 7.73195876e-03 1.71821306e-03
  0.00000000e+00 1.37457045e-02 9.11512027e-01 0.00000000e+00
  8.59106529e-04 1.37457045e-02 8.59106529e-04 0.00000000e+00
  1.28865979e-02 1.37457045e-02]
 [2.61282660e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.66745843e-01
  2.37529691e-03 0.00000000e+00 0.00000000e+00 2.37529691e-03
  2.37529691e-03 0.00000000e+00]
 [7.48129676e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.90024938e-01 0.00000000e+00 0.00000000e+00 2.49376559e-03
  0.00000000e+00 0.00000000e+00]
 [2.52525253e-03 0.00000000e+00 2.52525253e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 7.57575758e-03 0.00000000e+00
  0.00000000e+00 9.87373737e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.59489633e-03 6.37958533e-03 1.43540670e-02
  0.00000000e+00 0.00000000e+00 1.59489633e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.60127592e-01 0.00000000e+00
  0.00000000e+00 1.59489633e-03]
 [5.15463918e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.48453608e-01
  0.00000000e+00 0.00000000e+00]
 [7.66283525e-02 0.00000000e+00 2.91187739e-01 7.66283525e-03
  0.00000000e+00 7.66283525e-03 7.66283525e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  5.40229885e-01 0.00000000e+00]
 [4.25531915e-03 4.25531915e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.10638298e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 8.80851064e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8438    0.8836    0.8632      1143
           1     0.8824    0.8671    0.8746       173
           2     0.7565    0.7733    0.7648       450
           3     0.7736    0.7785    0.7760       158
           4     0.8299    0.8414    0.8356       145
           5     0.8049    0.8163    0.8105       283
           6     0.5578    0.5622    0.5600       249
           7     0.8415    0.7667    0.8023        90
           8     0.6042    0.6824    0.6409        85
           9     0.8372    0.8571    0.8471        84
          10     0.8099    0.7313    0.7686       134
          11     0.7021    0.5323    0.6055        62
          12     0.2593    0.1250    0.1687        56
          13     0.8750    0.5600    0.6829        50

    accuracy                         0.7872      3162
   macro avg     0.7413    0.6984    0.7143      3162
weighted avg     0.7826    0.7872    0.7833      3162

validation confusion matrix:
[[0.88363955 0.00262467 0.03237095 0.00437445 0.         0.00262467
  0.03324584 0.00612423 0.02012248 0.00174978 0.00174978 0.00787402
  0.00349956 0.        ]
 [0.01734104 0.86705202 0.00578035 0.         0.06358382 0.01734104
  0.01156069 0.00578035 0.         0.00578035 0.00578035 0.
  0.         0.        ]
 [0.12       0.         0.77333333 0.01777778 0.00222222 0.02
  0.03333333 0.00222222 0.         0.00222222 0.01111111 0.00444444
  0.01111111 0.00222222]
 [0.01265823 0.01265823 0.08227848 0.77848101 0.         0.01898734
  0.03797468 0.01898734 0.         0.         0.03797468 0.
  0.         0.        ]
 [0.00689655 0.02758621 0.         0.0137931  0.84137931 0.09655172
  0.         0.         0.00689655 0.         0.         0.
  0.00689655 0.        ]
 [0.04240283 0.01060071 0.02120141 0.01766784 0.04240283 0.81625442
  0.02120141 0.         0.01766784 0.         0.00706714 0.00353357
  0.         0.        ]
 [0.17269076 0.00401606 0.09638554 0.02811245 0.00401606 0.04016064
  0.562249   0.         0.00401606 0.03614458 0.01204819 0.00401606
  0.02811245 0.00803213]
 [0.16666667 0.         0.02222222 0.         0.         0.
  0.03333333 0.76666667 0.01111111 0.         0.         0.
  0.         0.        ]
 [0.16470588 0.01176471 0.         0.         0.         0.09411765
  0.02352941 0.         0.68235294 0.         0.01176471 0.01176471
  0.         0.        ]
 [0.03571429 0.         0.04761905 0.         0.         0.
  0.04761905 0.         0.         0.85714286 0.         0.
  0.         0.01190476]
 [0.05223881 0.02238806 0.01492537 0.00746269 0.         0.02985075
  0.09701493 0.         0.02238806 0.         0.73134328 0.
  0.02238806 0.        ]
 [0.29032258 0.0483871  0.03225806 0.01612903 0.         0.
  0.         0.01612903 0.0483871  0.         0.01612903 0.53225806
  0.         0.        ]
 [0.19642857 0.         0.30357143 0.10714286 0.         0.01785714
  0.17857143 0.         0.01785714 0.01785714 0.03571429 0.
  0.125      0.        ]
 [0.08       0.         0.08       0.02       0.         0.02
  0.24       0.         0.         0.         0.         0.
  0.         0.56      ]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8409    0.8725    0.8564      1145
           1     0.8466    0.7886    0.8166       175
           2     0.7560    0.7627    0.7594       451
           3     0.8037    0.8239    0.8137       159
           4     0.8239    0.8014    0.8125       146
           5     0.7945    0.8169    0.8056       284
           6     0.5498    0.5520    0.5509       250
           7     0.7444    0.7363    0.7403        91
           8     0.5490    0.6437    0.5926        87
           9     0.8111    0.8488    0.8295        86
          10     0.8142    0.6765    0.7390       136
          11     0.7021    0.5156    0.5946        64
          12     0.1818    0.1404    0.1584        57
          13     0.7442    0.6154    0.6737        52

    accuracy                         0.7729      3183
   macro avg     0.7116    0.6853    0.6959      3183
weighted avg     0.7707    0.7729    0.7708      3183

test confusion matrix:
[[0.87248908 0.00174672 0.02620087 0.00436681 0.00087336 0.00786026
  0.0279476  0.0139738  0.01834061 0.00349345 0.00436681 0.00786026
  0.0069869  0.00349345]
 [0.04571429 0.78857143 0.01142857 0.         0.05714286 0.04571429
  0.00571429 0.         0.02285714 0.         0.01714286 0.00571429
  0.         0.        ]
 [0.10643016 0.00665188 0.76274945 0.01552106 0.00221729 0.00665188
  0.06430155 0.00443459 0.00221729 0.00221729 0.00443459 0.
  0.01995565 0.00221729]
 [0.02515723 0.         0.04402516 0.82389937 0.         0.03144654
  0.03144654 0.         0.         0.         0.01886792 0.00628931
  0.01886792 0.        ]
 [0.01369863 0.04109589 0.00684932 0.         0.80136986 0.12328767
  0.         0.         0.00684932 0.         0.         0.00684932
  0.         0.        ]
 [0.03873239 0.02816901 0.02816901 0.00704225 0.03169014 0.81690141
  0.02464789 0.         0.02112676 0.         0.00352113 0.
  0.         0.        ]
 [0.132      0.004      0.132      0.032      0.008      0.016
  0.552      0.012      0.012      0.028      0.012      0.004
  0.04       0.016     ]
 [0.21978022 0.         0.02197802 0.         0.         0.
  0.01098901 0.73626374 0.01098901 0.         0.         0.
  0.         0.        ]
 [0.2183908  0.01149425 0.01149425 0.         0.01149425 0.04597701
  0.03448276 0.01149425 0.64367816 0.         0.         0.01149425
  0.         0.        ]
 [0.04651163 0.         0.         0.         0.         0.
  0.08139535 0.         0.         0.84883721 0.         0.
  0.01162791 0.01162791]
 [0.05147059 0.02205882 0.03676471 0.03676471 0.         0.02941176
  0.07352941 0.00735294 0.02205882 0.00735294 0.67647059 0.
  0.02941176 0.00735294]
 [0.203125   0.         0.03125    0.015625   0.015625   0.046875
  0.046875   0.         0.0625     0.03125    0.015625   0.515625
  0.015625   0.        ]
 [0.24561404 0.         0.31578947 0.05263158 0.         0.03508772
  0.0877193  0.         0.03508772 0.03508772 0.05263158 0.
  0.14035088 0.        ]
 [0.11538462 0.01923077 0.03846154 0.01923077 0.         0.
  0.19230769 0.         0.         0.         0.         0.
  0.         0.61538462]]
---------------------------------------
program finished.
