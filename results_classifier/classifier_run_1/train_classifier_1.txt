seed:  666
save trained model at:  ../trained_models/trained_classifier_model_1.pt
save loss at:  ./results/train_classifier_results_1.json
how to merge clusters:  [[0, 9, 12], [1, 5, 11], 2, [3, 8, 13], 4, 6, 7, [10, 16], 15, 17, 18]
features to use:  ['r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  120
learning rate decay at epoch:  60
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  11
number of pockets in training set:  14097
number of pockets in validation set:  3016
number of pockets in test set:  3031
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=8, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=8, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=11, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
loss function:
CrossEntropyLoss()
begin training...
epoch: 1, train loss: 2.2959010652712055, acc: 0.14974817337022062, val loss: 2.208689052166926, acc: 0.15318302387267904, test loss: 2.192860700320348, acc: 0.15902342461233915
epoch: 2, train loss: 2.0542106979356856, acc: 0.22146556004823723, val loss: 2.5158408061262785, acc: 0.15218832891246684, test loss: 2.5273072942968486, acc: 0.1534147146156384
epoch: 3, train loss: 1.949894850665207, acc: 0.2609775129460169, val loss: 2.1847181958924553, acc: 0.20689655172413793, test loss: 2.1874717178615315, acc: 0.2157703728142527
epoch: 4, train loss: 1.828958391898015, acc: 0.29552387032701993, val loss: 2.323404885729681, acc: 0.21187002652519893, test loss: 2.3344676105231192, acc: 0.21544044869679974
epoch: 5, train loss: 1.792424387869923, acc: 0.2970135489820529, val loss: 1.8376436410595314, acc: 0.3047082228116711, test loss: 1.8548670760557149, acc: 0.28241504453975586
epoch: 6, train loss: 1.7164380898786502, acc: 0.32545931758530183, val loss: 1.719343691985234, acc: 0.35046419098143233, test loss: 1.7350881420240587, acc: 0.34147146156384034
epoch: 7, train loss: 1.635075010906308, acc: 0.34333546144569765, val loss: 1.8485419525708064, acc: 0.30437665782493367, test loss: 1.846353076582732, acc: 0.30847904981854174
epoch: 8, train loss: 1.571450192175529, acc: 0.35532382776477267, val loss: 1.767859514259217, acc: 0.3255968169761273, test loss: 1.8011952804205482, acc: 0.32002639392939625
epoch: 9, train loss: 1.5388597347655686, acc: 0.38760019862382067, val loss: 1.7813353867366397, acc: 0.34681697612732093, test loss: 1.7865302170200026, acc: 0.35697789508413064
epoch: 10, train loss: 1.5127849314173392, acc: 0.3916436121160531, val loss: 1.7486818342689494, acc: 0.3789787798408488, test loss: 1.7216423872633366, acc: 0.3794127350709337
epoch: 11, train loss: 1.459456183609831, acc: 0.43002057175285524, val loss: 1.8740596455035223, acc: 0.3133289124668435, test loss: 1.8846956585469083, acc: 0.31276806334543056
epoch: 12, train loss: 1.405315014405598, acc: 0.43555366390012057, val loss: 1.4123465686008847, acc: 0.488395225464191, test loss: 1.4264173858120905, acc: 0.48036951501154734
epoch: 13, train loss: 1.350169191971361, acc: 0.44541391785486273, val loss: 1.3411938499076297, acc: 0.501657824933687, test loss: 1.3670024472233173, acc: 0.5051138238205213
epoch: 14, train loss: 1.341643043880878, acc: 0.464354117897425, val loss: 1.5609972084865014, acc: 0.4244031830238727, test loss: 1.5553694449411843, acc: 0.42758165621906963
epoch: 15, train loss: 1.2967065511599174, acc: 0.46215506845428106, val loss: 1.4445436313866937, acc: 0.4953580901856764, test loss: 1.4594542889341746, acc: 0.49455625206202575
epoch: 16, train loss: 1.2902810804436475, acc: 0.4783287224232106, val loss: 1.4948387027419214, acc: 0.46750663129973474, test loss: 1.511238972745843, acc: 0.47344110854503463
epoch: 17, train loss: 1.2430563252462836, acc: 0.4898205291906079, val loss: 1.3278265848400106, acc: 0.5089522546419099, test loss: 1.368568465934654, acc: 0.4929066314747608
epoch: 18, train loss: 1.2234624193992785, acc: 0.5006029651698943, val loss: 1.401324930494597, acc: 0.496684350132626, test loss: 1.422298038332268, acc: 0.4797096667766414
epoch: 19, train loss: 1.2050718005400156, acc: 0.5117400865432361, val loss: 1.2940434530495966, acc: 0.541445623342175, test loss: 1.303207490319114, acc: 0.5519630484988453
epoch: 20, train loss: 1.1901844905827699, acc: 0.5067744910264596, val loss: 1.3526458580550844, acc: 0.5155835543766578, test loss: 1.3682703339042777, acc: 0.5268888155724183
epoch: 21, train loss: 1.2385817271746278, acc: 0.4977654820174505, val loss: 1.3426390627018652, acc: 0.508289124668435, test loss: 1.3595175334070824, acc: 0.5163312438139228
epoch: 22, train loss: 1.1870143936992308, acc: 0.5051429382138044, val loss: 1.2815632089695816, acc: 0.5278514588859416, test loss: 1.3200394005635436, acc: 0.5229297261629825
epoch: 23, train loss: 1.1301214764868612, acc: 0.5308930978222317, val loss: 1.3472592182437684, acc: 0.5235411140583555, test loss: 1.3704583747518295, acc: 0.5160013196964698
epoch: 24, train loss: 1.1127512874168277, acc: 0.5289068596155211, val loss: 1.3313523862975345, acc: 0.5039787798408488, test loss: 1.3727789380219226, acc: 0.4975255691191026
epoch: 25, train loss: 1.144475254842032, acc: 0.5182662977938568, val loss: 1.3194043525650268, acc: 0.5125994694960212, test loss: 1.3701048567522403, acc: 0.5054437479379743
epoch: 26, train loss: 1.0816200374161646, acc: 0.5345818259204086, val loss: 1.3301606707927087, acc: 0.504973474801061, test loss: 1.3794061822570303, acc: 0.5047838997030683
epoch: 27, train loss: 1.105003776588854, acc: 0.5342271405263531, val loss: 2.5624701768080813, acc: 0.4330238726790451, test loss: 2.3898666931512293, acc: 0.4252721873968987
epoch: 28, train loss: 1.3058058252958336, acc: 0.47378874937930054, val loss: 1.6425743400260056, acc: 0.3942307692307692, test loss: 1.61244153115113, acc: 0.4120752226987793
epoch: 29, train loss: 1.1724347347248156, acc: 0.5145066326168688, val loss: 1.2927016125749846, acc: 0.5484084880636605, test loss: 1.2940357906399131, acc: 0.5489937314417684
epoch: 30, train loss: 1.1054517068008485, acc: 0.5320280910832091, val loss: 1.4211085649004667, acc: 0.4890583554376658, test loss: 1.417470727962694, acc: 0.49026723853513693
epoch: 31, train loss: 1.0944993443173483, acc: 0.534865574235653, val loss: 1.199919895088641, acc: 0.5593501326259946, test loss: 1.218043045295972, acc: 0.5645001649620587
epoch: 32, train loss: 1.0567984513638755, acc: 0.5534510888841597, val loss: 1.442956148155172, acc: 0.4827586206896552, test loss: 1.5090753501966345, acc: 0.4764104256021115
epoch: 33, train loss: 1.0640548948802921, acc: 0.5447258281903952, val loss: 1.309361120750164, acc: 0.5188992042440318, test loss: 1.3488501998623854, acc: 0.511382382052128
epoch: 34, train loss: 1.052849447709445, acc: 0.55735262821877, val loss: 1.2504046845499337, acc: 0.5550397877984085, test loss: 1.267595536439028, acc: 0.5394259320356318
epoch: 35, train loss: 1.0853779053568815, acc: 0.53791586862453, val loss: 1.2292978308245104, acc: 0.5513925729442971, test loss: 1.25746696772492, acc: 0.5423952490927086
epoch: 36, train loss: 1.0086256616917402, acc: 0.5657941405972902, val loss: 1.1641109614220475, acc: 0.5822281167108754, test loss: 1.1800519934938671, acc: 0.5773672055427251
epoch: 37, train loss: 1.009640998059641, acc: 0.5638079023905795, val loss: 1.2235314531414831, acc: 0.5646551724137931, test loss: 1.2748466757452162, acc: 0.558231606730452
epoch: 38, train loss: 1.0611391492606037, acc: 0.556643257430659, val loss: 1.4216705478470901, acc: 0.4824270557029178, test loss: 1.4269643949620678, acc: 0.4915869350049489
epoch: 39, train loss: 1.012386004519038, acc: 0.5662197630701568, val loss: 1.1736412294979752, acc: 0.5799071618037135, test loss: 1.2174081466489797, acc: 0.5724183437809304
epoch: 40, train loss: 0.9812412007595619, acc: 0.5732425338724552, val loss: 1.2216992384558965, acc: 0.5610079575596817, test loss: 1.265990986579403, acc: 0.5476740349719564
epoch: 41, train loss: 1.068651851605141, acc: 0.5469958147123501, val loss: 1.35457588475326, acc: 0.508289124668435, test loss: 1.3727952893989308, acc: 0.5080831408775982
epoch: 42, train loss: 0.9925955695413212, acc: 0.561112293395758, val loss: 1.2187907509209306, acc: 0.5557029177718833, test loss: 1.2421180139041033, acc: 0.55691191026064
epoch: 43, train loss: 0.95851822419311, acc: 0.5726750372419663, val loss: 1.1901720495059573, acc: 0.5755968169761273, test loss: 1.2161788453720694, acc: 0.5677994061365886
epoch: 44, train loss: 0.9610872801046824, acc: 0.5831737249060084, val loss: 1.2660417457158117, acc: 0.5563660477453581, test loss: 1.2989727399483721, acc: 0.5516331243813922
epoch: 45, train loss: 0.9540321899490373, acc: 0.5853727743491522, val loss: 1.3354375270696788, acc: 0.5222148541114059, test loss: 1.3973603578575056, acc: 0.5173210161662818
epoch: 46, train loss: 0.914060349987696, acc: 0.5936014754912393, val loss: 1.1332432864831676, acc: 0.6067639257294429, test loss: 1.1694796082604564, acc: 0.6034312108215111
epoch: 47, train loss: 0.9312649827193916, acc: 0.6003404979782933, val loss: 1.4665679745079667, acc: 0.5288461538461539, test loss: 1.4710658185751828, acc: 0.5136918508742989
epoch: 48, train loss: 1.0589745114980593, acc: 0.5162800595871462, val loss: 1.3161105350727111, acc: 0.5371352785145889, test loss: 1.3573173070182032, acc: 0.5232596502804355
epoch: 49, train loss: 0.97847537011925, acc: 0.5795559338866425, val loss: 1.2455424750199053, acc: 0.5653183023872679, test loss: 1.286151959612125, acc: 0.5612009237875288
epoch: 50, train loss: 0.9224596117425152, acc: 0.5993473788749379, val loss: 1.2734270579618865, acc: 0.5659814323607427, test loss: 1.308260052481802, acc: 0.559881227317717
epoch: 51, train loss: 1.0517087503807878, acc: 0.5574945023763921, val loss: 1.1604226903510664, acc: 0.5951591511936339, test loss: 1.1875019503756743, acc: 0.6014516661167931
epoch: 52, train loss: 0.9136098383707113, acc: 0.6121869901397461, val loss: 1.1243428706490393, acc: 0.6070954907161804, test loss: 1.1468594203841682, acc: 0.6096997690531177
epoch: 53, train loss: 0.9354036505056407, acc: 0.6009789316875931, val loss: 1.21762405861594, acc: 0.5736074270557029, test loss: 1.2521896032325823, acc: 0.5687891784889475
epoch: 54, train loss: 0.9166261423240446, acc: 0.6086401361991913, val loss: 1.1183613453366712, acc: 0.6097480106100795, test loss: 1.1627159944386107, acc: 0.6001319696469812
epoch: 55, train loss: 0.9107703183351311, acc: 0.6126126126126126, val loss: 1.5591803814435194, acc: 0.4525862068965517, test loss: 1.5950961238438837, acc: 0.44803695150115475
epoch: 56, train loss: 1.1083789760528444, acc: 0.5490529899978719, val loss: 1.3001869080553636, acc: 0.5348143236074271, test loss: 1.3376996372446923, acc: 0.5255691191026064
epoch: 57, train loss: 0.96959060771318, acc: 0.558203873164503, val loss: 1.1144093183370738, acc: 0.616710875331565, test loss: 1.1872938714063588, acc: 0.5885846255361267
epoch: 58, train loss: 0.8965801425907657, acc: 0.6116194935092573, val loss: 1.1471010228367005, acc: 0.6047745358090185, test loss: 1.1981896225747712, acc: 0.5935334872979214
epoch: 59, train loss: 0.8673447370233203, acc: 0.6270128396112649, val loss: 1.2136342378446847, acc: 0.5739389920424404, test loss: 1.2415015696102698, acc: 0.5780270537776312
epoch: 60, train loss: 0.8362058110716428, acc: 0.6324749946797191, val loss: 1.0502573080973536, acc: 0.6303050397877984, test loss: 1.105390831429901, acc: 0.6225668096337842
epoch: 61, train loss: 0.8016690727902819, acc: 0.6397105767184508, val loss: 1.0004319594456599, acc: 0.6462201591511937, test loss: 1.0508752497105425, acc: 0.6413724843286044
epoch: 62, train loss: 0.7697172903114831, acc: 0.6599276441796127, val loss: 1.0328712616738338, acc: 0.6462201591511937, test loss: 1.0998030316904408, acc: 0.628835367865391
epoch: 63, train loss: 0.7594365861903457, acc: 0.6605660778889125, val loss: 1.0776900192154497, acc: 0.6240053050397878, test loss: 1.1242119745624533, acc: 0.6179478719894425
epoch: 64, train loss: 0.7413790019140131, acc: 0.6662410441938001, val loss: 1.007203890885219, acc: 0.6545092838196287, test loss: 1.0601521295432956, acc: 0.6562190696139888
epoch: 65, train loss: 0.7325056244897921, acc: 0.6731928779172873, val loss: 1.0401108764527014, acc: 0.6462201591511937, test loss: 1.0969623797248984, acc: 0.6390630155064335
epoch: 66, train loss: 0.7173322183980818, acc: 0.6746825565723203, val loss: 1.0435318165812, acc: 0.6369363395225465, test loss: 1.091642573539749, acc: 0.6443418013856813
epoch: 67, train loss: 0.727234730921274, acc: 0.6778747251188196, val loss: 1.0105412483847742, acc: 0.646551724137931, test loss: 1.0602732603796163, acc: 0.6456614978554932
epoch: 68, train loss: 0.7034132620996894, acc: 0.6800028374831525, val loss: 1.009250202292789, acc: 0.6634615384615384, test loss: 1.0538148001372243, acc: 0.6628175519630485
epoch: 69, train loss: 0.709488606422499, acc: 0.6836206285025183, val loss: 1.0875646871976574, acc: 0.6276525198938993, test loss: 1.104151547545059, acc: 0.628835367865391
epoch: 70, train loss: 0.7503486030311664, acc: 0.6670922891395332, val loss: 0.9900352708224592, acc: 0.6571618037135278, test loss: 1.0483823388136484, acc: 0.6479709666776642
epoch: 71, train loss: 0.7118388444360726, acc: 0.678158473434064, val loss: 1.0148207891524945, acc: 0.6591511936339522, test loss: 1.0603474273967963, acc: 0.6591883866710656
epoch: 72, train loss: 0.7272372220842316, acc: 0.6807831453500744, val loss: 1.0264800184601497, acc: 0.6518567639257294, test loss: 1.0755629192205673, acc: 0.6469811943253052
epoch: 73, train loss: 0.7258221756069932, acc: 0.6710647655529546, val loss: 1.2561667270622456, acc: 0.5868700265251989, test loss: 1.2937186189666907, acc: 0.5770372814252722
epoch: 74, train loss: 0.8752803081670213, acc: 0.6439668014471164, val loss: 1.1468975494964053, acc: 0.6107427055702918, test loss: 1.1849330299485048, acc: 0.6073903002309469
epoch: 75, train loss: 0.7652278940094324, acc: 0.663971057671845, val loss: 1.0155986577509568, acc: 0.639920424403183, test loss: 1.0471260773392252, acc: 0.6552292972616298
epoch: 76, train loss: 0.7052527526054618, acc: 0.6790806554586082, val loss: 0.9873716616187868, acc: 0.6611405835543767, test loss: 1.047179461233453, acc: 0.6595183107885186
epoch: 77, train loss: 0.718585292743505, acc: 0.6792225296162304, val loss: 1.164859825325265, acc: 0.5941644562334217, test loss: 1.256788260116313, acc: 0.5925437149455626
epoch: 78, train loss: 0.9088632918473059, acc: 0.6141022912676456, val loss: 1.092569398943246, acc: 0.6203580901856764, test loss: 1.127094919238693, acc: 0.6245463543385021
epoch: 79, train loss: 0.7823514345056362, acc: 0.6582960913669575, val loss: 1.020980471799481, acc: 0.6485411140583555, test loss: 1.0496219805230091, acc: 0.6519300560871
epoch: 80, train loss: 0.6879327213957841, acc: 0.6848265588423069, val loss: 1.0455207982809538, acc: 0.6435676392572944, test loss: 1.096542273445595, acc: 0.6413724843286044
epoch: 81, train loss: 0.6920753110002235, acc: 0.6795772150102859, val loss: 1.0806207429192742, acc: 0.6206896551724138, test loss: 1.129831190138357, acc: 0.6275156713955791
epoch: 82, train loss: 0.7072717640056098, acc: 0.6770234801730864, val loss: 1.0496246887454936, acc: 0.6488726790450928, test loss: 1.0856933982330594, acc: 0.648960739030023
epoch: 83, train loss: 0.6774129172377598, acc: 0.6868127970490175, val loss: 1.0248616544890468, acc: 0.6631299734748011, test loss: 1.0553369076092303, acc: 0.6608380072583305
epoch: 84, train loss: 0.6420405339440395, acc: 0.7018514577569696, val loss: 0.9423483216477958, acc: 0.6787135278514589, test loss: 1.0055415049436185, acc: 0.6654569449026724
epoch: 85, train loss: 0.6404478570643644, acc: 0.7039795701213024, val loss: 0.9997191033881918, acc: 0.6687665782493368, test loss: 1.0408237944220364, acc: 0.6608380072583305
epoch: 86, train loss: 0.6215063308607376, acc: 0.7149748173370221, val loss: 1.0165356304348305, acc: 0.6561671087533156, test loss: 1.073120082931053, acc: 0.6545694490267239
epoch: 87, train loss: 0.6125335298109166, acc: 0.7076682982194793, val loss: 1.0662954460720802, acc: 0.6475464190981433, test loss: 1.1240648491002672, acc: 0.6387330913889805
epoch: 88, train loss: 0.6881116847243725, acc: 0.6892955948074059, val loss: 1.2432195485744932, acc: 0.6011273209549072, test loss: 1.2912765379512086, acc: 0.5958429561200924
epoch: 89, train loss: 0.6469073004567675, acc: 0.6941902532453713, val loss: 1.0728702010779545, acc: 0.6455570291777188, test loss: 1.1384590377669175, acc: 0.6298251402177499
epoch: 90, train loss: 0.7261864789844514, acc: 0.67595942399092, val loss: 1.0057128081271123, acc: 0.6700928381962865, test loss: 1.0624821767363364, acc: 0.6578686902012537
epoch: 91, train loss: 0.6375633175680719, acc: 0.7025608285450805, val loss: 0.9603376159301171, acc: 0.6777188328912467, test loss: 1.0269199247920289, acc: 0.6789838337182448
epoch: 92, train loss: 0.6191153658584745, acc: 0.7128467049726892, val loss: 1.2115936453209633, acc: 0.628315649867374, test loss: 1.2512671699306823, acc: 0.625866050808314
epoch: 93, train loss: 0.5980884532789106, acc: 0.7176704263318436, val loss: 1.0290135915146583, acc: 0.6637931034482759, test loss: 1.0946531191748465, acc: 0.6565489937314418
epoch: 94, train loss: 0.5914548515825312, acc: 0.7173157409377882, val loss: 0.9870658058386582, acc: 0.6767241379310345, test loss: 1.0442269212615172, acc: 0.6697459584295612
epoch: 95, train loss: 0.7081798193065243, acc: 0.6734056891537207, val loss: 1.01133747410711, acc: 0.6621352785145889, test loss: 1.069526113267387, acc: 0.6585285384361597
epoch: 96, train loss: 0.6045254881249901, acc: 0.7097964105838122, val loss: 1.0289740722122496, acc: 0.6621352785145889, test loss: 1.0885006729347484, acc: 0.6618277796106895
epoch: 97, train loss: 0.5921137655441783, acc: 0.7234872667943534, val loss: 1.08767825017241, acc: 0.6551724137931034, test loss: 1.1325284620591494, acc: 0.6598482349059717
epoch: 98, train loss: 0.695375207227841, acc: 0.6949705611122934, val loss: 1.0727643656793893, acc: 0.6505305039787799, test loss: 1.133314103153585, acc: 0.6367535466842627
epoch: 99, train loss: 0.6982914174018265, acc: 0.6983046038164148, val loss: 1.010338165557669, acc: 0.6687665782493368, test loss: 1.0788119522819657, acc: 0.6548993731441768
epoch: 100, train loss: 0.6003504824983387, acc: 0.7189472937504433, val loss: 0.9593403927527309, acc: 0.6889920424403183, test loss: 1.0256825558465135, acc: 0.6829429231276807
epoch: 101, train loss: 0.5697854318994744, acc: 0.7307937859118961, val loss: 0.9748677587635637, acc: 0.6820291777188329, test loss: 1.0464069746147089, acc: 0.6793137578356978
epoch: 102, train loss: 0.5679302995632987, acc: 0.7347662623253175, val loss: 1.0449583832401812, acc: 0.6681034482758621, test loss: 1.1018044848679631, acc: 0.6558891454965358
epoch: 103, train loss: 0.570918591607291, acc: 0.7288075477051855, val loss: 1.0286988113223716, acc: 0.6717506631299734, test loss: 1.0618904015090511, acc: 0.6654569449026724
epoch: 104, train loss: 0.5402666885116941, acc: 0.7441299567283819, val loss: 1.0538162110022589, acc: 0.6724137931034483, test loss: 1.1185326163267704, acc: 0.6628175519630485
epoch: 105, train loss: 0.5539665138534581, acc: 0.7361140668227283, val loss: 0.9883059581015408, acc: 0.6793766578249337, test loss: 1.049768212485809, acc: 0.6806334543055097
epoch: 106, train loss: 0.7432513714064991, acc: 0.6734766262325318, val loss: 1.1034699754310224, acc: 0.6342838196286472, test loss: 1.1550093657582634, acc: 0.6222368855163313
epoch: 107, train loss: 0.590084284495367, acc: 0.7168901184649216, val loss: 1.0804417300919955, acc: 0.65815649867374, test loss: 1.138264619057346, acc: 0.6397228637413395
epoch: 108, train loss: 0.6721799284285278, acc: 0.6948286869546713, val loss: 0.974988906231736, acc: 0.6840185676392573, test loss: 1.0126593455510027, acc: 0.67403497195645
epoch: 109, train loss: 0.5858464305993808, acc: 0.7241966375824643, val loss: 1.0123760611688426, acc: 0.6797082228116711, test loss: 1.0747990916562686, acc: 0.6664467172550314
epoch: 110, train loss: 0.5455806754773125, acc: 0.7417180960488047, val loss: 1.008513384535711, acc: 0.6753978779840849, test loss: 1.0635799598237619, acc: 0.6611679313757836
epoch: 111, train loss: 0.5317390710908216, acc: 0.7437043342555154, val loss: 0.9727764630823617, acc: 0.6929708222811671, test loss: 1.0349312278155405, acc: 0.6852523919498515
epoch: 112, train loss: 0.5121997026058683, acc: 0.7513655387671135, val loss: 0.9636247322477144, acc: 0.6976127320954907, test loss: 1.041447679463962, acc: 0.6859122401847575
epoch: 113, train loss: 0.53004423881487, acc: 0.7499467971908916, val loss: 1.0159306292192374, acc: 0.6856763925729443, test loss: 1.066463646028192, acc: 0.6783239854833388
epoch: 114, train loss: 0.5386503593054208, acc: 0.744910264595304, val loss: 1.1072265414091258, acc: 0.6767241379310345, test loss: 1.1951786579373094, acc: 0.650940283734741
epoch: 115, train loss: 0.5625076771856429, acc: 0.7388096758175499, val loss: 1.1173871946904008, acc: 0.6690981432360743, test loss: 1.1806660691569324, acc: 0.6525899043220059
epoch: 116, train loss: 0.529381984344161, acc: 0.7449812016741151, val loss: 1.0596079880109832, acc: 0.6747347480106101, test loss: 1.1496811547872454, acc: 0.6581986143187067
epoch: 117, train loss: 0.5592697106701876, acc: 0.7343406398524509, val loss: 1.0206045016685912, acc: 0.666445623342175, test loss: 1.0584854237595158, acc: 0.6680963378422963
epoch: 118, train loss: 0.5738605443971448, acc: 0.7307937859118961, val loss: 1.0667878734021983, acc: 0.6684350132625995, test loss: 1.11479315652347, acc: 0.6539096007918179
epoch: 119, train loss: 0.5071539248846486, acc: 0.7564020713627013, val loss: 0.9305175476428368, acc: 0.7105437665782494, test loss: 0.9782823103955381, acc: 0.7076872319366546
epoch: 120, train loss: 0.5151051624096766, acc: 0.7493083634815918, val loss: 1.0840848086683441, acc: 0.6760610079575596, test loss: 1.141496705481889, acc: 0.6779940613658858
best val loss 0.9305175476428368 at epoch 119.
