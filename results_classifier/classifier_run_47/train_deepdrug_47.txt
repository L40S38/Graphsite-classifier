seed:  7
save trained model at:  ../trained_models/trained_classifier_model_47.pt
save loss at:  ./results/train_classifier_results_47.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['2btoB00', '5oj7A00', '6d4vA01', '2qtzA01', '6ef3R01']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1vgvC00', '5kviA01', '5j5xA00', '4usjD00', '4cvlA00']
model architecture:
DeepDruG(
  (embedding_net): JKMCNMMEmbeddingNet(
    (conv0): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=33, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2ae66a6f4880>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.018056193131363, acc: 0.3973008168580561; test loss: 1.7941501237517186, acc: 0.4502481682817301
epoch: 2, train loss: 1.696316946663489, acc: 0.47845388895465846; test loss: 1.5926188745839014, acc: 0.494682108248641
epoch: 3, train loss: 1.5945799501602418, acc: 0.5090564697525749; test loss: 1.5412356251662742, acc: 0.49988182462774755
epoch: 4, train loss: 1.4894231570265173, acc: 0.5456375044394459; test loss: 1.3988450417082463, acc: 0.5800047270148901
epoch: 5, train loss: 1.4360661000715453, acc: 0.5641055996211672; test loss: 1.375835689802988, acc: 0.5840226896714724
epoch: 6, train loss: 1.3604138143474398, acc: 0.5857700958920327; test loss: 1.5169202211308328, acc: 0.5506972346962893
epoch: 7, train loss: 1.3132039792610792, acc: 0.6041789984609921; test loss: 1.2641399979619545, acc: 0.6022216969983455
epoch: 8, train loss: 1.259633690559809, acc: 0.6172013732686161; test loss: 1.2797237136239448, acc: 0.600094540297802
epoch: 9, train loss: 1.2081776312911272, acc: 0.6329466082632887; test loss: 1.1722487438037812, acc: 0.6369652564405578
epoch: 10, train loss: 1.168638172722066, acc: 0.6469160648751036; test loss: 1.265361615203284, acc: 0.6147482864571023
epoch: 11, train loss: 1.1285469852087129, acc: 0.6567420385935835; test loss: 1.143473594922936, acc: 0.6433467265421886
epoch: 12, train loss: 1.1060345084537087, acc: 0.667751864567302; test loss: 1.1538319479514736, acc: 0.6435830772866935
epoch: 13, train loss: 1.0766752194240556, acc: 0.6728424292648277; test loss: 1.189566472945442, acc: 0.6499645473883243
epoch: 14, train loss: 1.0840311721328928, acc: 0.6746182076476855; test loss: 1.3852029277257845, acc: 0.5755140628692981
epoch: 15, train loss: 1.1385680043910458, acc: 0.6535456375044394; test loss: 1.0442498410968064, acc: 0.6780902859844008
epoch: 16, train loss: 1.0242932263642242, acc: 0.6863383449745472; test loss: 1.1152475975106946, acc: 0.644292129520208
epoch: 17, train loss: 0.9961987252106705, acc: 0.6988871788800758; test loss: 1.0079105276217333, acc: 0.6847081068305365
epoch: 18, train loss: 0.9644780202886826, acc: 0.7069965668284598; test loss: 1.0423671144933122, acc: 0.6771448830063814
epoch: 19, train loss: 0.954714977841441, acc: 0.7088907304368415; test loss: 1.1671497873650238, acc: 0.635074450484519
epoch: 20, train loss: 0.9301715948686976, acc: 0.719071859831893; test loss: 1.0225334780009412, acc: 0.6889624202316237
epoch: 21, train loss: 0.9152364508137839, acc: 0.7212027938913224; test loss: 1.1843782024613454, acc: 0.6785629874734106
epoch: 22, train loss: 0.8987897493380039, acc: 0.7262341659760861; test loss: 1.066509489358331, acc: 0.6754904277948476
epoch: 23, train loss: 0.9026870366326589, acc: 0.7234521131762756; test loss: 1.0014580665990496, acc: 0.7010163082013708
epoch: 24, train loss: 0.8750918644609516, acc: 0.7351722505031372; test loss: 0.9526881411463847, acc: 0.7166154573386906
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.6763689892523752, acc: 0.7396116964602818; test loss: 0.881923652320741, acc: 0.6726542188607895
epoch: 26, train loss: 0.6324161113656257, acc: 0.7521605303658103; test loss: 0.7780999151760454, acc: 0.69463483809974
epoch: 27, train loss: 0.638885095696118, acc: 0.7527524564934296; test loss: 0.7662085048753977, acc: 0.7104703379815647
epoch: 28, train loss: 0.6428994219007048, acc: 0.7478394696341897; test loss: 0.9291393395660336, acc: 0.6511463011108485
epoch: 29, train loss: 0.6276327080242279, acc: 0.7562448206463833; test loss: 0.8516995248657061, acc: 0.6832900023635075
epoch: 30, train loss: 0.6044961211243028, acc: 0.7657748313010536; test loss: 0.8046030668016586, acc: 0.7128338454266131
epoch: 31, train loss: 0.6101120359462663, acc: 0.761453770569433; test loss: 0.8264319276279999, acc: 0.69463483809974
epoch: 32, train loss: 0.6105075714657882, acc: 0.7605066887652421; test loss: 0.7567025647606306, acc: 0.7255967856298747
epoch: 33, train loss: 0.593144546863919, acc: 0.7702142772581981; test loss: 0.9387570076083438, acc: 0.6839990545970219
epoch: 34, train loss: 0.5913657967728441, acc: 0.7697407363561027; test loss: 0.7105702453402305, acc: 0.7307965020089813
epoch: 35, train loss: 0.5749447348767855, acc: 0.775245649342962; test loss: 0.7039147570634164, acc: 0.7307965020089813
epoch: 36, train loss: 0.5573561039283683, acc: 0.7815200662957263; test loss: 0.701988995709168, acc: 0.7359962183880879
epoch: 37, train loss: 0.5550573985647475, acc: 0.785071623061442; test loss: 0.7082497313974372, acc: 0.7378870243441267
epoch: 38, train loss: 0.550944581290907, acc: 0.7834142299041079; test loss: 0.7002890162985254, acc: 0.7430867407232333
epoch: 39, train loss: 0.5346972892605314, acc: 0.7925890848822067; test loss: 0.6951463137414545, acc: 0.7362325691325927
epoch: 40, train loss: 0.5446334345546491, acc: 0.7840653486444892; test loss: 0.7432617006277086, acc: 0.7168518080831955
epoch: 41, train loss: 0.5201270639917561, acc: 0.7931218183970641; test loss: 0.7614799321782529, acc: 0.7123611439376034
epoch: 42, train loss: 0.5145948276874567, acc: 0.7949567893926838; test loss: 0.7232456146416949, acc: 0.7348144646655637
epoch: 43, train loss: 0.5314121284724214, acc: 0.7922339292056352; test loss: 0.7885681050124108, acc: 0.7043252186244386
epoch: 44, train loss: 0.4886122897657203, acc: 0.8120634544808808; test loss: 0.724101463634335, acc: 0.7348144646655637
epoch: 45, train loss: 0.4790515993507117, acc: 0.8096365573576417; test loss: 0.9291136185346949, acc: 0.6759631292838573
epoch: 46, train loss: 0.5032174665998168, acc: 0.8021782881496389; test loss: 0.6952978067999893, acc: 0.735759867643583
epoch: 47, train loss: 0.48747081494520467, acc: 0.8075648159109743; test loss: 0.6819607898772566, acc: 0.7383597258331364
epoch: 48, train loss: 0.5023730530553391, acc: 0.8014679767964958; test loss: 0.9985563169554654, acc: 0.6414559205861499
epoch: 49, train loss: 0.48374637573996254, acc: 0.8109387948384041; test loss: 0.7337542116092581, acc: 0.7475774048688253
epoch: 50, train loss: 0.47421682289511236, acc: 0.8150230851189771; test loss: 0.7860686546858533, acc: 0.7045615693689435
epoch: 51, train loss: 0.5004306631679858, acc: 0.8058482301408784; test loss: 0.689515610841171, acc: 0.7485228078468447
epoch: 52, train loss: 0.4543369978902692, acc: 0.8174499822422162; test loss: 0.6611207562173633, acc: 0.7575041361380288
epoch: 53, train loss: 0.4533811808655611, acc: 0.8166212856635492; test loss: 0.6648923414758238, acc: 0.7485228078468447
epoch: 54, train loss: 0.4611044241766175, acc: 0.817627560080502; test loss: 0.8867122541502896, acc: 0.6934530843772158
epoch: 55, train loss: 0.45753952549648974, acc: 0.8188114123357405; test loss: 0.7983496811789448, acc: 0.7017253604348853
epoch: 56, train loss: 0.45331126512176567, acc: 0.8228365100035515; test loss: 0.7860559187045871, acc: 0.7149610021271567
epoch: 57, train loss: 0.46532532979189695, acc: 0.8163253225997396; test loss: 0.7409173906398874, acc: 0.7442684944457575
epoch: 58, train loss: 0.43407419606526393, acc: 0.8294660826328875; test loss: 0.6271849178416992, acc: 0.7771212479319309
epoch: 59, train loss: 0.41561169267349923, acc: 0.83384633597727; test loss: 0.658058502352342, acc: 0.7553769794374853
epoch: 60, train loss: 0.4522728005661631, acc: 0.8223629691014561; test loss: 0.7735797841620484, acc: 0.74048688253368
epoch: 61, train loss: 0.43047896296763205, acc: 0.8296436604711732; test loss: 0.9517462037184987, acc: 0.7000709052233515
epoch: 62, train loss: 0.4280653885727001, acc: 0.8297620456966971; test loss: 0.7218694024772346, acc: 0.7355235168990782
epoch: 63, train loss: 0.4039072223537784, acc: 0.8398839824789867; test loss: 0.6795738530592895, acc: 0.7657764121956984
epoch: 64, train loss: 0.4074845990972913, acc: 0.8405942938321298; test loss: 0.7295224731398429, acc: 0.7395414795556606
epoch: 65, train loss: 0.3995337105332232, acc: 0.8382265893216526; test loss: 0.691336411106702, acc: 0.7416686362562042
epoch: 66, train loss: 0.4225158019363351, acc: 0.8312418610157453; test loss: 0.6642266286797232, acc: 0.7622311510281258
epoch: 67, train loss: 0.42069814641814096, acc: 0.8350893808452705; test loss: 0.8808289542882275, acc: 0.662491136847081
epoch: 68, train loss: 0.39839173224856617, acc: 0.8407126790576536; test loss: 0.7381043472461344, acc: 0.7393051288111557
epoch: 69, train loss: 0.3735029872591092, acc: 0.8502426897123239; test loss: 0.6906760968206285, acc: 0.767903568896242
Epoch    69: reducing learning rate of group 0 to 1.5000e-03.
epoch: 70, train loss: 0.31130678679543133, acc: 0.8730910382384278; test loss: 0.6291919513054137, acc: 0.7934294493027653
epoch: 71, train loss: 0.2549759944797451, acc: 0.8967680833431988; test loss: 0.6963623185947808, acc: 0.7792484046324746
epoch: 72, train loss: 0.25564701206884227, acc: 0.8946371492837694; test loss: 0.6495409653108472, acc: 0.780193807610494
epoch: 73, train loss: 0.2463035401161061, acc: 0.8966496981176749; test loss: 0.6720083955049345, acc: 0.7835027180335618
epoch: 74, train loss: 0.25717051064204616, acc: 0.8938676453178643; test loss: 0.6929705610119742, acc: 0.7733396360198534
epoch: 75, train loss: 0.24935952760389432, acc: 0.8952882680241506; test loss: 0.6895243062445583, acc: 0.7870479792011345
epoch: 76, train loss: 0.2302409174831235, acc: 0.9040487747129158; test loss: 0.69720315859972, acc: 0.7747577404868825
epoch: 77, train loss: 0.2433127921012445, acc: 0.8954658458624364; test loss: 0.7118881599076446, acc: 0.7738123375088631
epoch: 78, train loss: 0.2295149091970318, acc: 0.9040487747129158; test loss: 0.6758215546861548, acc: 0.7910659418577168
epoch: 79, train loss: 0.23646190960272906, acc: 0.899017402628152; test loss: 0.7523191504319546, acc: 0.7473410541243205
epoch: 80, train loss: 0.23912133112932588, acc: 0.8958801941517699; test loss: 0.700954351715939, acc: 0.7837390687780666
epoch: 81, train loss: 0.23611498101302064, acc: 0.9004972179472002; test loss: 0.6895431546955293, acc: 0.7889387851571732
epoch: 82, train loss: 0.21368226912426105, acc: 0.9062389013851071; test loss: 0.7646478826175258, acc: 0.7615220987946112
epoch: 83, train loss: 0.21766984746754486, acc: 0.9062389013851071; test loss: 0.7879337372438363, acc: 0.7764121956984165
epoch: 84, train loss: 0.2210843775651152, acc: 0.9077187167041553; test loss: 0.7033275854105759, acc: 0.7827936658000473
epoch: 85, train loss: 0.20411421305839444, acc: 0.9125725109506334; test loss: 0.8734654180548823, acc: 0.7201607185062633
epoch: 86, train loss: 0.2675242493163062, acc: 0.8877708062033858; test loss: 0.7151792935236383, acc: 0.7738123375088631
epoch: 87, train loss: 0.22196898619896055, acc: 0.9061797087723452; test loss: 0.7271499415431228, acc: 0.784684471756086
epoch: 88, train loss: 0.20542644604827096, acc: 0.9106191547294897; test loss: 0.756352480605713, acc: 0.7865752777121248
epoch: 89, train loss: 0.20615382752038042, acc: 0.9117438143719664; test loss: 0.7088531014260545, acc: 0.7775939494209406
epoch: 90, train loss: 0.1990034489169041, acc: 0.914466674559015; test loss: 0.776071725909351, acc: 0.7598676435830772
epoch: 91, train loss: 0.1858002828930036, acc: 0.9169527642950159; test loss: 0.7055741007856037, acc: 0.7875206806901441
epoch: 92, train loss: 0.18583764720099966, acc: 0.9184325796140642; test loss: 0.7655881193337275, acc: 0.772630583786339
epoch: 93, train loss: 0.19771020131533396, acc: 0.9175446904226352; test loss: 0.6914211230141647, acc: 0.7972110612148429
epoch: 94, train loss: 0.19058855867419736, acc: 0.9156505268142536; test loss: 0.7035036323144345, acc: 0.7827936658000473
epoch: 95, train loss: 0.17668712369165324, acc: 0.9205635136734935; test loss: 0.7653257317701938, acc: 0.7676672181517372
epoch: 96, train loss: 0.20775445837960155, acc: 0.9114478513081568; test loss: 0.723079740367187, acc: 0.7827936658000473
epoch: 97, train loss: 0.18852358783040793, acc: 0.9183141943885403; test loss: 0.7147806590117903, acc: 0.7882297329236587
epoch: 98, train loss: 0.15860578515155083, acc: 0.9273706641411152; test loss: 0.7464115052210808, acc: 0.7839754195225715
epoch: 99, train loss: 0.16449032826043733, acc: 0.9252989226944477; test loss: 0.7458085577641782, acc: 0.7988655164263767
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.12702933723325063, acc: 0.9240558778264473; test loss: 0.6863698171008369, acc: 0.7615220987946112
epoch: 101, train loss: 0.1315417763194601, acc: 0.9192612761927311; test loss: 0.6150371041373422, acc: 0.7924840463247459
epoch: 102, train loss: 0.1096423551099772, acc: 0.9321060731620694; test loss: 0.6784403258760723, acc: 0.7861025762231151
epoch: 103, train loss: 0.12146776092360115, acc: 0.925890848822067; test loss: 0.6189894912316708, acc: 0.7898841881351927
epoch: 104, train loss: 0.09877106741845178, acc: 0.937255830472357; test loss: 0.6809094321274132, acc: 0.7894114866461829
epoch: 105, train loss: 0.1157817280640387, acc: 0.9312773765834024; test loss: 0.6492140530074806, acc: 0.7653037107066887
epoch: 106, train loss: 0.12807554089135356, acc: 0.9213330176393986; test loss: 0.687232448287789, acc: 0.7702670763412904
epoch: 107, train loss: 0.11593626077582712, acc: 0.9286137090091157; test loss: 0.6529101018955228, acc: 0.7686126211297566
epoch: 108, train loss: 0.13018899116097532, acc: 0.9208002841245413; test loss: 0.5597521931926175, acc: 0.7920113448357362
epoch: 109, train loss: 0.11497447259633291, acc: 0.9292056351367349; test loss: 0.5941081848924421, acc: 0.7972110612148429
epoch: 110, train loss: 0.11572854014911128, acc: 0.9276074345921629; test loss: 0.6800783641285942, acc: 0.7705034270857953
epoch: 111, train loss: 0.10397431452757397, acc: 0.934118621995975; test loss: 0.6138123631674582, acc: 0.7931930985582605
epoch: 112, train loss: 0.11116645258390595, acc: 0.9296791760388303; test loss: 0.6021935866533287, acc: 0.7835027180335618
epoch: 113, train loss: 0.11588063542026027, acc: 0.9263051971114005; test loss: 0.6693278489116236, acc: 0.7702670763412904
epoch: 114, train loss: 0.10100634506242831, acc: 0.9345921628980703; test loss: 0.6559766321648832, acc: 0.781611912077523
epoch: 115, train loss: 0.11687465168017953, acc: 0.9271338936900675; test loss: 0.6674793350924783, acc: 0.7830300165445521
epoch: 116, train loss: 0.1260364167696557, acc: 0.9220433289925417; test loss: 0.6069824219989709, acc: 0.7927203970692508
epoch: 117, train loss: 0.11785111358014172, acc: 0.9270155084645436; test loss: 0.633505608561585, acc: 0.7705034270857953
epoch: 118, train loss: 0.10439745216725549, acc: 0.9329347697407363; test loss: 0.6560086984212898, acc: 0.7792484046324746
epoch: 119, train loss: 0.11913861576002477, acc: 0.9235823369243519; test loss: 0.6319802164383403, acc: 0.7697943748522807
epoch: 120, train loss: 0.11518570917470082, acc: 0.9279033976559725; test loss: 0.6518915581720092, acc: 0.7993382179153864
epoch: 121, train loss: 0.12483684390490474, acc: 0.9214514028649224; test loss: 0.6647896815687453, acc: 0.7723942330418341
epoch: 122, train loss: 0.13625875225649425, acc: 0.9151769859121581; test loss: 0.5885722826795639, acc: 0.7901205388796975
epoch: 123, train loss: 0.12392178791820671, acc: 0.9225760625073991; test loss: 0.6100077924929, acc: 0.7797211061214843
epoch: 124, train loss: 0.11923959059370641, acc: 0.9209186693500652; test loss: 0.7541812710997569, acc: 0.7506499645473883
epoch: 125, train loss: 0.12523653150677355, acc: 0.9219841363797798; test loss: 0.6163859117456543, acc: 0.7887024344126684
epoch: 126, train loss: 0.09690210073560078, acc: 0.9345329702853084; test loss: 0.664694673230084, acc: 0.7844481210115812
epoch: 127, train loss: 0.10885177807785625, acc: 0.9310406061323547; test loss: 0.6868261179442778, acc: 0.7811392105885133
epoch: 128, train loss: 0.10842931402464145, acc: 0.9320468805493074; test loss: 0.6923045795653002, acc: 0.7787757031434649
epoch: 129, train loss: 0.10462727005084874, acc: 0.9354208594767373; test loss: 0.7021624514240092, acc: 0.7827936658000473
epoch: 130, train loss: 0.12374146616436313, acc: 0.9216881733159702; test loss: 0.5993354629326363, acc: 0.796974710470338
epoch: 131, train loss: 0.105264892370105, acc: 0.9332899254173079; test loss: 0.6282777865145455, acc: 0.787757031434649
Epoch   131: reducing learning rate of group 0 to 7.5000e-04.
epoch: 132, train loss: 0.07632871974505263, acc: 0.9458387593228366; test loss: 0.6407509320341817, acc: 0.8005199716379107
epoch: 133, train loss: 0.04529512835955626, acc: 0.9679767964957974; test loss: 0.6925971306009908, acc: 0.8130465610966675
epoch: 134, train loss: 0.04182680319486193, acc: 0.96898307091275; test loss: 0.6653219407908152, acc: 0.8128102103521626
epoch: 135, train loss: 0.03687357618554508, acc: 0.9709364271338937; test loss: 0.6817694248473551, acc: 0.8024107775939494
epoch: 136, train loss: 0.03240318187317637, acc: 0.9744879838996093; test loss: 0.7453605899909967, acc: 0.8069014417395415
epoch: 137, train loss: 0.03394378808087656, acc: 0.9750799100272286; test loss: 0.7195050491562016, acc: 0.8142283148191917
epoch: 138, train loss: 0.033812971392275314, acc: 0.9767373031845625; test loss: 0.7139794206089569, acc: 0.8078468447175609
epoch: 139, train loss: 0.03138258497369519, acc: 0.977210844086658; test loss: 0.7606687507739841, acc: 0.8083195462065705
epoch: 140, train loss: 0.039311040608723716, acc: 0.9709364271338937; test loss: 0.7166311211463154, acc: 0.8097376506735996
epoch: 141, train loss: 0.03744406761616782, acc: 0.9722386646146561; test loss: 0.6985408480751742, acc: 0.8054833372725124
epoch: 142, train loss: 0.04734611432729027, acc: 0.9665561737895111; test loss: 0.7310958063312243, acc: 0.8054833372725124
epoch: 143, train loss: 0.05078599525996393, acc: 0.9644252397300817; test loss: 0.7018072257169283, acc: 0.7889387851571732
epoch: 144, train loss: 0.0522169104405828, acc: 0.9637741209897005; test loss: 0.6731229966144927, acc: 0.7934294493027653
epoch: 145, train loss: 0.03967069497443775, acc: 0.9713507754232272; test loss: 0.7613751019915349, acc: 0.8043015835499882
epoch: 146, train loss: 0.045768101775174846, acc: 0.9683911447851308; test loss: 0.7623980562051851, acc: 0.7915386433467265
epoch: 147, train loss: 0.04124721867130807, acc: 0.9719427015508465; test loss: 0.702294325665136, acc: 0.8033561805719688
epoch: 148, train loss: 0.03661528944142707, acc: 0.973955250384752; test loss: 0.7631443301094702, acc: 0.7929567478137556
epoch: 149, train loss: 0.04387449869177984, acc: 0.9679767964957974; test loss: 0.7542202923490037, acc: 0.7955566060033089
epoch: 150, train loss: 0.05644026990043626, acc: 0.9605185272877945; test loss: 0.7412027706916212, acc: 0.7879933821791538
epoch: 151, train loss: 0.056954576397815, acc: 0.9611696460281757; test loss: 0.6995221138902095, acc: 0.7960293074923186
epoch: 152, train loss: 0.05201284803923762, acc: 0.9644252397300817; test loss: 0.8396621522427845, acc: 0.755849680926495
epoch: 153, train loss: 0.0504461045074748, acc: 0.9660826328874157; test loss: 0.707379365074412, acc: 0.7950839045142992
epoch: 154, train loss: 0.0510799195654924, acc: 0.9646620101811294; test loss: 0.7044235664010189, acc: 0.7995745686598913
epoch: 155, train loss: 0.04426197622183499, acc: 0.969870960104179; test loss: 0.7268063477155757, acc: 0.8021744268494446
epoch: 156, train loss: 0.0395560632000608, acc: 0.9729489759677992; test loss: 0.7348846719214832, acc: 0.7974474119593477
epoch: 157, train loss: 0.04229060365975334, acc: 0.9711140049721795; test loss: 0.7786123550859531, acc: 0.7861025762231151
epoch: 158, train loss: 0.040855501914809016, acc: 0.9722386646146561; test loss: 0.7165023649423519, acc: 0.800047270148901
epoch: 159, train loss: 0.04902575512865288, acc: 0.9663785959512253; test loss: 0.7412281803306016, acc: 0.7875206806901441
epoch: 160, train loss: 0.06502815493080084, acc: 0.9561382739434119; test loss: 0.7027185295411977, acc: 0.798392814937367
epoch: 161, train loss: 0.05191636237220904, acc: 0.9639516988279863; test loss: 0.77724100714512, acc: 0.7991018671708816
epoch: 162, train loss: 0.03850193255860678, acc: 0.9725346276784658; test loss: 0.763933790628861, acc: 0.7986291656818719
epoch: 163, train loss: 0.04188475874707419, acc: 0.9710548123594176; test loss: 0.8363328036391913, acc: 0.7641219569841645
epoch: 164, train loss: 0.05192249239097901, acc: 0.9647212027938913; test loss: 0.7184642791973449, acc: 0.8002836208934058
epoch: 165, train loss: 0.03495096632439215, acc: 0.9750799100272286; test loss: 0.7564203198765841, acc: 0.8066650909950366
epoch: 166, train loss: 0.03428969395711773, acc: 0.9751982952527525; test loss: 0.8254238963549765, acc: 0.7773575986764358
epoch: 167, train loss: 0.04255912078687167, acc: 0.9709956197466556; test loss: 0.7647137554563604, acc: 0.800047270148901
epoch: 168, train loss: 0.03872125457283857, acc: 0.9715875458742749; test loss: 0.7512863470684746, acc: 0.7988655164263767
epoch: 169, train loss: 0.03951435586069552, acc: 0.9736000947081804; test loss: 0.7250217645339047, acc: 0.8047742850389978
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.02902600758780569, acc: 0.9717651237125606; test loss: 0.653377500360273, acc: 0.7901205388796975
epoch: 171, train loss: 0.031782457603240236, acc: 0.9696341896531313; test loss: 0.6265066607684228, acc: 0.7993382179153864
epoch: 172, train loss: 0.03095284536623353, acc: 0.9692198413637978; test loss: 0.615834921084237, acc: 0.7981564641928622
epoch: 173, train loss: 0.024176401111730722, acc: 0.9765005327335149; test loss: 0.658234482706368, acc: 0.7995745686598913
epoch: 174, train loss: 0.027021475896156368, acc: 0.9735409020954184; test loss: 0.6043276330365889, acc: 0.8047742850389978
epoch: 175, train loss: 0.019893086471926627, acc: 0.980466437788564; test loss: 0.6523026379815401, acc: 0.7998109194043961
epoch: 176, train loss: 0.028773730510045992, acc: 0.9717059310997987; test loss: 0.6202785793530746, acc: 0.7856298747341054
epoch: 177, train loss: 0.035855475054156136, acc: 0.966615366402273; test loss: 0.6474717593424091, acc: 0.7891751359016781
epoch: 178, train loss: 0.04515331458797381, acc: 0.9594530602580797; test loss: 0.5661006624663139, acc: 0.7948475537697943
epoch: 179, train loss: 0.03488470301957325, acc: 0.9673256777554161; test loss: 0.6031168885502627, acc: 0.8012290238714252
epoch: 180, train loss: 0.02309209001317269, acc: 0.975553450929324; test loss: 0.6413619693248048, acc: 0.803119829827464
epoch: 181, train loss: 0.032020687678551214, acc: 0.9693974192020836; test loss: 0.6327936683696259, acc: 0.7934294493027653
epoch: 182, train loss: 0.031112123337544866, acc: 0.9704628862317982; test loss: 0.6281148028751266, acc: 0.7903568896242024
Epoch   182: reducing learning rate of group 0 to 3.7500e-04.
epoch: 183, train loss: 0.017151368840469566, acc: 0.9831892979756126; test loss: 0.6100784185450568, acc: 0.8139919640746869
epoch: 184, train loss: 0.012000497785142516, acc: 0.987036817805138; test loss: 0.6168881525415192, acc: 0.8132829118411723
epoch: 185, train loss: 0.010715519524718215, acc: 0.9899964484432343; test loss: 0.6250939515430971, acc: 0.8135192625856772
epoch: 186, train loss: 0.009105949342693829, acc: 0.9917130342133301; test loss: 0.6354150146886942, acc: 0.8139919640746869
epoch: 187, train loss: 0.01003779398874542, acc: 0.9900556410559962; test loss: 0.6383638897564248, acc: 0.8147010163082014
epoch: 188, train loss: 0.009161035561581519, acc: 0.9904699893453297; test loss: 0.6500862927280174, acc: 0.8113921058851336
epoch: 189, train loss: 0.007945996562198266, acc: 0.993488812596188; test loss: 0.6479830801332005, acc: 0.8147010163082014
epoch: 190, train loss: 0.007064773717173422, acc: 0.9937847756599977; test loss: 0.6659532964046445, acc: 0.8163554715197353
epoch: 191, train loss: 0.008727384701735475, acc: 0.991061915472949; test loss: 0.6607364406261115, acc: 0.8069014417395415
epoch: 192, train loss: 0.007788003971829977, acc: 0.9920681898899018; test loss: 0.6628837925805687, acc: 0.8154100685417159
epoch: 193, train loss: 0.007058452733449362, acc: 0.9935480052089499; test loss: 0.6916746730867556, acc: 0.8061923895060269
epoch: 194, train loss: 0.010474861243271582, acc: 0.9905291819580916; test loss: 0.6651849660769729, acc: 0.8113921058851336
epoch: 195, train loss: 0.00801627815843186, acc: 0.9921865751154256; test loss: 0.6972370685287977, acc: 0.8135192625856772
epoch: 196, train loss: 0.009223107297938859, acc: 0.9911211080857109; test loss: 0.7129742957209617, acc: 0.8024107775939494
epoch: 197, train loss: 0.014318186477950384, acc: 0.9866224695158045; test loss: 0.6856342511829601, acc: 0.8043015835499882
epoch: 198, train loss: 0.01913374851133515, acc: 0.9825973718479933; test loss: 0.6911671832503227, acc: 0.7967383597258332
epoch: 199, train loss: 0.01692655650523727, acc: 0.9846691132946608; test loss: 0.6820454824565292, acc: 0.7998109194043961
epoch: 200, train loss: 0.017198559504504652, acc: 0.98372203149047; test loss: 0.6561066545907049, acc: 0.8076104939730561
best test acc 0.8163554715197353 at epoch 190.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9997    1.0000    0.9998      6100
           1     1.0000    0.9989    0.9995       926
           2     0.9901    0.9992    0.9946      2400
           3     1.0000    1.0000    1.0000       843
           4     0.9974    0.9987    0.9981       774
           5     0.9960    1.0000    0.9980      1512
           6     1.0000    0.9932    0.9966      1330
           7     1.0000    1.0000    1.0000       481
           8     1.0000    1.0000    1.0000       458
           9     0.9956    1.0000    0.9978       452
          10     1.0000    1.0000    1.0000       717
          11     1.0000    1.0000    1.0000       333
          12     0.9928    0.9164    0.9530       299
          13     1.0000    1.0000    1.0000       269

    accuracy                         0.9978     16894
   macro avg     0.9980    0.9933    0.9955     16894
weighted avg     0.9978    0.9978    0.9977     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8691    0.9233    0.8954      1525
           1     0.9163    0.8017    0.8552       232
           2     0.8127    0.8087    0.8107       601
           3     0.8804    0.7678    0.8203       211
           4     0.8376    0.8505    0.8440       194
           5     0.8295    0.8492    0.8392       378
           6     0.6261    0.6336    0.6299       333
           7     0.8692    0.7686    0.8158       121
           8     0.6847    0.6609    0.6726       115
           9     0.8796    0.8333    0.8559       114
          10     0.8506    0.7278    0.7844       180
          11     0.8806    0.7024    0.7815        84
          12     0.1667    0.2400    0.1967        75
          13     0.8600    0.6324    0.7288        68

    accuracy                         0.8164      4231
   macro avg     0.7831    0.7286    0.7522      4231
weighted avg     0.8223    0.8164    0.8177      4231

---------------------------------------
program finished.
