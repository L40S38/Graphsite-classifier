seed:  666
save trained model at:  ../trained_models/trained_classifier_model_21.pt
save loss at:  ./results/train_classifier_results_21.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  15
number of pockets in training set:  17282
number of pockets in validation set:  3698
number of pockets in test set:  3720
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=1, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=15, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.001
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b10b5a71be0>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0661281315958373, acc: 0.36511977780349497, val loss: 1.8387103881495523, acc: 0.41481882098431583, test loss: 1.855460407913372, acc: 0.42016129032258065
epoch: 2, train loss: 1.7680419462861983, acc: 0.4475755120935077, val loss: 1.6720894920045843, acc: 0.4853975121687399, test loss: 1.677007580828923, acc: 0.4801075268817204
epoch: 3, train loss: 1.6583726899986149, acc: 0.4812521698877445, val loss: 1.5975298578510935, acc: 0.508112493239589, test loss: 1.5963831963077668, acc: 0.5067204301075269
epoch: 4, train loss: 1.5659394079495768, acc: 0.513250781159588, val loss: 1.651324190275936, acc: 0.49459167117360736, test loss: 1.6656421763922578, acc: 0.4908602150537634
epoch: 5, train loss: 1.5043782388263107, acc: 0.5285267908806851, val loss: 1.6030634092988034, acc: 0.49621416982152516, test loss: 1.5959803986293013, acc: 0.5075268817204301
epoch: 6, train loss: 1.4898231464077403, acc: 0.5339659761601666, val loss: 1.4746698648495054, acc: 0.53677663601947, test loss: 1.4617746829986573, acc: 0.5443548387096774
epoch: 7, train loss: 1.4597840020259543, acc: 0.5429927091771786, val loss: 1.4185870632472846, acc: 0.544889129259059, test loss: 1.4343682847997195, acc: 0.5540322580645162
epoch: 8, train loss: 1.4157933231416007, acc: 0.5577479458396019, val loss: 1.4052264016018874, acc: 0.5511087074094105, test loss: 1.4103512343539986, acc: 0.5543010752688172
epoch: 9, train loss: 1.3782000548171025, acc: 0.5675269066080315, val loss: 1.3784891901434015, acc: 0.5600324499729583, test loss: 1.3952458063761393, acc: 0.553494623655914
epoch: 10, train loss: 1.3359117027979568, acc: 0.5803726420553177, val loss: 1.4402570188077737, acc: 0.5381287182260681, test loss: 1.4576370567403814, acc: 0.5293010752688172
epoch: 11, train loss: 1.31835481046099, acc: 0.5891100567063997, val loss: 1.333580436804541, acc: 0.5819361817198486, test loss: 1.3608432979993923, acc: 0.568010752688172
epoch: 12, train loss: 1.2955031285971876, acc: 0.5961694248350885, val loss: 1.3264074810393247, acc: 0.5827474310438074, test loss: 1.333814851699337, acc: 0.5801075268817204
epoch: 13, train loss: 1.2926687820547365, acc: 0.5979631987038537, val loss: 1.7522874999007643, acc: 0.46619794483504595, test loss: 1.7828859811188071, acc: 0.46478494623655914
epoch: 14, train loss: 1.264963135635421, acc: 0.6002777456312927, val loss: 1.236828463215516, acc: 0.6005949161709032, test loss: 1.238189628560056, acc: 0.6048387096774194
epoch: 15, train loss: 1.2474619771799353, acc: 0.6099988427265363, val loss: 1.588106465378344, acc: 0.5305570578691184, test loss: 1.64137238840903, acc: 0.5271505376344086
epoch: 16, train loss: 1.2444144731428237, acc: 0.6113297072098137, val loss: 1.4463314280244324, acc: 0.5427257977285019, test loss: 1.47074374844951, acc: 0.5379032258064517
epoch: 17, train loss: 1.247003974123777, acc: 0.6099409790533503, val loss: 1.3235824435643082, acc: 0.575716603569497, test loss: 1.3392019143668554, acc: 0.5733870967741935
epoch: 18, train loss: 1.2066900280804673, acc: 0.6206457585927555, val loss: 1.274832948341184, acc: 0.6003244997295836, test loss: 1.2895610650380454, acc: 0.5924731182795699
epoch: 19, train loss: 1.1994645680256595, acc: 0.6204721675731976, val loss: 1.3638390010598544, acc: 0.57355327203894, test loss: 1.3841162866161716, acc: 0.5723118279569892
epoch: 20, train loss: 1.1820197619990114, acc: 0.628225899780118, val loss: 1.2892000507573427, acc: 0.6003244997295836, test loss: 1.3041665533537505, acc: 0.592741935483871
epoch: 21, train loss: 1.1689482192007594, acc: 0.6345909038305751, val loss: 1.4016132621651665, acc: 0.5562466197944835, test loss: 1.398305332532493, acc: 0.5526881720430108
epoch: 22, train loss: 1.1757808317534972, acc: 0.6362110866797824, val loss: 1.2898526212342563, acc: 0.5881557598702001, test loss: 1.3100847562154134, acc: 0.578494623655914
epoch: 23, train loss: 1.1590039950528501, acc: 0.6406665895151025, val loss: 1.2048391370015121, acc: 0.6216873985938345, test loss: 1.2230371357292258, acc: 0.6134408602150537
epoch: 24, train loss: 1.1411991791238996, acc: 0.6452956833699803, val loss: 1.3473074139615147, acc: 0.5749053542455381, test loss: 1.3562666159804149, acc: 0.5709677419354838
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9298351180041613, acc: 0.6424024997106816, val loss: 0.9653613016758821, acc: 0.6254732287723094, test loss: 0.9936682808783747, acc: 0.6080645161290322
epoch: 26, train loss: 0.8912971767199949, acc: 0.6538595070015044, val loss: 0.9804205300035574, acc: 0.6181719848566792, test loss: 1.0035022771486672, acc: 0.6112903225806452
epoch: 27, train loss: 0.8900285942137014, acc: 0.652702233537785, val loss: 0.9723082946531834, acc: 0.6097890751757706, test loss: 0.9836002560072048, acc: 0.6120967741935484
epoch: 28, train loss: 0.9032151656713133, acc: 0.6484203217220229, val loss: 1.0674566657817834, acc: 0.5968090859924283, test loss: 1.0988239483166766, acc: 0.5887096774193549
epoch: 29, train loss: 0.8822856812189608, acc: 0.6584307371831963, val loss: 0.974032279928031, acc: 0.6233098972417523, test loss: 0.9999287128448486, acc: 0.6150537634408603
epoch: 30, train loss: 0.8614677481526593, acc: 0.6631755583844462, val loss: 1.001674004152957, acc: 0.6081665765278529, test loss: 1.0317729006531418, acc: 0.6005376344086022
epoch: 31, train loss: 0.8607062080187622, acc: 0.6664159240828608, val loss: 1.0515830760520493, acc: 0.6097890751757706, test loss: 1.0812374991755331, acc: 0.5887096774193549
epoch: 32, train loss: 0.8614862346632615, acc: 0.6625390579794005, val loss: 0.977128806276667, acc: 0.623580313683072, test loss: 0.9938891123699886, acc: 0.6150537634408603
epoch: 33, train loss: 0.8457886911544958, acc: 0.6705821085522509, val loss: 0.9345301925459057, acc: 0.6435911303407247, test loss: 0.959374798497846, acc: 0.6268817204301075
epoch: 34, train loss: 0.846980638573559, acc: 0.6655479689850712, val loss: 0.9602179634306739, acc: 0.6260140616549487, test loss: 1.00878539957026, acc: 0.6067204301075269
epoch: 35, train loss: 0.8427364267473515, acc: 0.668036106932068, val loss: 1.0082585682670382, acc: 0.6149269875608437, test loss: 1.0354370486351752, acc: 0.5981182795698925
epoch: 36, train loss: 0.8354851221916866, acc: 0.6754426570998727, val loss: 0.9162400047349698, acc: 0.6430502974580855, test loss: 0.9256078797001992, acc: 0.635483870967742
epoch: 37, train loss: 0.8263575708875777, acc: 0.6764842032172202, val loss: 0.9654411067828544, acc: 0.620605732828556, test loss: 0.9773872683125158, acc: 0.6166666666666667
epoch: 38, train loss: 0.8070506415575719, acc: 0.6816919338039579, val loss: 0.9488865555525471, acc: 0.6392644672796106, test loss: 0.9501689962161485, acc: 0.6336021505376344
epoch: 39, train loss: 0.8154801365029907, acc: 0.6808818423793542, val loss: 0.875074414061881, acc: 0.6546782044348296, test loss: 0.8922966239272907, acc: 0.6395161290322581
epoch: 40, train loss: 0.8122670248818196, acc: 0.6819812521698877, val loss: 0.988269633509263, acc: 0.6262844780962683, test loss: 0.9966372489929199, acc: 0.6169354838709677
epoch: 41, train loss: 0.8161521760309036, acc: 0.6760212938317325, val loss: 0.9580859990942601, acc: 0.6316928069226609, test loss: 0.9759382083851804, acc: 0.625
epoch: 42, train loss: 0.7910841588841008, acc: 0.6844693901168846, val loss: 0.897587005997942, acc: 0.6408869659275284, test loss: 0.9089776064759941, acc: 0.6422043010752688
epoch: 43, train loss: 0.7997247994932474, acc: 0.6862053003124639, val loss: 0.9074597688801809, acc: 0.6454840454299622, test loss: 0.938153947296963, acc: 0.6370967741935484
epoch: 44, train loss: 0.8083227253832098, acc: 0.6829649346140493, val loss: 0.9792026563100779, acc: 0.6173607355327204, test loss: 0.984181030847693, acc: 0.6126344086021506
epoch: 45, train loss: 0.8194939439501727, acc: 0.6775836130077537, val loss: 0.9524497387536865, acc: 0.6295294753921038, test loss: 0.941507278462892, acc: 0.6365591397849463
epoch: 46, train loss: 0.7889581556094492, acc: 0.6883462562203448, val loss: 0.9332647987673126, acc: 0.6306111411573824, test loss: 0.9470825795204408, acc: 0.6295698924731182
epoch: 47, train loss: 0.7737628441899342, acc: 0.6940747598657563, val loss: 0.9591507597057287, acc: 0.633044889129259, test loss: 0.9714228071192259, acc: 0.6282258064516129
epoch: 48, train loss: 0.7773828596970009, acc: 0.6928017590556649, val loss: 1.112303746204366, acc: 0.5724716062736614, test loss: 1.1278131044039161, acc: 0.5701612903225807
epoch: 49, train loss: 0.7717841241097867, acc: 0.6922231223238051, val loss: 0.8838449393561751, acc: 0.6563007030827475, test loss: 0.9021709426756828, acc: 0.6494623655913978
epoch: 50, train loss: 0.784087942638768, acc: 0.6878254831616711, val loss: 0.8966281771208545, acc: 0.6606273661438615, test loss: 0.9173239677183089, acc: 0.6534946236559139
epoch: 51, train loss: 0.7678721928166068, acc: 0.6928596227288508, val loss: 0.9043638427686923, acc: 0.6603569497025419, test loss: 0.9058230225757886, acc: 0.6543010752688172
epoch: 52, train loss: 0.7648275318058563, acc: 0.6987038537206341, val loss: 0.8708805547913324, acc: 0.6533261222282315, test loss: 0.8998016613785939, acc: 0.6478494623655914
epoch: 53, train loss: 0.7893716525443795, acc: 0.6871889827566254, val loss: 0.9020029326398157, acc: 0.6530557057869119, test loss: 0.9177559344999252, acc: 0.6505376344086021
epoch: 54, train loss: 0.762059282530873, acc: 0.6960421247540793, val loss: 0.9163902378391743, acc: 0.6446727961060033, test loss: 0.9426236706395303, acc: 0.6271505376344086
epoch: 55, train loss: 0.7470881653706684, acc: 0.7025807198240944, val loss: 0.9222938748808923, acc: 0.6373715521903732, test loss: 0.9337673607692923, acc: 0.6408602150537634
epoch: 56, train loss: 0.7420262125076181, acc: 0.7025807198240944, val loss: 0.8961471095479715, acc: 0.6606273661438615, test loss: 0.9077119155596661, acc: 0.6513440860215054
epoch: 57, train loss: 0.7289796530986574, acc: 0.7108552250896887, val loss: 0.924761064249867, acc: 0.6430502974580855, test loss: 0.9307575605248892, acc: 0.6373655913978494
epoch: 58, train loss: 0.7419761677277252, acc: 0.7050109940979054, val loss: 0.9211340008586597, acc: 0.6476473769605192, test loss: 0.9399739255187332, acc: 0.6317204301075269
epoch: 59, train loss: 0.7232878849594947, acc: 0.7104501793773869, val loss: 0.8658342036509785, acc: 0.6627906976744186, test loss: 0.898108434677124, acc: 0.646236559139785
epoch: 60, train loss: 0.7129624106516759, acc: 0.7157157736373105, val loss: 0.8566881050091295, acc: 0.667658193618172, test loss: 0.8686631694916755, acc: 0.6653225806451613
epoch: 61, train loss: 0.7167072741332229, acc: 0.7118967712070362, val loss: 0.8584820496965061, acc: 0.6581936181719849, test loss: 0.8724866959356492, acc: 0.6599462365591398
epoch: 62, train loss: 0.7158093828871327, acc: 0.7126489989584539, val loss: 0.837374689902661, acc: 0.6725256895619254, test loss: 0.8423411733360701, acc: 0.6696236559139785
epoch: 63, train loss: 0.7049416831047112, acc: 0.7180303205647495, val loss: 0.903415732025262, acc: 0.6517036235803136, test loss: 0.9057144631621659, acc: 0.6553763440860215
epoch: 64, train loss: 0.7030127194382749, acc: 0.7167573197546581, val loss: 0.9008192794267521, acc: 0.6519740400216333, test loss: 0.9069378201679517, acc: 0.65
epoch: 65, train loss: 0.7099822672852872, acc: 0.7157736373104965, val loss: 0.8589312647277436, acc: 0.6660356949702542, test loss: 0.8660026252910655, acc: 0.6586021505376344
epoch: 66, train loss: 0.702413850906146, acc: 0.7193033213748409, val loss: 0.8494812952240717, acc: 0.6717144402379664, test loss: 0.883678283486315, acc: 0.6629032258064517
epoch: 67, train loss: 0.6840304528628078, acc: 0.7269991899085754, val loss: 0.8234957726727697, acc: 0.6733369388858843, test loss: 0.860576940864645, acc: 0.6706989247311828
epoch: 68, train loss: 0.7323934473024034, acc: 0.708077768776762, val loss: 0.9347420147523937, acc: 0.6419686316928069, test loss: 0.9587028764909313, acc: 0.6298387096774194
epoch: 69, train loss: 0.7150265164168921, acc: 0.7127068626316398, val loss: 0.830589668759789, acc: 0.680908599242834, test loss: 0.8537774111634941, acc: 0.6704301075268817
epoch: 70, train loss: 0.702810261808547, acc: 0.7198819581067006, val loss: 1.014438246442796, acc: 0.6219578150351541, test loss: 1.036152098768501, acc: 0.6115591397849462
epoch: 71, train loss: 0.7347536198299056, acc: 0.7081934961231339, val loss: 0.9016847485397363, acc: 0.6546782044348296, test loss: 0.9501781443113921, acc: 0.6486559139784946
epoch: 72, train loss: 0.7086305037484016, acc: 0.7145006365004051, val loss: 0.8175869643075715, acc: 0.6855056787452677, test loss: 0.83464401460463, acc: 0.671236559139785
epoch: 73, train loss: 0.7016615027865938, acc: 0.7180881842379354, val loss: 0.8577446708039634, acc: 0.6698215251487291, test loss: 0.8700024568906395, acc: 0.6663978494623656
epoch: 74, train loss: 0.688961035509157, acc: 0.7242217335956487, val loss: 0.8849503230635961, acc: 0.6600865332612222, test loss: 0.9061905635300503, acc: 0.65
epoch: 75, train loss: 0.6729015022319661, acc: 0.7308181923388497, val loss: 0.8603955948913723, acc: 0.6649540292049757, test loss: 0.8628772063921857, acc: 0.6758064516129032
epoch: 76, train loss: 0.676175659866662, acc: 0.728098599699109, val loss: 0.8351822125067125, acc: 0.6744186046511628, test loss: 0.8615700896068286, acc: 0.6620967741935484
epoch: 77, train loss: 0.6745624012547556, acc: 0.7260733711375998, val loss: 0.8412781171762601, acc: 0.672796106003245, test loss: 0.8690979752489315, acc: 0.6596774193548387
epoch: 78, train loss: 0.6819325779519303, acc: 0.7290244184700845, val loss: 0.8504469483655617, acc: 0.6611681990265008, test loss: 0.8908788358011553, acc: 0.6481182795698924
epoch: 79, train loss: 0.6555866675649497, acc: 0.7346371947691239, val loss: 0.8626446281657985, acc: 0.678745267712277, test loss: 0.897315240162675, acc: 0.6580645161290323
epoch: 80, train loss: 0.6604567758188026, acc: 0.7384561971993983, val loss: 0.8461399475518532, acc: 0.6822606814494321, test loss: 0.8478148803916029, acc: 0.6690860215053763
epoch: 81, train loss: 0.6591615054690442, acc: 0.738919106584886, val loss: 0.9285799507968422, acc: 0.6371011357490536, test loss: 0.9325114014328167, acc: 0.6352150537634409
epoch: 82, train loss: 0.6669886128078276, acc: 0.7329012845735448, val loss: 0.9907724587836093, acc: 0.6260140616549487, test loss: 0.9992842458909558, acc: 0.6266129032258064
epoch: 83, train loss: 0.6559887395359247, acc: 0.7396134706631177, val loss: 0.8542368038078461, acc: 0.6703623580313683, test loss: 0.9058286133632865, acc: 0.6594086021505376
Epoch    83: reducing learning rate of group 0 to 1.5000e-03.
epoch: 84, train loss: 0.5763517466839673, acc: 0.7676194884851291, val loss: 0.7622043984590059, acc: 0.7049756625202812, test loss: 0.7706538456742481, acc: 0.6932795698924731
epoch: 85, train loss: 0.5445929289540129, acc: 0.7769934035412568, val loss: 0.7897980891413918, acc: 0.7055164954029205, test loss: 0.8178778725285684, acc: 0.6997311827956989
epoch: 86, train loss: 0.5313978597889094, acc: 0.7824904524939243, val loss: 0.7858032254414535, acc: 0.7090319091400757, test loss: 0.806899614744289, acc: 0.7032258064516129
epoch: 87, train loss: 0.5289346264711258, acc: 0.7819696794352505, val loss: 0.7648178607854023, acc: 0.7106544077879935, test loss: 0.7850319549601565, acc: 0.7137096774193549
epoch: 88, train loss: 0.5186466758566561, acc: 0.7874667283879181, val loss: 0.8320944892064245, acc: 0.6909140075716603, test loss: 0.8287179347007505, acc: 0.6846774193548387
epoch: 89, train loss: 0.5185744585903717, acc: 0.7915750491841222, val loss: 0.8065903577500256, acc: 0.7074094104921579, test loss: 0.8224480536676222, acc: 0.7002688172043011
epoch: 90, train loss: 0.5092195977754662, acc: 0.7931952320333294, val loss: 0.756331557128931, acc: 0.7114656571119524, test loss: 0.7783325133785125, acc: 0.6991935483870968
epoch: 91, train loss: 0.4955412463007281, acc: 0.7990394630251129, val loss: 0.8130968186970077, acc: 0.6976744186046512, test loss: 0.8357796592097129, acc: 0.693010752688172
epoch: 92, train loss: 0.5108132666079703, acc: 0.7904756393935887, val loss: 0.8044600583463827, acc: 0.6884802595997837, test loss: 0.8490613163158458, acc: 0.6865591397849462
epoch: 93, train loss: 0.5196662869335891, acc: 0.7886240018516375, val loss: 0.807330877991094, acc: 0.7011898323418064, test loss: 0.8109556336556711, acc: 0.6967741935483871
epoch: 94, train loss: 0.4994053505882733, acc: 0.7967827797708599, val loss: 0.793498428916209, acc: 0.7098431584640346, test loss: 0.8128439949404809, acc: 0.7010752688172043
epoch: 95, train loss: 0.4988323600876319, acc: 0.7963198703853721, val loss: 0.8454595026033772, acc: 0.7003785830178475, test loss: 0.861475993228215, acc: 0.6884408602150538
epoch: 96, train loss: 0.4896404983689423, acc: 0.7964934614049299, val loss: 0.7985881891297031, acc: 0.6984856679286101, test loss: 0.8211324061116865, acc: 0.6908602150537635
epoch: 97, train loss: 0.4880651872876259, acc: 0.7995023724106006, val loss: 0.8478236950172354, acc: 0.6922660897782585, test loss: 0.8434484774066556, acc: 0.685752688172043
epoch: 98, train loss: 0.48281363308767716, acc: 0.7995602360837866, val loss: 0.792264032905459, acc: 0.7101135749053542, test loss: 0.7989972247872301, acc: 0.7120967741935483
epoch: 99, train loss: 0.49294818222984893, acc: 0.7992709177178567, val loss: 0.8142991033742852, acc: 0.7106544077879935, test loss: 0.81228867243695, acc: 0.706989247311828
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3767130717492024, acc: 0.8024534197430853, val loss: 0.7153673738063897, acc: 0.7055164954029205, test loss: 0.7230993424692461, acc: 0.696236559139785
epoch: 101, train loss: 0.3882092031674064, acc: 0.7973614165027196, val loss: 0.668687058887719, acc: 0.7144402379664684, test loss: 0.666732680925759, acc: 0.7026881720430107
epoch: 102, train loss: 0.3532458978620286, acc: 0.8127531535701886, val loss: 0.693626193499939, acc: 0.7106544077879935, test loss: 0.6865331854871525, acc: 0.6951612903225807
epoch: 103, train loss: 0.3733492540707537, acc: 0.8039000115727346, val loss: 0.6738275550648353, acc: 0.7125473228772309, test loss: 0.7081691388160952, acc: 0.7088709677419355
epoch: 104, train loss: 0.37632910218342025, acc: 0.800833236893878, val loss: 0.6653999521514414, acc: 0.7020010816657652, test loss: 0.6790049301680698, acc: 0.703494623655914
epoch: 105, train loss: 0.354716403946365, acc: 0.8085291054276126, val loss: 0.6836002738234673, acc: 0.709572742022715, test loss: 0.7159406610714492, acc: 0.6973118279569892
epoch: 106, train loss: 0.35207491446777484, acc: 0.8099756972572619, val loss: 0.6928961144195112, acc: 0.6998377501352082, test loss: 0.7103652164500247, acc: 0.6938172043010753
epoch: 107, train loss: 0.35775049720741975, acc: 0.8079504686957528, val loss: 0.7332262591841158, acc: 0.6946998377501352, test loss: 0.779783389388874, acc: 0.696505376344086
epoch: 108, train loss: 0.35738197804021554, acc: 0.8078926050225669, val loss: 0.7055578533541905, acc: 0.6971335857220119, test loss: 0.740682953147478, acc: 0.6919354838709677
epoch: 109, train loss: 0.34285360647839896, acc: 0.8158199282490453, val loss: 0.6900456106424203, acc: 0.7214710654407788, test loss: 0.7044065634409586, acc: 0.7094086021505376
epoch: 110, train loss: 0.3669796268251011, acc: 0.8042471936118505, val loss: 0.6988318459029582, acc: 0.7076798269334775, test loss: 0.7030336708150884, acc: 0.7010752688172043
epoch: 111, train loss: 0.3716067531196873, acc: 0.8036106932068048, val loss: 0.725782922437347, acc: 0.7036235803136831, test loss: 0.7199633618836762, acc: 0.6997311827956989
epoch: 112, train loss: 0.3534056907926027, acc: 0.8065617405392894, val loss: 0.6976527636472827, acc: 0.7103839913466738, test loss: 0.70977934868105, acc: 0.7026881720430107
epoch: 113, train loss: 0.33387466727783666, acc: 0.8128110172433746, val loss: 0.7036727453451533, acc: 0.7063277447268794, test loss: 0.7112214621677193, acc: 0.6991935483870968
epoch: 114, train loss: 0.35616399146881167, acc: 0.803205647494503, val loss: 0.6737334755319334, acc: 0.7090319091400757, test loss: 0.6841129420905985, acc: 0.7104838709677419
epoch: 115, train loss: 0.34837841640991024, acc: 0.8106700613354936, val loss: 0.6941612260673032, acc: 0.7036235803136831, test loss: 0.6904922167460124, acc: 0.703763440860215
epoch: 116, train loss: 0.3523350753189306, acc: 0.8094549241985881, val loss: 0.6748464325429814, acc: 0.7155219037317468, test loss: 0.6849411215833439, acc: 0.7024193548387097
epoch: 117, train loss: 0.358123412260288, acc: 0.8059831038074297, val loss: 0.710285173357339, acc: 0.6974040021633315, test loss: 0.7137624407327303, acc: 0.6967741935483871
epoch: 118, train loss: 0.3498911110115415, acc: 0.8082976507348687, val loss: 0.7082734644380758, acc: 0.7060573282855598, test loss: 0.7283497877018426, acc: 0.6989247311827957
epoch: 119, train loss: 0.33126394323570946, acc: 0.8126374262238167, val loss: 0.7133947371662855, acc: 0.6946998377501352, test loss: 0.7357544309349471, acc: 0.6879032258064516
epoch: 120, train loss: 0.33599795129147136, acc: 0.8137368360143502, val loss: 0.7164337145050775, acc: 0.7076798269334775, test loss: 0.7269071702034243, acc: 0.7086021505376344
epoch: 121, train loss: 0.3276581284818019, acc: 0.8139104270339081, val loss: 0.7210206556732813, acc: 0.7049756625202812, test loss: 0.7282329215798327, acc: 0.6978494623655914
epoch: 122, train loss: 0.3441114754033108, acc: 0.8086448327739845, val loss: 0.691521596367002, acc: 0.7065981611681991, test loss: 0.6956443894294, acc: 0.6967741935483871
epoch: 123, train loss: 0.3297807068312117, acc: 0.8161671102881611, val loss: 0.7150577544650237, acc: 0.7041644131963224, test loss: 0.723297361148301, acc: 0.7024193548387097
epoch: 124, train loss: 0.31788658246634227, acc: 0.8215484318944567, val loss: 0.7230135442889529, acc: 0.7033531638723635, test loss: 0.7287588893726308, acc: 0.7021505376344086
epoch: 125, train loss: 0.33782379359037923, acc: 0.8114801527600972, val loss: 0.7278684837357814, acc: 0.6890210924824229, test loss: 0.7201132774353027, acc: 0.6903225806451613
epoch: 126, train loss: 0.32048193133135333, acc: 0.8181344751764842, val loss: 0.7009044855333393, acc: 0.6998377501352082, test loss: 0.7454213862778039, acc: 0.6854838709677419
epoch: 127, train loss: 0.31295425895187207, acc: 0.8206226131234811, val loss: 0.7223992051144688, acc: 0.6995673336938886, test loss: 0.7348621978554675, acc: 0.6903225806451613
epoch: 128, train loss: 0.32857127717876444, acc: 0.8092813331790302, val loss: 0.7304225644915732, acc: 0.7057869118442401, test loss: 0.767461307074434, acc: 0.6967741935483871
epoch: 129, train loss: 0.3327297031562122, acc: 0.8133896539752343, val loss: 0.7260517363550858, acc: 0.7017306652244456, test loss: 0.7244275595552178, acc: 0.7005376344086022
epoch: 130, train loss: 0.3174753521944469, acc: 0.816456428654091, val loss: 0.7355388039959773, acc: 0.7030827474310438, test loss: 0.772122274932041, acc: 0.6935483870967742
epoch: 131, train loss: 0.3323117652544751, acc: 0.811885198472399, val loss: 0.7086029743877471, acc: 0.7047052460789616, test loss: 0.7169567205572641, acc: 0.7002688172043011
epoch: 132, train loss: 0.3185898612362549, acc: 0.8174979747714385, val loss: 0.7219701880954806, acc: 0.7074094104921579, test loss: 0.740260071908274, acc: 0.696505376344086
epoch: 133, train loss: 0.3018140142479369, acc: 0.8219534776067585, val loss: 0.7114999296859899, acc: 0.7009194159004868, test loss: 0.7532157072456934, acc: 0.6884408602150538
epoch: 134, train loss: 0.3288901838342592, acc: 0.8124638352042588, val loss: 0.7626344235669348, acc: 0.6833423472147107, test loss: 0.7981855628310993, acc: 0.6809139784946237
Epoch   134: reducing learning rate of group 0 to 7.5000e-04.
epoch: 135, train loss: 0.27250708957959235, acc: 0.8356671681518343, val loss: 0.7138723635944306, acc: 0.7212006489994591, test loss: 0.7418371764562464, acc: 0.7096774193548387
epoch: 136, train loss: 0.23179389531724695, acc: 0.858465455387108, val loss: 0.7057922918000951, acc: 0.7193077339102217, test loss: 0.7497802021682903, acc: 0.7161290322580646
epoch: 137, train loss: 0.21627384535782923, acc: 0.8639625043397755, val loss: 0.7376794862772982, acc: 0.7217414818820984, test loss: 0.769699094628775, acc: 0.7126344086021505
epoch: 138, train loss: 0.21492759982388462, acc: 0.8652933688230529, val loss: 0.7865066359402877, acc: 0.7230935640886966, test loss: 0.8060713706477995, acc: 0.7155913978494624
epoch: 139, train loss: 0.20620037587707676, acc: 0.867318597384562, val loss: 0.7766321133509915, acc: 0.7212006489994591, test loss: 0.7850445352574831, acc: 0.7137096774193549
epoch: 140, train loss: 0.2019144455332017, acc: 0.8727577826640435, val loss: 0.76626342383509, acc: 0.72065981611682, test loss: 0.7740867645509781, acc: 0.7137096774193549
epoch: 141, train loss: 0.21730533158416315, acc: 0.862631639856498, val loss: 0.7557793552002564, acc: 0.7171444023796647, test loss: 0.7901583717715356, acc: 0.7155913978494624
epoch: 142, train loss: 0.20753066074984752, acc: 0.8702117810438607, val loss: 0.7565555085099149, acc: 0.7252568956192537, test loss: 0.7991135694647348, acc: 0.7080645161290322
epoch: 143, train loss: 0.20956075258752205, acc: 0.8684758708482815, val loss: 0.777126246365165, acc: 0.722011898323418, test loss: 0.7794375722126294, acc: 0.7163978494623656
epoch: 144, train loss: 0.20555385501451506, acc: 0.8700381900243027, val loss: 0.8187053915356997, acc: 0.7160627366143861, test loss: 0.8432507648262927, acc: 0.7080645161290322
epoch: 145, train loss: 0.20670771281513098, acc: 0.8692280985996991, val loss: 0.7881304107529851, acc: 0.7109248242293131, test loss: 0.7992091471149075, acc: 0.7018817204301075
epoch: 146, train loss: 0.19561474664166298, acc: 0.8743201018400648, val loss: 0.7798713867957041, acc: 0.7203893996755003, test loss: 0.8189383696484309, acc: 0.7118279569892473
epoch: 147, train loss: 0.1930583554166236, acc: 0.8731628283763453, val loss: 0.797609506522468, acc: 0.7171444023796647, test loss: 0.8302993148885748, acc: 0.7115591397849462
epoch: 148, train loss: 0.2083871040910141, acc: 0.8670871426918181, val loss: 0.7665201218596274, acc: 0.7103839913466738, test loss: 0.8266290110926474, acc: 0.703494623655914
epoch: 149, train loss: 0.20597733268720134, acc: 0.867145006365004, val loss: 0.7750502602353877, acc: 0.7106544077879935, test loss: 0.7970460958378289, acc: 0.7096774193548387
epoch: 150, train loss: 0.19786172721027542, acc: 0.8711375998148363, val loss: 0.8149677287442675, acc: 0.7076798269334775, test loss: 0.8287153561909993, acc: 0.6981182795698925
epoch: 151, train loss: 0.19699982356009776, acc: 0.8686494618678394, val loss: 0.7960079103111899, acc: 0.7136289886425095, test loss: 0.7983394156220138, acc: 0.7064516129032258
epoch: 152, train loss: 0.20323893424744766, acc: 0.8684180071750954, val loss: 0.7750751533786822, acc: 0.7138994050838291, test loss: 0.7869050031067223, acc: 0.7137096774193549
epoch: 153, train loss: 0.18948499344659203, acc: 0.873857192454577, val loss: 0.8282927863336113, acc: 0.7063277447268794, test loss: 0.8698886281700544, acc: 0.6967741935483871
epoch: 154, train loss: 0.19760176936739393, acc: 0.8724106006249277, val loss: 0.8323802945419154, acc: 0.7011898323418064, test loss: 0.8348160610404066, acc: 0.6924731182795699
epoch: 155, train loss: 0.19564019474162642, acc: 0.8723527369517416, val loss: 0.8193392681392094, acc: 0.7093023255813954, test loss: 0.8215085660257647, acc: 0.7008064516129032
epoch: 156, train loss: 0.1974931951319505, acc: 0.869691007985187, val loss: 0.8042480636120101, acc: 0.7155219037317468, test loss: 0.8601379512458719, acc: 0.7013440860215053
epoch: 157, train loss: 0.19192073889531108, acc: 0.8702117810438607, val loss: 0.8107349109237035, acc: 0.717685235262304, test loss: 0.8238192589052262, acc: 0.7123655913978495
epoch: 158, train loss: 0.19901013344150356, acc: 0.8665663696331443, val loss: 0.8093253729342125, acc: 0.7184964845862628, test loss: 0.8179627377499816, acc: 0.7029569892473119
epoch: 159, train loss: 0.18651477793703033, acc: 0.8766925124406897, val loss: 0.8221712093085066, acc: 0.7117360735532721, test loss: 0.8253581370076826, acc: 0.7120967741935483
epoch: 160, train loss: 0.17515296070212333, acc: 0.8804536511977781, val loss: 0.8079299483188364, acc: 0.7214710654407788, test loss: 0.8574955760791737, acc: 0.7118279569892473
epoch: 161, train loss: 0.17385425506822075, acc: 0.8832311075107048, val loss: 0.8179670178355108, acc: 0.72065981611682, test loss: 0.8461869229552567, acc: 0.7126344086021505
epoch: 162, train loss: 0.18177395504791005, acc: 0.8749566022451105, val loss: 0.8504864611194997, acc: 0.7111952406706328, test loss: 0.8520029488430229, acc: 0.7061827956989247
epoch: 163, train loss: 0.18317575449745435, acc: 0.878139104270339, val loss: 0.8047592489057905, acc: 0.717685235262304, test loss: 0.8636201622665569, acc: 0.7064516129032258
epoch: 164, train loss: 0.19254959067451555, acc: 0.8741465108205069, val loss: 0.8592884443849406, acc: 0.7160627366143861, test loss: 0.8935834274497083, acc: 0.693010752688172
epoch: 165, train loss: 0.20205640767841837, acc: 0.8687651892142113, val loss: 0.7938641043467545, acc: 0.7166035694970254, test loss: 0.8226251058681037, acc: 0.7120967741935483
epoch: 166, train loss: 0.19403527842091128, acc: 0.8688230528873973, val loss: 0.8146123559878542, acc: 0.7036235803136831, test loss: 0.8316353741512503, acc: 0.7002688172043011
epoch: 167, train loss: 0.1830190265069004, acc: 0.8789491956949427, val loss: 0.887956080403181, acc: 0.701460248783126, test loss: 0.8940180691339636, acc: 0.692741935483871
epoch: 168, train loss: 0.18531037911043277, acc: 0.8773290128457355, val loss: 0.8259082048115954, acc: 0.7182260681449432, test loss: 0.854793191725208, acc: 0.7018817204301075
epoch: 169, train loss: 0.17881743753553747, acc: 0.8788913320217567, val loss: 0.8257776600016072, acc: 0.7093023255813954, test loss: 0.8687281347090198, acc: 0.7056451612903226
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.143981539117367, acc: 0.874030783474135, val loss: 0.7348233790833477, acc: 0.7128177393185505, test loss: 0.7392987358954645, acc: 0.703763440860215
epoch: 171, train loss: 0.12601691807099977, acc: 0.8813794699687536, val loss: 0.7286973031808905, acc: 0.7141698215251487, test loss: 0.7458391584375853, acc: 0.7096774193548387
epoch: 172, train loss: 0.10946090224457344, acc: 0.8944566601087837, val loss: 0.7408151767136536, acc: 0.7214710654407788, test loss: 0.7756418658841041, acc: 0.7032258064516129
epoch: 173, train loss: 0.11247293499436675, acc: 0.8911005670639972, val loss: 0.7317493578238768, acc: 0.7184964845862628, test loss: 0.7623266307256555, acc: 0.7102150537634409
epoch: 174, train loss: 0.10713961403422466, acc: 0.8946881148015277, val loss: 0.754225903875187, acc: 0.7163331530557058, test loss: 0.7857850705423662, acc: 0.7061827956989247
epoch: 175, train loss: 0.11018176045203676, acc: 0.8891332021756742, val loss: 0.7271717858353197, acc: 0.7155219037317468, test loss: 0.7409863197675315, acc: 0.7075268817204301
epoch: 176, train loss: 0.11563773156540245, acc: 0.8904640666589515, val loss: 0.7566909182709961, acc: 0.7130881557598702, test loss: 0.7718472926847396, acc: 0.7008064516129032
epoch: 177, train loss: 0.12400602157255826, acc: 0.8827103344520311, val loss: 0.7605900875357694, acc: 0.6992969172525689, test loss: 0.7912627850809405, acc: 0.6900537634408602
epoch: 178, train loss: 0.12966343510097106, acc: 0.8791227867145006, val loss: 0.7671718723777825, acc: 0.6995673336938886, test loss: 0.7784653479053129, acc: 0.7048387096774194
epoch: 179, train loss: 0.15315715918426726, acc: 0.8725263279712996, val loss: 1.1972251700478158, acc: 0.5694970254191455, test loss: 1.2100611978961575, acc: 0.556989247311828
epoch: 180, train loss: 0.3157148071701722, acc: 0.7873510010415461, val loss: 0.620948055874921, acc: 0.7171444023796647, test loss: 0.6407935232244512, acc: 0.7016129032258065
epoch: 181, train loss: 0.18606408116283854, acc: 0.8483393125795625, val loss: 0.664253999246914, acc: 0.717685235262304, test loss: 0.7226774792517385, acc: 0.7018817204301075
epoch: 182, train loss: 0.15086410297672584, acc: 0.8647147320911931, val loss: 0.6896627160779459, acc: 0.722011898323418, test loss: 0.6992952551893009, acc: 0.7134408602150538
epoch: 183, train loss: 0.13642685799780246, acc: 0.8712533271612082, val loss: 0.695442925665171, acc: 0.7157923201730665, test loss: 0.7220869479640838, acc: 0.714247311827957
epoch: 184, train loss: 0.12349742692459266, acc: 0.8786598773290129, val loss: 0.7103555104486229, acc: 0.7152514872904273, test loss: 0.7410412267972064, acc: 0.7083333333333334
epoch: 185, train loss: 0.11966478204322957, acc: 0.8845041083207962, val loss: 0.7327104313686901, acc: 0.7130881557598702, test loss: 0.7542660561941004, acc: 0.7059139784946237
Epoch   185: reducing learning rate of group 0 to 3.7500e-04.
epoch: 186, train loss: 0.10385441689900454, acc: 0.895903251938433, val loss: 0.7264534383931632, acc: 0.7217414818820984, test loss: 0.7648713050350067, acc: 0.7118279569892473
epoch: 187, train loss: 0.09329149885946589, acc: 0.902615438028006, val loss: 0.7487622926917316, acc: 0.7233639805300163, test loss: 0.7801616373882498, acc: 0.7088709677419355
epoch: 188, train loss: 0.08666424518098656, acc: 0.9044670755699572, val loss: 0.7556667103517243, acc: 0.7201189832341807, test loss: 0.7945768807523994, acc: 0.7123655913978495
epoch: 189, train loss: 0.08377160687038791, acc: 0.9082860780002314, val loss: 0.7640052914554973, acc: 0.7222823147647377, test loss: 0.80832168568847, acc: 0.710752688172043
epoch: 190, train loss: 0.08484912554625353, acc: 0.9088068510589052, val loss: 0.7752905767501658, acc: 0.7152514872904273, test loss: 0.7961761284899967, acc: 0.7118279569892473
epoch: 191, train loss: 0.08233929689063503, acc: 0.9076495775951857, val loss: 0.7857897877370815, acc: 0.7241752298539751, test loss: 0.8204703110520558, acc: 0.7123655913978495
epoch: 192, train loss: 0.07517085675080747, acc: 0.9154033098021063, val loss: 0.7944495449716042, acc: 0.7166035694970254, test loss: 0.8369771859979117, acc: 0.7094086021505376
epoch: 193, train loss: 0.07682526546859476, acc: 0.9149982640898044, val loss: 0.7928692631749864, acc: 0.7090319091400757, test loss: 0.8536573266470304, acc: 0.7056451612903226
epoch: 194, train loss: 0.08132815999681096, acc: 0.9126258534891795, val loss: 0.7998901968585149, acc: 0.7163331530557058, test loss: 0.8450686039463167, acc: 0.7048387096774194
epoch: 195, train loss: 0.08127796559742984, acc: 0.9076495775951857, val loss: 0.7961115595970752, acc: 0.7184964845862628, test loss: 0.8488417625427246, acc: 0.6997311827956989
epoch: 196, train loss: 0.08057136767448037, acc: 0.9105427612544844, val loss: 0.7950545854346697, acc: 0.7187669010275824, test loss: 0.8542699957406649, acc: 0.7075268817204301
epoch: 197, train loss: 0.07493805706573435, acc: 0.9162134012267099, val loss: 0.8181142846979664, acc: 0.7203893996755003, test loss: 0.8568546254147765, acc: 0.7096774193548387
epoch: 198, train loss: 0.0751085039290713, acc: 0.9152297187825483, val loss: 0.8253783955323883, acc: 0.7152514872904273, test loss: 0.874882225836477, acc: 0.7040322580645161
epoch: 199, train loss: 0.07971201709624366, acc: 0.9110635343131582, val loss: 0.8483933930528558, acc: 0.7125473228772309, test loss: 0.8674875818273072, acc: 0.7056451612903226
epoch: 200, train loss: 0.08512454262908004, acc: 0.9079388959611157, val loss: 0.8598775409762442, acc: 0.7003785830178475, test loss: 0.8676352336842527, acc: 0.6986559139784946
best val acc 0.7252568956192537 at epoch 142.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9607    0.9578    0.9593      5337
           1     0.8237    0.8237    0.8237      2502
           2     0.9384    0.9407    0.9396       810
           3     0.8918    0.9457    0.9180      2100
           4     0.9572    0.9104    0.9332       737
           5     0.9355    0.9645    0.9498       677
           6     0.8110    0.9342    0.8683      1323
           7     0.8314    0.7921    0.8113      1164
           8     0.9245    0.8432    0.8820       421
           9     0.8085    0.9476    0.8726       401
          10     0.9572    0.9596    0.9584       396
          11     0.9255    0.8915    0.9082       627
          12     0.9401    0.8625    0.8996       291
          13     0.8485    0.1073    0.1905       261
          14     0.8112    0.8043    0.8077       235

    accuracy                         0.8995     17282
   macro avg     0.8910    0.8457    0.8481     17282
weighted avg     0.9003    0.8995    0.8949     17282

train confusion matrix:
[[9.57841484e-01 6.55799138e-03 5.62113547e-04 6.18324902e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 8.05696084e-03
  5.24639310e-03 1.25538692e-02 3.74742365e-04 7.49484729e-04
  9.36855912e-04 7.49484729e-04 1.87371182e-04]
 [8.39328537e-03 8.23741007e-01 1.59872102e-03 4.87609912e-02
  3.59712230e-03 0.00000000e+00 1.03517186e-01 6.79456435e-03
  0.00000000e+00 3.99680256e-04 0.00000000e+00 2.39808153e-03
  3.99680256e-04 0.00000000e+00 3.99680256e-04]
 [1.23456790e-03 0.00000000e+00 9.40740741e-01 1.23456790e-03
  0.00000000e+00 4.44444444e-02 4.93827160e-03 1.23456790e-03
  0.00000000e+00 2.46913580e-03 0.00000000e+00 3.70370370e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [1.28571429e-02 2.71428571e-02 0.00000000e+00 9.45714286e-01
  4.76190476e-04 2.38095238e-03 1.90476190e-03 7.61904762e-03
  0.00000000e+00 0.00000000e+00 9.52380952e-04 0.00000000e+00
  4.76190476e-04 4.76190476e-04 0.00000000e+00]
 [1.35685210e-03 4.07055631e-02 0.00000000e+00 2.71370421e-03
  9.10447761e-01 0.00000000e+00 5.42740841e-03 1.35685210e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.30664858e-02
  2.71370421e-03 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 3.39734121e-02 0.00000000e+00
  0.00000000e+00 9.64549483e-01 1.47710487e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [2.26757370e-03 5.36659108e-02 2.26757370e-03 0.00000000e+00
  2.26757370e-03 1.51171580e-03 9.34240363e-01 7.55857899e-04
  0.00000000e+00 2.26757370e-03 0.00000000e+00 0.00000000e+00
  7.55857899e-04 0.00000000e+00 0.00000000e+00]
 [4.03780069e-02 7.47422680e-02 0.00000000e+00 1.46048110e-02
  1.03092784e-02 0.00000000e+00 1.20274914e-02 7.92096220e-01
  8.59106529e-04 8.59106529e-04 9.45017182e-03 8.59106529e-03
  8.59106529e-04 0.00000000e+00 3.52233677e-02]
 [1.33016627e-01 9.50118765e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.50118765e-03
  8.43230404e-01 0.00000000e+00 0.00000000e+00 4.75059382e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [1.49625935e-02 2.49376559e-03 1.99501247e-02 0.00000000e+00
  2.49376559e-03 0.00000000e+00 0.00000000e+00 2.49376559e-03
  0.00000000e+00 9.47630923e-01 0.00000000e+00 4.98753117e-03
  4.98753117e-03 0.00000000e+00 0.00000000e+00]
 [1.26262626e-02 0.00000000e+00 0.00000000e+00 5.05050505e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.76767677e-02
  0.00000000e+00 0.00000000e+00 9.59595960e-01 0.00000000e+00
  2.52525253e-03 0.00000000e+00 2.52525253e-03]
 [4.78468900e-03 1.27591707e-02 6.37958533e-03 3.18979266e-03
  4.78468900e-03 0.00000000e+00 0.00000000e+00 6.69856459e-02
  0.00000000e+00 7.97448166e-03 0.00000000e+00 8.91547049e-01
  1.59489633e-03 0.00000000e+00 0.00000000e+00]
 [4.12371134e-02 3.43642612e-03 1.37457045e-02 1.71821306e-02
  0.00000000e+00 3.43642612e-03 3.43642612e-03 6.87285223e-03
  0.00000000e+00 3.78006873e-02 6.87285223e-03 3.43642612e-03
  8.62542955e-01 0.00000000e+00 0.00000000e+00]
 [9.96168582e-02 5.63218391e-01 0.00000000e+00 2.18390805e-01
  0.00000000e+00 0.00000000e+00 3.83141762e-03 7.66283525e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.07279693e-01 0.00000000e+00]
 [4.25531915e-03 0.00000000e+00 4.25531915e-03 0.00000000e+00
  4.25531915e-03 4.25531915e-03 0.00000000e+00 1.74468085e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.25531915e-03 0.00000000e+00 8.04255319e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8637    0.8259    0.8444      1143
           1     0.6007    0.6119    0.6063       536
           2     0.7586    0.7630    0.7608       173
           3     0.6904    0.7533    0.7205       450
           4     0.8519    0.7278    0.7850       158
           5     0.7947    0.8276    0.8108       145
           6     0.6975    0.7986    0.7446       283
           7     0.5018    0.5502    0.5249       249
           8     0.7826    0.6000    0.6792        90
           9     0.5048    0.6235    0.5579        85
          10     0.7684    0.8690    0.8156        84
          11     0.7424    0.7313    0.7368       134
          12     0.6349    0.6452    0.6400        62
          13     0.0833    0.0179    0.0294        56
          14     0.6286    0.4400    0.5176        50

    accuracy                         0.7253      3698
   macro avg     0.6603    0.6524    0.6516      3698
weighted avg     0.7237    0.7253    0.7225      3698

validation confusion matrix:
[[0.82589676 0.03412073 0.0096238  0.04024497 0.00174978 0.
  0.00437445 0.03149606 0.01049869 0.02362205 0.00262467 0.00349956
  0.00699913 0.00349956 0.00174978]
 [0.05410448 0.6119403  0.01119403 0.10447761 0.01119403 0.00746269
  0.10074627 0.05783582 0.00373134 0.0130597  0.00186567 0.01119403
  0.00186567 0.00559701 0.00373134]
 [0.02890173 0.01734104 0.76300578 0.02312139 0.00578035 0.08092486
  0.06358382 0.01156069 0.         0.00578035 0.         0.
  0.         0.         0.        ]
 [0.04444444 0.12444444 0.00444444 0.75333333 0.00444444 0.
  0.00888889 0.03333333 0.         0.00444444 0.00222222 0.00444444
  0.00888889 0.00666667 0.        ]
 [0.01265823 0.09493671 0.00632911 0.01898734 0.7278481  0.
  0.01898734 0.06962025 0.         0.         0.01265823 0.02531646
  0.01265823 0.         0.        ]
 [0.02068966 0.00689655 0.05517241 0.0137931  0.         0.82758621
  0.06206897 0.         0.         0.         0.         0.
  0.0137931  0.         0.        ]
 [0.02120141 0.06713781 0.02120141 0.00706714 0.01060071 0.03886926
  0.79858657 0.01060071 0.         0.01413428 0.         0.01060071
  0.         0.         0.        ]
 [0.08032129 0.15261044 0.         0.08433735 0.01606426 0.
  0.01606426 0.5502008  0.         0.00803213 0.02811245 0.02811245
  0.00803213 0.00401606 0.02409639]
 [0.26666667 0.04444444 0.         0.01111111 0.         0.
  0.         0.03333333 0.6        0.02222222 0.         0.01111111
  0.01111111 0.         0.        ]
 [0.14117647 0.05882353 0.03529412 0.         0.         0.02352941
  0.04705882 0.01176471 0.         0.62352941 0.         0.04705882
  0.01176471 0.         0.        ]
 [0.02380952 0.         0.         0.02380952 0.         0.
  0.         0.05952381 0.         0.         0.86904762 0.
  0.01190476 0.         0.01190476]
 [0.03731343 0.07462687 0.02985075 0.02238806 0.01492537 0.
  0.00746269 0.05223881 0.         0.01492537 0.01492537 0.73134328
  0.         0.         0.        ]
 [0.11290323 0.0483871  0.01612903 0.         0.         0.
  0.03225806 0.01612903 0.         0.06451613 0.0483871  0.01612903
  0.64516129 0.         0.        ]
 [0.14285714 0.39285714 0.         0.21428571 0.         0.
  0.01785714 0.07142857 0.01785714 0.01785714 0.03571429 0.01785714
  0.01785714 0.01785714 0.03571429]
 [0.12       0.06       0.         0.         0.         0.
  0.         0.34       0.         0.         0.02       0.02
  0.         0.         0.44      ]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8522    0.8210    0.8363      1145
           1     0.5612    0.5810    0.5709       537
           2     0.8313    0.7886    0.8094       175
           3     0.7100    0.7384    0.7239       451
           4     0.7516    0.7233    0.7372       159
           5     0.8165    0.8836    0.8487       146
           6     0.6780    0.7711    0.7216       284
           7     0.4172    0.4840    0.4481       250
           8     0.7927    0.7143    0.7514        91
           9     0.4417    0.6092    0.5121        87
          10     0.8356    0.7093    0.7673        86
          11     0.7323    0.6838    0.7072       136
          12     0.6200    0.4844    0.5439        64
          13     0.4444    0.0702    0.1212        57
          14     0.4878    0.3846    0.4301        52

    accuracy                         0.7081      3720
   macro avg     0.6648    0.6298    0.6353      3720
weighted avg     0.7126    0.7081    0.7069      3720

test confusion matrix:
[[0.8209607  0.03930131 0.00611354 0.03231441 0.00436681 0.
  0.00262009 0.04104803 0.01222707 0.02358079 0.00174672 0.00349345
  0.0069869  0.         0.00524017]
 [0.0744879  0.58100559 0.00372439 0.09869646 0.02420857 0.00931099
  0.12290503 0.04655493 0.0018622  0.01489758 0.0018622  0.01303538
  0.0018622  0.0018622  0.00372439]
 [0.02857143 0.02285714 0.78857143 0.01142857 0.         0.05714286
  0.02857143 0.00571429 0.         0.04571429 0.         0.01142857
  0.         0.         0.        ]
 [0.03991131 0.12195122 0.00443459 0.7383592  0.00665188 0.00443459
  0.00221729 0.05986696 0.         0.00221729 0.00665188 0.00443459
  0.00221729 0.00443459 0.00221729]
 [0.01886792 0.11320755 0.         0.03144654 0.72327044 0.
  0.01257862 0.03773585 0.         0.         0.         0.04402516
  0.01257862 0.00628931 0.        ]
 [0.         0.00684932 0.06164384 0.00684932 0.00684932 0.88356164
  0.03424658 0.         0.         0.         0.         0.
  0.         0.         0.        ]
 [0.02112676 0.1056338  0.00704225 0.01056338 0.01408451 0.02816901
  0.77112676 0.00704225 0.00352113 0.03169014 0.         0.
  0.         0.         0.        ]
 [0.128      0.148      0.004      0.06       0.036      0.
  0.052      0.484      0.004      0.008      0.008      0.016
  0.004      0.         0.048     ]
 [0.16483516 0.05494505 0.         0.         0.         0.01098901
  0.         0.02197802 0.71428571 0.01098901 0.         0.01098901
  0.01098901 0.         0.        ]
 [0.14942529 0.03448276 0.03448276 0.02298851 0.         0.01149425
  0.03448276 0.02298851 0.         0.6091954  0.         0.02298851
  0.04597701 0.01149425 0.        ]
 [0.05813953 0.01162791 0.         0.03488372 0.01162791 0.
  0.01162791 0.15116279 0.         0.         0.70930233 0.01162791
  0.         0.         0.        ]
 [0.01470588 0.08088235 0.         0.01470588 0.00735294 0.
  0.00735294 0.16176471 0.         0.02941176 0.         0.68382353
  0.         0.         0.        ]
 [0.140625   0.0625     0.03125    0.         0.         0.03125
  0.0625     0.         0.         0.109375   0.03125    0.046875
  0.484375   0.         0.        ]
 [0.10526316 0.49122807 0.         0.19298246 0.01754386 0.
  0.         0.0877193  0.         0.         0.01754386 0.
  0.01754386 0.07017544 0.        ]
 [0.17307692 0.03846154 0.         0.03846154 0.         0.
  0.         0.32692308 0.         0.         0.01923077 0.01923077
  0.         0.         0.38461538]]
---------------------------------------
program finished.
