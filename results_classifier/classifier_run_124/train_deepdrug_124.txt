seed:  24
save trained model at:  ../trained_models/trained_classifier_model_124.pt
save loss at:  ./results/train_classifier_results_124.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['5aqgE00', '4xruA00', '2qrcF00', '1vjtA00', '3zf6A00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['4c5nC01', '2gdzA00', '5vsvB00', '5akdJ00', '1rb0A00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2ac08a056730>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.05803844395849, acc: 0.3857582573694803; test loss: 1.855903441673248, acc: 0.4429212952020799
epoch: 2, train loss: 1.7550925264503034, acc: 0.4605777199005564; test loss: 1.6422777906204618, acc: 0.47293783975419523
epoch: 3, train loss: 1.650888502054473, acc: 0.4881614774476145; test loss: 1.5336509088454475, acc: 0.5284802647128338
epoch: 4, train loss: 1.5668771307299345, acc: 0.5252752456493429; test loss: 1.4791973563577232, acc: 0.548097376506736
epoch: 5, train loss: 1.5387883881810178, acc: 0.5327335148573458; test loss: 1.4939073254047233, acc: 0.5320255258804065
epoch: 6, train loss: 1.4958350050445137, acc: 0.5451047709245886; test loss: 1.542646456655107, acc: 0.5258804065232805
epoch: 7, train loss: 1.4597522690548224, acc: 0.5532733514857345; test loss: 1.4748547662096196, acc: 0.5426613093831245
epoch: 8, train loss: 1.448675369883312, acc: 0.5615603172724044; test loss: 1.411271920245862, acc: 0.551169936185299
epoch: 9, train loss: 1.4138574559293133, acc: 0.5696697052207884; test loss: 1.38360885400576, acc: 0.5733869061687544
epoch: 10, train loss: 1.3649525765457053, acc: 0.5836983544453652; test loss: 1.3406150942292796, acc: 0.5887497045615694
epoch: 11, train loss: 1.3551706927929374, acc: 0.588729726530129; test loss: 1.3726442981572107, acc: 0.5705506972346963
epoch: 12, train loss: 1.319375085014979, acc: 0.5970166923167989; test loss: 1.3532553741876128, acc: 0.578586622547861
epoch: 13, train loss: 1.3257259014613307, acc: 0.5919853202320351; test loss: 1.3796283827411793, acc: 0.5703143464901914
epoch: 14, train loss: 1.3351601938446611, acc: 0.5988516633124186; test loss: 1.340294601054542, acc: 0.586386197116521
epoch: 15, train loss: 1.2884891366653786, acc: 0.6039422280099443; test loss: 1.2492189535827565, acc: 0.6166390924131411
epoch: 16, train loss: 1.2734223495535106, acc: 0.6122291937966142; test loss: 1.2789334042138225, acc: 0.5989127865752777
epoch: 17, train loss: 1.254262458449696, acc: 0.6193914999408073; test loss: 1.2201520054933732, acc: 0.6239659654927913
epoch: 18, train loss: 1.251148351398575, acc: 0.6250147981531905; test loss: 1.2395224322159222, acc: 0.6192389506026944
epoch: 19, train loss: 1.2582821008565512, acc: 0.6178524920089973; test loss: 1.3309980596782636, acc: 0.5930040179626566
epoch: 20, train loss: 1.2346316052284751, acc: 0.6219959749023322; test loss: 1.2408975117935184, acc: 0.6239659654927913
epoch: 21, train loss: 1.2294146807303552, acc: 0.6273233100509057; test loss: 1.2032761841455946, acc: 0.6272748759158592
epoch: 22, train loss: 1.2083576165304617, acc: 0.6330058008760506; test loss: 1.185398054697699, acc: 0.6400378161191208
epoch: 23, train loss: 1.2007801824876567, acc: 0.6333017639398603; test loss: 1.236207673071237, acc: 0.6291656818718979
epoch: 24, train loss: 1.1801500135559266, acc: 0.6378595951225287; test loss: 1.2452638200540347, acc: 0.6201843535807138
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9595967532827174, acc: 0.6389842547650053; test loss: 0.9636855301240521, acc: 0.6383833609075868
epoch: 26, train loss: 0.9430558242726724, acc: 0.6429501598200544; test loss: 0.9645642013703077, acc: 0.6360198534625384
epoch: 27, train loss: 0.9489521983602109, acc: 0.6414703445010063; test loss: 1.0057374438730093, acc: 0.6145119357125975
epoch: 28, train loss: 0.9401106094628152, acc: 0.6462649461347224; test loss: 0.9590834975101739, acc: 0.6303474355944221
epoch: 29, train loss: 0.9209920294654289, acc: 0.6512963182194862; test loss: 0.9610257172409847, acc: 0.6286929803828882
epoch: 30, train loss: 0.9158492428008601, acc: 0.6516514738960578; test loss: 0.9423806172007788, acc: 0.6360198534625384
epoch: 31, train loss: 0.9144211659364282, acc: 0.6521250147981532; test loss: 0.9915819888812547, acc: 0.6232569132592768
epoch: 32, train loss: 0.9090414538642592, acc: 0.6544927193086303; test loss: 0.9589889961279642, acc: 0.63365634601749
epoch: 33, train loss: 0.8934708215844015, acc: 0.6565644607552977; test loss: 0.8938463072888368, acc: 0.6577641219569842
epoch: 34, train loss: 0.8907469228991421, acc: 0.6589913578785368; test loss: 0.9371544275123779, acc: 0.6372016071850626
epoch: 35, train loss: 0.8760713894677413, acc: 0.664496270865396; test loss: 0.9414042109041117, acc: 0.635074450484519
epoch: 36, train loss: 0.9401118716319186, acc: 0.6409968035989109; test loss: 0.9696105078753781, acc: 0.6301110848499173
epoch: 37, train loss: 0.8808018547607708, acc: 0.6652065822185391; test loss: 0.9377813294153071, acc: 0.6348380997400142
epoch: 38, train loss: 0.873728673832366, acc: 0.666094471409968; test loss: 1.027588761802318, acc: 0.6003308910423067
epoch: 39, train loss: 0.8649550871452778, acc: 0.665324967444063; test loss: 0.960921538464991, acc: 0.6294020326164027
epoch: 40, train loss: 0.8469910232840423, acc: 0.6683437906949212; test loss: 0.8989438058861715, acc: 0.645710233987237
epoch: 41, train loss: 0.8391376924370821, acc: 0.6766307564815911; test loss: 0.9651802169377853, acc: 0.6324745922949657
epoch: 42, train loss: 0.8260500200742855, acc: 0.6770451047709246; test loss: 0.8869946773270292, acc: 0.6532734578113921
epoch: 43, train loss: 0.8212793338746167, acc: 0.678347342251687; test loss: 0.8741948751094802, acc: 0.6620184353580714
epoch: 44, train loss: 0.8261822166281423, acc: 0.6798863501834971; test loss: 0.8578580338688389, acc: 0.6589458756795084
epoch: 45, train loss: 0.8084149130832744, acc: 0.6796495797324494; test loss: 1.044347766442548, acc: 0.6029307492318601
epoch: 46, train loss: 0.8025564318424684, acc: 0.6809518172132117; test loss: 0.8847927497763691, acc: 0.6669818009926731
epoch: 47, train loss: 0.8047983836348669, acc: 0.68243163253226; test loss: 0.9630407014656112, acc: 0.6284566296383833
epoch: 48, train loss: 0.8210833972059539, acc: 0.6790576536048302; test loss: 1.0664981239880549, acc: 0.6083668163554715
epoch: 49, train loss: 0.7864539464281849, acc: 0.6919616431869303; test loss: 0.8964989456378718, acc: 0.6669818009926731
epoch: 50, train loss: 0.7785011899602531, acc: 0.6949804664377885; test loss: 0.8275997367093423, acc: 0.6759631292838573
epoch: 51, train loss: 0.7651054881106499, acc: 0.6995974902332189; test loss: 1.3231931827051988, acc: 0.540534152682581
epoch: 52, train loss: 0.7738310890365976, acc: 0.6965786669823606; test loss: 0.8617073487119735, acc: 0.659418577168518
epoch: 53, train loss: 0.7662546316575648, acc: 0.6944477329229313; test loss: 0.8570810850842628, acc: 0.6617820846135666
epoch: 54, train loss: 0.7729362663509183, acc: 0.6964602817568367; test loss: 0.8771611321651465, acc: 0.6485464429212952
epoch: 55, train loss: 0.7659538470669951, acc: 0.700366994199124; test loss: 0.8319054698583449, acc: 0.6726542188607895
epoch: 56, train loss: 0.7476219313658665, acc: 0.7028530839351249; test loss: 0.8808243243695996, acc: 0.6523280548333728
epoch: 57, train loss: 0.727530885331233, acc: 0.710903279270747; test loss: 0.9852708878739171, acc: 0.615220987946112
epoch: 58, train loss: 0.733046597921878, acc: 0.705990292411507; test loss: 0.9325601537299195, acc: 0.6511463011108485
epoch: 59, train loss: 0.7280678338125018, acc: 0.7139221025216053; test loss: 0.9015331711224759, acc: 0.6485464429212952
epoch: 60, train loss: 0.7251031994255136, acc: 0.713389369006748; test loss: 0.9791684569379338, acc: 0.6204207043252187
epoch: 61, train loss: 0.7250073917225097, acc: 0.7118503610749378; test loss: 0.8510706903128105, acc: 0.6669818009926731
Epoch    61: reducing learning rate of group 0 to 1.5000e-03.
epoch: 62, train loss: 0.6735926018221866, acc: 0.7293121818397064; test loss: 0.7335367822613285, acc: 0.7159064051051761
epoch: 63, train loss: 0.6306332360149997, acc: 0.7457677281875222; test loss: 0.7814546245628146, acc: 0.6910895769321673
epoch: 64, train loss: 0.6207509999502703, acc: 0.7464188469279034; test loss: 0.7615684077295111, acc: 0.7036161663909242
epoch: 65, train loss: 0.6195196225697632, acc: 0.7497336332425714; test loss: 0.7730953350215903, acc: 0.7095249350035453
epoch: 66, train loss: 0.6080366388727141, acc: 0.7511542559488575; test loss: 0.7715670694501314, acc: 0.7005436067123612
epoch: 67, train loss: 0.6084437395985259, acc: 0.7522789155913342; test loss: 0.7891751894909167, acc: 0.6953438903332545
epoch: 68, train loss: 0.5984426823130328, acc: 0.7568367467740026; test loss: 0.732158713576304, acc: 0.7215788229732923
epoch: 69, train loss: 0.587158523255641, acc: 0.7586125251568604; test loss: 0.748085824571527, acc: 0.7116520917040888
epoch: 70, train loss: 0.5849521777764487, acc: 0.7623416597608619; test loss: 0.7758461324015117, acc: 0.7040888678799339
epoch: 71, train loss: 0.5906690754692301, acc: 0.7610394222800995; test loss: 0.7418435442619531, acc: 0.7130701961711179
epoch: 72, train loss: 0.5725421256337121, acc: 0.7671362613945779; test loss: 0.748345037634788, acc: 0.7163791065941858
epoch: 73, train loss: 0.5737640898834816, acc: 0.7640582455309577; test loss: 0.7379083599612606, acc: 0.7109430394705744
epoch: 74, train loss: 0.5703916827983456, acc: 0.765538060850006; test loss: 0.8162594034271132, acc: 0.6854171590640511
epoch: 75, train loss: 0.5736493497074349, acc: 0.7668994909435303; test loss: 0.8555961312821672, acc: 0.6856535098085559
epoch: 76, train loss: 0.5526977556124656, acc: 0.7731739078962946; test loss: 0.8059363334180615, acc: 0.6877806665090995
epoch: 77, train loss: 0.5575021561312368, acc: 0.7737066414111519; test loss: 0.7914923251703596, acc: 0.708343181281021
epoch: 78, train loss: 0.550342858088176, acc: 0.7741801823132473; test loss: 0.7514160465501043, acc: 0.71756086031671
epoch: 79, train loss: 0.54697877656952, acc: 0.7791523617852492; test loss: 0.8015722325157538, acc: 0.6988891515008272
epoch: 80, train loss: 0.5452190040060229, acc: 0.7774357760151533; test loss: 0.7478176567521228, acc: 0.7225242259513117
epoch: 81, train loss: 0.5534793118675799, acc: 0.7723452113176276; test loss: 0.8143618506929943, acc: 0.6939257858662254
epoch: 82, train loss: 0.5477179919415937, acc: 0.7758967680833432; test loss: 0.8241089111635573, acc: 0.6868352635310802
epoch: 83, train loss: 0.5287973496904596, acc: 0.7844205043210607; test loss: 0.7351655401086954, acc: 0.7232332781848263
epoch: 84, train loss: 0.5282941489187489, acc: 0.7826447259382029; test loss: 0.7979016780177066, acc: 0.7133065469156228
epoch: 85, train loss: 0.533999236798278, acc: 0.7788563987214395; test loss: 0.7482458147081859, acc: 0.7234696289293311
epoch: 86, train loss: 0.5110092480619857, acc: 0.791523617852492; test loss: 0.764603778223314, acc: 0.7263058378633893
epoch: 87, train loss: 0.5174779501471616, acc: 0.7872025571208713; test loss: 0.8078083475271147, acc: 0.7076341290475066
epoch: 88, train loss: 0.525002269388106, acc: 0.7867290162187759; test loss: 0.751001735072811, acc: 0.7213424722287876
epoch: 89, train loss: 0.5077310679324974, acc: 0.7949567893926838; test loss: 0.8096752017982968, acc: 0.7036161663909242
epoch: 90, train loss: 0.5091224707829765, acc: 0.7897478394696342; test loss: 0.9025349094072602, acc: 0.6759631292838573
epoch: 91, train loss: 0.5024697172531676, acc: 0.790694921273825; test loss: 0.7427272507553263, acc: 0.7213424722287876
epoch: 92, train loss: 0.4937733006192281, acc: 0.795548715520303; test loss: 0.7633443317692828, acc: 0.7215788229732923
epoch: 93, train loss: 0.499467045506232, acc: 0.7940689002012549; test loss: 0.7629052036283597, acc: 0.7168518080831955
epoch: 94, train loss: 0.49490810003000646, acc: 0.7963182194862081; test loss: 0.8264541239412759, acc: 0.6991255022453321
epoch: 95, train loss: 0.47993930036448385, acc: 0.8015863620220196; test loss: 0.7822690703180828, acc: 0.7130701961711179
epoch: 96, train loss: 0.5000081287977667, acc: 0.7924115070439209; test loss: 0.7850960750327384, acc: 0.7095249350035453
epoch: 97, train loss: 0.4862487718411786, acc: 0.7950751746182076; test loss: 0.7763692081693047, acc: 0.7095249350035453
epoch: 98, train loss: 0.4864447200257383, acc: 0.7976796495797325; test loss: 0.7449282795169679, acc: 0.7298510990309619
epoch: 99, train loss: 0.48052640291901333, acc: 0.8005208949923049; test loss: 0.7648870265954382, acc: 0.7289056960529425
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3677090931160289, acc: 0.8043684148218302; test loss: 0.6205911574085338, acc: 0.7381233750886316
epoch: 101, train loss: 0.3675989730338084, acc: 0.8044276074345922; test loss: 0.6308132235363172, acc: 0.7367052706216024
epoch: 102, train loss: 0.35912275864370363, acc: 0.8084527051024032; test loss: 0.6665141317642093, acc: 0.7241786811628457
epoch: 103, train loss: 0.35554598488651873, acc: 0.8060850005919261; test loss: 0.6661237911309116, acc: 0.7185062632947293
epoch: 104, train loss: 0.3702197769477944, acc: 0.8024742512134485; test loss: 0.6239789748862403, acc: 0.7244150319073505
epoch: 105, train loss: 0.36692509363851167, acc: 0.8018231324730674; test loss: 0.641717079372672, acc: 0.7187426140392342
epoch: 106, train loss: 0.36486960191606443, acc: 0.7998697762519238; test loss: 0.6513684565112033, acc: 0.7085795320255259
epoch: 107, train loss: 0.36800692802112284, acc: 0.8001065467029714; test loss: 0.6436465342144571, acc: 0.7248877333963601
epoch: 108, train loss: 0.3544717687847516, acc: 0.8091630164555463; test loss: 0.609363147621317, acc: 0.7322146064760104
epoch: 109, train loss: 0.35701161409900556, acc: 0.8041908369835444; test loss: 0.6262313865989806, acc: 0.7338690616875443
epoch: 110, train loss: 0.3526654102272489, acc: 0.8073872380726885; test loss: 0.6541631138372636, acc: 0.7133065469156228
epoch: 111, train loss: 0.3995716302221564, acc: 0.7859595122528709; test loss: 0.6776674588896991, acc: 0.7021980619238951
epoch: 112, train loss: 0.3710947483513388, acc: 0.7961998342606843; test loss: 0.624057286401585, acc: 0.7161427558496809
Epoch   112: reducing learning rate of group 0 to 7.5000e-04.
epoch: 113, train loss: 0.31196353080442873, acc: 0.8238427844205043; test loss: 0.6527530565466956, acc: 0.7277239423304184
epoch: 114, train loss: 0.29314629268022463, acc: 0.8326032911092696; test loss: 0.6061321587124154, acc: 0.7440321437012527
epoch: 115, train loss: 0.2841379043247145, acc: 0.835444536521842; test loss: 0.6148054219396705, acc: 0.7447411959347672
epoch: 116, train loss: 0.2738167420320713, acc: 0.8410678347342252; test loss: 0.6379035134823772, acc: 0.7407232332781848
epoch: 117, train loss: 0.27857061524821325, acc: 0.8368059666153664; test loss: 0.6560352310112142, acc: 0.7376506735996219
epoch: 118, train loss: 0.2688493402583283, acc: 0.8404167159938439; test loss: 0.6484285169277764, acc: 0.7348144646655637
epoch: 119, train loss: 0.2622154847703009, acc: 0.8443234284361312; test loss: 0.6597380627134228, acc: 0.7367052706216024
epoch: 120, train loss: 0.2647680903491543, acc: 0.8427252278915591; test loss: 0.6508343749732673, acc: 0.7423776884897187
epoch: 121, train loss: 0.257612967146802, acc: 0.8474014442997514; test loss: 0.6437152359283328, acc: 0.7437957929567478
epoch: 122, train loss: 0.2613938509980447, acc: 0.8417781460873683; test loss: 0.6600426672705801, acc: 0.7468683526353108
epoch: 123, train loss: 0.26144273572769955, acc: 0.8465135551083225; test loss: 0.6448813834333499, acc: 0.7466320018908059
epoch: 124, train loss: 0.254150996440908, acc: 0.8481709482656564; test loss: 0.6714650058656177, acc: 0.7378870243441267
epoch: 125, train loss: 0.24962179478320962, acc: 0.8503018823250859; test loss: 0.6788813166234038, acc: 0.7359962183880879
epoch: 126, train loss: 0.2554960210245432, acc: 0.8481709482656564; test loss: 0.6733330971973215, acc: 0.738832427322146
epoch: 127, train loss: 0.25738587069119084, acc: 0.8442642358233693; test loss: 0.6640944909722606, acc: 0.7383597258331364
epoch: 128, train loss: 0.25965860942926605, acc: 0.8456256659168936; test loss: 0.6675224083587369, acc: 0.7336327109430395
epoch: 129, train loss: 0.2455800999702611, acc: 0.8487628743932757; test loss: 0.6935795595448114, acc: 0.7300874497754668
epoch: 130, train loss: 0.2623123517964484, acc: 0.8442642358233693; test loss: 0.6904914273744609, acc: 0.7312692034979911
epoch: 131, train loss: 0.24237962191141865, acc: 0.8519592754824198; test loss: 0.7546936440935543, acc: 0.7116520917040888
epoch: 132, train loss: 0.25337021405829246, acc: 0.8445601988871789; test loss: 0.6819620154422948, acc: 0.725124084140865
epoch: 133, train loss: 0.2451212488893674, acc: 0.8511305789037528; test loss: 0.6856406874984636, acc: 0.7371779721106122
epoch: 134, train loss: 0.24770334288892457, acc: 0.8489996448443234; test loss: 0.7030458286337694, acc: 0.7338690616875443
epoch: 135, train loss: 0.2455578664216513, acc: 0.8487036817805138; test loss: 0.7425567093709208, acc: 0.7121247931930985
epoch: 136, train loss: 0.2389867866326468, acc: 0.8524920089972772; test loss: 0.6809305837322877, acc: 0.7371779721106122
epoch: 137, train loss: 0.23817646184503305, acc: 0.8533798981887061; test loss: 0.7524293007734565, acc: 0.7234696289293311
epoch: 138, train loss: 0.2280143153491748, acc: 0.8595359299159465; test loss: 0.6896525198504367, acc: 0.7383597258331364
epoch: 139, train loss: 0.22753734857538996, acc: 0.8574049958565171; test loss: 0.6817449340581386, acc: 0.734341763176554
epoch: 140, train loss: 0.22676726631141914, acc: 0.8551556765715639; test loss: 0.7974936697280764, acc: 0.7095249350035453
epoch: 141, train loss: 0.2405981266408245, acc: 0.8545637504439446; test loss: 0.6983272855768133, acc: 0.7303238005199716
epoch: 142, train loss: 0.2228309243275341, acc: 0.8613117082988043; test loss: 0.6985221968506335, acc: 0.7419049870007091
epoch: 143, train loss: 0.22607706540630346, acc: 0.8565762992778502; test loss: 0.6829565182219045, acc: 0.7367052706216024
epoch: 144, train loss: 0.22537491214166694, acc: 0.8603646264946134; test loss: 0.7117802671043902, acc: 0.7289056960529425
epoch: 145, train loss: 0.2190069642605493, acc: 0.861548478749852; test loss: 0.7203498070078295, acc: 0.7411959347671945
epoch: 146, train loss: 0.2234321051452856, acc: 0.8595951225287084; test loss: 0.7151998733972663, acc: 0.7359962183880879
epoch: 147, train loss: 0.2206204536328926, acc: 0.8614300935243282; test loss: 0.7213024751031633, acc: 0.7348144646655637
epoch: 148, train loss: 0.21786775574312134, acc: 0.8604238191073754; test loss: 0.7303573764601771, acc: 0.7355235168990782
epoch: 149, train loss: 0.2180894321170067, acc: 0.8609565526222327; test loss: 0.6863687361872977, acc: 0.7471047033798156
epoch: 150, train loss: 0.2075886042305488, acc: 0.868888362732331; test loss: 0.7564322081786525, acc: 0.7229969274403214
epoch: 151, train loss: 0.20019735484112222, acc: 0.8683556292174737; test loss: 0.7183290701390327, acc: 0.7428503899787284
epoch: 152, train loss: 0.20608883292608407, acc: 0.8643897241624245; test loss: 0.7210465134021323, acc: 0.7317419049870008
epoch: 153, train loss: 0.21149190138031834, acc: 0.862377175328519; test loss: 0.7333820799906692, acc: 0.7407232332781848
epoch: 154, train loss: 0.2147349406413472, acc: 0.8621995974902332; test loss: 0.7324255652192129, acc: 0.7312692034979911
epoch: 155, train loss: 0.20311773923412363, acc: 0.868059666153664; test loss: 0.7377186620018494, acc: 0.7433230914677381
epoch: 156, train loss: 0.2003023735680523, acc: 0.8701314076003315; test loss: 0.7449215484606518, acc: 0.7341054124320492
epoch: 157, train loss: 0.20896418095411204, acc: 0.864804072451758; test loss: 0.7478046229589467, acc: 0.7326873079650201
epoch: 158, train loss: 0.21840821096667654, acc: 0.8617260565881378; test loss: 0.7530300279115566, acc: 0.7317419049870008
epoch: 159, train loss: 0.21618963033754726, acc: 0.8644489167751864; test loss: 0.6989705596401798, acc: 0.7355235168990782
epoch: 160, train loss: 0.20284214428476827, acc: 0.8668758138984255; test loss: 0.7284163698691393, acc: 0.7298510990309619
epoch: 161, train loss: 0.2131934252394379, acc: 0.8610749378477566; test loss: 0.7174687694922826, acc: 0.7312692034979911
epoch: 162, train loss: 0.1981417726567896, acc: 0.8675861252515686; test loss: 0.738663013481243, acc: 0.7324509572205152
epoch: 163, train loss: 0.19830966261811436, acc: 0.8659287320942346; test loss: 0.7443148784394017, acc: 0.7324509572205152
Epoch   163: reducing learning rate of group 0 to 3.7500e-04.
epoch: 164, train loss: 0.16312660558083728, acc: 0.8891322362969102; test loss: 0.7374789819061488, acc: 0.7485228078468447
epoch: 165, train loss: 0.15019383973075862, acc: 0.8929205635136735; test loss: 0.7528573233194876, acc: 0.7473410541243205
epoch: 166, train loss: 0.1472956123183546, acc: 0.8968272759559607; test loss: 0.7709028157351626, acc: 0.7482864571023399
epoch: 167, train loss: 0.14308896893574752, acc: 0.9012075293003433; test loss: 0.7729608473330348, acc: 0.7433230914677381
epoch: 168, train loss: 0.14247899992311863, acc: 0.9019770332662483; test loss: 0.7849755309710573, acc: 0.7506499645473883
epoch: 169, train loss: 0.1391238285879334, acc: 0.9029833076832011; test loss: 0.7952692696907875, acc: 0.7504136138028835
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.10241826841894573, acc: 0.9005564105599622; test loss: 0.7293103623125351, acc: 0.7393051288111557
epoch: 171, train loss: 0.1018303225525416, acc: 0.9006747957854859; test loss: 0.7333294593591945, acc: 0.7371779721106122
epoch: 172, train loss: 0.10168456440460502, acc: 0.899076595240914; test loss: 0.6948431479792819, acc: 0.7475774048688253
epoch: 173, train loss: 0.09625492255014119, acc: 0.9041079673256778; test loss: 0.7073256285639695, acc: 0.7437957929567478
epoch: 174, train loss: 0.0974099333079959, acc: 0.9029833076832011; test loss: 0.7149482724289611, acc: 0.7381233750886316
epoch: 175, train loss: 0.09949056836374556, acc: 0.9007339883982479; test loss: 0.6993144118061756, acc: 0.748050106357835
epoch: 176, train loss: 0.09884464434332942, acc: 0.9002012548833905; test loss: 0.6923164342825251, acc: 0.7440321437012527
epoch: 177, train loss: 0.0993375182187076, acc: 0.900615603172724; test loss: 0.7105233305257366, acc: 0.7473410541243205
epoch: 178, train loss: 0.10080562319994199, acc: 0.8979519355984373; test loss: 0.7359842301203945, acc: 0.7341054124320492
epoch: 179, train loss: 0.10778393664768748, acc: 0.8923286373860542; test loss: 0.6965548000953293, acc: 0.7367052706216024
epoch: 180, train loss: 0.11953923739144488, acc: 0.8832129750207174; test loss: 0.6962303586165073, acc: 0.7322146064760104
epoch: 181, train loss: 0.11541546762829294, acc: 0.8883035397182432; test loss: 0.7033525095344351, acc: 0.7402505317891751
epoch: 182, train loss: 0.09916440075645083, acc: 0.8995501361430094; test loss: 0.7002826684813386, acc: 0.7494682108248641
epoch: 183, train loss: 0.09299355394881591, acc: 0.9016218775896768; test loss: 0.6972828547573067, acc: 0.7426140392342235
epoch: 184, train loss: 0.09413870797046363, acc: 0.9023913815555819; test loss: 0.7358682521323283, acc: 0.7433230914677381
epoch: 185, train loss: 0.09508692360007581, acc: 0.9019178406534865; test loss: 0.7236716345505522, acc: 0.7485228078468447
epoch: 186, train loss: 0.09170594507665399, acc: 0.9010891440748194; test loss: 0.7200056546350315, acc: 0.7445048451902624
epoch: 187, train loss: 0.09283322397087175, acc: 0.9035160411980585; test loss: 0.7515513734089457, acc: 0.7352871661545733
epoch: 188, train loss: 0.09658281051740006, acc: 0.9007931810110098; test loss: 0.7134456681855827, acc: 0.7364689198770976
epoch: 189, train loss: 0.10541488683596298, acc: 0.8956434237007221; test loss: 0.7035697938927633, acc: 0.7409595840226897
epoch: 190, train loss: 0.0906352394252638, acc: 0.9032792707470108; test loss: 0.7182935384451934, acc: 0.7440321437012527
epoch: 191, train loss: 0.09596408437705962, acc: 0.9037528116491061; test loss: 0.7210229662286843, acc: 0.7333963601985346
epoch: 192, train loss: 0.10279732570096249, acc: 0.8948147271220551; test loss: 0.7004562102150788, acc: 0.7376506735996219
epoch: 193, train loss: 0.09362251945638199, acc: 0.9009707588492956; test loss: 0.7172244436434893, acc: 0.7362325691325927
epoch: 194, train loss: 0.09291073210855849, acc: 0.904640700840535; test loss: 0.7139535204960195, acc: 0.7390687780666509
epoch: 195, train loss: 0.09214670346557875, acc: 0.9048182786788209; test loss: 0.7527749942172309, acc: 0.7352871661545733
epoch: 196, train loss: 0.09325690548970626, acc: 0.9022138037172961; test loss: 0.7304072946387525, acc: 0.7376506735996219
epoch: 197, train loss: 0.09709789030006034, acc: 0.8981295134367231; test loss: 0.7388434097068695, acc: 0.7322146064760104
epoch: 198, train loss: 0.09514349760233869, acc: 0.9006747957854859; test loss: 0.738581684570565, acc: 0.735759867643583
epoch: 199, train loss: 0.08967139018690486, acc: 0.9052918195809163; test loss: 0.7578395661155937, acc: 0.7331600094540298
epoch: 200, train loss: 0.09723079858061773, acc: 0.8996093287557713; test loss: 0.7379102319218368, acc: 0.7383597258331364
best test acc 0.7506499645473883 at epoch 168.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9618    0.9818    0.9717      6100
           1     0.9710    0.9039    0.9362       926
           2     0.8537    0.9850    0.9147      2400
           3     0.9547    0.9004    0.9267       843
           4     0.9006    0.9599    0.9293       774
           5     0.9530    0.9656    0.9593      1512
           6     0.8610    0.8617    0.8613      1330
           7     0.9222    0.8877    0.9047       481
           8     0.9234    0.8690    0.8954       458
           9     0.9690    0.9690    0.9690       452
          10     0.9175    0.9149    0.9162       717
          11     0.9668    0.8739    0.9180       333
          12     0.6429    0.0301    0.0575       299
          13     0.9661    0.6357    0.7668       269

    accuracy                         0.9286     16894
   macro avg     0.9117    0.8385    0.8519     16894
weighted avg     0.9257    0.9286    0.9208     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8156    0.8557    0.8352      1525
           1     0.8389    0.7629    0.7991       232
           2     0.6483    0.8037    0.7177       601
           3     0.8216    0.7204    0.7677       211
           4     0.8402    0.8402    0.8402       194
           5     0.8298    0.8254    0.8276       378
           6     0.5145    0.5315    0.5229       333
           7     0.6822    0.6033    0.6404       121
           8     0.5980    0.5304    0.5622       115
           9     0.8384    0.7281    0.7793       114
          10     0.7178    0.6500    0.6822       180
          11     0.7115    0.4405    0.5441        84
          12     0.0000    0.0000    0.0000        75
          13     0.8372    0.5294    0.6486        68

    accuracy                         0.7506      4231
   macro avg     0.6924    0.6301    0.6548      4231
weighted avg     0.7427    0.7506    0.7435      4231

---------------------------------------
program finished.
