seed:  666
save trained model at:  ../trained_models/trained_classifier_model_10.pt
save loss at:  ./results/train_classifier_results_10.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  150
learning rate decay at epoch:  90
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=64, out_features=64, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(64, 128)
  )
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [20, 60, 130]
loss function:
FocalLoss(gamma=0, alpha=1, reduction=mean)
begin training...
epoch: 1, train loss: 2.311163132482687, acc: 0.33222951755638025, val loss: 2.078614973595596, acc: 0.37123031758740327, test loss: 2.0770281463513824, acc: 0.3812301166489926
epoch: 2, train loss: 2.018513310224439, acc: 0.39931487296602913, val loss: 1.938992618909988, acc: 0.4120629837203096, test loss: 1.936807473416293, acc: 0.4088016967126193
epoch: 3, train loss: 1.9224851925986308, acc: 0.42215244076505853, val loss: 1.843399419891443, acc: 0.45182812917000265, test loss: 1.8424825814976293, acc: 0.4483032873806999
epoch: 4, train loss: 1.8542557484757176, acc: 0.44145018555523835, val loss: 1.7833284442189983, acc: 0.4550306912196424, test loss: 1.778250711972994, acc: 0.4623541887592789
epoch: 5, train loss: 1.7995702147858164, acc: 0.4612617756208964, val loss: 1.791560485008592, acc: 0.4566319722444622, test loss: 1.7994758872945387, acc: 0.4483032873806999
epoch: 6, train loss: 1.757773886882609, acc: 0.47142449329146446, val loss: 1.8541805652545107, acc: 0.4494262076327729, test loss: 1.8460274660448255, acc: 0.4459172852598091
epoch: 7, train loss: 1.7139133737184578, acc: 0.48192977447901797, val loss: 1.7037799544165158, acc: 0.4734454230050707, test loss: 1.6872447303092366, acc: 0.47799575821845175
epoch: 8, train loss: 1.6797983122048907, acc: 0.49060805024264914, val loss: 1.676683119465136, acc: 0.4923939151321057, test loss: 1.6641290910929156, acc: 0.4976139978791092
epoch: 9, train loss: 1.6599072123063894, acc: 0.5020268341421639, val loss: 1.7207962493117663, acc: 0.47050974112623434, test loss: 1.700513987374331, acc: 0.4909862142099682
epoch: 10, train loss: 1.6289874038933143, acc: 0.5089922923208678, val loss: 1.5839974567162887, acc: 0.5196156925540433, test loss: 1.5712675815675317, acc: 0.5241251325556734
epoch: 11, train loss: 1.600196814135487, acc: 0.5156151869825863, val loss: 1.696296069155638, acc: 0.48331998932479314, test loss: 1.6820253132509655, acc: 0.4886002120890774
epoch: 12, train loss: 1.5830599359400164, acc: 0.5222380816443049, val loss: 2.0874184871120773, acc: 0.40539097945022684, test loss: 2.0909769107782195, acc: 0.40906680805938495
epoch: 13, train loss: 1.560063633917401, acc: 0.5298886668569798, val loss: 1.5324720337386446, acc: 0.5278889778489458, test loss: 1.5221473091101216, acc: 0.527306468716861
epoch: 14, train loss: 1.5413569725550076, acc: 0.5324007993148729, val loss: 1.5842385333412197, acc: 0.5070723245262877, test loss: 1.5705447156507668, acc: 0.5188229056203606
epoch: 15, train loss: 1.5440540182090234, acc: 0.5339423351413074, val loss: 1.5257828791745351, acc: 0.5385641846810783, test loss: 1.5110106465672384, acc: 0.5397667020148462
epoch: 16, train loss: 1.518853064261264, acc: 0.536682843277191, val loss: 1.6463845805037585, acc: 0.510808646917534, test loss: 1.6512469656267703, acc: 0.5045068928950159
epoch: 17, train loss: 1.510552520551853, acc: 0.5381672851841279, val loss: 1.61515671684483, acc: 0.5204163330664532, test loss: 1.6159811373859296, acc: 0.51033934252386
epoch: 18, train loss: 1.4718750070035032, acc: 0.5498144447616329, val loss: 1.6209652524392764, acc: 0.5020016012810248, test loss: 1.5911844904324157, acc: 0.5071580063626723
epoch: 19, train loss: 1.4742732446329136, acc: 0.5500428204396232, val loss: 1.4835161497219422, acc: 0.5398985855350947, test loss: 1.4692941601936285, acc: 0.5432131495227995
epoch 20, gamma increased to 1.
epoch: 20, train loss: 1.2254081995664854, acc: 0.5535255495289751, val loss: 1.3716585260726688, acc: 0.5025353616226315, test loss: 1.3566567194929557, acc: 0.5039766702014846
epoch: 21, train loss: 1.204167243646888, acc: 0.5593491293177276, val loss: 1.2354604480169218, acc: 0.5452361889511609, test loss: 1.2248442183117345, acc: 0.546659597030753
epoch: 22, train loss: 1.18471182667457, acc: 0.5617470739366257, val loss: 1.1973469169546453, acc: 0.5516413130504404, test loss: 1.2032295583035255, acc: 0.5527571580063627
epoch: 23, train loss: 1.1685309704392084, acc: 0.5690550956323152, val loss: 1.24013402266664, acc: 0.5455030691219642, test loss: 1.227769160447672, acc: 0.5445387062566278
epoch: 24, train loss: 1.154325683634859, acc: 0.572023979446189, val loss: 1.3225829079528475, acc: 0.5105417667467307, test loss: 1.3364982501320217, acc: 0.5198833510074231
epoch: 25, train loss: 1.155051261011342, acc: 0.5727091064801598, val loss: 1.2291827598889413, acc: 0.5468374699759808, test loss: 1.2151580008570488, acc: 0.559915164369035
epoch: 26, train loss: 1.152379955239477, acc: 0.5733371395946332, val loss: 1.2032979650754498, acc: 0.5529757139044569, test loss: 1.1996966940474283, acc: 0.5601802757158006
epoch: 27, train loss: 1.1287376376311846, acc: 0.5792749072223808, val loss: 1.1379233100810686, acc: 0.5748598879103283, test loss: 1.1342724904275774, acc: 0.5837751855779427
epoch: 28, train loss: 1.096688250198114, acc: 0.591036254638881, val loss: 1.1374143280281777, acc: 0.5759274085935415, test loss: 1.1276360842481272, acc: 0.5795334040296924
epoch: 29, train loss: 1.0903572354356188, acc: 0.5886383100199829, val loss: 1.114020372919633, acc: 0.5940752602081665, test loss: 1.1187899959681522, acc: 0.5851007423117709
epoch: 30, train loss: 1.090326094552513, acc: 0.5935483870967742, val loss: 1.2706145190989013, acc: 0.5439017880971444, test loss: 1.2470451756474321, acc: 0.549575821845175
epoch: 31, train loss: 1.0953031392496313, acc: 0.5906365972023979, val loss: 1.1420677397325576, acc: 0.5788630904723779, test loss: 1.1418581901400617, acc: 0.573170731707317
epoch: 32, train loss: 1.067615717191748, acc: 0.5969169283471311, val loss: 1.1041167668733782, acc: 0.5906058179877235, test loss: 1.0969067701984943, acc: 0.5972958642629904
epoch: 33, train loss: 1.038588398934908, acc: 0.611647159577505, val loss: 1.1476796606810595, acc: 0.5817987723512144, test loss: 1.1353983712221634, acc: 0.5848356309650053
epoch: 34, train loss: 1.0328343633580406, acc: 0.6135883528404225, val loss: 1.043431485784127, acc: 0.6047504670402989, test loss: 1.0480106847789357, acc: 0.6105514316012726
epoch: 35, train loss: 1.0112231228305992, acc: 0.6229517556380245, val loss: 1.1373146107777743, acc: 0.578062449959968, test loss: 1.135404993595273, acc: 0.588016967126193
epoch: 36, train loss: 1.0561818582345308, acc: 0.6045675135598059, val loss: 1.148638687421393, acc: 0.5649853215906058, test loss: 1.1270872150979392, acc: 0.5861611876988335
epoch: 37, train loss: 1.0375044306176, acc: 0.6114758778190122, val loss: 1.0740490075521925, acc: 0.5914064585001334, test loss: 1.05576474138181, acc: 0.6108165429480382
epoch: 38, train loss: 0.9960501889722401, acc: 0.6196403083071653, val loss: 1.0761611467366032, acc: 0.6146250333600214, test loss: 1.0657660589491196, acc: 0.6097560975609756
epoch: 39, train loss: 0.9909061793970646, acc: 0.6245503853839566, val loss: 1.052150794783369, acc: 0.5972778222578062, test loss: 1.0685776192714656, acc: 0.6010074231177094
epoch: 40, train loss: 0.9798650044902134, acc: 0.6284327719097916, val loss: 1.026338605903644, acc: 0.6135575126768081, test loss: 1.0267873101118135, acc: 0.619034994697773
epoch: 41, train loss: 0.9895791746159128, acc: 0.6204396231801313, val loss: 1.055124415949182, acc: 0.6148919135308246, test loss: 1.0507195769539568, acc: 0.6179745493107105
epoch: 42, train loss: 0.9531846282416128, acc: 0.6379103625463888, val loss: 1.2392150685728025, acc: 0.5623165198825727, test loss: 1.253246612791545, acc: 0.5633616118769883
epoch: 43, train loss: 0.9652027446135364, acc: 0.6321438766771339, val loss: 1.2582711850861914, acc: 0.5273552175073392, test loss: 1.2500013056135126, acc: 0.538441145281018
epoch: 44, train loss: 0.9845940325248592, acc: 0.6267199543248644, val loss: 1.125048562244062, acc: 0.5898051774753136, test loss: 1.1040913191478805, acc: 0.5941145281018028
epoch: 45, train loss: 0.9406682274561421, acc: 0.6376819868683985, val loss: 1.2482881612831158, acc: 0.5241526554576995, test loss: 1.2325350315421661, acc: 0.5304878048780488
epoch: 46, train loss: 0.9767651143939775, acc: 0.6300314016557237, val loss: 1.113040224023905, acc: 0.5911395783293302, test loss: 1.1336414341203482, acc: 0.5949098621420997
epoch: 47, train loss: 0.9501300117951137, acc: 0.6417356551527262, val loss: 1.0030212201275506, acc: 0.6314384841206299, test loss: 0.9996322005323489, acc: 0.6370625662778366
epoch: 48, train loss: 0.9317691097489569, acc: 0.6463031687125321, val loss: 1.0969598281214896, acc: 0.6082199092607419, test loss: 1.1125935603047732, acc: 0.6086956521739131
epoch: 49, train loss: 0.9121940960347772, acc: 0.6483585498144447, val loss: 1.051275587794556, acc: 0.6159594342140379, test loss: 1.0701753423170977, acc: 0.6296394485683987
epoch: 50, train loss: 0.9050706184628416, acc: 0.6501855552383671, val loss: 0.9992578836450712, acc: 0.6282359220709901, test loss: 1.0219007759559444, acc: 0.6320254506892895
epoch: 51, train loss: 0.8809157418721477, acc: 0.6624036540108479, val loss: 1.217152289972644, acc: 0.5852682145716573, test loss: 1.215127332339484, acc: 0.5853658536585366
epoch: 52, train loss: 0.8882707735146586, acc: 0.6565800742220953, val loss: 1.1896188689258087, acc: 0.577261809447558, test loss: 1.1571937914491333, acc: 0.5835100742311771
epoch: 53, train loss: 0.8729734065870949, acc: 0.6616614330573793, val loss: 1.005094311095442, acc: 0.6271684013877769, test loss: 1.0091921661211096, acc: 0.6312301166489925
epoch: 54, train loss: 0.8908928207489479, acc: 0.6584641735655152, val loss: 0.9647572424878176, acc: 0.6343741659994663, test loss: 0.9992137941297772, acc: 0.6306998939554613
epoch: 55, train loss: 0.8622700393931579, acc: 0.6627462175278332, val loss: 1.006105567221009, acc: 0.6287696824125968, test loss: 1.0165103715651866, acc: 0.626458112407211
epoch: 56, train loss: 0.8638011116250528, acc: 0.6608621181844133, val loss: 0.9860530161113144, acc: 0.633306645316253, test loss: 1.0022826665026765, acc: 0.6344114528101803
epoch: 57, train loss: 0.8684431937841426, acc: 0.6662860405366828, val loss: 0.9750170885864372, acc: 0.6423805711235655, test loss: 0.9834391974694908, acc: 0.6476670201484623
epoch: 58, train loss: 0.8549501535520192, acc: 0.6708535540964887, val loss: 1.2737277807602732, acc: 0.5641846810781959, test loss: 1.3041227513633882, acc: 0.5572640509013785
epoch: 59, train loss: 0.8350796889006462, acc: 0.672566371681416, val loss: 1.0879478042272686, acc: 0.6218307979717107, test loss: 1.074756641782354, acc: 0.6261930010604454
epoch 60, gamma increased to 2.
epoch: 60, train loss: 0.6731513669890333, acc: 0.6833571224664573, val loss: 1.022947809427046, acc: 0.5823325326928209, test loss: 1.0508073873004125, acc: 0.5792682926829268
epoch: 61, train loss: 0.6963195218074808, acc: 0.6735369683128747, val loss: 0.8107246305899903, acc: 0.6429143314651722, test loss: 0.8449913618546908, acc: 0.6444856839872747
epoch: 62, train loss: 0.6844433324288682, acc: 0.678447045389666, val loss: 0.9746762884118126, acc: 0.6138243928476115, test loss: 1.0093367266629685, acc: 0.6089607635206787
epoch: 63, train loss: 0.6972671947171611, acc: 0.673765343990865, val loss: 0.8365934880727254, acc: 0.6375767280491059, test loss: 0.8538226856280233, acc: 0.6312301166489925
epoch: 64, train loss: 0.6797798420203403, acc: 0.6798743933771053, val loss: 0.9999808025003784, acc: 0.5954096610621831, test loss: 0.9957936697521999, acc: 0.5893425238600212
epoch: 65, train loss: 0.6821791488988856, acc: 0.6783328575506709, val loss: 0.9076212495366573, acc: 0.6162263143848412, test loss: 0.9139199871273587, acc: 0.6203605514316013
epoch: 66, train loss: 0.6821974156381333, acc: 0.6773051669997145, val loss: 0.8252976025967462, acc: 0.6434480918067788, test loss: 0.8393021269542414, acc: 0.6426299045599152
epoch: 67, train loss: 0.663292970665516, acc: 0.6825007136739937, val loss: 0.9714218085690883, acc: 0.6050173472111022, test loss: 0.9889051820668031, acc: 0.6076352067868505
epoch: 68, train loss: 0.6764896080421101, acc: 0.6762774764487582, val loss: 0.8429425157544961, acc: 0.6431812116359754, test loss: 0.8476927047822533, acc: 0.644220572640509
epoch: 69, train loss: 0.6671592724925615, acc: 0.6766771338852412, val loss: 0.8280898242560519, acc: 0.6543901788097144, test loss: 0.8621446044675113, acc: 0.6489925768822906
epoch: 70, train loss: 0.6658078737508015, acc: 0.6837567799029404, val loss: 0.882920289567734, acc: 0.622097678142514, test loss: 0.9089366203906807, acc: 0.616118769883351
epoch: 71, train loss: 0.665382869111583, acc: 0.6852983157293748, val loss: 0.8344252072622657, acc: 0.6261008807045636, test loss: 0.8288303008135152, acc: 0.6362672322375398
epoch: 72, train loss: 0.6366399788904149, acc: 0.6921495860690836, val loss: 0.9080752050650733, acc: 0.6234320789965305, test loss: 0.9203895847466187, acc: 0.6193001060445387
epoch: 73, train loss: 0.6553124566776495, acc: 0.6848415643733943, val loss: 0.8337495676320109, acc: 0.6439818521483853, test loss: 0.8517718881463449, acc: 0.647136797454931
epoch: 74, train loss: 0.6342070276928465, acc: 0.6961461604339138, val loss: 0.9083485443305104, acc: 0.6295703229250067, test loss: 0.9373222578115453, acc: 0.6224814422057264
epoch: 75, train loss: 0.6287540949755589, acc: 0.69129317727662, val loss: 0.8449883004777234, acc: 0.6453162530024019, test loss: 0.8565140076431971, acc: 0.6468716861081655
epoch: 76, train loss: 0.6422868277731876, acc: 0.6899800171281758, val loss: 0.8481587415890659, acc: 0.6349079263410728, test loss: 0.846363699828074, acc: 0.6349416755037116
epoch: 77, train loss: 0.6411068309350522, acc: 0.6915786468741079, val loss: 0.8552859457518407, acc: 0.6301040832666133, test loss: 0.8639102143927824, acc: 0.623541887592789
epoch: 78, train loss: 0.6242855084102358, acc: 0.6983728232943192, val loss: 0.8489352915488723, acc: 0.6426474512943688, test loss: 0.8641579864133833, acc: 0.6322905620360552
epoch: 79, train loss: 0.6349167303667387, acc: 0.6882671995432487, val loss: 0.8785958829039346, acc: 0.6234320789965305, test loss: 0.8897534473576956, acc: 0.626458112407211
epoch: 80, train loss: 0.6383564056837657, acc: 0.6874678846702826, val loss: 0.8429802444606646, acc: 0.6453162530024019, test loss: 0.8289549826058704, acc: 0.6444856839872747
epoch: 81, train loss: 0.6287774732206128, acc: 0.6967741935483871, val loss: 0.9145837482974661, acc: 0.6180944755804644, test loss: 0.9199877381956842, acc: 0.6195652173913043
epoch: 82, train loss: 0.6122299306432826, acc: 0.6944333428489866, val loss: 0.950827305972942, acc: 0.6023485455030692, test loss: 0.9337555769266025, acc: 0.6092258748674443
epoch: 83, train loss: 0.6181134821651937, acc: 0.6977447901798458, val loss: 1.0408870710001838, acc: 0.5775286896183613, test loss: 1.057632892280975, acc: 0.5853658536585366
epoch: 84, train loss: 0.6217631397028157, acc: 0.6972309449043677, val loss: 0.8365908316176256, acc: 0.6413130504403523, test loss: 0.8301217836640925, acc: 0.6386532343584306
epoch: 85, train loss: 0.6073929325601152, acc: 0.7004852983157294, val loss: 0.8605130672582092, acc: 0.6386442487323192, test loss: 0.8965433933322781, acc: 0.6389183457051962
epoch: 86, train loss: 0.6289367592236058, acc: 0.6905509563231516, val loss: 0.9001511542485942, acc: 0.6295703229250067, test loss: 0.9360421783471538, acc: 0.6219512195121951
epoch: 87, train loss: 0.6195360894513545, acc: 0.6953468455609477, val loss: 0.8400393341075651, acc: 0.6511876167600748, test loss: 0.839137214870999, acc: 0.6474019088016967
epoch: 88, train loss: 0.6036128010404066, acc: 0.699857265201256, val loss: 0.8334464160544032, acc: 0.6394448892447291, test loss: 0.8481526956325625, acc: 0.6460763520678685
epoch: 89, train loss: 0.6131698180871659, acc: 0.6984299172138168, val loss: 0.9007769254547519, acc: 0.6322391246330398, test loss: 0.8939603085482487, acc: 0.6320254506892895
epoch: 90, train loss: 0.5357506400426728, acc: 0.7328575506708536, val loss: 0.7650497875310339, acc: 0.6752068321323725, test loss: 0.7618498665656922, acc: 0.6770943796394485
epoch: 91, train loss: 0.494965937084516, acc: 0.7430202683414217, val loss: 0.8074448347918854, acc: 0.6693354683746997, test loss: 0.8110865416481285, acc: 0.6850477200424178
epoch: 92, train loss: 0.48421529447253076, acc: 0.743648301455895, val loss: 0.8200445673387465, acc: 0.6728049105951428, test loss: 0.844319372642331, acc: 0.6598621420996819
epoch: 93, train loss: 0.4780634856754938, acc: 0.7492435055666572, val loss: 0.8232310548094645, acc: 0.6631972244462236, test loss: 0.8707132824910914, acc: 0.6635737009544008
epoch: 94, train loss: 0.4723173240032191, acc: 0.7474165001427348, val loss: 0.7917952611664629, acc: 0.6770749933279957, test loss: 0.8327219266274575, acc: 0.6667550371155886
epoch: 95, train loss: 0.4603879617548928, acc: 0.7525549528975164, val loss: 0.863033684247457, acc: 0.6554576994929276, test loss: 0.8730529148530606, acc: 0.6553552492046659
epoch: 96, train loss: 0.4585935203987566, acc: 0.7498144447616328, val loss: 0.8440145776848936, acc: 0.6682679476914866, test loss: 0.863559126474698, acc: 0.6508483563096501
epoch: 97, train loss: 0.4630563952260585, acc: 0.7528975164145019, val loss: 0.811041814026783, acc: 0.6794769148652255, test loss: 0.8446301614998001, acc: 0.679745493107105
epoch: 98, train loss: 0.460655992694014, acc: 0.7508421353125893, val loss: 0.8669602779951419, acc: 0.6623965839338137, test loss: 0.8667274487233491, acc: 0.6680805938494168
epoch: 99, train loss: 0.4495111995242917, acc: 0.7562089637453612, val loss: 1.025028860534004, acc: 0.6370429677074994, test loss: 1.0275495722336836, acc: 0.6288441145281018
epoch: 100, train loss: 0.4626231542594631, acc: 0.7511276049100771, val loss: 0.844216178377883, acc: 0.6610621830797971, test loss: 0.837600636204126, acc: 0.6702014846235419
epoch: 101, train loss: 0.45592510426789734, acc: 0.7547816157579218, val loss: 0.8251327758857273, acc: 0.6714705097411262, test loss: 0.8386621524521554, acc: 0.6680805938494168
epoch: 102, train loss: 0.45069145696216945, acc: 0.7536397373679703, val loss: 0.8241339880274173, acc: 0.6706698692287163, test loss: 0.88051163176843, acc: 0.6672852598091198
epoch: 103, train loss: 0.443445953089545, acc: 0.7562089637453612, val loss: 0.8511753606579925, acc: 0.6752068321323725, test loss: 0.8964454700433563, acc: 0.6651643690349947
epoch: 104, train loss: 0.44884260483819893, acc: 0.7564373394233515, val loss: 0.9380193291138037, acc: 0.6373098478783027, test loss: 0.9386857636786468, acc: 0.6468716861081655
epoch: 105, train loss: 0.45992576423080656, acc: 0.7521552954610334, val loss: 0.8570936739778786, acc: 0.6602615425673872, test loss: 0.8814007009706467, acc: 0.6683457051961824
epoch: 106, train loss: 0.4503110631002959, acc: 0.7557522123893805, val loss: 0.9352340955304119, acc: 0.6437149719775821, test loss: 0.962085246786965, acc: 0.6484623541887593
epoch: 107, train loss: 0.4518816663900376, acc: 0.7551241792749073, val loss: 0.8116363119309127, acc: 0.6706698692287163, test loss: 0.8485404408496359, acc: 0.6702014846235419
epoch: 108, train loss: 0.43154987425182056, acc: 0.76306023408507, val loss: 0.8482803271425734, acc: 0.6722711502535361, test loss: 0.9077089373911172, acc: 0.6625132555673383
epoch: 109, train loss: 0.42572829718423716, acc: 0.7657436483014559, val loss: 0.8152845906740448, acc: 0.677341873498799, test loss: 0.8555496618831145, acc: 0.6802757158006363
epoch: 110, train loss: 0.42157839150078935, acc: 0.76306023408507, val loss: 0.9563492260712256, acc: 0.6599946623965839, test loss: 0.9871386457728429, acc: 0.6574761399787911
epoch: 111, train loss: 0.4285090118076609, acc: 0.7618612617756209, val loss: 0.8468403180567525, acc: 0.6720042700827329, test loss: 0.8839734336351412, acc: 0.6675503711558854
epoch: 112, train loss: 0.4217673041634719, acc: 0.7627176705680845, val loss: 1.082998239806343, acc: 0.6212970376301041, test loss: 1.1055888055111671, acc: 0.6171792152704135
epoch: 113, train loss: 0.4151567999546847, acc: 0.7663716814159292, val loss: 0.8765111999700061, acc: 0.6706698692287163, test loss: 0.9326411643690674, acc: 0.6580063626723224
epoch: 114, train loss: 0.4132766405973916, acc: 0.7655152726234656, val loss: 0.8608805577151133, acc: 0.6575927408593542, test loss: 0.8911900002023448, acc: 0.651643690349947
epoch: 115, train loss: 0.44702430126532944, acc: 0.7575221238938054, val loss: 0.9264194109549928, acc: 0.6541232986389112, test loss: 0.9551649318646525, acc: 0.6450159066808059
epoch: 116, train loss: 0.4424096620494626, acc: 0.7544961461604339, val loss: 0.8800003017652566, acc: 0.6501200960768615, test loss: 0.8874113347345799, acc: 0.6426299045599152
epoch: 117, train loss: 0.4086349273081885, acc: 0.767342278047388, val loss: 0.9295923315686546, acc: 0.6538564184681078, test loss: 0.9306332220581152, acc: 0.6633085896076352
epoch: 118, train loss: 0.4053033856725543, acc: 0.7723094490436768, val loss: 0.8965505984804933, acc: 0.6655991459834534, test loss: 0.9398953282567628, acc: 0.66118769883351
epoch: 119, train loss: 0.38024103654952107, acc: 0.782186697116757, val loss: 0.875277715267612, acc: 0.6770749933279957, test loss: 0.9070571409429751, acc: 0.6694061505832449
epoch: 120, train loss: 0.4102976248191487, acc: 0.7679132172423637, val loss: 0.8637837003255355, acc: 0.6717373899119295, test loss: 0.9096705018525918, acc: 0.6572110286320254
epoch: 121, train loss: 0.40626215447434827, acc: 0.7696831287467885, val loss: 0.8452096192843761, acc: 0.6730717907659461, test loss: 0.8814567359103125, acc: 0.6606574761399788
epoch: 122, train loss: 0.3954870111970196, acc: 0.7737367970311162, val loss: 0.8852817291955105, acc: 0.6709367493995196, test loss: 0.9180937255806786, acc: 0.6609225874867445
epoch: 123, train loss: 0.3898464429235581, acc: 0.7719668855266915, val loss: 0.9946908454497974, acc: 0.644515612489992, test loss: 0.9862078710173752, acc: 0.6413043478260869
epoch: 124, train loss: 0.3967506715562593, acc: 0.7684841564373395, val loss: 0.9086782822202993, acc: 0.6762743528155858, test loss: 0.9443296084095539, acc: 0.6670201484623541
epoch: 125, train loss: 0.39432841871181556, acc: 0.7712246645732229, val loss: 0.8946335651030183, acc: 0.6744061916199626, test loss: 0.9555661961483677, acc: 0.6686108165429481
epoch: 126, train loss: 0.38474204224788766, acc: 0.7759634598915216, val loss: 0.9128223117714346, acc: 0.6647985054710435, test loss: 0.9375550180578788, acc: 0.6588016967126193
epoch: 127, train loss: 0.40529725405103373, acc: 0.7705395375392521, val loss: 0.9191313340309495, acc: 0.6573258606885508, test loss: 0.9475370462221406, acc: 0.6585365853658537
epoch: 128, train loss: 0.39778968154141947, acc: 0.772823294319155, val loss: 0.8806173697070245, acc: 0.6682679476914866, test loss: 0.9331124319377911, acc: 0.6603923647932132
epoch: 129, train loss: 0.367162461206636, acc: 0.7850413930916358, val loss: 0.8823366915349296, acc: 0.6778756338404056, test loss: 0.8979838493646518, acc: 0.6808059384941676
epoch 130, gamma increased to 3.
epoch: 130, train loss: 0.2959309640115852, acc: 0.7832143876677133, val loss: 0.7856600980997913, acc: 0.669602348545503, test loss: 0.7886876728729377, acc: 0.6699363732767762
epoch: 131, train loss: 0.2910462163837508, acc: 0.779788752497859, val loss: 0.7569587235454562, acc: 0.6717373899119295, test loss: 0.8127609458480382, acc: 0.6654294803817603
epoch: 132, train loss: 0.291314047796196, acc: 0.7765343990864972, val loss: 0.7543908747985327, acc: 0.6832132372564719, test loss: 0.7943308548103082, acc: 0.6702014846235419
epoch: 133, train loss: 0.2959747768727977, acc: 0.7751641450185556, val loss: 0.819527774834461, acc: 0.6730717907659461, test loss: 0.839483745783283, acc: 0.6659597030752916
epoch: 134, train loss: 0.2866146658493252, acc: 0.7831572937482159, val loss: 0.8192651348175097, acc: 0.6680010675206832, test loss: 0.8686283196017446, acc: 0.6606574761399788
epoch: 135, train loss: 0.2935510671278289, acc: 0.7800171281758492, val loss: 0.7741660308863342, acc: 0.6725380304243395, test loss: 0.8487686683618377, acc: 0.6572110286320254
epoch: 136, train loss: 0.29632302264877297, acc: 0.7764202112475022, val loss: 0.7754031917016156, acc: 0.6733386709367494, test loss: 0.7927185669936405, acc: 0.6725874867444327
epoch: 137, train loss: 0.2855059612973091, acc: 0.7790465315443905, val loss: 0.8123269780400533, acc: 0.6623965839338137, test loss: 0.8377027096955673, acc: 0.6577412513255567
epoch: 138, train loss: 0.2738811126920519, acc: 0.78635455324008, val loss: 0.9160168986655185, acc: 0.6589271417133707, test loss: 0.9590473574520042, acc: 0.644220572640509
epoch: 139, train loss: 0.2974831387243235, acc: 0.7764202112475022, val loss: 0.8572001503663665, acc: 0.6543901788097144, test loss: 0.8759494382781416, acc: 0.6439554612937434
epoch: 140, train loss: 0.3156229779766588, acc: 0.7675706537253782, val loss: 0.8818206860919237, acc: 0.6365092073658927, test loss: 0.9193810898324718, acc: 0.634676564156946
epoch: 141, train loss: 0.2953773843588708, acc: 0.7723665429631744, val loss: 0.8449097849828324, acc: 0.6639978649586336, test loss: 0.8877730217869941, acc: 0.6511134676564156
epoch: 142, train loss: 0.28724704752504704, acc: 0.77670568084499, val loss: 0.8328560103026269, acc: 0.655724579663731, test loss: 0.8628898948778656, acc: 0.6519088016967126
epoch: 143, train loss: 0.36585671402813874, acc: 0.7425635169854411, val loss: 0.7673374916478287, acc: 0.6493194555644516, test loss: 0.8079874566099424, acc: 0.647932131495228
epoch: 144, train loss: 0.30908513000410964, acc: 0.7696831287467885, val loss: 0.8204843048925937, acc: 0.6597277822257807, test loss: 0.8418342031830821, acc: 0.6564156945917285
epoch: 145, train loss: 0.28502195420994814, acc: 0.7796174707393663, val loss: 0.8025856388388298, acc: 0.6685348278622898, test loss: 0.8343820432842979, acc: 0.6580063626723224
epoch: 146, train loss: 0.2900637419443964, acc: 0.7769911504424779, val loss: 0.8015312377949031, acc: 0.6720042700827329, test loss: 0.8324961547254884, acc: 0.662778366914104
epoch: 147, train loss: 0.26970548455013466, acc: 0.7848130174136454, val loss: 0.8636867852410794, acc: 0.6613290632506005, test loss: 0.8942717996407162, acc: 0.6638388123011665
epoch: 148, train loss: 0.28202636961830774, acc: 0.7796745646588639, val loss: 0.8639834079228308, acc: 0.6546570589805177, test loss: 0.8645656538667001, acc: 0.6495227995758218
epoch: 149, train loss: 0.31788996638245764, acc: 0.7653439908649728, val loss: 0.8297557161431456, acc: 0.6570589805177476, test loss: 0.823731113845445, acc: 0.6492576882290562
epoch: 150, train loss: 0.2794083281181691, acc: 0.7803596916928347, val loss: 0.8076575747928462, acc: 0.6685348278622898, test loss: 0.8169111578487263, acc: 0.6688759278897137
best val acc 0.6832132372564719 at epoch 132.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.8745    0.9700    0.9198      5337
           1     0.6927    0.7298    0.7108      2502
           2     0.9735    0.8148    0.8871       810
           3     0.7866    0.8212    0.8035      1840
           4     0.9349    0.7788    0.8497       737
           5     0.8384    0.9808    0.9040       677
           6     0.6895    0.9433    0.7967      1323
           7     0.6276    0.5612    0.5925       907
           8     0.8454    0.8575    0.8514       421
           9     0.8009    0.8728    0.8353       401
          10     0.9298    0.9369    0.9333       396
          11     0.9754    0.9548    0.9650       332
          12     0.8543    0.5763    0.6883       295
          13     0.9493    0.7079    0.8110       291
          14     0.0000    0.0000    0.0000       261
          15     0.9485    0.1862    0.3113       494
          16     0.7381    0.1211    0.2081       256
          17     0.8316    0.6723    0.7435       235

    accuracy                         0.8122     17515
   macro avg     0.7939    0.6937    0.7117     17515
weighted avg     0.8064    0.8122    0.7946     17515

train confusion matrix:
[[5177   26    1   15    0    0    2    3   50   56    1    0    0    5
     0    0    0    1]
 [ 153 1826    2  128   14    0  360    9    0    6    0    1    2    0
     0    0    0    1]
 [   0    3  660    0    2  118   16    4    0    3    0    0    4    0
     0    0    0    0]
 [  93  219    0 1511    1    0    3    1    0    1    2    0    0    3
     0    5    0    1]
 [   3  128    0    6  574    0    3   15    0    0    3    2    2    0
     0    0    1    0]
 [   0    0    2    0    0  664   10    0    0    1    0    0    0    0
     0    0    0    0]
 [   5   49    6    0    0    7 1248    0    0    5    0    0    3    0
     0    0    0    0]
 [ 159  137    0   17   10    0    9  509    7    1   13    0   17    1
     0    0   10   17]
 [  59    0    0    0    0    0    0    0  361    0    0    0    1    0
     0    0    0    0]
 [  43    6    0    0    0    0    1    0    0  350    0    1    0    0
     0    0    0    0]
 [  16    2    0    3    0    0    0    3    0    0  371    0    0    0
     0    0    0    1]
 [   1    3    0    1    0    0    1    0    0    9    0  317    0    0
     0    0    0    0]
 [  16   37    4    0    8    0    2   56    0    0    1    0  170    0
     0    0    0    1]
 [  67    2    1    1    0    1    0    1    3    5    0    4    0  206
     0    0    0    0]
 [  67  148    0   39    0    0    0    4    1    0    2    0    0    0
     0    0    0    0]
 [  11   38    1  195    0    2  154    1    0    0    0    0    0    0
     0   92    0    0]
 [  26   10    0    3    5    0    0  160    5    0    4    0    0    2
     0    0   31   10]
 [  24    2    1    2    0    0    1   45    0    0    2    0    0    0
     0    0    0  158]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.7592    0.8801    0.8152      1143
           1     0.5466    0.5802    0.5629       536
           2     0.9000    0.6763    0.7723       173
           3     0.6552    0.6751    0.6650       394
           4     0.8772    0.6329    0.7353       158
           5     0.7602    0.8966    0.8228       145
           6     0.5935    0.8410    0.6959       283
           7     0.4313    0.3557    0.3898       194
           8     0.6813    0.6889    0.6851        90
           9     0.4476    0.5529    0.4947        85
          10     0.7882    0.7976    0.7929        84
          11     0.9167    0.7746    0.8397        71
          12     0.6667    0.3175    0.4301        63
          13     0.8049    0.5323    0.6408        62
          14     0.0000    0.0000    0.0000        56
          15     0.6111    0.1048    0.1789       105
          16     0.6667    0.0364    0.0690        55
          17     0.6842    0.5200    0.5909        50

    accuracy                         0.6832      3747
   macro avg     0.6550    0.5479    0.5656      3747
weighted avg     0.6759    0.6832    0.6635      3747

validation confusion matrix:
[[1006   38    1   22    1    6    5   12   20   24    4    0    1    0
     0    1    0    2]
 [  71  311    5   42    3    2   74   14    1    7    0    2    1    0
     0    0    0    3]
 [   5    2  117    2    0   28   12    1    0    3    0    0    1    2
     0    0    0    0]
 [  48   50    0  266    0    0    5   10    0    1    1    0    2    2
     0    6    0    3]
 [   5   33    1    2  100    0    7    4    1    0    2    0    1    2
     0    0    0    0]
 [   3    1    0    0    0  130   11    0    0    0    0    0    0    0
     0    0    0    0]
 [  12   21    2    1    0    2  238    1    0    5    0    0    0    1
     0    0    0    0]
 [  47   33    1   18    3    0    7   69    2    1    7    1    2    0
     0    0    1    2]
 [  19    5    0    1    0    0    0    1   62    2    0    0    0    0
     0    0    0    0]
 [  20    7    2    0    1    1    5    0    0   47    0    1    0    1
     0    0    0    0]
 [  10    2    0    0    1    0    0    2    1    0   67    0    0    0
     0    0    0    1]
 [   2    3    0    2    2    0    0    0    0    7    0   55    0    0
     0    0    0    0]
 [  12   13    0    0    1    0    4   12    0    1    0    0   20    0
     0    0    0    0]
 [  17    1    1    2    0    1    0    2    1    4    0    0    0   33
     0    0    0    0]
 [  20   22    0    9    0    0    1    1    1    1    0    0    1    0
     0    0    0    0]
 [  11   14    0   32    0    1   32    2    0    2    0    0    0    0
     0   11    0    0]
 [   4   13    0    6    2    0    0   21    2    0    2    1    1    0
     0    0    2    1]
 [  13    0    0    1    0    0    0    8    0    0    2    0    0    0
     0    0    0   26]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.7414    0.8664    0.7990      1145
           1     0.5124    0.5791    0.5437       537
           2     0.8707    0.7314    0.7950       175
           3     0.6623    0.6405    0.6512       395
           4     0.9273    0.6415    0.7584       159
           5     0.7485    0.8356    0.7896       146
           6     0.6010    0.8380    0.7000       284
           7     0.4451    0.3949    0.4185       195
           8     0.6771    0.7143    0.6952        91
           9     0.4824    0.4713    0.4767        87
          10     0.7778    0.7326    0.7545        86
          11     0.9821    0.7639    0.8594        72
          12     0.5455    0.2812    0.3711        64
          13     0.5581    0.3750    0.4486        64
          14     0.0000    0.0000    0.0000        57
          15     0.7895    0.1402    0.2381       107
          16     0.2500    0.0179    0.0333        56
          17     0.5897    0.4423    0.5055        52

    accuracy                         0.6702      3772
   macro avg     0.6200    0.5259    0.5466      3772
weighted avg     0.6610    0.6702    0.6514      3772

test confusion matrix:
[[992  45   5  18   0   1   6  16  22  22   4   0   0  11   0   0   0   3]
 [ 73 311   3  45   3   1  81   9   0   6   2   0   0   2   0   0   0   1]
 [  4   6 128   1   0  26   5   0   0   2   1   0   1   1   0   0   0   0]
 [ 52  70   0 253   0   2   4   5   1   1   2   0   1   0   0   3   0   1]
 [  8  25   0   8 102   0   2   7   1   0   0   0   4   2   0   0   0   0]
 [  1   1   4   0   0 122  15   0   0   2   0   0   0   0   0   1   0   0]
 [ 12  20   1   1   0   8 238   0   0   3   1   0   0   0   0   0   0   0]
 [ 42  33   0  13   0   1   8  77   3   0   5   1   4   0   0   0   2   6]
 [ 19   3   0   0   0   0   0   1  65   0   0   0   1   2   0   0   0   0]
 [ 29   9   2   1   0   0   3   1   0  41   0   0   0   1   0   0   0   0]
 [ 12   1   0   1   0   0   0   7   0   0  63   0   0   0   0   0   1   1]
 [  6   5   1   2   1   0   0   0   0   2   0  55   0   0   0   0   0   0]
 [ 11  16   1   0   1   0   1  16   0   0   0   0  18   0   0   0   0   0]
 [ 24   9   1   0   0   1   1   0   1   3   0   0   0  24   0   0   0   0]
 [ 16  25   0   7   1   0   2   3   1   0   0   0   2   0   0   0   0   0]
 [  7  21   1  27   0   1  29   2   0   2   0   0   1   0   0  15   0   1]
 [ 17   5   0   4   2   0   1  18   2   1   2   0   0   0   0   0   1   3]
 [ 13   2   0   1   0   0   0  11   0   0   1   0   1   0   0   0   0  23]]
---------------------------------------
program finished.
