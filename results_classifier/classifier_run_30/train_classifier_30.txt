seed:  666
save trained model at:  ../trained_models/trained_classifier_model_30.pt
save loss at:  ./results/train_classifier_results_30.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  14780
number of pockets in validation set:  3162
number of pockets in test set:  3183
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKMCNMMEmbeddingNet(
    (conv0): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=33, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): MCNMMConv(
      (nn): Sequential(
        (0): Linear(in_features=288, out_features=96, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=96, out_features=96, bias=True)
      )
      (NMMs): ModuleList(
        (0): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (1): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
        (2): NMMConv(edge_nn=Sequential(
          (0): Linear(in_features=1, out_features=8, bias=True)
          (1): LeakyReLU(negative_slope=0.01)
          (2): Linear(in_features=8, out_features=1, bias=True)
          (3): ELU(alpha=1.0)
        ))
      )
    )
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b9eac2dc700>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0239298601757043, acc: 0.393234100135318, val loss: 1.7917153390429579, acc: 0.4468690702087287, test loss: 1.7888593056499302, acc: 0.4530317310713164
epoch: 2, train loss: 1.7249145486519366, acc: 0.47273342354533154, val loss: 1.6051650080478772, acc: 0.48798228969006957, test loss: 1.5988814361882666, acc: 0.4938737040527804
epoch: 3, train loss: 1.6218532263829357, acc: 0.5039242219215155, val loss: 1.637363722172061, acc: 0.48134092346616064, test loss: 1.6480902951354692, acc: 0.4816211121583412
epoch: 4, train loss: 1.5460032616965018, acc: 0.527807848443843, val loss: 1.5064522922981547, acc: 0.5436432637571158, test loss: 1.4845584321988292, acc: 0.5391140433553252
epoch: 5, train loss: 1.459977153641284, acc: 0.5571041948579161, val loss: 1.5019121952708352, acc: 0.5439595192915876, test loss: 1.4947619751268986, acc: 0.5394282123782596
epoch: 6, train loss: 1.3882471968582422, acc: 0.5798376184032477, val loss: 1.3955016991218987, acc: 0.5708412397216951, test loss: 1.37763799723686, acc: 0.5802701853597235
epoch: 7, train loss: 1.3555138394377713, acc: 0.591339648173207, val loss: 1.418091327611741, acc: 0.5543959519291588, test loss: 1.4210584164414808, acc: 0.5661325793276782
epoch: 8, train loss: 1.2913671162841447, acc: 0.6078484438430312, val loss: 1.3193527380927013, acc: 0.5895003162555345, test loss: 1.318903203209803, acc: 0.6032045240339302
epoch: 9, train loss: 1.2426446141022307, acc: 0.627063599458728, val loss: 1.2311970658275162, acc: 0.6192283364958887, test loss: 1.2068656824910713, acc: 0.6189129751806471
epoch: 10, train loss: 1.1967142664531247, acc: 0.6375507442489852, val loss: 1.3071353661719942, acc: 0.6283997469955724, test loss: 1.2906362449377542, acc: 0.628023876845743
epoch: 11, train loss: 1.1596963608861777, acc: 0.6554127198917457, val loss: 1.2896105513249976, acc: 0.6005692599620494, test loss: 1.2676506010271864, acc: 0.6057178762174049
epoch: 12, train loss: 1.1487583394302243, acc: 0.6592016238159675, val loss: 1.7587533705902583, acc: 0.5913978494623656, test loss: 1.7320330841826674, acc: 0.5956644674835061
epoch: 13, train loss: 1.11947276389002, acc: 0.6662381596752368, val loss: 1.2569761464748106, acc: 0.6081593927893738, test loss: 1.2446835760871007, acc: 0.6041470311027333
epoch: 14, train loss: 1.1228984351732412, acc: 0.6615020297699594, val loss: 1.3025785094194213, acc: 0.5825426944971537, test loss: 1.298167085108276, acc: 0.5849827207037386
epoch: 15, train loss: 1.0731191662877435, acc: 0.6805818673883627, val loss: 1.3090548091565108, acc: 0.6176470588235294, test loss: 1.298974195441996, acc: 0.6173421300659755
epoch: 16, train loss: 1.021193907709341, acc: 0.6936400541271989, val loss: 1.099897767983231, acc: 0.6717267552182163, test loss: 1.0783799117816681, acc: 0.6757775683317625
epoch: 17, train loss: 1.009242554704617, acc: 0.6996617050067659, val loss: 1.117401026080938, acc: 0.6603415559772297, test loss: 1.1115397502894375, acc: 0.6657241595978637
epoch: 18, train loss: 0.9962195283505204, acc: 0.6993910690121786, val loss: 1.0742885915634377, acc: 0.6619228336495888, test loss: 1.0769614328425747, acc: 0.6628966383914546
epoch: 19, train loss: 0.974403162286472, acc: 0.7079837618403247, val loss: 1.0482671962064856, acc: 0.6859582542694497, test loss: 1.0333488580755867, acc: 0.6826892868363179
epoch: 20, train loss: 0.9631867955760156, acc: 0.7058186738836265, val loss: 1.1159626894702948, acc: 0.6445287792536369, test loss: 1.114115466870791, acc: 0.6468740182218033
epoch: 21, train loss: 0.9202154149867201, acc: 0.7221244925575101, val loss: 1.0808217911536298, acc: 0.685325743200506, test loss: 1.0599388578123092, acc: 0.683631793905121
epoch: 22, train loss: 0.9135240329618545, acc: 0.7215832205683356, val loss: 1.1894641504643613, acc: 0.6546489563567363, test loss: 1.189801234102384, acc: 0.6515865535658184
epoch: 23, train loss: 0.9165643602166027, acc: 0.7218538565629229, val loss: 1.060218182339991, acc: 0.6808981657179001, test loss: 1.0476430534305268, acc: 0.6877159912032673
epoch: 24, train loss: 0.9046103809296037, acc: 0.7287550744248985, val loss: 1.1300073212871515, acc: 0.6698292220113852, test loss: 1.1277706026094647, acc: 0.6729500471253534
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.7069500028683787, acc: 0.7327469553450608, val loss: 0.8176792013275707, acc: 0.6865907653383935, test loss: 0.8341725142991584, acc: 0.68645931511153
epoch: 26, train loss: 0.6707394141466918, acc: 0.745872801082544, val loss: 0.8663183133259821, acc: 0.6644528779253637, test loss: 0.858611798084198, acc: 0.6767200754005656
epoch: 27, train loss: 0.667709949390814, acc: 0.7464817320703654, val loss: 0.9779604417775568, acc: 0.6261859582542695, test loss: 0.9634650104120023, acc: 0.6339930882814955
epoch: 28, train loss: 0.6591486861644482, acc: 0.7476995940460082, val loss: 0.8949854162808538, acc: 0.6834282099936749, test loss: 0.9390934587760875, acc: 0.6899151743638078
epoch: 29, train loss: 0.6364397578536254, acc: 0.7571718538565629, val loss: 0.8168636347054681, acc: 0.7141049968374447, test loss: 0.836687248831008, acc: 0.7056236255105247
epoch: 30, train loss: 0.6321890340283695, acc: 0.7546684709066306, val loss: 0.8623170477163181, acc: 0.6695129664769134, test loss: 0.8845521402254144, acc: 0.663210807414389
epoch: 31, train loss: 0.661171771870253, acc: 0.7445196211096076, val loss: 0.7457509625945794, acc: 0.7109424414927261, test loss: 0.7728113784304663, acc: 0.6980835689601005
epoch: 32, train loss: 0.6247523266343206, acc: 0.7561569688768606, val loss: 0.9984054942435663, acc: 0.6672991777356104, test loss: 0.9723239413904687, acc: 0.681746779767515
epoch: 33, train loss: 0.6229961336707552, acc: 0.7566982408660352, val loss: 0.9243033820507573, acc: 0.661606578115117, test loss: 0.9458723194075574, acc: 0.668865849827207
epoch: 34, train loss: 0.6139089110417036, acc: 0.7646143437077131, val loss: 0.76427409695644, acc: 0.7163187855787476, test loss: 0.7749296897543028, acc: 0.7203895695884386
epoch: 35, train loss: 0.6103038091136896, acc: 0.7640730717185386, val loss: 0.8262674060522341, acc: 0.7125237191650854, test loss: 0.8439885328821967, acc: 0.7081369776939994
epoch: 36, train loss: 0.6104261083596453, acc: 0.7631258457374831, val loss: 0.7830591310359948, acc: 0.704617330803289, test loss: 0.7687976021007139, acc: 0.7100219918316054
epoch: 37, train loss: 0.5884237331692356, acc: 0.772936400541272, val loss: 0.8158877128140222, acc: 0.6941808981657179, test loss: 0.8700670400506252, acc: 0.6814326107445806
epoch: 38, train loss: 0.5731967724547173, acc: 0.7753044654939107, val loss: 0.7545606548615756, acc: 0.7179000632511069, test loss: 0.7706457946122984, acc: 0.7103361608545398
epoch: 39, train loss: 0.5876124002614105, acc: 0.775575101488498, val loss: 0.7338529366017292, acc: 0.7280202403542062, test loss: 0.7516030117733614, acc: 0.7295004712535345
epoch: 40, train loss: 0.5840273788720571, acc: 0.7714479025710419, val loss: 0.7317897418418464, acc: 0.724225173940544, test loss: 0.7553038742420174, acc: 0.7169337103361608
epoch: 41, train loss: 0.5424497885697588, acc: 0.7869418132611637, val loss: 0.8386449667564455, acc: 0.7001897533206831, test loss: 0.8575394960308165, acc: 0.6899151743638078
epoch: 42, train loss: 0.5643368280143635, acc: 0.7794993234100135, val loss: 0.8130263764974364, acc: 0.7077798861480076, test loss: 0.8509940620192259, acc: 0.700282752120641
epoch: 43, train loss: 0.5278746256486327, acc: 0.7914749661705007, val loss: 0.7671825341376076, acc: 0.7115749525616698, test loss: 0.7678408592780206, acc: 0.7090794847628024
epoch: 44, train loss: 0.528001477769973, acc: 0.7906630581867389, val loss: 0.7070014810049405, acc: 0.7321315623023403, test loss: 0.7182578684734616, acc: 0.7364121897580899
epoch: 45, train loss: 0.51493640960634, acc: 0.7995263870094722, val loss: 0.8981232730592827, acc: 0.6581277672359266, test loss: 0.9275204889661218, acc: 0.6481306943135406
epoch: 46, train loss: 0.5258556064959308, acc: 0.7930987821380243, val loss: 0.9255649969594377, acc: 0.674573055028463, test loss: 0.9359663895305282, acc: 0.6820609487904492
epoch: 47, train loss: 0.5277765764354531, acc: 0.7895128552097429, val loss: 0.9400515693565624, acc: 0.6872232764073372, test loss: 0.9552032742304057, acc: 0.68645931511153
epoch: 48, train loss: 0.5380221619496326, acc: 0.7875507442489851, val loss: 0.7946494512358925, acc: 0.713472485768501, test loss: 0.8107360174848417, acc: 0.7103361608545398
epoch: 49, train loss: 0.49512049785647566, acc: 0.8047361299052774, val loss: 0.8540029507660549, acc: 0.6929158760278304, test loss: 0.8771295899532893, acc: 0.6952560477536914
epoch: 50, train loss: 0.4773638535578293, acc: 0.81319350473613, val loss: 0.843852436293687, acc: 0.7087286527514232, test loss: 0.875343543944627, acc: 0.7012252591894439
epoch: 51, train loss: 0.47926067391332333, acc: 0.8054127198917456, val loss: 0.8078877755466102, acc: 0.7033523086654017, test loss: 0.830884216642964, acc: 0.7090794847628024
epoch: 52, train loss: 0.4863866949113683, acc: 0.8065629228687415, val loss: 0.6942528298502855, acc: 0.7390891840607211, test loss: 0.7057280924722454, acc: 0.7389255419415646
epoch: 53, train loss: 0.46956841940808847, acc: 0.8100811907983761, val loss: 0.7026417299888317, acc: 0.7340290955091714, test loss: 0.7355959006461414, acc: 0.7191328934967012
epoch: 54, train loss: 0.4574743581156931, acc: 0.8163734776725304, val loss: 0.6871495914941796, acc: 0.7400379506641366, test loss: 0.6985252294531417, acc: 0.7329563305058121
epoch: 55, train loss: 0.44982869467006165, acc: 0.822192151556157, val loss: 0.9493372012059347, acc: 0.7043010752688172, test loss: 1.014398520748607, acc: 0.6914860194784794
epoch: 56, train loss: 0.4631670735524375, acc: 0.8133288227334236, val loss: 0.7469776133658538, acc: 0.7318153067678684, test loss: 0.7516133849665012, acc: 0.7295004712535345
epoch: 57, train loss: 0.43921593977730716, acc: 0.8234776725304466, val loss: 0.9384462019071633, acc: 0.687539531941809, test loss: 0.9043285330623492, acc: 0.6908576814326107
epoch: 58, train loss: 0.49407023441485043, acc: 0.8048037889039242, val loss: 0.7034480292762404, acc: 0.7343453510436433, test loss: 0.702514714128073, acc: 0.7335846685516808
epoch: 59, train loss: 0.4599636659005982, acc: 0.8192151556156969, val loss: 0.7321078615954675, acc: 0.7254901960784313, test loss: 0.7644425749291873, acc: 0.7053094564875904
epoch: 60, train loss: 0.43403174128680816, acc: 0.8243572395128552, val loss: 0.7354435956907905, acc: 0.74573055028463, test loss: 0.7250601288435634, acc: 0.743952246308514
epoch: 61, train loss: 0.42652591579821825, acc: 0.8286874154262517, val loss: 0.7026540251339183, acc: 0.7473118279569892, test loss: 0.7056325977042603, acc: 0.7489789506754634
epoch: 62, train loss: 0.42132538824023347, acc: 0.834573748308525, val loss: 0.6776085670502559, acc: 0.7577482605945604, test loss: 0.6866292640393611, acc: 0.7508639648130694
epoch: 63, train loss: 0.4109578010032561, acc: 0.834979702300406, val loss: 0.7717639359396368, acc: 0.734977862112587, test loss: 0.7797474058177012, acc: 0.7276154571159283
epoch: 64, train loss: 0.4459001689786356, acc: 0.8226657645466847, val loss: 0.6763859555210063, acc: 0.7530044275774826, test loss: 0.688938613212075, acc: 0.7492931196983977
epoch: 65, train loss: 0.42035975515116536, acc: 0.831596752368065, val loss: 0.8386799259173727, acc: 0.6979759645793802, test loss: 0.8471225284758582, acc: 0.6814326107445806
epoch: 66, train loss: 0.399464381152464, acc: 0.8388362652232747, val loss: 0.9284002471769407, acc: 0.6818469323213157, test loss: 0.9142670305577457, acc: 0.6880301602262017
epoch: 67, train loss: 0.4076782188654269, acc: 0.8351150202976996, val loss: 0.8009159751966888, acc: 0.7128399746995573, test loss: 0.8018807281760199, acc: 0.7078228086710651
epoch: 68, train loss: 0.40183566778051355, acc: 0.8422868741542625, val loss: 0.7101048851676715, acc: 0.75426944971537, test loss: 0.7588421968854976, acc: 0.7351555136663525
epoch: 69, train loss: 0.37872228905538424, acc: 0.8497293640054128, val loss: 0.7164378779964459, acc: 0.7330803289057558, test loss: 0.7147138541949982, acc: 0.7320138234370092
epoch: 70, train loss: 0.41687264601335794, acc: 0.8319350473612991, val loss: 0.8078559743385387, acc: 0.7324478178368121, test loss: 0.8125240049233349, acc: 0.7295004712535345
epoch: 71, train loss: 0.38586400469036647, acc: 0.8442489851150203, val loss: 0.8033578774357506, acc: 0.7248576850094877, test loss: 0.8021081477413003, acc: 0.725416273955388
epoch: 72, train loss: 0.37538642081582013, acc: 0.8462787550744248, val loss: 1.1202978027688786, acc: 0.6695129664769134, test loss: 1.0811776912628088, acc: 0.6710650329877474
epoch: 73, train loss: 0.4499846046445173, acc: 0.8203653585926928, val loss: 0.786977091444813, acc: 0.7356103731815307, test loss: 0.7764491930795921, acc: 0.7408105560791706
Epoch    73: reducing learning rate of group 0 to 1.5000e-03.
epoch: 74, train loss: 0.30927088945418474, acc: 0.8735453315290934, val loss: 0.6280354238777957, acc: 0.7849462365591398, test loss: 0.6436558340541829, acc: 0.7797675149230285
epoch: 75, train loss: 0.2551547543484723, acc: 0.8926251691474966, val loss: 0.6509960901126464, acc: 0.7789373814041746, test loss: 0.6748792686965756, acc: 0.7731699654414075
epoch: 76, train loss: 0.22818318336032564, acc: 0.9019621109607577, val loss: 0.718934309324232, acc: 0.7773561037318153, test loss: 0.7423433497234444, acc: 0.7734841344643418
epoch: 77, train loss: 0.2417947374435497, acc: 0.8981732070365359, val loss: 0.7001489357130954, acc: 0.7830487033523087, test loss: 0.7087721082350011, acc: 0.7835375431982406
epoch: 78, train loss: 0.21499679492678145, acc: 0.9092692828146144, val loss: 0.7920507361510372, acc: 0.7748260594560404, test loss: 0.7830108309031806, acc: 0.7681432610744581
epoch: 79, train loss: 0.2215047720723933, acc: 0.9058186738836265, val loss: 0.8764381766847084, acc: 0.7447817836812144, test loss: 0.933867765004778, acc: 0.7213320766572416
epoch: 80, train loss: 0.21738556121779715, acc: 0.9036535859269282, val loss: 0.7967377443392016, acc: 0.7517394054395952, test loss: 0.775563602210814, acc: 0.7552623311341502
epoch: 81, train loss: 0.2308460668475444, acc: 0.9019621109607577, val loss: 0.719143746247554, acc: 0.7741935483870968, test loss: 0.7472626607522177, acc: 0.7766258246936852
epoch: 82, train loss: 0.2106278200272133, acc: 0.9087956698240865, val loss: 0.7270029908414886, acc: 0.7669196710942442, test loss: 0.7284081875060738, acc: 0.7744266415331448
epoch: 83, train loss: 0.2059395838372601, acc: 0.9106224627875508, val loss: 0.6926599708559542, acc: 0.7896900695762176, test loss: 0.6924173444687403, acc: 0.783851712221175
epoch: 84, train loss: 0.20559591292044468, acc: 0.9089986468200271, val loss: 0.7560857648264374, acc: 0.7789373814041746, test loss: 0.7587487177769418, acc: 0.7703424442349984
epoch: 85, train loss: 0.20853256732023778, acc: 0.9085250338294993, val loss: 0.7680484091157328, acc: 0.7754585705249842, test loss: 0.7719700809098848, acc: 0.7706566132579328
epoch: 86, train loss: 0.1832198819062384, acc: 0.919553450608931, val loss: 0.7897294638354442, acc: 0.7697659709044908, test loss: 0.7764444439208175, acc: 0.7709707822808671
epoch: 87, train loss: 0.20163980499817327, acc: 0.9125845737483085, val loss: 0.843926326147293, acc: 0.7624920936116382, test loss: 0.8715795746323001, acc: 0.7565190072258875
epoch: 88, train loss: 0.19955339016788545, acc: 0.9138024357239513, val loss: 0.8406685736268326, acc: 0.7678684376976597, test loss: 0.8717803089940019, acc: 0.7587181903864278
epoch: 89, train loss: 0.1940003412421566, acc: 0.913937753721245, val loss: 0.7322225104699325, acc: 0.7808349146110057, test loss: 0.7548428644671067, acc: 0.7769399937166196
epoch: 90, train loss: 0.19673100956721945, acc: 0.9146143437077131, val loss: 0.7205782465207886, acc: 0.7817836812144212, test loss: 0.7345654051745046, acc: 0.7759974866478165
epoch: 91, train loss: 0.18705350234197182, acc: 0.9184709066305818, val loss: 0.8603171098843019, acc: 0.7526881720430108, test loss: 0.8665330962044168, acc: 0.7530631479736098
epoch: 92, train loss: 0.1731958988917858, acc: 0.9211772665764547, val loss: 0.7553665135798011, acc: 0.7912713472485768, test loss: 0.7555764435298631, acc: 0.7785108388312912
epoch: 93, train loss: 0.18557104832103352, acc: 0.9181326116373477, val loss: 0.7179686799221292, acc: 0.7814674256799494, test loss: 0.7017330184059251, acc: 0.7882500785422557
epoch: 94, train loss: 0.18269808507739932, acc: 0.918403247631935, val loss: 0.9514075616429984, acc: 0.7188488298545225, test loss: 0.9348542134491408, acc: 0.7238454288407163
epoch: 95, train loss: 0.19102615458192618, acc: 0.9147496617050067, val loss: 0.7923710629730417, acc: 0.7884250474383302, test loss: 0.7610108286863297, acc: 0.7895067546339931
epoch: 96, train loss: 0.15278088189106995, acc: 0.9297699594046008, val loss: 0.8035886737080332, acc: 0.7849462365591398, test loss: 0.7659901038603404, acc: 0.7891925856110588
epoch: 97, train loss: 0.15602354779133779, acc: 0.930108254397835, val loss: 0.8352872854844443, acc: 0.7732447817836812, test loss: 0.8128422481849663, acc: 0.7709707822808671
epoch: 98, train loss: 0.17405638232605383, acc: 0.9215155615696887, val loss: 0.766833203114866, acc: 0.7820999367488931, test loss: 0.7540583859215059, acc: 0.7803958529688972
epoch: 99, train loss: 0.1675806273102599, acc: 0.9246278755074425, val loss: 0.7927437142282253, acc: 0.7830487033523087, test loss: 0.8011258867526256, acc: 0.781966698083569
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.12786070348768014, acc: 0.9224627875507443, val loss: 0.6732712374089415, acc: 0.7786211258697027, test loss: 0.6817715260130858, acc: 0.7775683317624882
epoch: 101, train loss: 0.09034709128499516, acc: 0.9390392422192152, val loss: 0.7223453413150794, acc: 0.7681846932321316, test loss: 0.7165927251404725, acc: 0.7722274583726044
epoch: 102, train loss: 0.09789223936720372, acc: 0.9347767253044655, val loss: 0.67503085730575, acc: 0.7877925363693865, test loss: 0.6688116290379799, acc: 0.7917059377945335
epoch: 103, train loss: 0.0979856030947462, acc: 0.9349120433017591, val loss: 0.6334664518511951, acc: 0.7912713472485768, test loss: 0.6277891479870301, acc: 0.7901350926798618
epoch: 104, train loss: 0.10352880337120554, acc: 0.9315967523680649, val loss: 0.6788808040269035, acc: 0.7798861480075902, test loss: 0.6688074894832284, acc: 0.783851712221175
epoch: 105, train loss: 0.1585653917674928, acc: 0.9042625169147497, val loss: 0.6524561389498437, acc: 0.7770398481973435, test loss: 0.6611164354356699, acc: 0.7759974866478165
epoch: 106, train loss: 0.12728375875175563, acc: 0.9198240866035182, val loss: 0.6938170990862476, acc: 0.7732447817836812, test loss: 0.6959040622609313, acc: 0.7734841344643418
epoch: 107, train loss: 0.10501114266731743, acc: 0.929702300405954, val loss: 0.8255914280039084, acc: 0.7558507273877293, test loss: 0.8337298757506361, acc: 0.7549481621112158
epoch: 108, train loss: 0.12861730777765643, acc: 0.9170500676589987, val loss: 0.6737358258691941, acc: 0.7713472485768501, test loss: 0.6669061883155782, acc: 0.7797675149230285
epoch: 109, train loss: 0.1054311605485431, acc: 0.930040595399188, val loss: 0.7016753699491478, acc: 0.7703984819734345, test loss: 0.6985179275377269, acc: 0.7788250078542256
epoch: 110, train loss: 0.09385658878233177, acc: 0.9348443843031123, val loss: 0.6413669847069776, acc: 0.7975964579380139, test loss: 0.6456788218025138, acc: 0.7885642475651901
epoch: 111, train loss: 0.102648568044496, acc: 0.9318673883626523, val loss: 0.6978958284907669, acc: 0.7767235926628716, test loss: 0.6661131701827461, acc: 0.7775683317624882
epoch: 112, train loss: 0.1050027712494014, acc: 0.9288227334235454, val loss: 0.6601112528287633, acc: 0.7890575585072739, test loss: 0.6726982989777119, acc: 0.7844800502670437
epoch: 113, train loss: 0.0881489337014894, acc: 0.9384979702300406, val loss: 0.6624091918675075, acc: 0.7877925363693865, test loss: 0.6245738896340871, acc: 0.7866792334275841
epoch: 114, train loss: 0.08351935498890277, acc: 0.9424221921515562, val loss: 0.6491155673400425, acc: 0.7881087919038583, test loss: 0.6650666466232669, acc: 0.7816525290606346
epoch: 115, train loss: 0.07799226766066558, acc: 0.9451962110960758, val loss: 0.7060804024744004, acc: 0.775774826059456, test loss: 0.6979187345714492, acc: 0.7763116556707509
epoch: 116, train loss: 0.09656971045372774, acc: 0.9339648173207037, val loss: 0.6921094314423186, acc: 0.7817836812144212, test loss: 0.6713614919595362, acc: 0.7835375431982406
epoch: 117, train loss: 0.09647629857668212, acc: 0.9360622462787551, val loss: 0.7203510415320303, acc: 0.7827324478178368, test loss: 0.711712766285404, acc: 0.7728557964184731
epoch: 118, train loss: 0.11981668023037169, acc: 0.9246278755074425, val loss: 0.670869038423204, acc: 0.7624920936116382, test loss: 0.7027578669601067, acc: 0.7631165567075087
epoch: 119, train loss: 0.13218854598811902, acc: 0.9179972936400541, val loss: 0.6399522924031131, acc: 0.7770398481973435, test loss: 0.6737279455279016, acc: 0.7737983034872762
epoch: 120, train loss: 0.08600990246169783, acc: 0.936874154262517, val loss: 0.7117868435676907, acc: 0.7836812144212524, test loss: 0.6953494805717408, acc: 0.7885642475651901
epoch: 121, train loss: 0.1090753077775119, acc: 0.9288227334235454, val loss: 0.7333810686838921, acc: 0.7653383934218849, test loss: 0.7304329833181407, acc: 0.7580898523405593
epoch: 122, train loss: 0.10693983531207935, acc: 0.9294316644113667, val loss: 0.6634799796218438, acc: 0.7798861480075902, test loss: 0.6496049953338451, acc: 0.783851712221175
epoch: 123, train loss: 0.11320802195751296, acc: 0.9278755074424898, val loss: 0.639374585637256, acc: 0.7735610373181531, test loss: 0.6221699656235734, acc: 0.787621740496387
epoch: 124, train loss: 0.093336974378771, acc: 0.9352503382949933, val loss: 0.703583235834159, acc: 0.7688172043010753, test loss: 0.701844667511993, acc: 0.7719132893496701
Epoch   124: reducing learning rate of group 0 to 7.5000e-04.
epoch: 125, train loss: 0.1002678278657193, acc: 0.9327469553450609, val loss: 0.6343601994089203, acc: 0.7881087919038583, test loss: 0.636519861453515, acc: 0.7835375431982406
epoch: 126, train loss: 0.06243238363601519, acc: 0.9545331529093369, val loss: 0.6601423567265818, acc: 0.7969639468690702, test loss: 0.635004329411743, acc: 0.7992459943449576
epoch: 127, train loss: 0.04515145166054953, acc: 0.9628552097428958, val loss: 0.6954038112243469, acc: 0.8020240354206198, test loss: 0.6796253231610662, acc: 0.7979893182532203
epoch: 128, train loss: 0.032133332586498156, acc: 0.9732070365358593, val loss: 0.7230903609955635, acc: 0.797280202403542, test loss: 0.7052400388921389, acc: 0.7945334590009425
epoch: 129, train loss: 0.029308848368843288, acc: 0.9757104194857916, val loss: 0.7414062064633499, acc: 0.7956989247311828, test loss: 0.7544013722452996, acc: 0.7954759660697456
epoch: 130, train loss: 0.027405430106332725, acc: 0.9767253044654939, val loss: 0.754784035727919, acc: 0.8010752688172043, test loss: 0.734754018052509, acc: 0.8011310084825636
epoch: 131, train loss: 0.026094569796154397, acc: 0.9780108254397835, val loss: 0.7511710905712062, acc: 0.8013915243516762, test loss: 0.7553639674838369, acc: 0.7976751492302859
epoch: 132, train loss: 0.03293879671836415, acc: 0.9741542625169147, val loss: 0.783163087719381, acc: 0.793168880455408, test loss: 0.769466254608533, acc: 0.7929626138862708
epoch: 133, train loss: 0.03433768729173928, acc: 0.9713802435723952, val loss: 0.7580867591800002, acc: 0.7919038583175205, test loss: 0.7816452399163062, acc: 0.7895067546339931
epoch: 134, train loss: 0.04837442276865768, acc: 0.9634641407307172, val loss: 0.8043218826207084, acc: 0.7783048703352309, test loss: 0.8058261742503472, acc: 0.7785108388312912
epoch: 135, train loss: 0.05249588111811223, acc: 0.9592692828146143, val loss: 0.6862003611734138, acc: 0.7893738140417458, test loss: 0.7182582752756304, acc: 0.783851712221175
epoch: 136, train loss: 0.03680080570915716, acc: 0.9696887686062247, val loss: 0.6983571723621906, acc: 0.7966476913345983, test loss: 0.7319786547116427, acc: 0.7904492617027961
epoch: 137, train loss: 0.044827161483335556, acc: 0.965426251691475, val loss: 0.7240447655725449, acc: 0.782416192283365, test loss: 0.698437857133504, acc: 0.7863650644046497
epoch: 138, train loss: 0.04657815581941233, acc: 0.963531799729364, val loss: 0.727946631294651, acc: 0.7947501581277673, test loss: 0.6940100782141534, acc: 0.7957901350926798
epoch: 139, train loss: 0.03222771149950356, acc: 0.9734776725304466, val loss: 0.7847254062136241, acc: 0.7881087919038583, test loss: 0.7864723989433962, acc: 0.7851083883129123
epoch: 140, train loss: 0.06408990096497438, acc: 0.9574424898511502, val loss: 0.7870938025419656, acc: 0.7710309930423782, test loss: 0.7770018484695056, acc: 0.7693999371661954
epoch: 141, train loss: 0.07191266612589602, acc: 0.9535182679296347, val loss: 0.6719837647913982, acc: 0.7900063251106895, test loss: 0.701377816032472, acc: 0.7935909519321395
epoch: 142, train loss: 0.046718824456202644, acc: 0.9646143437077132, val loss: 0.7416871010993268, acc: 0.7805186590765338, test loss: 0.7488127334890147, acc: 0.7863650644046497
epoch: 143, train loss: 0.03843397367871663, acc: 0.9696887686062247, val loss: 0.7702249102318008, acc: 0.7900063251106895, test loss: 0.7765186958405869, acc: 0.7898209236569275
epoch: 144, train loss: 0.03338573105199734, acc: 0.9731393775372125, val loss: 0.7602811592579792, acc: 0.7903225806451613, test loss: 0.7522156404842193, acc: 0.7929626138862708
epoch: 145, train loss: 0.050603707810216086, acc: 0.9631935047361299, val loss: 0.7190208033924236, acc: 0.7919038583175205, test loss: 0.7286662249830284, acc: 0.7885642475651901
epoch: 146, train loss: 0.043037749409514284, acc: 0.966914749661705, val loss: 0.705434366089871, acc: 0.8010752688172043, test loss: 0.7275729316456154, acc: 0.7945334590009425
epoch: 147, train loss: 0.031172257700635066, acc: 0.9753721244925575, val loss: 0.7311761733627561, acc: 0.7938013915243517, test loss: 0.7503771757952192, acc: 0.7979893182532203
epoch: 148, train loss: 0.04010186314441677, acc: 0.9706359945872801, val loss: 0.7208579322494034, acc: 0.7938013915243517, test loss: 0.745187667455083, acc: 0.7979893182532203
epoch: 149, train loss: 0.04534441723875167, acc: 0.9670500676589987, val loss: 0.7163829151246459, acc: 0.786527514231499, test loss: 0.7125922588293859, acc: 0.7932767829092051
epoch: 150, train loss: 0.040321764774388646, acc: 0.9711096075778078, val loss: 0.7340585297229243, acc: 0.7830487033523087, test loss: 0.7375737252641391, acc: 0.7869934024505184
epoch: 151, train loss: 0.03290390420518564, acc: 0.9746955345060893, val loss: 0.775075123552881, acc: 0.790955091714105, test loss: 0.787058342038705, acc: 0.787621740496387
epoch: 152, train loss: 0.029658849315569107, acc: 0.9779431664411367, val loss: 0.7714185787860839, acc: 0.7928526249209361, test loss: 0.7865929521182555, acc: 0.7945334590009425
epoch: 153, train loss: 0.035586213110788585, acc: 0.974830852503383, val loss: 0.798968983631508, acc: 0.786527514231499, test loss: 0.8072540170922431, acc: 0.7895067546339931
epoch: 154, train loss: 0.0412161650441093, acc: 0.9715832205683356, val loss: 0.7405899159143726, acc: 0.786527514231499, test loss: 0.7598840570884271, acc: 0.7910775997486648
epoch: 155, train loss: 0.042854486087259, acc: 0.9692828146143437, val loss: 0.729766800914815, acc: 0.786527514231499, test loss: 0.7190996096113962, acc: 0.7882500785422557
epoch: 156, train loss: 0.056197129739435824, acc: 0.9627875507442489, val loss: 0.6893654034003511, acc: 0.7941176470588235, test loss: 0.7208174438623374, acc: 0.7904492617027961
epoch: 157, train loss: 0.0549192131099988, acc: 0.963531799729364, val loss: 0.7686239695111684, acc: 0.782416192283365, test loss: 0.778666407815548, acc: 0.782909205152372
epoch: 158, train loss: 0.04299283208317944, acc: 0.968809201623816, val loss: 0.7271127401462919, acc: 0.7874762808349146, test loss: 0.7215160132578023, acc: 0.7907634307257304
epoch: 159, train loss: 0.04689752706836944, acc: 0.9672530446549391, val loss: 0.8095620979921943, acc: 0.7678684376976597, test loss: 0.7580886624198719, acc: 0.7734841344643418
epoch: 160, train loss: 0.05582579662657881, acc: 0.9598105548037889, val loss: 0.7538295917161125, acc: 0.7896900695762176, test loss: 0.7374634929621627, acc: 0.7891925856110588
epoch: 161, train loss: 0.04179086387056621, acc: 0.969553450608931, val loss: 0.756310964369608, acc: 0.7836812144212524, test loss: 0.7534585213309146, acc: 0.7825950361294376
epoch: 162, train loss: 0.03699775375274586, acc: 0.9732070365358593, val loss: 0.7241130710628347, acc: 0.7833649588867805, test loss: 0.7226568954454591, acc: 0.7860508953817154
epoch: 163, train loss: 0.0364930309738374, acc: 0.9723274695534506, val loss: 0.826835657508218, acc: 0.7912713472485768, test loss: 0.8055013007350662, acc: 0.7913917687715991
epoch: 164, train loss: 0.030227984265198727, acc: 0.9765223274695535, val loss: 0.7386734432091372, acc: 0.7919038583175205, test loss: 0.7610297409798266, acc: 0.7879359095193214
epoch: 165, train loss: 0.03957625178264829, acc: 0.9741542625169147, val loss: 0.7642870073752792, acc: 0.782416192283365, test loss: 0.7590282473068285, acc: 0.7797675149230285
epoch: 166, train loss: 0.047804419540060386, acc: 0.9672530446549391, val loss: 0.7356496205592291, acc: 0.786527514231499, test loss: 0.777116142725967, acc: 0.782909205152372
epoch: 167, train loss: 0.03626560986152924, acc: 0.9725981055480379, val loss: 0.7888757027379325, acc: 0.7849462365591398, test loss: 0.8005738312667022, acc: 0.7873075714734528
epoch: 168, train loss: 0.03566525605518537, acc: 0.9755074424898511, val loss: 0.7335479394459106, acc: 0.790955091714105, test loss: 0.7931485457349939, acc: 0.7747408105560791
epoch: 169, train loss: 0.03718058751626331, acc: 0.9720568335588633, val loss: 0.8072305252701327, acc: 0.7751423149905123, test loss: 0.8386910895749978, acc: 0.7646874018221803
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.028819970673653366, acc: 0.9716508795669824, val loss: 0.6165031966041116, acc: 0.7928526249209361, test loss: 0.6638891472218735, acc: 0.781966698083569
epoch: 171, train loss: 0.02944410099989466, acc: 0.9684032476319351, val loss: 0.6322637204502596, acc: 0.7912713472485768, test loss: 0.6474088398570577, acc: 0.7854225573358466
epoch: 172, train loss: 0.02070540585662257, acc: 0.9742895805142084, val loss: 0.6002416991493809, acc: 0.7969639468690702, test loss: 0.6281857820865009, acc: 0.7939051209550738
epoch: 173, train loss: 0.016039336676909734, acc: 0.9807848443843031, val loss: 0.6548352720313703, acc: 0.790955091714105, test loss: 0.6862182795083863, acc: 0.7891925856110588
epoch: 174, train loss: 0.018182644484935173, acc: 0.9792286874154262, val loss: 0.6375782522199125, acc: 0.7985452245414295, test loss: 0.6466213011194841, acc: 0.7935909519321395
epoch: 175, train loss: 0.014938524694097541, acc: 0.9827469553450608, val loss: 0.612947333846795, acc: 0.8067678684376977, test loss: 0.6552781364358224, acc: 0.7942192899780082
epoch: 176, train loss: 0.014010681088948282, acc: 0.9849120433017592, val loss: 0.6820012606977286, acc: 0.7839974699557243, test loss: 0.7072592862851583, acc: 0.7772541627395538
epoch: 177, train loss: 0.023991578349563846, acc: 0.9756427604871448, val loss: 0.6258336666790011, acc: 0.7950664136622391, test loss: 0.6362070316718578, acc: 0.7983034872761545
epoch: 178, train loss: 0.02737008707701192, acc: 0.9725981055480379, val loss: 0.6391282374914653, acc: 0.7893738140417458, test loss: 0.6700901076586188, acc: 0.7769399937166196
epoch: 179, train loss: 0.03645706871002711, acc: 0.9679296346414074, val loss: 0.6081355654838944, acc: 0.7802024035420619, test loss: 0.6392251663600503, acc: 0.7794533459000943
epoch: 180, train loss: 0.02948300454447976, acc: 0.9696211096075779, val loss: 0.6280409857715556, acc: 0.7881087919038583, test loss: 0.6507579583699247, acc: 0.7851083883129123
epoch: 181, train loss: 0.05564302114898362, acc: 0.9529093369418132, val loss: 0.5973854083791705, acc: 0.7855787476280834, test loss: 0.6158353999617562, acc: 0.7816525290606346
epoch: 182, train loss: 0.044435438772862594, acc: 0.9605548037889039, val loss: 0.5877967057991148, acc: 0.7862112586970272, test loss: 0.6139040481658465, acc: 0.7917059377945335
epoch: 183, train loss: 0.031893260198795265, acc: 0.9674560216508795, val loss: 0.6014260800860488, acc: 0.7884250474383302, test loss: 0.6231312685105698, acc: 0.7863650644046497
epoch: 184, train loss: 0.045382749789340246, acc: 0.9581190798376183, val loss: 0.6592444213110763, acc: 0.7722960151802657, test loss: 0.6599237934958062, acc: 0.7709707822808671
epoch: 185, train loss: 0.054405238258580554, acc: 0.9512855209742895, val loss: 0.6405141564730524, acc: 0.7792536369386465, test loss: 0.6359542654326904, acc: 0.7778825007854225
epoch: 186, train loss: 0.026993290527505705, acc: 0.9673207036535859, val loss: 0.6183869074146481, acc: 0.7944339025932954, test loss: 0.6161051953022412, acc: 0.7851083883129123
Epoch   186: reducing learning rate of group 0 to 3.7500e-04.
epoch: 187, train loss: 0.018717087157504163, acc: 0.9790257104194858, val loss: 0.6073404204460883, acc: 0.8004427577482606, test loss: 0.6145053336802346, acc: 0.7983034872761545
epoch: 188, train loss: 0.010826626468666594, acc: 0.9858592692828146, val loss: 0.6244286236168839, acc: 0.8051865907653384, test loss: 0.6328900317294845, acc: 0.8001885014137606
epoch: 189, train loss: 0.009450035685917844, acc: 0.9884979702300406, val loss: 0.6284302152613158, acc: 0.8055028462998103, test loss: 0.6480965292981837, acc: 0.8008168394596292
epoch: 190, train loss: 0.0086201842687772, acc: 0.9889039242219215, val loss: 0.6385801791541565, acc: 0.7998102466793169, test loss: 0.6528016664380365, acc: 0.7986176562990889
epoch: 191, train loss: 0.007767486506228679, acc: 0.9912043301759134, val loss: 0.6438341590892206, acc: 0.8032890575585073, test loss: 0.6619530645062479, acc: 0.7976751492302859
epoch: 192, train loss: 0.006885985994814859, acc: 0.9914073071718539, val loss: 0.6379946409939664, acc: 0.8051865907653384, test loss: 0.6548011493577997, acc: 0.7970468111844172
epoch: 193, train loss: 0.006102334207909356, acc: 0.9936400541271989, val loss: 0.6404364930158322, acc: 0.8105629348513599, test loss: 0.6625763461007811, acc: 0.8020735155513666
epoch: 194, train loss: 0.006089415800340565, acc: 0.9940460081190798, val loss: 0.6508402564267732, acc: 0.8036053130929791, test loss: 0.6718846292492732, acc: 0.7983034872761545
epoch: 195, train loss: 0.006376060893877595, acc: 0.9941136671177266, val loss: 0.6641913163820299, acc: 0.8074003795066413, test loss: 0.688245463775757, acc: 0.7983034872761545
epoch: 196, train loss: 0.007250326247671582, acc: 0.9930311231393776, val loss: 0.6636971682404356, acc: 0.806135357368754, test loss: 0.6887390769355671, acc: 0.7976751492302859
epoch: 197, train loss: 0.0060632417709421725, acc: 0.9935723951285521, val loss: 0.7044753592501405, acc: 0.7979127134724858, test loss: 0.7166158982953147, acc: 0.7929626138862708
epoch: 198, train loss: 0.009108812791321814, acc: 0.9907307171853856, val loss: 0.6824947496820748, acc: 0.793168880455408, test loss: 0.6835074253214106, acc: 0.7913917687715991
epoch: 199, train loss: 0.00851957191069168, acc: 0.9907307171853856, val loss: 0.6754747650277984, acc: 0.8007590132827325, test loss: 0.68925572557716, acc: 0.7945334590009425
epoch: 200, train loss: 0.010606613129639496, acc: 0.9884979702300406, val loss: 0.6571527914786777, acc: 0.7938013915243517, test loss: 0.6866537190592218, acc: 0.7913917687715991
best val acc 0.8105629348513599 at epoch 193.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000      5337
           1     0.9988    0.9963    0.9975       810
           2     0.9971    0.9990    0.9981      2100
           3     1.0000    1.0000    1.0000       737
           4     0.9941    1.0000    0.9971       677
           5     0.9947    1.0000    0.9974      1323
           6     1.0000    0.9948    0.9974      1164
           7     1.0000    1.0000    1.0000       421
           8     1.0000    1.0000    1.0000       401
           9     1.0000    1.0000    1.0000       396
          10     1.0000    0.9984    0.9992       627
          11     1.0000    1.0000    1.0000       291
          12     1.0000    0.9770    0.9884       261
          13     1.0000    1.0000    1.0000       235

    accuracy                         0.9988     14780
   macro avg     0.9989    0.9975    0.9982     14780
weighted avg     0.9988    0.9988    0.9988     14780

train confusion matrix:
[[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 9.96296296e-01 0.00000000e+00 0.00000000e+00
  3.70370370e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 9.99047619e-01 0.00000000e+00
  4.76190476e-04 4.76190476e-04 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.15463918e-03 9.94845361e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 1.59489633e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 9.98405104e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 2.29885057e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  9.77011494e-01 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.00000000e+00]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8708    0.9081    0.8891      1143
           1     0.8800    0.8902    0.8851       173
           2     0.8486    0.7844    0.8152       450
           3     0.8671    0.7848    0.8239       158
           4     0.8786    0.8483    0.8632       145
           5     0.8640    0.8304    0.8468       283
           6     0.5512    0.6265    0.5865       249
           7     0.8462    0.7333    0.7857        90
           8     0.7059    0.7059    0.7059        85
           9     0.8554    0.8452    0.8503        84
          10     0.8696    0.7463    0.8032       134
          11     0.7400    0.5968    0.6607        62
          12     0.1915    0.3214    0.2400        56
          13     0.7778    0.5600    0.6512        50

    accuracy                         0.8106      3162
   macro avg     0.7676    0.7273    0.7433      3162
weighted avg     0.8209    0.8106    0.8140      3162

validation confusion matrix:
[[9.08136483e-01 2.62467192e-03 1.92475941e-02 2.62467192e-03
  8.74890639e-04 1.74978128e-03 2.97462817e-02 7.87401575e-03
  6.99912511e-03 1.74978128e-03 1.74978128e-03 2.62467192e-03
  1.31233596e-02 8.74890639e-04]
 [2.31213873e-02 8.90173410e-01 5.78034682e-03 5.78034682e-03
  3.46820809e-02 1.15606936e-02 5.78034682e-03 0.00000000e+00
  1.15606936e-02 0.00000000e+00 5.78034682e-03 0.00000000e+00
  5.78034682e-03 0.00000000e+00]
 [8.22222222e-02 0.00000000e+00 7.84444444e-01 6.66666667e-03
  2.22222222e-03 6.66666667e-03 4.88888889e-02 0.00000000e+00
  2.22222222e-03 2.22222222e-03 0.00000000e+00 2.22222222e-03
  5.55555556e-02 6.66666667e-03]
 [3.16455696e-02 6.32911392e-03 2.53164557e-02 7.84810127e-01
  0.00000000e+00 0.00000000e+00 8.22784810e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.26582278e-02 1.89873418e-02
  3.79746835e-02 0.00000000e+00]
 [0.00000000e+00 7.58620690e-02 1.37931034e-02 0.00000000e+00
  8.48275862e-01 6.20689655e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.24028269e-02 0.00000000e+00 1.76678445e-02 1.41342756e-02
  2.47349823e-02 8.30388693e-01 1.41342756e-02 0.00000000e+00
  2.47349823e-02 3.53356890e-03 0.00000000e+00 7.06713781e-03
  2.12014134e-02 0.00000000e+00]
 [1.56626506e-01 0.00000000e+00 4.81927711e-02 4.01606426e-03
  4.01606426e-03 4.41767068e-02 6.26506024e-01 8.03212851e-03
  0.00000000e+00 2.81124498e-02 2.81124498e-02 0.00000000e+00
  4.41767068e-02 8.03212851e-03]
 [1.88888889e-01 0.00000000e+00 1.11111111e-02 1.11111111e-02
  0.00000000e+00 1.11111111e-02 0.00000000e+00 7.33333333e-01
  3.33333333e-02 0.00000000e+00 0.00000000e+00 1.11111111e-02
  0.00000000e+00 0.00000000e+00]
 [1.05882353e-01 2.35294118e-02 2.35294118e-02 0.00000000e+00
  1.17647059e-02 7.05882353e-02 1.17647059e-02 0.00000000e+00
  7.05882353e-01 0.00000000e+00 0.00000000e+00 1.17647059e-02
  2.35294118e-02 1.17647059e-02]
 [2.38095238e-02 0.00000000e+00 2.38095238e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 8.33333333e-02 0.00000000e+00
  0.00000000e+00 8.45238095e-01 0.00000000e+00 0.00000000e+00
  1.19047619e-02 1.19047619e-02]
 [1.49253731e-02 1.49253731e-02 7.46268657e-03 1.49253731e-02
  0.00000000e+00 0.00000000e+00 1.41791045e-01 0.00000000e+00
  1.49253731e-02 0.00000000e+00 7.46268657e-01 7.46268657e-03
  3.73134328e-02 0.00000000e+00]
 [1.77419355e-01 1.61290323e-02 3.22580645e-02 3.22580645e-02
  0.00000000e+00 1.61290323e-02 4.83870968e-02 1.61290323e-02
  3.22580645e-02 0.00000000e+00 0.00000000e+00 5.96774194e-01
  3.22580645e-02 0.00000000e+00]
 [2.14285714e-01 1.78571429e-02 1.25000000e-01 3.57142857e-02
  0.00000000e+00 1.78571429e-02 1.96428571e-01 0.00000000e+00
  0.00000000e+00 1.78571429e-02 3.57142857e-02 1.78571429e-02
  3.21428571e-01 0.00000000e+00]
 [8.00000000e-02 0.00000000e+00 4.00000000e-02 0.00000000e+00
  0.00000000e+00 2.00000000e-02 2.40000000e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.00000000e-02 0.00000000e+00
  4.00000000e-02 5.60000000e-01]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8654    0.9039    0.8842      1145
           1     0.8354    0.7829    0.8083       175
           2     0.8442    0.8049    0.8241       451
           3     0.8732    0.7799    0.8239       159
           4     0.8467    0.7945    0.8198       146
           5     0.8856    0.8451    0.8649       284
           6     0.5480    0.6160    0.5800       250
           7     0.7955    0.7692    0.7821        91
           8     0.7241    0.7241    0.7241        87
           9     0.8675    0.8372    0.8521        86
          10     0.7600    0.6985    0.7280       136
          11     0.7358    0.6094    0.6667        64
          12     0.2022    0.3158    0.2466        57
          13     0.7297    0.5192    0.6067        52

    accuracy                         0.8021      3183
   macro avg     0.7510    0.7143    0.7294      3183
weighted avg     0.8101    0.8021    0.8048      3183

test confusion matrix:
[[9.03930131e-01 8.73362445e-04 1.57205240e-02 0.00000000e+00
  0.00000000e+00 2.62008734e-03 2.96943231e-02 1.13537118e-02
  1.04803493e-02 8.73362445e-04 6.98689956e-03 3.49344978e-03
  1.04803493e-02 3.49344978e-03]
 [3.42857143e-02 7.82857143e-01 1.71428571e-02 1.14285714e-02
  7.42857143e-02 3.42857143e-02 1.14285714e-02 0.00000000e+00
  1.71428571e-02 0.00000000e+00 0.00000000e+00 5.71428571e-03
  1.14285714e-02 0.00000000e+00]
 [7.09534368e-02 4.43458980e-03 8.04878049e-01 2.21729490e-03
  2.21729490e-03 0.00000000e+00 4.43458980e-02 0.00000000e+00
  0.00000000e+00 4.43458980e-03 0.00000000e+00 0.00000000e+00
  5.98669623e-02 6.65188470e-03]
 [1.25786164e-02 0.00000000e+00 2.51572327e-02 7.79874214e-01
  0.00000000e+00 1.88679245e-02 4.40251572e-02 6.28930818e-03
  0.00000000e+00 0.00000000e+00 4.40251572e-02 1.88679245e-02
  4.40251572e-02 6.28930818e-03]
 [0.00000000e+00 8.90410959e-02 6.84931507e-03 1.36986301e-02
  7.94520548e-01 7.53424658e-02 0.00000000e+00 0.00000000e+00
  2.05479452e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.92957746e-02 1.76056338e-02 1.76056338e-02 1.05633803e-02
  2.11267606e-02 8.45070423e-01 2.46478873e-02 0.00000000e+00
  3.52112676e-03 0.00000000e+00 7.04225352e-03 3.52112676e-03
  0.00000000e+00 0.00000000e+00]
 [1.40000000e-01 1.20000000e-02 5.60000000e-02 1.60000000e-02
  0.00000000e+00 1.20000000e-02 6.16000000e-01 1.20000000e-02
  0.00000000e+00 2.80000000e-02 4.00000000e-02 0.00000000e+00
  6.40000000e-02 4.00000000e-03]
 [1.53846154e-01 0.00000000e+00 2.19780220e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.29670330e-02 7.69230769e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.09890110e-02
  1.09890110e-02 0.00000000e+00]
 [1.72413793e-01 0.00000000e+00 2.29885057e-02 0.00000000e+00
  1.14942529e-02 1.14942529e-02 1.14942529e-02 0.00000000e+00
  7.24137931e-01 0.00000000e+00 1.14942529e-02 3.44827586e-02
  0.00000000e+00 0.00000000e+00]
 [3.48837209e-02 0.00000000e+00 1.16279070e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 8.13953488e-02 1.16279070e-02
  1.16279070e-02 8.37209302e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.16279070e-02]
 [6.61764706e-02 1.47058824e-02 2.20588235e-02 2.20588235e-02
  0.00000000e+00 1.47058824e-02 1.17647059e-01 0.00000000e+00
  1.47058824e-02 0.00000000e+00 6.98529412e-01 0.00000000e+00
  2.94117647e-02 0.00000000e+00]
 [2.03125000e-01 0.00000000e+00 1.56250000e-02 1.56250000e-02
  0.00000000e+00 3.12500000e-02 3.12500000e-02 0.00000000e+00
  3.12500000e-02 0.00000000e+00 3.12500000e-02 6.09375000e-01
  3.12500000e-02 0.00000000e+00]
 [2.28070175e-01 1.75438596e-02 2.28070175e-01 3.50877193e-02
  0.00000000e+00 0.00000000e+00 1.40350877e-01 0.00000000e+00
  0.00000000e+00 1.75438596e-02 0.00000000e+00 1.75438596e-02
  3.15789474e-01 0.00000000e+00]
 [9.61538462e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.84615385e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.19230769e-01]]
---------------------------------------
program finished.
