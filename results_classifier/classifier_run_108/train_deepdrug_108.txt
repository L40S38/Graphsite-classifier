seed:  8
save trained model at:  ../trained_models/trained_classifier_model_108.pt
save loss at:  ./results/train_classifier_results_108.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['5a07A00', '4k8aA00', '3qunA00', '6iy3K00', '6cxmB00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['4nahF00', '5fubA00', '5x1tA00', '2ii6A00', '4r7yA00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2b151c437610>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0146804045851843, acc: 0.39895821001539006; test loss: 1.7636121549495876, acc: 0.44646655636965255
epoch: 2, train loss: 1.713723431300225, acc: 0.476382147507991; test loss: 1.7499242570489835, acc: 0.4727014890096904
epoch: 3, train loss: 1.6103958487581245, acc: 0.5072214987569551; test loss: 1.5645245322378722, acc: 0.5235168990782321
epoch: 4, train loss: 1.5692581632538944, acc: 0.5253344382621049; test loss: 1.5324018608450298, acc: 0.5353344363034743
epoch: 5, train loss: 1.5091545810865428, acc: 0.5427370664141116; test loss: 1.6166365920891579, acc: 0.5135901678090286
epoch: 6, train loss: 1.447309542664751, acc: 0.5642239848466911; test loss: 1.4346403504229415, acc: 0.5677144883006382
epoch: 7, train loss: 1.4425539585687965, acc: 0.5638096365573576; test loss: 1.4723938014089737, acc: 0.5447884660836682
epoch: 8, train loss: 1.4116510288862416, acc: 0.5726293358588848; test loss: 1.4529514549415856, acc: 0.5644055778775703
epoch: 9, train loss: 1.3915849986606426, acc: 0.5799100272286019; test loss: 1.41045196956282, acc: 0.5651146301110849
epoch: 10, train loss: 1.3640569441132508, acc: 0.587131525985557; test loss: 1.3712737460531994, acc: 0.5757504136138029
epoch: 11, train loss: 1.33482267422776, acc: 0.5962471883508939; test loss: 1.3689781863253834, acc: 0.5769321673363271
epoch: 12, train loss: 1.3430938856466517, acc: 0.5924588611341305; test loss: 1.3898667014538326, acc: 0.581659182226424
epoch: 13, train loss: 1.3142318947859681, acc: 0.603468687107849; test loss: 1.365206348053588, acc: 0.5797683762703852
epoch: 14, train loss: 1.2929143466878334, acc: 0.6054220433289925; test loss: 1.320024580348048, acc: 0.5913495627511227
epoch: 15, train loss: 1.282674666536028, acc: 0.610749378477566; test loss: 1.2991507824248836, acc: 0.6041125029543843
epoch: 16, train loss: 1.266826709967697, acc: 0.6189179590387119; test loss: 1.3370247258556225, acc: 0.5941857716851808
epoch: 17, train loss: 1.2653503324543214, acc: 0.6163726766899491; test loss: 1.3464958890902745, acc: 0.590876861262113
epoch: 18, train loss: 1.2350971165586255, acc: 0.6263170356339529; test loss: 1.3018478055677638, acc: 0.601512644764831
epoch: 19, train loss: 1.247136033214986, acc: 0.6238901385107138; test loss: 1.2668719908160482, acc: 0.6112030252895296
epoch: 20, train loss: 1.2184982659253083, acc: 0.6323546821356695; test loss: 1.3012447319986464, acc: 0.5982037343417632
epoch: 21, train loss: 1.2093905352467937, acc: 0.6317627560080502; test loss: 1.2802768038734142, acc: 0.6012762940203261
epoch: 22, train loss: 1.2032318965237394, acc: 0.6362022019651947; test loss: 1.2682814899558184, acc: 0.6097849208225006
epoch: 23, train loss: 1.1968874538722398, acc: 0.6348999644844323; test loss: 1.2856680723094287, acc: 0.598440085086268
epoch: 24, train loss: 1.1733851250183007, acc: 0.6469752574878656; test loss: 1.2481750040520903, acc: 0.621366107303238
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9472133587969539, acc: 0.6427133893690068; test loss: 1.0085423996307077, acc: 0.6204207043252187
epoch: 26, train loss: 0.943747855854328, acc: 0.6466201018112939; test loss: 1.039456916289306, acc: 0.6086031670999764
epoch: 27, train loss: 0.9325135258707025, acc: 0.6481591097431041; test loss: 0.9954467730949292, acc: 0.6190025998581895
epoch: 28, train loss: 0.9290902562870759, acc: 0.6510003551556766; test loss: 1.0671944736560957, acc: 0.6003308910423067
epoch: 29, train loss: 0.9211035997779804, acc: 0.6514147034450101; test loss: 1.0030664452647688, acc: 0.6227842117702671
epoch: 30, train loss: 0.9050488940100423, acc: 0.6581034686871079; test loss: 1.028455722588395, acc: 0.6097849208225006
epoch: 31, train loss: 0.903039935249908, acc: 0.6583994317509175; test loss: 0.9877320241488567, acc: 0.6265658236823446
epoch: 32, train loss: 0.8954692936245614, acc: 0.6553806085000592; test loss: 1.0554324161693633, acc: 0.6041125029543843
epoch: 33, train loss: 0.8905741598417847, acc: 0.6590505504912987; test loss: 0.9712816588113956, acc: 0.624438666981801
epoch: 34, train loss: 0.8775819334261824, acc: 0.6649698117674914; test loss: 0.9502516140528702, acc: 0.6357835027180335
epoch: 35, train loss: 0.8700925098222516, acc: 0.6626021072570143; test loss: 0.9701833533384197, acc: 0.6265658236823446
epoch: 36, train loss: 0.8689676273979773, acc: 0.6673375162779686; test loss: 1.0301268808499657, acc: 0.6256204207043252
epoch: 37, train loss: 0.8554617862271263, acc: 0.6755060968391144; test loss: 1.0345172799808482, acc: 0.613802883479083
epoch: 38, train loss: 0.8555787780361936, acc: 0.6689357168225405; test loss: 1.0040105245153381, acc: 0.6284566296383833
epoch: 39, train loss: 0.8368696784484848, acc: 0.6753877116135906; test loss: 0.8967264764727613, acc: 0.6546915622784212
epoch: 40, train loss: 0.8258277454488557, acc: 0.6812477802770214; test loss: 0.9426208198338191, acc: 0.6518553533443631
epoch: 41, train loss: 0.8285698782936192, acc: 0.6797679649579732; test loss: 1.1325939989236478, acc: 0.5880406523280548
epoch: 42, train loss: 0.8320498096317712, acc: 0.6753285190008287; test loss: 0.8924792946666951, acc: 0.6532734578113921
epoch: 43, train loss: 0.8144261136031848, acc: 0.6861015745234995; test loss: 0.9124862643962942, acc: 0.6568187189789648
epoch: 44, train loss: 0.8062726521726273, acc: 0.6863975375873091; test loss: 1.0207897495981142, acc: 0.6242023162372962
epoch: 45, train loss: 0.8259516699830357, acc: 0.681543743340831; test loss: 0.9834681159073341, acc: 0.6204207043252187
epoch: 46, train loss: 0.8526125950433382, acc: 0.6679294424055878; test loss: 1.1121425783625734, acc: 0.5932403687071615
epoch: 47, train loss: 0.8033038258227961, acc: 0.6844441813661655; test loss: 1.0190180739283534, acc: 0.6187662491136847
epoch: 48, train loss: 0.7953890524800182, acc: 0.6885876642595005; test loss: 0.9510756824293252, acc: 0.6376743086740724
epoch: 49, train loss: 0.786990249793431, acc: 0.6936190363442643; test loss: 0.9174779839223791, acc: 0.6428740250531789
epoch: 50, train loss: 0.7891130432851698, acc: 0.6885876642595005; test loss: 1.2117177089374698, acc: 0.5665327345781139
epoch: 51, train loss: 0.784407747381747, acc: 0.6894755534509294; test loss: 1.0644026141841765, acc: 0.6062396596549279
epoch: 52, train loss: 0.7791948751103769, acc: 0.6971705931099799; test loss: 1.0348704083820686, acc: 0.6130938312455684
epoch: 53, train loss: 0.7725077697450881, acc: 0.6979400970758849; test loss: 1.074222957177862, acc: 0.6097849208225006
epoch: 54, train loss: 0.7654193345718839, acc: 0.6981176749141708; test loss: 0.9711390272490777, acc: 0.6338926967619948
Epoch    54: reducing learning rate of group 0 to 1.5000e-03.
epoch: 55, train loss: 0.7056154342633078, acc: 0.7164081922576062; test loss: 0.8036470684253474, acc: 0.6984164500118175
epoch: 56, train loss: 0.6735734119843743, acc: 0.7286610630993252; test loss: 0.8161801164861944, acc: 0.6870716142755849
epoch: 57, train loss: 0.6666404133550342, acc: 0.7304960340949449; test loss: 0.788144887409377, acc: 0.6967619948002837
epoch: 58, train loss: 0.6613910391413419, acc: 0.7333964721202794; test loss: 0.7982541977218126, acc: 0.6913259276766722
epoch: 59, train loss: 0.6502553172488036, acc: 0.7383094589795194; test loss: 0.802436379894977, acc: 0.7000709052233515
epoch: 60, train loss: 0.650996822654333, acc: 0.7387829998816148; test loss: 0.8772366816088596, acc: 0.6759631292838573
epoch: 61, train loss: 0.6468849907517503, acc: 0.7372439919498046; test loss: 0.7741273828387909, acc: 0.703852517135429
epoch: 62, train loss: 0.6404812942175551, acc: 0.7409139339410442; test loss: 0.8048295980366487, acc: 0.6927440321437013
epoch: 63, train loss: 0.6442525671602947, acc: 0.7397300816858056; test loss: 0.821478203379271, acc: 0.6955802410777594
epoch: 64, train loss: 0.6279767374816245, acc: 0.7445838759322837; test loss: 0.8037880799697663, acc: 0.6953438903332545
epoch: 65, train loss: 0.6182847104228374, acc: 0.7481946253107612; test loss: 0.7821105988812881, acc: 0.7017253604348853
epoch: 66, train loss: 0.6204351447482949, acc: 0.7477210844086658; test loss: 0.783657683798732, acc: 0.6998345544788466
epoch: 67, train loss: 0.6178834637664713, acc: 0.7468331952172369; test loss: 0.7896581466416044, acc: 0.6972346962892934
epoch: 68, train loss: 0.6164518631011251, acc: 0.7480762400852373; test loss: 0.8041176752569658, acc: 0.7010163082013708
epoch: 69, train loss: 0.6144638865660447, acc: 0.7493192849532379; test loss: 0.7822501511191736, acc: 0.7057433230914677
epoch: 70, train loss: 0.6059638805025016, acc: 0.7530484195572392; test loss: 0.7959753914212989, acc: 0.6943984873552351
epoch: 71, train loss: 0.5973466887083987, acc: 0.7554753166804783; test loss: 0.8118959120671763, acc: 0.7000709052233515
epoch: 72, train loss: 0.5927525884028058, acc: 0.7597963774120989; test loss: 0.823818155170135, acc: 0.6910895769321673
epoch: 73, train loss: 0.5867737003497235, acc: 0.7625784302119095; test loss: 0.8325004341528507, acc: 0.6849444575750414
epoch: 74, train loss: 0.5728009839205822, acc: 0.762164081922576; test loss: 0.8123841575910171, acc: 0.6988891515008272
epoch: 75, train loss: 0.5780357267498984, acc: 0.765478868237244; test loss: 0.7876950874026575, acc: 0.7019617111793902
epoch: 76, train loss: 0.5761901517090493, acc: 0.7619865040842903; test loss: 0.798553230463261, acc: 0.7109430394705744
epoch: 77, train loss: 0.5689754528915781, acc: 0.7712205516751509; test loss: 0.7778096125780113, acc: 0.7085795320255259
epoch: 78, train loss: 0.5665329746823488, acc: 0.7660707943648633; test loss: 0.7960908281636897, acc: 0.7102339872370598
epoch: 79, train loss: 0.5714224545902312, acc: 0.7660116017521014; test loss: 0.8459922470006219, acc: 0.7003072559678563
epoch: 80, train loss: 0.5603207881117312, acc: 0.7708653959985794; test loss: 0.7518815492744058, acc: 0.718978964783739
epoch: 81, train loss: 0.5714896518520537, acc: 0.7674914170711495; test loss: 0.7726752137608124, acc: 0.7123611439376034
epoch: 82, train loss: 0.5729565624091644, acc: 0.7656564460755297; test loss: 1.0065905821292402, acc: 0.6317655400614512
epoch: 83, train loss: 0.5513075242222336, acc: 0.7695039659050551; test loss: 0.8125127424615631, acc: 0.7130701961711179
epoch: 84, train loss: 0.5408574304905843, acc: 0.7754232271812478; test loss: 0.8531742097074274, acc: 0.7050342708579532
epoch: 85, train loss: 0.5401936314812184, acc: 0.7790931691724873; test loss: 0.786986998310328, acc: 0.7123611439376034
epoch: 86, train loss: 0.5429101966465241, acc: 0.7731147152835326; test loss: 0.8243479319031259, acc: 0.7014890096903805
epoch: 87, train loss: 0.5348967536181349, acc: 0.7753048419557239; test loss: 0.7664101931874675, acc: 0.7111793902150791
epoch: 88, train loss: 0.5356058722644836, acc: 0.7786196282703919; test loss: 0.7929825497642811, acc: 0.7064523753249823
epoch: 89, train loss: 0.519688224854576, acc: 0.7808097549425832; test loss: 0.8044614239072608, acc: 0.7177972110612149
epoch: 90, train loss: 0.5281055882384371, acc: 0.7796259026873447; test loss: 0.780950850669556, acc: 0.7144883006381471
epoch: 91, train loss: 0.5143482189947887, acc: 0.7845980821593466; test loss: 0.7742880729602263, acc: 0.7133065469156228
epoch: 92, train loss: 0.5254828255882458, acc: 0.7828814963892506; test loss: 0.7972811527270229, acc: 0.7199243677617585
epoch: 93, train loss: 0.5128932083035498, acc: 0.7852492008997277; test loss: 0.9043849553094917, acc: 0.6887260694871189
epoch: 94, train loss: 0.5000640074854223, acc: 0.7901621877589676; test loss: 0.8861244568388615, acc: 0.696289293311274
epoch: 95, train loss: 0.52039156227376, acc: 0.782526340712679; test loss: 0.7566773150441101, acc: 0.7203970692507682
epoch: 96, train loss: 0.5081230926911647, acc: 0.7869657866698236; test loss: 0.8203073765081424, acc: 0.7177972110612149
epoch: 97, train loss: 0.49525929854948364, acc: 0.7936545519119214; test loss: 0.8265131673923313, acc: 0.7118884424485937
epoch: 98, train loss: 0.5040127945620873, acc: 0.7885639872143957; test loss: 0.7972079018728301, acc: 0.7116520917040888
epoch: 99, train loss: 0.49453472688210226, acc: 0.7927074701077306; test loss: 0.9488575003778813, acc: 0.676199480028362
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.38222505850976524, acc: 0.7979164200307801; test loss: 0.7027239745966624, acc: 0.7047979201134483
epoch: 101, train loss: 0.37837474295637713, acc: 0.7937729371374452; test loss: 0.6422187801343954, acc: 0.7201607185062633
epoch: 102, train loss: 0.38480108610141683, acc: 0.7908133064993489; test loss: 0.6958634625707386, acc: 0.7047979201134483
epoch: 103, train loss: 0.3799090003408426, acc: 0.7950751746182076; test loss: 0.675586172999628, acc: 0.7255967856298747
epoch: 104, train loss: 0.38337090233239646, acc: 0.7967917603883036; test loss: 0.6238592343250365, acc: 0.7305601512644765
epoch: 105, train loss: 0.3743248390201649, acc: 0.7981531904818279; test loss: 0.6161840153033501, acc: 0.7322146064760104
epoch: 106, train loss: 0.3820950833244184, acc: 0.7930034331715402; test loss: 0.7361446936568367, acc: 0.6787993382179154
epoch: 107, train loss: 0.4042320620792073, acc: 0.7853083935124896; test loss: 0.678130911586133, acc: 0.7107066887260695
epoch: 108, train loss: 0.3793438768350148, acc: 0.7948384041671599; test loss: 0.7038190227342649, acc: 0.7019617111793902
epoch: 109, train loss: 0.38592486998887265, acc: 0.78992541730792; test loss: 0.6672083839573384, acc: 0.7097612857480501
epoch: 110, train loss: 0.3592440580473548, acc: 0.8008168580561146; test loss: 0.7909913534702236, acc: 0.6780902859844008
epoch: 111, train loss: 0.3700977150146617, acc: 0.7946608263288741; test loss: 0.6473017319864315, acc: 0.7248877333963601
epoch: 112, train loss: 0.36444760782980873, acc: 0.7938913223629691; test loss: 0.6669607686534387, acc: 0.7156700543606712
epoch: 113, train loss: 0.3573072269158461, acc: 0.7999881614774477; test loss: 0.6669628844422332, acc: 0.7213424722287876
epoch: 114, train loss: 0.3683668598347449, acc: 0.7986859239966853; test loss: 0.6959628354457782, acc: 0.7121247931930985
epoch: 115, train loss: 0.3608543496767247, acc: 0.8006392802178288; test loss: 0.6762801970299522, acc: 0.7265421886078941
epoch: 116, train loss: 0.3602877267404493, acc: 0.802119095536877; test loss: 0.6751740903500489, acc: 0.7133065469156228
Epoch   116: reducing learning rate of group 0 to 7.5000e-04.
epoch: 117, train loss: 0.3157195832809217, acc: 0.8198768793654552; test loss: 0.6220271942942905, acc: 0.7426140392342235
epoch: 118, train loss: 0.2830642452779939, acc: 0.8352669586835563; test loss: 0.6374152470807098, acc: 0.7452138974237769
epoch: 119, train loss: 0.2878868696822829, acc: 0.8320113649816503; test loss: 0.6461365625836331, acc: 0.7419049870007091
epoch: 120, train loss: 0.27892896887591934, acc: 0.833076832011365; test loss: 0.640690179447056, acc: 0.7449775466792721
epoch: 121, train loss: 0.2776327115043996, acc: 0.8385817449982242; test loss: 0.6693586098953388, acc: 0.7411959347671945
epoch: 122, train loss: 0.27664273578951004, acc: 0.835503729134604; test loss: 0.6848645239568326, acc: 0.7419049870007091
epoch: 123, train loss: 0.2716741081460829, acc: 0.8392920563513674; test loss: 0.7094246467161167, acc: 0.7255967856298747
epoch: 124, train loss: 0.28293128100826825, acc: 0.8321297502071742; test loss: 0.6839139704387596, acc: 0.7310328527534862
epoch: 125, train loss: 0.26633746563732036, acc: 0.8414821830235587; test loss: 0.6692280191756456, acc: 0.7395414795556606
epoch: 126, train loss: 0.2629581484319586, acc: 0.8410678347342252; test loss: 0.7001683034110199, acc: 0.7416686362562042
epoch: 127, train loss: 0.2631747261248948, acc: 0.8422516869894637; test loss: 0.6758349556585938, acc: 0.7279602930749232
epoch: 128, train loss: 0.27099824107312814, acc: 0.834734225168699; test loss: 0.69390493125618, acc: 0.7355235168990782
epoch: 129, train loss: 0.26509057316848395, acc: 0.8363324257132709; test loss: 0.6884283707852454, acc: 0.7348144646655637
epoch: 130, train loss: 0.26381700413176556, acc: 0.8410086421214632; test loss: 0.6781413698838806, acc: 0.7371779721106122
epoch: 131, train loss: 0.2692638277351271, acc: 0.833136024624127; test loss: 0.6907470111333008, acc: 0.7331600094540298
epoch: 132, train loss: 0.2624803906670686, acc: 0.8401207529300343; test loss: 0.7047052403708999, acc: 0.7369416213661073
epoch: 133, train loss: 0.2633104647978447, acc: 0.8429028057298449; test loss: 0.6790315166654611, acc: 0.7430867407232333
epoch: 134, train loss: 0.2645382718904718, acc: 0.8351485734580324; test loss: 0.7059893195044766, acc: 0.7300874497754668
epoch: 135, train loss: 0.24988706131736188, acc: 0.846868710784894; test loss: 0.7104548301236958, acc: 0.7369416213661073
epoch: 136, train loss: 0.2692707422708384, acc: 0.8410678347342252; test loss: 0.6878466723348987, acc: 0.7336327109430395
epoch: 137, train loss: 0.24927039054232658, acc: 0.8475790221380372; test loss: 0.6838564636069983, acc: 0.7350508154100686
epoch: 138, train loss: 0.25200801643589144, acc: 0.8450929324020362; test loss: 0.6760562740863285, acc: 0.7378870243441267
epoch: 139, train loss: 0.23402553371272736, acc: 0.8492364152953711; test loss: 0.7233735873214011, acc: 0.7341054124320492
epoch: 140, train loss: 0.23560600318795227, acc: 0.8530247425121344; test loss: 0.7059722615595659, acc: 0.7341054124320492
epoch: 141, train loss: 0.24499642550839437, acc: 0.8490588374570853; test loss: 0.7237222768882295, acc: 0.7322146064760104
epoch: 142, train loss: 0.257535409614068, acc: 0.8437315023085119; test loss: 0.6966687360009356, acc: 0.7345781139210589
epoch: 143, train loss: 0.25686237348654406, acc: 0.842014916538416; test loss: 0.7346843721398336, acc: 0.7272512408414087
epoch: 144, train loss: 0.2393162473745144, acc: 0.8478749852018468; test loss: 0.7061967916224927, acc: 0.735759867643583
epoch: 145, train loss: 0.23781741139090554, acc: 0.8512489641292766; test loss: 0.7000254685364274, acc: 0.7423776884897187
epoch: 146, train loss: 0.23887929504505062, acc: 0.8489404522315614; test loss: 0.7272421927126382, acc: 0.7324509572205152
epoch: 147, train loss: 0.24073988859351744, acc: 0.8491180300698473; test loss: 0.7183144605348534, acc: 0.7397778303001654
epoch: 148, train loss: 0.23294001058973587, acc: 0.8508346158399431; test loss: 0.7519319377534921, acc: 0.7364689198770976
epoch: 149, train loss: 0.2328374985302498, acc: 0.8536166686397537; test loss: 0.7084662285405562, acc: 0.7333963601985346
epoch: 150, train loss: 0.2281217037417754, acc: 0.8548597135077542; test loss: 0.7382271198537778, acc: 0.7317419049870008
epoch: 151, train loss: 0.2358994438487481, acc: 0.8504202675506097; test loss: 0.7640205620874104, acc: 0.7270148900969038
epoch: 152, train loss: 0.2306205320318761, acc: 0.8514857345803244; test loss: 0.7127342743671862, acc: 0.7336327109430395
epoch: 153, train loss: 0.2368140469006537, acc: 0.8512489641292766; test loss: 0.7278948589240201, acc: 0.7341054124320492
epoch: 154, train loss: 0.22158968781520308, acc: 0.8560435657629928; test loss: 0.7375488794038719, acc: 0.7326873079650201
epoch: 155, train loss: 0.22660334884407704, acc: 0.8543861726056589; test loss: 0.7503741996785873, acc: 0.7303238005199716
epoch: 156, train loss: 0.2298496228160838, acc: 0.8547413282822304; test loss: 0.7168606572386729, acc: 0.7341054124320492
epoch: 157, train loss: 0.22385961863082077, acc: 0.8551556765715639; test loss: 0.7354530005048168, acc: 0.7317419049870008
epoch: 158, train loss: 0.2094213434196131, acc: 0.8639161832603292; test loss: 0.7385847137431336, acc: 0.7407232332781848
epoch: 159, train loss: 0.21331683886330607, acc: 0.861548478749852; test loss: 0.743534733628697, acc: 0.7367052706216024
epoch: 160, train loss: 0.21919621782633375, acc: 0.8555108322481354; test loss: 0.7327225117558313, acc: 0.7371779721106122
epoch: 161, train loss: 0.21372246934180147, acc: 0.8604238191073754; test loss: 0.7381366054883833, acc: 0.7315055542424959
epoch: 162, train loss: 0.21885437526208373, acc: 0.8572274180182313; test loss: 0.7276688013142149, acc: 0.7333963601985346
epoch: 163, train loss: 0.2154079465078887, acc: 0.8595359299159465; test loss: 0.7396443230523705, acc: 0.7279602930749232
epoch: 164, train loss: 0.21090913599689318, acc: 0.8650408429028057; test loss: 0.7590827490527757, acc: 0.7362325691325927
epoch: 165, train loss: 0.21904572643400494, acc: 0.8603054338818515; test loss: 0.7478067539623515, acc: 0.7371779721106122
epoch: 166, train loss: 0.21582083633041416, acc: 0.8578193441458506; test loss: 0.7675554754603869, acc: 0.7192153155282439
epoch: 167, train loss: 0.2079945545486124, acc: 0.8620812122647094; test loss: 0.7548095252434431, acc: 0.7364689198770976
Epoch   167: reducing learning rate of group 0 to 3.7500e-04.
epoch: 168, train loss: 0.18023554781414114, acc: 0.8783591807742394; test loss: 0.7329792612355754, acc: 0.7468683526353108
epoch: 169, train loss: 0.16042721479676017, acc: 0.8902568959393867; test loss: 0.7747257550933124, acc: 0.7411959347671945
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.11855815846362283, acc: 0.890079318101101; test loss: 0.6990051589038068, acc: 0.7430867407232333
epoch: 171, train loss: 0.11263951386035201, acc: 0.8920326743222445; test loss: 0.697343882644354, acc: 0.7445048451902624
epoch: 172, train loss: 0.11327140622623322, acc: 0.8944003788327217; test loss: 0.6909646080673459, acc: 0.7468683526353108
epoch: 173, train loss: 0.11455411465122557, acc: 0.8906120516159584; test loss: 0.6992624530666012, acc: 0.7475774048688253
epoch: 174, train loss: 0.11252066050250953, acc: 0.8891914289096721; test loss: 0.6915116871933203, acc: 0.743559442212243
epoch: 175, train loss: 0.10930598533455543, acc: 0.8903752811649106; test loss: 0.6873388003018189, acc: 0.7478137556133302
epoch: 176, train loss: 0.10782641687519638, acc: 0.8920918669350065; test loss: 0.6995173129480913, acc: 0.7447411959347672
epoch: 177, train loss: 0.10409405099663972, acc: 0.8942819936071978; test loss: 0.7044585728245237, acc: 0.743559442212243
epoch: 178, train loss: 0.10881372600550396, acc: 0.8939268379306262; test loss: 0.7161646529986373, acc: 0.7419049870007091
epoch: 179, train loss: 0.10725253173443951, acc: 0.8961761572155795; test loss: 0.7152346293432046, acc: 0.7383597258331364
epoch: 180, train loss: 0.11088255233731907, acc: 0.8919142890967208; test loss: 0.724388629881323, acc: 0.7463956511463011
epoch: 181, train loss: 0.113600959226699, acc: 0.890138510713863; test loss: 0.6929843569444275, acc: 0.7397778303001654
epoch: 182, train loss: 0.11027946037728645, acc: 0.8914999408073873; test loss: 0.6850422971554609, acc: 0.7416686362562042
epoch: 183, train loss: 0.10549142138819251, acc: 0.8932165265774832; test loss: 0.6888084347381628, acc: 0.738832427322146
epoch: 184, train loss: 0.10954041250611926, acc: 0.890908014679768; test loss: 0.7173199911146856, acc: 0.7364689198770976
epoch: 185, train loss: 0.10100373543902122, acc: 0.8955250384751983; test loss: 0.7257184055335582, acc: 0.7390687780666509
epoch: 186, train loss: 0.10445593347404926, acc: 0.8944003788327217; test loss: 0.7120815564824796, acc: 0.7419049870007091
epoch: 187, train loss: 0.1033968390510804, acc: 0.8942228009944359; test loss: 0.7368489810124945, acc: 0.7419049870007091
epoch: 188, train loss: 0.10545166164400226, acc: 0.8994909435302474; test loss: 0.7213757382417452, acc: 0.7426140392342235
epoch: 189, train loss: 0.10344354996998606, acc: 0.8961169646028175; test loss: 0.7102598169683367, acc: 0.7376506735996219
epoch: 190, train loss: 0.10238092089702748, acc: 0.8945779566710075; test loss: 0.7181854169156303, acc: 0.74048688253368
epoch: 191, train loss: 0.10923824301517629, acc: 0.890848822067006; test loss: 0.7075957149175627, acc: 0.7411959347671945
epoch: 192, train loss: 0.10555478606500526, acc: 0.8936308748668166; test loss: 0.7128644919908236, acc: 0.7359962183880879
epoch: 193, train loss: 0.10993900233413308, acc: 0.8923286373860542; test loss: 0.7133557373286702, acc: 0.7381233750886316
epoch: 194, train loss: 0.10581355407610016, acc: 0.8961169646028175; test loss: 0.704135954112371, acc: 0.7433230914677381
epoch: 195, train loss: 0.0988915263495107, acc: 0.8974783946963419; test loss: 0.7142532805521926, acc: 0.7376506735996219
epoch: 196, train loss: 0.09977924095902681, acc: 0.8959393867645318; test loss: 0.7302323253237589, acc: 0.738832427322146
epoch: 197, train loss: 0.09795428044777361, acc: 0.8973600094708181; test loss: 0.7463493887004884, acc: 0.7397778303001654
epoch: 198, train loss: 0.10287049655464396, acc: 0.8959393867645318; test loss: 0.7332389551368736, acc: 0.7329236587095249
epoch: 199, train loss: 0.11494075719408697, acc: 0.8838640937610986; test loss: 0.7147879965112046, acc: 0.7378870243441267
epoch: 200, train loss: 0.1072819376986408, acc: 0.8928613709009116; test loss: 0.7118565182515674, acc: 0.7423776884897187
best test acc 0.7478137556133302 at epoch 175.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9541    0.9777    0.9658      6100
           1     0.9854    0.8747    0.9268       926
           2     0.8772    0.9762    0.9241      2400
           3     0.9411    0.9288    0.9349       843
           4     0.8711    0.9780    0.9215       774
           5     0.9351    0.9716    0.9530      1512
           6     0.8814    0.8158    0.8473      1330
           7     0.9339    0.8524    0.8913       481
           8     0.8690    0.9563    0.9106       458
           9     0.9610    0.9801    0.9704       452
          10     0.9708    0.8815    0.9240       717
          11     0.9408    0.9069    0.9235       333
          12     0.8261    0.0635    0.1180       299
          13     0.8871    0.8178    0.8511       269

    accuracy                         0.9278     16894
   macro avg     0.9167    0.8558    0.8616     16894
weighted avg     0.9274    0.9278    0.9207     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8177    0.8590    0.8379      1525
           1     0.8010    0.7112    0.7534       232
           2     0.6870    0.7854    0.7329       601
           3     0.8000    0.7393    0.7685       211
           4     0.7557    0.8608    0.8048       194
           5     0.7626    0.7989    0.7804       378
           6     0.5318    0.5015    0.5162       333
           7     0.7033    0.5289    0.6038       121
           8     0.5966    0.6174    0.6068       115
           9     0.7925    0.7368    0.7636       114
          10     0.7812    0.6944    0.7353       180
          11     0.6000    0.5714    0.5854        84
          12     0.3333    0.0267    0.0494        75
          13     0.6458    0.4559    0.5345        68

    accuracy                         0.7478      4231
   macro avg     0.6863    0.6348    0.6481      4231
weighted avg     0.7399    0.7478    0.7397      4231

---------------------------------------
program finished.
