seed:  16
save trained model at:  ../trained_models/trained_classifier_model_116.pt
save loss at:  ./results/train_classifier_results_116.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['5hr5A02', '5n2sA00', '1cjaB00', '4fxsA00', '3ngtE00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['4bnfF02', '1ztfA00', '2zroA00', '4at8C00', '3kycB00']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2ad8c5ec5610>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 2.0608421644685224, acc: 0.384396827275956; test loss: 1.7597312470807784, acc: 0.4493027653037107
epoch: 2, train loss: 1.7268267143431735, acc: 0.47584941399313363; test loss: 1.6128890276011552, acc: 0.494682108248641
epoch: 3, train loss: 1.6459092138447788, acc: 0.5006511187403812; test loss: 1.5786837370675721, acc: 0.5098085558969511
epoch: 4, train loss: 1.576696714833317, acc: 0.5220196519474369; test loss: 1.48296459080113, acc: 0.5443157645946585
epoch: 5, train loss: 1.5376468023577003, acc: 0.5295371137682018; test loss: 1.5791856938292022, acc: 0.5105176081304656
epoch: 6, train loss: 1.5006878788518923, acc: 0.548005208949923; test loss: 1.4423463562086551, acc: 0.5601512644764831
epoch: 7, train loss: 1.4896769977623547, acc: 0.5459334675032556; test loss: 1.4358997418789738, acc: 0.567950839045143
epoch: 8, train loss: 1.4352840175568824, acc: 0.5652894518764058; test loss: 1.4500190798651944, acc: 0.5620420704325219
epoch: 9, train loss: 1.4194704755828467, acc: 0.5653486444891678; test loss: 1.46297878504532, acc: 0.5488064287402505
epoch: 10, train loss: 1.395350188027476, acc: 0.5758849295607908; test loss: 1.3601545879559662, acc: 0.5797683762703852
epoch: 11, train loss: 1.3815742646842026, acc: 0.57985083461584; test loss: 1.4332452709582983, acc: 0.5606239659654928
epoch: 12, train loss: 1.3649824287562, acc: 0.5811530720966024; test loss: 1.3920711448811887, acc: 0.5724415031907351
epoch: 13, train loss: 1.3509651607517041, acc: 0.5867171776962236; test loss: 1.3653804012799877, acc: 0.5726778539352398
epoch: 14, train loss: 1.325891744290299, acc: 0.5940570616787025; test loss: 1.358922000586127, acc: 0.578586622547861
epoch: 15, train loss: 1.3225483374298148, acc: 0.6006274416952764; test loss: 1.318681524087847, acc: 0.5951311746632002
epoch: 16, train loss: 1.2899175746011582, acc: 0.605066887652421; test loss: 1.385397489232618, acc: 0.5606239659654928
epoch: 17, train loss: 1.2749848755291966, acc: 0.6131170829880431; test loss: 1.2606610715938669, acc: 0.6038761522098794
epoch: 18, train loss: 1.2641426470063206, acc: 0.6127619273114715; test loss: 1.3518036183835316, acc: 0.5901678090285984
epoch: 19, train loss: 1.2569160533823516, acc: 0.6169646028175684; test loss: 1.2529699515011596, acc: 0.6048215551878988
epoch: 20, train loss: 1.2264370644347997, acc: 0.6243044868000474; test loss: 1.2575046557451472, acc: 0.6067123611439376
epoch: 21, train loss: 1.2340606980257744, acc: 0.6219367822895703; test loss: 1.260047663416149, acc: 0.6048215551878988
epoch: 22, train loss: 1.2055088297014225, acc: 0.6282111992423346; test loss: 1.356486408796696, acc: 0.5814228314819192
epoch: 23, train loss: 1.2087549772208381, acc: 0.6322362969101456; test loss: 1.3395922098337858, acc: 0.5901678090285984
epoch: 24, train loss: 1.216635082930069, acc: 0.6313484077187167; test loss: 1.2336067358840586, acc: 0.6185298983691798
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9541407511956003, acc: 0.642476618917959; test loss: 0.993095080301965, acc: 0.624438666981801
epoch: 26, train loss: 0.9351714808968831, acc: 0.6476263762282467; test loss: 1.0743369119608552, acc: 0.600094540297802
epoch: 27, train loss: 0.9364924459213532, acc: 0.6453178643305315; test loss: 0.9720423923150723, acc: 0.6268021744268495
epoch: 28, train loss: 0.920050178926711, acc: 0.650645199479105; test loss: 0.9201883688741642, acc: 0.6466556369652564
epoch: 29, train loss: 0.9273540725634057, acc: 0.6492837693855806; test loss: 0.9701323709260259, acc: 0.6201843535807138
epoch: 30, train loss: 0.9128357520946825, acc: 0.6517106665088197; test loss: 1.021996350413489, acc: 0.601512644764831
epoch: 31, train loss: 0.9059840163154914, acc: 0.6537232153427253; test loss: 0.9263391486298416, acc: 0.6341290475064997
epoch: 32, train loss: 0.8899869553588672, acc: 0.6691132946608264; test loss: 0.930613566222582, acc: 0.6364925549515481
epoch: 33, train loss: 0.8808303191292931, acc: 0.6665680123120634; test loss: 0.9573323128212713, acc: 0.6346017489955094
epoch: 34, train loss: 0.8755058288658895, acc: 0.6686989463714929; test loss: 0.9437622724766146, acc: 0.627511226660364
epoch: 35, train loss: 0.8720959831345387, acc: 0.6641411151888245; test loss: 1.1271262245520832, acc: 0.5840226896714724
epoch: 36, train loss: 0.855355482475707, acc: 0.6724280809754942; test loss: 0.9074782668713052, acc: 0.6490191444103048
epoch: 37, train loss: 0.8460367080783426, acc: 0.6737895110690186; test loss: 0.9724049668479997, acc: 0.621366107303238
epoch: 38, train loss: 0.8436430356580716, acc: 0.6744998224221617; test loss: 1.4438324521208339, acc: 0.5031907350508155
epoch: 39, train loss: 0.8302394257673388, acc: 0.6779329939623535; test loss: 0.9678354604789141, acc: 0.6223115102812574
epoch: 40, train loss: 0.8223526718151616, acc: 0.6793536166686398; test loss: 0.9085161995870851, acc: 0.6450011817537226
epoch: 41, train loss: 0.8205958378163967, acc: 0.6831419438854031; test loss: 0.9170748733848693, acc: 0.645710233987237
epoch: 42, train loss: 0.8085479396637395, acc: 0.6882917011956908; test loss: 0.9910699213348475, acc: 0.6178208461356653
epoch: 43, train loss: 0.8301562867009937, acc: 0.6821356694684504; test loss: 1.0045576750372804, acc: 0.6237296147482865
epoch: 44, train loss: 0.792109715355717, acc: 0.6924943767017876; test loss: 1.0691094629535212, acc: 0.5996218388087923
epoch: 45, train loss: 0.809568355369105, acc: 0.6877589676808334; test loss: 0.8041273840379275, acc: 0.6816355471519735
epoch: 46, train loss: 0.7791173927414498, acc: 0.6943293476974074; test loss: 0.9210377657450111, acc: 0.6386197116520917
epoch: 47, train loss: 0.7737208833154792, acc: 0.6975257487865515; test loss: 0.8623505593247798, acc: 0.6537461593004018
epoch: 48, train loss: 0.772287852265672, acc: 0.6982952527524565; test loss: 0.8979637762018762, acc: 0.6572914204679745
epoch: 49, train loss: 0.7628733299251871, acc: 0.7034450100627442; test loss: 0.9267893257334958, acc: 0.6402741668636256
epoch: 50, train loss: 0.7737974688486614, acc: 0.6953356221143602; test loss: 0.8886295640138947, acc: 0.6492554951548097
epoch: 51, train loss: 0.7644239884305173, acc: 0.6991831419438854; test loss: 0.9279478916573034, acc: 0.6381470101630821
epoch: 52, train loss: 0.7573902701589219, acc: 0.7026755060968392; test loss: 0.9481845535980672, acc: 0.6428740250531789
epoch: 53, train loss: 0.7459817124327583, acc: 0.7082988043092222; test loss: 0.8733336245920663, acc: 0.6577641219569842
epoch: 54, train loss: 0.7482352890784294, acc: 0.7093642713389369; test loss: 1.0529182663878998, acc: 0.6109666745450248
epoch: 55, train loss: 0.718865271854841, acc: 0.7167041553214158; test loss: 0.8607990102716102, acc: 0.6565823682344599
epoch: 56, train loss: 0.7206556295041743, acc: 0.7187167041553214; test loss: 0.7978846467828897, acc: 0.6882533679981092
epoch: 57, train loss: 0.7250470889028154, acc: 0.7127382502663667; test loss: 0.830072036789146, acc: 0.6743086740723233
epoch: 58, train loss: 0.7199547369620977, acc: 0.7139812951343673; test loss: 0.9828580438428611, acc: 0.6237296147482865
epoch: 59, train loss: 0.7077636186915967, acc: 0.7223866461465609; test loss: 0.9172680237084183, acc: 0.6478373906877807
epoch: 60, train loss: 0.7111938455351234, acc: 0.7194862081212264; test loss: 0.9332143022388015, acc: 0.6483100921767904
epoch: 61, train loss: 0.6965871118322214, acc: 0.7242216171421807; test loss: 0.962244391694922, acc: 0.6329472937839754
epoch: 62, train loss: 0.6944717273170798, acc: 0.7242808097549426; test loss: 0.812825802679294, acc: 0.6745450248168282
epoch: 63, train loss: 0.6941745570104786, acc: 0.7232153427252279; test loss: 1.094081145317891, acc: 0.6116757267785393
epoch: 64, train loss: 0.7026747144353845, acc: 0.7192494376701788; test loss: 0.7871116105334129, acc: 0.6981800992673127
epoch: 65, train loss: 0.7024837924599492, acc: 0.7212027938913224; test loss: 0.8288603575622407, acc: 0.676199480028362
epoch: 66, train loss: 0.6821623478074659, acc: 0.7262341659760861; test loss: 0.8664255101076116, acc: 0.6681635547151974
epoch: 67, train loss: 0.6848718831069769, acc: 0.7292529892269445; test loss: 0.80296931450487, acc: 0.6906168754431576
epoch: 68, train loss: 0.6704621676073789, acc: 0.7397892742985676; test loss: 0.7809689548161542, acc: 0.6960529425667691
epoch: 69, train loss: 0.6760522788468798, acc: 0.7325085829288505; test loss: 0.9091633374289231, acc: 0.6490191444103048
epoch: 70, train loss: 0.678997189198159, acc: 0.7309103823842784; test loss: 1.0250429750982117, acc: 0.6454738832427322
epoch: 71, train loss: 0.6653817013564075, acc: 0.733751627796851; test loss: 0.8340528304907651, acc: 0.6894351217206334
epoch: 72, train loss: 0.6637067039563019, acc: 0.7324493903160886; test loss: 0.7839396882355847, acc: 0.69463483809974
epoch: 73, train loss: 0.6651048059292456, acc: 0.7360009470818042; test loss: 0.7869275809179607, acc: 0.691562278421177
epoch: 74, train loss: 0.6621371592532055, acc: 0.737835918077424; test loss: 0.8706757710127085, acc: 0.676199480028362
epoch: 75, train loss: 0.6580411897504966, acc: 0.7399076595240914; test loss: 0.8381264303288993, acc: 0.6875443157645946
Epoch    75: reducing learning rate of group 0 to 1.5000e-03.
epoch: 76, train loss: 0.5903386606384878, acc: 0.762164081922576; test loss: 0.7338084927464005, acc: 0.7066887260694871
epoch: 77, train loss: 0.5583228553994048, acc: 0.7755416124067717; test loss: 0.7499136481265936, acc: 0.7090522335145356
epoch: 78, train loss: 0.5496397679432392, acc: 0.7764295015982006; test loss: 0.7612574070346916, acc: 0.703852517135429
epoch: 79, train loss: 0.551869211386573, acc: 0.7738842192494376; test loss: 0.736892488064199, acc: 0.7187426140392342
epoch: 80, train loss: 0.553091023325906, acc: 0.777731739078963; test loss: 0.7442883021215603, acc: 0.7187426140392342
epoch: 81, train loss: 0.5412985301940727, acc: 0.7798626731383923; test loss: 0.7737049948264735, acc: 0.7088158827700307
epoch: 82, train loss: 0.5331865276784548, acc: 0.7820527998105836; test loss: 0.7152020431415772, acc: 0.7293783975419522
epoch: 83, train loss: 0.5231396298274721, acc: 0.7868474014442998; test loss: 0.7373836175174303, acc: 0.7265421886078941
epoch: 84, train loss: 0.5282436189911726, acc: 0.7886823724399195; test loss: 0.7277014282008374, acc: 0.7227605766958166
epoch: 85, train loss: 0.5185319034992851, acc: 0.7860187048656327; test loss: 0.7470232809838097, acc: 0.7201607185062633
epoch: 86, train loss: 0.5208267833024521, acc: 0.7883272167633479; test loss: 0.723850340066692, acc: 0.7310328527534862
epoch: 87, train loss: 0.509232543164546, acc: 0.7875577127974429; test loss: 0.7513701547772799, acc: 0.7239423304183408
epoch: 88, train loss: 0.5202965723405552, acc: 0.7879128684740144; test loss: 0.815834310257077, acc: 0.6969983455447885
epoch: 89, train loss: 0.5054300701997824, acc: 0.7941872854267787; test loss: 0.7562427072766205, acc: 0.711415740959584
epoch: 90, train loss: 0.5109377226011561, acc: 0.7903397655972535; test loss: 0.7409955579545362, acc: 0.7303238005199716
epoch: 91, train loss: 0.5049521789206998, acc: 0.7934769740736356; test loss: 0.7653473277825662, acc: 0.7227605766958166
epoch: 92, train loss: 0.5015455627809745, acc: 0.7927666627204925; test loss: 0.8220024933293197, acc: 0.7066887260694871
epoch: 93, train loss: 0.5028844210711177, acc: 0.7948975967799219; test loss: 0.7551792131138592, acc: 0.7199243677617585
epoch: 94, train loss: 0.49108504156001126, acc: 0.7972061086776371; test loss: 0.7183461821521605, acc: 0.7291420467974474
epoch: 95, train loss: 0.4894715772481505, acc: 0.7983307683201136; test loss: 0.7544730296796502, acc: 0.726778539352399
epoch: 96, train loss: 0.4942848789130243, acc: 0.7986859239966853; test loss: 0.7721983854777986, acc: 0.7121247931930985
epoch: 97, train loss: 0.4892299825148539, acc: 0.7946016337161123; test loss: 0.7545189314546723, acc: 0.7203970692507682
epoch: 98, train loss: 0.47576475247865996, acc: 0.8002841245412573; test loss: 0.7769812898584022, acc: 0.7161427558496809
epoch: 99, train loss: 0.4893482159591644, acc: 0.7976204569669705; test loss: 0.7548185706110435, acc: 0.7281966438194281
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.38392970022738043, acc: 0.8001657393157334; test loss: 0.6124948846958569, acc: 0.7258331363743796
epoch: 101, train loss: 0.3659521567266877, acc: 0.8044276074345922; test loss: 0.6079411543280597, acc: 0.7371779721106122
epoch: 102, train loss: 0.3721792031402798, acc: 0.8017047472475435; test loss: 0.62732847983379, acc: 0.7149610021271567
epoch: 103, train loss: 0.355398135349936, acc: 0.8096957499704037; test loss: 0.6418568858354262, acc: 0.7187426140392342
epoch: 104, train loss: 0.3623255758796681, acc: 0.8036581034686872; test loss: 0.6102566956290396, acc: 0.7310328527534862
epoch: 105, train loss: 0.3593586468832075, acc: 0.8031253699538298; test loss: 0.628713110298503, acc: 0.7241786811628457
epoch: 106, train loss: 0.3712640920482162, acc: 0.8017047472475435; test loss: 0.6202682671437616, acc: 0.7265421886078941
epoch: 107, train loss: 0.3631347097792512, acc: 0.8030069847283059; test loss: 0.6929798229905455, acc: 0.7121247931930985
epoch: 108, train loss: 0.3755193398918106, acc: 0.7969101456138274; test loss: 0.6549949025859846, acc: 0.7156700543606712
epoch: 109, train loss: 0.3526014322195587, acc: 0.80774239374926; test loss: 0.6252598939452491, acc: 0.7329236587095249
epoch: 110, train loss: 0.3594903575219642, acc: 0.8035397182431633; test loss: 0.6562475691278954, acc: 0.7173245095722052
epoch: 111, train loss: 0.3646788952109855, acc: 0.7983307683201136; test loss: 0.6685334356702887, acc: 0.7151973528716615
epoch: 112, train loss: 0.33891811597498267, acc: 0.8120042618681188; test loss: 0.6263648956781639, acc: 0.7248877333963601
epoch: 113, train loss: 0.33480569396030835, acc: 0.8104060613235469; test loss: 0.6438286123870817, acc: 0.7244150319073505
epoch: 114, train loss: 0.3539327630788402, acc: 0.8046643778856398; test loss: 0.6436310492066789, acc: 0.7199243677617585
epoch: 115, train loss: 0.34428346730399106, acc: 0.811057180063928; test loss: 0.7134064902258495, acc: 0.7000709052233515
epoch: 116, train loss: 0.35681437398535804, acc: 0.8027702142772581; test loss: 0.6860907329957385, acc: 0.7109430394705744
epoch: 117, train loss: 0.35240889245114326, acc: 0.8050787261749733; test loss: 0.6463180861555355, acc: 0.7182699125502245
epoch: 118, train loss: 0.34940851819971064, acc: 0.8092222090683083; test loss: 0.6271066096792038, acc: 0.7289056960529425
epoch: 119, train loss: 0.34255969053727714, acc: 0.8092222090683083; test loss: 0.7110889509929548, acc: 0.7147246513826518
epoch: 120, train loss: 0.3340006084911807, acc: 0.8141943885403101; test loss: 0.7878331989859949, acc: 0.7017253604348853
epoch: 121, train loss: 0.3446294521070365, acc: 0.8087486681662128; test loss: 0.6594919836907149, acc: 0.7222878752068069
epoch: 122, train loss: 0.3481881560144078, acc: 0.80620338581745; test loss: 0.6696959129886632, acc: 0.7192153155282439
epoch: 123, train loss: 0.33611037139640243, acc: 0.8114715283532615; test loss: 0.646992410054812, acc: 0.7137792484046325
epoch: 124, train loss: 0.3391318872466911, acc: 0.8093405942938321; test loss: 0.6480531345386366, acc: 0.7187426140392342
epoch: 125, train loss: 0.34241071250984284, acc: 0.8073872380726885; test loss: 0.6268428959933277, acc: 0.725124084140865
epoch: 126, train loss: 0.33465939211227264, acc: 0.8099917130342134; test loss: 0.683844336238321, acc: 0.7185062632947293
Epoch   126: reducing learning rate of group 0 to 7.5000e-04.
epoch: 127, train loss: 0.289202948918184, acc: 0.8297620456966971; test loss: 0.6224853944000807, acc: 0.738832427322146
epoch: 128, train loss: 0.2662038013323166, acc: 0.84367230969575; test loss: 0.6388863096618111, acc: 0.7390687780666509
epoch: 129, train loss: 0.2613617297642553, acc: 0.8455072806913697; test loss: 0.6490633718738091, acc: 0.7359962183880879
epoch: 130, train loss: 0.25933333970293204, acc: 0.8460400142062271; test loss: 0.6350476466800777, acc: 0.7433230914677381
epoch: 131, train loss: 0.25687718410470495, acc: 0.8421333017639399; test loss: 0.6582622367477508, acc: 0.738832427322146
epoch: 132, train loss: 0.24956665990464685, acc: 0.8450929324020362; test loss: 0.6530566014907682, acc: 0.7345781139210589
epoch: 133, train loss: 0.2492674283230814, acc: 0.8462767846572747; test loss: 0.6663034097208984, acc: 0.7362325691325927
epoch: 134, train loss: 0.26031462770390684, acc: 0.8416597608618445; test loss: 0.6870637204004106, acc: 0.7395414795556606
epoch: 135, train loss: 0.2530153361198196, acc: 0.8482893334911803; test loss: 0.7166466317653544, acc: 0.7199243677617585
epoch: 136, train loss: 0.23847366733262845, acc: 0.8552740617970878; test loss: 0.6650070130078399, acc: 0.7317419049870008
epoch: 137, train loss: 0.24243767623211543, acc: 0.8474014442997514; test loss: 0.7024707536104582, acc: 0.726778539352399
epoch: 138, train loss: 0.2475643037454862, acc: 0.8485852965549899; test loss: 0.6898428697277259, acc: 0.7341054124320492
epoch: 139, train loss: 0.2459164117339999, acc: 0.846868710784894; test loss: 0.6761088399170307, acc: 0.7449775466792721
epoch: 140, train loss: 0.2360894385845658, acc: 0.8539718243163253; test loss: 0.6517119035459359, acc: 0.7454502481682818
epoch: 141, train loss: 0.2305958287397249, acc: 0.8541494021546111; test loss: 0.7093396508025943, acc: 0.7355235168990782
epoch: 142, train loss: 0.24314568499157213, acc: 0.8500651118740381; test loss: 0.7225329402884364, acc: 0.7270148900969038
epoch: 143, train loss: 0.25583725614817, acc: 0.8413046051852728; test loss: 0.6822717547895666, acc: 0.735759867643583
epoch: 144, train loss: 0.23065568189007354, acc: 0.8533207055759441; test loss: 0.7033033638917873, acc: 0.7397778303001654
epoch: 145, train loss: 0.22985775824678117, acc: 0.851781697644134; test loss: 0.699382237214846, acc: 0.7355235168990782
epoch: 146, train loss: 0.23967970676064787, acc: 0.8497691488102285; test loss: 0.674812344027253, acc: 0.7437957929567478
epoch: 147, train loss: 0.23050642576996103, acc: 0.854267787380135; test loss: 0.7054394704178134, acc: 0.7393051288111557
epoch: 148, train loss: 0.23694868480875547, acc: 0.8507162306144194; test loss: 0.7126076617609437, acc: 0.723705979673836
epoch: 149, train loss: 0.22604551818740004, acc: 0.8581153072096602; test loss: 0.6672169798177288, acc: 0.7428503899787284
epoch: 150, train loss: 0.22376809609392148, acc: 0.8592399668521369; test loss: 0.683131078010303, acc: 0.738832427322146
epoch: 151, train loss: 0.23190806831137392, acc: 0.8532615129631822; test loss: 0.6989058036100946, acc: 0.7411959347671945
epoch: 152, train loss: 0.22647565909323641, acc: 0.8607789747839469; test loss: 0.6560211430145927, acc: 0.7409595840226897
epoch: 153, train loss: 0.224274654331695, acc: 0.8553332544098496; test loss: 0.6998435293239106, acc: 0.7331600094540298
epoch: 154, train loss: 0.22652206024192728, acc: 0.8540310169290872; test loss: 0.6724907604410022, acc: 0.7419049870007091
epoch: 155, train loss: 0.22220932209196775, acc: 0.8594767373031845; test loss: 0.6988660271808459, acc: 0.7345781139210589
epoch: 156, train loss: 0.230384028344602, acc: 0.8555700248608974; test loss: 0.6920038960668049, acc: 0.7352871661545733
epoch: 157, train loss: 0.2113027408219829, acc: 0.8635018349709956; test loss: 0.7211129768496004, acc: 0.725124084140865
epoch: 158, train loss: 0.21600849147278753, acc: 0.8621995974902332; test loss: 0.6905704015987868, acc: 0.7348144646655637
epoch: 159, train loss: 0.21824604099433606, acc: 0.8594767373031845; test loss: 0.7201977399076814, acc: 0.7310328527534862
epoch: 160, train loss: 0.21684729794687077, acc: 0.8623179827157571; test loss: 0.6943791741260933, acc: 0.7369416213661073
epoch: 161, train loss: 0.21170760536103528, acc: 0.8611933230732804; test loss: 0.7172304426798215, acc: 0.726778539352399
epoch: 162, train loss: 0.21010220753090952, acc: 0.8637386054220433; test loss: 0.7095040060954859, acc: 0.7312692034979911
epoch: 163, train loss: 0.20578749250120928, acc: 0.8642121463241388; test loss: 0.7037282984072478, acc: 0.7348144646655637
epoch: 164, train loss: 0.2080441682126602, acc: 0.8626731383923286; test loss: 0.7136751382352612, acc: 0.7421413377452138
epoch: 165, train loss: 0.19839818113948432, acc: 0.866461465609092; test loss: 0.7318398672704352, acc: 0.7338690616875443
epoch: 166, train loss: 0.20487982605009428, acc: 0.8655735764176631; test loss: 0.7453386417322198, acc: 0.7263058378633893
epoch: 167, train loss: 0.1979495137347771, acc: 0.870486563276903; test loss: 0.7180852944674545, acc: 0.7350508154100686
epoch: 168, train loss: 0.2034036356894465, acc: 0.8655735764176631; test loss: 0.7054549624130422, acc: 0.7341054124320492
epoch: 169, train loss: 0.20517813119469724, acc: 0.8621995974902332; test loss: 0.7118846546868234, acc: 0.7411959347671945
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.1589488520242765, acc: 0.8662838877708062; test loss: 0.629374750566043, acc: 0.734341763176554
epoch: 171, train loss: 0.14472538154301903, acc: 0.8732094234639517; test loss: 0.6340022601012895, acc: 0.7411959347671945
epoch: 172, train loss: 0.15273797947075912, acc: 0.8666982360601397; test loss: 0.597145592368993, acc: 0.7378870243441267
epoch: 173, train loss: 0.14822042587453463, acc: 0.8691251331833787; test loss: 0.6347389731162605, acc: 0.7369416213661073
epoch: 174, train loss: 0.1490781593018627, acc: 0.8658103468687108; test loss: 0.626107154008599, acc: 0.7248877333963601
epoch: 175, train loss: 0.16062140038052408, acc: 0.8623179827157571; test loss: 0.6056387980647527, acc: 0.7411959347671945
epoch: 176, train loss: 0.155873744736074, acc: 0.860719782171185; test loss: 0.603822709679519, acc: 0.743559442212243
epoch: 177, train loss: 0.1520838544056714, acc: 0.8639161832603292; test loss: 0.6370220235886795, acc: 0.7317419049870008
Epoch   177: reducing learning rate of group 0 to 3.7500e-04.
epoch: 178, train loss: 0.13526182056948097, acc: 0.8744524683319522; test loss: 0.6116343565343317, acc: 0.7423776884897187
epoch: 179, train loss: 0.11556574369646098, acc: 0.8880667692671954; test loss: 0.6180871053667278, acc: 0.7426140392342235
epoch: 180, train loss: 0.1105732767071698, acc: 0.8917367112584349; test loss: 0.6305690494109992, acc: 0.7466320018908059
epoch: 181, train loss: 0.11014279438445339, acc: 0.8931573339647212; test loss: 0.6373937043194736, acc: 0.7461593004017962
epoch: 182, train loss: 0.10984942466338118, acc: 0.8914407481946253; test loss: 0.6478795742092615, acc: 0.7437957929567478
epoch: 183, train loss: 0.1126124926949586, acc: 0.8923878299988162; test loss: 0.6371437736618071, acc: 0.7440321437012527
epoch: 184, train loss: 0.10574189488497855, acc: 0.894163608381674; test loss: 0.6343392143855842, acc: 0.7489955093358545
epoch: 185, train loss: 0.10636172925486033, acc: 0.8948147271220551; test loss: 0.6487923593167237, acc: 0.7449775466792721
epoch: 186, train loss: 0.10952493097936104, acc: 0.8913223629691015; test loss: 0.681941084956424, acc: 0.7395414795556606
epoch: 187, train loss: 0.11057154751358843, acc: 0.8896057771990056; test loss: 0.6642458995333577, acc: 0.7433230914677381
epoch: 188, train loss: 0.1072372735222303, acc: 0.8903160885521487; test loss: 0.6460548746926766, acc: 0.7459229496572914
epoch: 189, train loss: 0.10739046212881574, acc: 0.890848822067006; test loss: 0.6740663180084157, acc: 0.7419049870007091
epoch: 190, train loss: 0.10354469075130403, acc: 0.8958210015390079; test loss: 0.6791727614442217, acc: 0.7378870243441267
epoch: 191, train loss: 0.10938107225405784, acc: 0.8938676453178643; test loss: 0.6693117866784342, acc: 0.7416686362562042
epoch: 192, train loss: 0.10725745117190154, acc: 0.8921510595477684; test loss: 0.6529875302309123, acc: 0.7440321437012527
epoch: 193, train loss: 0.10583029776843353, acc: 0.8940452231561501; test loss: 0.6877169196021329, acc: 0.7369416213661073
epoch: 194, train loss: 0.10533482418993712, acc: 0.891677518645673; test loss: 0.6653285503274751, acc: 0.7428503899787284
epoch: 195, train loss: 0.10621896781123459, acc: 0.8929797561264354; test loss: 0.6452705127300875, acc: 0.7463956511463011
epoch: 196, train loss: 0.10507805426632058, acc: 0.8952882680241506; test loss: 0.6654903190520829, acc: 0.743559442212243
epoch: 197, train loss: 0.1032315943140073, acc: 0.8942819936071978; test loss: 0.6608646947197586, acc: 0.7416686362562042
epoch: 198, train loss: 0.10430073066513905, acc: 0.8925654078371019; test loss: 0.6812701896132377, acc: 0.7440321437012527
epoch: 199, train loss: 0.11055482957193283, acc: 0.8914407481946253; test loss: 0.6572477976443553, acc: 0.7411959347671945
epoch: 200, train loss: 0.10700958675342269, acc: 0.8921510595477684; test loss: 0.6675115611022935, acc: 0.7442684944457575
best test acc 0.7489955093358545 at epoch 184.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9503    0.9775    0.9637      6100
           1     0.9767    0.9060    0.9401       926
           2     0.8954    0.9483    0.9211      2400
           3     0.9354    0.9454    0.9404       843
           4     0.9102    0.9819    0.9447       774
           5     0.9411    0.9821    0.9612      1512
           6     0.8858    0.8226    0.8530      1330
           7     0.9106    0.9106    0.9106       481
           8     0.9031    0.9563    0.9290       458
           9     0.9613    0.9889    0.9749       452
          10     0.9554    0.8954    0.9244       717
          11     0.9295    0.8709    0.8992       333
          12     0.8393    0.1572    0.2648       299
          13     0.8502    0.7807    0.8140       269

    accuracy                         0.9309     16894
   macro avg     0.9174    0.8660    0.8744     16894
weighted avg     0.9296    0.9309    0.9255     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8062    0.8813    0.8421      1525
           1     0.8300    0.7155    0.7685       232
           2     0.7152    0.7604    0.7371       601
           3     0.7333    0.6777    0.7044       211
           4     0.7718    0.8196    0.7950       194
           5     0.8172    0.8042    0.8107       378
           6     0.5151    0.5135    0.5143       333
           7     0.7857    0.6364    0.7032       121
           8     0.5766    0.5565    0.5664       115
           9     0.7870    0.7456    0.7658       114
          10     0.7801    0.6111    0.6854       180
          11     0.5867    0.5238    0.5535        84
          12     0.0526    0.0133    0.0213        75
          13     0.6471    0.6471    0.6471        68

    accuracy                         0.7490      4231
   macro avg     0.6718    0.6361    0.6510      4231
weighted avg     0.7387    0.7490    0.7419      4231

---------------------------------------
program finished.
