seed:  666
save trained model at:  ../trained_models/trained_classifier_model_100.pt
save loss at:  ./results/train_classifier_results_100.json
how to merge clusters:  [[0, 9, 12, 25], 2, [3, 8, 13, 27], 4, 6, [7, 19, 21], [10, 16, 28], 15, 17, 18, [20, 23], 24, 26, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  14
number of pockets in training set:  16894
number of pockets in test set:  4231
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['3c4vB00', '3jbzA00', '5a6nA00', '2oxdA00', '1a9cC01']
model architecture:
DeepDruG(
  (embedding_net): JKEGINEmbeddingNet(
    (conv0): GINConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv4): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn4): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv5): GINConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))
    (bn5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=14, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0007
)
learning rate scheduler: 
<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2af8608fc640>
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [25, 100, 170]
loss function:
FocalLoss(gamma=0, alpha=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), reduction=mean)
begin training...
epoch: 1, train loss: 1.9871206063694737, acc: 0.4013259145258672; test loss: 1.7687653056049821, acc: 0.4509572205152446
epoch: 2, train loss: 1.7005698680877686, acc: 0.47922339292056354; test loss: 1.6810692803173926, acc: 0.4875915859134956
epoch: 3, train loss: 1.5956888742188355, acc: 0.5077542322718125; test loss: 1.5744241802046075, acc: 0.5168990782320965
epoch: 4, train loss: 1.5319275399474854, acc: 0.5293595359299159; test loss: 1.5228213029842066, acc: 0.5350980855589695
epoch: 5, train loss: 1.5116637509377508, acc: 0.5410204806440156; test loss: 1.4922580363986566, acc: 0.5327345781139211
epoch: 6, train loss: 1.4776458278046114, acc: 0.5518527287794484; test loss: 1.460119800177682, acc: 0.5436067123611439
epoch: 7, train loss: 1.4479490530706798, acc: 0.561086776370309; test loss: 1.4296534934299716, acc: 0.5516426376743087
epoch: 8, train loss: 1.4164591576060388, acc: 0.5715046762164082; test loss: 1.44090124321615, acc: 0.5554242495863861
epoch: 9, train loss: 1.3858024578087709, acc: 0.5802651829051735; test loss: 1.3723006678427738, acc: 0.5731505554242496
epoch: 10, train loss: 1.3688851828771018, acc: 0.5841127027346987; test loss: 1.364691268408786, acc: 0.566296383833609
epoch: 11, train loss: 1.3534959674488374, acc: 0.5868355629217473; test loss: 1.381295836132431, acc: 0.5667690853226187
epoch: 12, train loss: 1.3445256010123057, acc: 0.5945306025807979; test loss: 1.3337013564248423, acc: 0.5811864807374143
epoch: 13, train loss: 1.3337796971235414, acc: 0.5937610986148929; test loss: 1.3755079245516606, acc: 0.583077286693453
epoch: 14, train loss: 1.3234161023516704, acc: 0.5996803598910856; test loss: 1.362790368911195, acc: 0.5726778539352398
epoch: 15, train loss: 1.2933526933157926, acc: 0.6031727240440393; test loss: 1.2917763112256837, acc: 0.603167099976365
epoch: 16, train loss: 1.2850527922557038, acc: 0.6129986977625193; test loss: 1.3317221352124828, acc: 0.5873316000945403
epoch: 17, train loss: 1.2813144711131694, acc: 0.60749378477566; test loss: 1.3388940073643079, acc: 0.5835499881824627
epoch: 18, train loss: 1.2938918340159575, acc: 0.6047709245886114; test loss: 1.3019273210095896, acc: 0.6003308910423067
epoch: 19, train loss: 1.2608075401296668, acc: 0.6170829880430923; test loss: 1.2858322246563234, acc: 0.5993854880642874
epoch: 20, train loss: 1.2363794730713598, acc: 0.6211080857109033; test loss: 1.382485225673433, acc: 0.5854407941385016
epoch: 21, train loss: 1.2263250563064807, acc: 0.624600449863857; test loss: 1.3052034359456124, acc: 0.5979673835972583
epoch: 22, train loss: 1.2060066827079934, acc: 0.6317035633952882; test loss: 1.2186202600024265, acc: 0.6232569132592768
epoch: 23, train loss: 1.1981313194736893, acc: 0.6363205871907186; test loss: 1.3830113625644764, acc: 0.584731741904987
epoch: 24, train loss: 1.1976003072184755, acc: 0.6348999644844323; test loss: 1.2556301550616105, acc: 0.6116757267785393
epoch 25, gamma increased to 1.
epoch: 25, train loss: 0.9578899235588229, acc: 0.6396945661181485; test loss: 1.011564124703999, acc: 0.6064760103994328
epoch: 26, train loss: 0.9411943519243873, acc: 0.6462057535219604; test loss: 1.017753108571647, acc: 0.6086031670999764
epoch: 27, train loss: 0.9302571834481452, acc: 0.6452586717177696; test loss: 0.9719092334818541, acc: 0.628929331127393
epoch: 28, train loss: 0.9325834370737628, acc: 0.6460281756836747; test loss: 0.9527740376521385, acc: 0.6353108012290238
epoch: 29, train loss: 0.9200604692695973, acc: 0.6525393630874867; test loss: 0.9071210120987087, acc: 0.6447648310092177
epoch: 30, train loss: 0.9132546825607641, acc: 0.6516514738960578; test loss: 0.9638120663191404, acc: 0.6317655400614512
epoch: 31, train loss: 0.8902367089956741, acc: 0.6629572629335859; test loss: 0.9579890723263392, acc: 0.6353108012290238
epoch: 32, train loss: 0.9034860406639421, acc: 0.6558541494021546; test loss: 1.017154618657472, acc: 0.6123847790120539
epoch: 33, train loss: 0.8766684935010226, acc: 0.6650881969930152; test loss: 0.9431784757061, acc: 0.6447648310092177
epoch: 34, train loss: 0.874081893398788, acc: 0.6647922339292056; test loss: 0.9430631070350028, acc: 0.6381470101630821
epoch: 35, train loss: 0.8643485606457618, acc: 0.6678702497928258; test loss: 0.9944818957537743, acc: 0.6223115102812574
epoch: 36, train loss: 0.852163050807523, acc: 0.6687581389842547; test loss: 1.036640557575609, acc: 0.6048215551878988
epoch: 37, train loss: 0.8539759440366737, acc: 0.6698236060139695; test loss: 1.4898348658171086, acc: 0.48617348144646655
epoch: 38, train loss: 0.8415686558996895, acc: 0.6751509411625429; test loss: 0.9821369573597197, acc: 0.6298747341054124
epoch: 39, train loss: 0.8497912404846831, acc: 0.671836154847875; test loss: 0.9699923963981778, acc: 0.6383833609075868
epoch: 40, train loss: 0.8097866941333819, acc: 0.6833787143364508; test loss: 0.9008410477914361, acc: 0.6570550697234696
epoch: 41, train loss: 0.8130172004611644, acc: 0.6860423819107375; test loss: 0.9801193622516081, acc: 0.6376743086740724
epoch: 42, train loss: 0.8156091757775821, acc: 0.6842074109151178; test loss: 1.1398399361367204, acc: 0.5842590404159773
epoch: 43, train loss: 0.800640999254735, acc: 0.6884692790339766; test loss: 0.9769937384075038, acc: 0.630583786338927
epoch: 44, train loss: 0.7882235544284194, acc: 0.6898899017402628; test loss: 0.8504824690772579, acc: 0.6683999054597022
epoch: 45, train loss: 0.7934038837139206, acc: 0.6882917011956908; test loss: 0.9374371579115455, acc: 0.6483100921767904
epoch: 46, train loss: 0.7790622422649153, acc: 0.697880904463123; test loss: 0.9155877105340527, acc: 0.6452375324982274
epoch: 47, train loss: 0.7737714590596718, acc: 0.6978217118503611; test loss: 0.9025769222433532, acc: 0.6461829354762467
epoch: 48, train loss: 0.7606081258789158, acc: 0.7002486089736001; test loss: 0.8764465680727579, acc: 0.676435830772867
epoch: 49, train loss: 0.7558105605035558, acc: 0.7042145140286492; test loss: 0.8693325961912476, acc: 0.6740723233278185
epoch: 50, train loss: 0.752860620513786, acc: 0.7022611578075056; test loss: 0.9772205155556772, acc: 0.6499645473883243
epoch: 51, train loss: 0.7479958164490115, acc: 0.7008405351012194; test loss: 0.9595695003440999, acc: 0.642637674308674
epoch: 52, train loss: 0.75067154494943, acc: 0.7016100390671244; test loss: 0.9346443820692805, acc: 0.645710233987237
epoch: 53, train loss: 0.7296500819680873, acc: 0.7140404877471291; test loss: 0.8861182998241586, acc: 0.6667454502481683
epoch: 54, train loss: 0.7192943149436024, acc: 0.7153427252278916; test loss: 1.0516718049275906, acc: 0.5958402268967147
epoch: 55, train loss: 0.7303557476518111, acc: 0.7103705457558896; test loss: 0.8292684452495968, acc: 0.6865989127865753
epoch: 56, train loss: 0.7274725630295492, acc: 0.7128566354918906; test loss: 0.8657912221312156, acc: 0.6688726069487119
epoch: 57, train loss: 0.7200608462486886, acc: 0.7142180655854149; test loss: 0.9527076600826934, acc: 0.645710233987237
epoch: 58, train loss: 0.712346456432478, acc: 0.7216171421806559; test loss: 0.9181042830449868, acc: 0.6584731741904987
epoch: 59, train loss: 0.7049763780085028, acc: 0.7200781342488457; test loss: 0.8677128912060854, acc: 0.6750177263058379
epoch: 60, train loss: 0.703324137325892, acc: 0.7242808097549426; test loss: 0.8677567417980565, acc: 0.6615457338690617
epoch: 61, train loss: 0.689677052029607, acc: 0.7261749733633243; test loss: 0.8724932881907295, acc: 0.6688726069487119
epoch: 62, train loss: 0.7030667720129465, acc: 0.7216171421806559; test loss: 0.8338769852650191, acc: 0.6714724651382652
epoch: 63, train loss: 0.6874049394543077, acc: 0.7295489522907541; test loss: 0.8430714585825614, acc: 0.6738359725833136
epoch: 64, train loss: 0.6827903708875906, acc: 0.7260565881378004; test loss: 0.7909825456195958, acc: 0.6917986291656819
epoch: 65, train loss: 0.6763726395446901, acc: 0.7310879602225642; test loss: 0.8416575079295574, acc: 0.677853935239896
epoch: 66, train loss: 0.6660213108611442, acc: 0.7335740499585651; test loss: 0.8315653475822498, acc: 0.6870716142755849
epoch: 67, train loss: 0.6588992033962161, acc: 0.7332780868947555; test loss: 0.9457521486496481, acc: 0.6511463011108485
epoch: 68, train loss: 0.6762670095790783, acc: 0.728897833550373; test loss: 0.9250444684065817, acc: 0.6390924131411014
epoch: 69, train loss: 0.671185353210978, acc: 0.7281283295844678; test loss: 0.9198837106714178, acc: 0.6480737414322855
epoch: 70, train loss: 0.6740048819425417, acc: 0.7333372795075175; test loss: 0.7987298651018822, acc: 0.696289293311274
epoch: 71, train loss: 0.657109380897381, acc: 0.7401444299751391; test loss: 0.850728156239224, acc: 0.6804537934294493
epoch: 72, train loss: 0.6542539791170628, acc: 0.7355274061797088; test loss: 0.8937192461333413, acc: 0.6773812337508863
epoch: 73, train loss: 0.6423028355112186, acc: 0.7432816384515213; test loss: 0.8367504304364285, acc: 0.6896714724651383
epoch: 74, train loss: 0.6534478259013223, acc: 0.740262815200663; test loss: 0.8380617336921449, acc: 0.6759631292838573
epoch: 75, train loss: 0.6631813931425634, acc: 0.7373623771753285; test loss: 0.8168531397898635, acc: 0.6880170172536043
epoch: 76, train loss: 0.6436082195767118, acc: 0.7417426305197111; test loss: 0.8283149894037249, acc: 0.6804537934294493
epoch: 77, train loss: 0.6324724326743958, acc: 0.7478986622469516; test loss: 0.8747192880161206, acc: 0.6740723233278185
epoch: 78, train loss: 0.6404301097749521, acc: 0.7473067361193323; test loss: 0.931974307438691, acc: 0.6646182935476247
epoch: 79, train loss: 0.6348253621199265, acc: 0.7460044986385699; test loss: 1.5128705927565487, acc: 0.48853698889151503
epoch: 80, train loss: 0.6433474593110602, acc: 0.7370664141115189; test loss: 0.8574766697429421, acc: 0.6837627038525171
epoch: 81, train loss: 0.6268580927783334, acc: 0.7471291582810465; test loss: 0.8914566015244882, acc: 0.6634365398251004
Epoch    81: reducing learning rate of group 0 to 1.5000e-03.
epoch: 82, train loss: 0.5697387152273048, acc: 0.7712797442879129; test loss: 0.7423134824959726, acc: 0.7196880170172536
epoch: 83, train loss: 0.5378743662302968, acc: 0.7802178288149639; test loss: 0.7533053413572336, acc: 0.7161427558496809
epoch: 84, train loss: 0.5154141521784092, acc: 0.7917603883035397; test loss: 0.782353794566285, acc: 0.7043252186244386
epoch: 85, train loss: 0.5085976809894769, acc: 0.7911684621759204; test loss: 0.7897004430329537, acc: 0.7116520917040888
epoch: 86, train loss: 0.514383886129853, acc: 0.7894518764058246; test loss: 0.7694412406469682, acc: 0.7104703379815647
epoch: 87, train loss: 0.5092919850343781, acc: 0.7914052326269682; test loss: 0.7663356807039748, acc: 0.71756086031671
epoch: 88, train loss: 0.514150984445041, acc: 0.7911684621759204; test loss: 0.7456176573433962, acc: 0.7180335618057196
epoch: 89, train loss: 0.5059886255164929, acc: 0.7933585888481117; test loss: 0.7870095286124763, acc: 0.7085795320255259
epoch: 90, train loss: 0.4963091090330361, acc: 0.794009707588493; test loss: 0.8173357958603176, acc: 0.7057433230914677
epoch: 91, train loss: 0.5028634961221266, acc: 0.7934769740736356; test loss: 0.798611880475763, acc: 0.7104703379815647
epoch: 92, train loss: 0.49306195647642853, acc: 0.7963774120989701; test loss: 0.8067742960020826, acc: 0.7066887260694871
epoch: 93, train loss: 0.48409674762691624, acc: 0.7997513910263999; test loss: 0.818036853323251, acc: 0.7090522335145356
epoch: 94, train loss: 0.48347768254910645, acc: 0.8007576654433527; test loss: 0.7872665882448698, acc: 0.7185062632947293
epoch: 95, train loss: 0.47636087955886036, acc: 0.805433881851545; test loss: 0.7885139459020786, acc: 0.7111793902150791
epoch: 96, train loss: 0.479196426274862, acc: 0.7992778501243045; test loss: 0.7977076644058967, acc: 0.723705979673836
epoch: 97, train loss: 0.4801661990881954, acc: 0.8024742512134485; test loss: 0.8096937681309468, acc: 0.7137792484046325
epoch: 98, train loss: 0.47050804788154355, acc: 0.8054930744643068; test loss: 0.8089290094048778, acc: 0.7064523753249823
epoch: 99, train loss: 0.46821521134916755, acc: 0.8031845625665917; test loss: 0.7985017343605418, acc: 0.7135428976601277
epoch 100, gamma increased to 2.
epoch: 100, train loss: 0.3713920177920296, acc: 0.8018823250858292; test loss: 0.6688356485752258, acc: 0.7076341290475066
epoch: 101, train loss: 0.3694682801469792, acc: 0.805374689238783; test loss: 0.6592107769220722, acc: 0.7142519498936422
epoch: 102, train loss: 0.3554743961870452, acc: 0.8086894755534509; test loss: 0.718320292914852, acc: 0.6951075395887497
epoch: 103, train loss: 0.3572221459390652, acc: 0.8067953119450693; test loss: 0.6482205190886225, acc: 0.7203970692507682
epoch: 104, train loss: 0.3487029342986119, acc: 0.8104060613235469; test loss: 0.6639159095397207, acc: 0.7076341290475066
epoch: 105, train loss: 0.349483800919448, acc: 0.8128329584467858; test loss: 0.7024035915306234, acc: 0.7026707634129048
epoch: 106, train loss: 0.36153755713189384, acc: 0.8001065467029714; test loss: 0.7828656163122095, acc: 0.6830536516190026
epoch: 107, train loss: 0.3692526585557298, acc: 0.8004025097667811; test loss: 0.7192254394990167, acc: 0.6861262112975656
epoch: 108, train loss: 0.3546662139083925, acc: 0.8081567420385936; test loss: 0.6658818638936139, acc: 0.7085795320255259
epoch: 109, train loss: 0.3439235862181767, acc: 0.8100509056469752; test loss: 0.6525038545843389, acc: 0.7166154573386906
epoch: 110, train loss: 0.3369973279448449, acc: 0.8101692908724991; test loss: 0.6542162588656414, acc: 0.7123611439376034
epoch: 111, train loss: 0.34279500144366554, acc: 0.8076832011364982; test loss: 0.7441431986922153, acc: 0.6875443157645946
epoch: 112, train loss: 0.3498097796249322, acc: 0.8059074227536404; test loss: 0.7224954182699875, acc: 0.6835263531080122
epoch: 113, train loss: 0.33963370835822027, acc: 0.8111755652894519; test loss: 0.6983647672201945, acc: 0.7104703379815647
epoch: 114, train loss: 0.3489645520504994, acc: 0.8091630164555463; test loss: 0.7069139676126739, acc: 0.7031434649019145
epoch: 115, train loss: 0.3365454295054969, acc: 0.8140168107020244; test loss: 0.6858167337824678, acc: 0.711415740959584
epoch: 116, train loss: 0.34587123662646146, acc: 0.8099325204214514; test loss: 0.7266400230322513, acc: 0.6965256440557788
epoch: 117, train loss: 0.33300497054319855, acc: 0.8150230851189771; test loss: 0.6648621120173382, acc: 0.7244150319073505
epoch: 118, train loss: 0.3477576382663725, acc: 0.8081567420385936; test loss: 0.6643511403736789, acc: 0.7097612857480501
epoch: 119, train loss: 0.3301046329714835, acc: 0.8153190481827868; test loss: 0.677868475166795, acc: 0.7085795320255259
epoch: 120, train loss: 0.3203868315186573, acc: 0.8215342725227892; test loss: 0.7421145373781475, acc: 0.6991255022453321
epoch: 121, train loss: 0.32687993530579623, acc: 0.8177459453060258; test loss: 0.6928320471994197, acc: 0.7161427558496809
epoch: 122, train loss: 0.3298318935729095, acc: 0.8141943885403101; test loss: 0.7206229771938878, acc: 0.7007799574568659
epoch: 123, train loss: 0.3466988455985331, acc: 0.8080383568130697; test loss: 0.6878036104232474, acc: 0.709997636492555
epoch: 124, train loss: 0.32843561988970904, acc: 0.8157925890848822; test loss: 0.6911055926762696, acc: 0.7071614275584968
epoch: 125, train loss: 0.30558531886164897, acc: 0.8224221617142181; test loss: 0.7641328991065209, acc: 0.697707397778303
epoch: 126, train loss: 0.32243571347621286, acc: 0.8212383094589795; test loss: 0.7136048336903611, acc: 0.7047979201134483
epoch: 127, train loss: 0.3241823055807113, acc: 0.8152598555700249; test loss: 0.7759372676920592, acc: 0.7040888678799339
epoch: 128, train loss: 0.311360499820611, acc: 0.8227773173907896; test loss: 0.6801470220582024, acc: 0.7147246513826518
epoch: 129, train loss: 0.3148363730294106, acc: 0.8217710429738369; test loss: 0.7320563179526423, acc: 0.7135428976601277
epoch: 130, train loss: 0.3180907563661081, acc: 0.819166568012312; test loss: 0.7042452091022294, acc: 0.7123611439376034
epoch: 131, train loss: 0.3199347383569257, acc: 0.8179235231443116; test loss: 0.6804619861590161, acc: 0.7234696289293311
epoch: 132, train loss: 0.31512164410167975, acc: 0.8212383094589795; test loss: 0.7203697095495453, acc: 0.7024344126683999
Epoch   132: reducing learning rate of group 0 to 7.5000e-04.
epoch: 133, train loss: 0.25808503812264305, acc: 0.8462175920445129; test loss: 0.6896127172464233, acc: 0.7274875915859135
epoch: 134, train loss: 0.23915045976469443, acc: 0.8549780987332781; test loss: 0.6975499293787827, acc: 0.7307965020089813
epoch: 135, train loss: 0.23845449525025436, acc: 0.8548005208949923; test loss: 0.7275412271164237, acc: 0.718978964783739
epoch: 136, train loss: 0.23095873476655382, acc: 0.8574049958565171; test loss: 0.7754929478752841, acc: 0.7092885842590404
epoch: 137, train loss: 0.22969581333366218, acc: 0.8586480407245176; test loss: 0.7250414719623304, acc: 0.7289056960529425
epoch: 138, train loss: 0.2341423942779696, acc: 0.8546821356694685; test loss: 0.7383195607832943, acc: 0.7192153155282439
epoch: 139, train loss: 0.2245760265487714, acc: 0.8603646264946134; test loss: 0.738724333376108, acc: 0.7281966438194281
epoch: 140, train loss: 0.22476429300038958, acc: 0.8592991594648988; test loss: 0.7348223461718234, acc: 0.7293783975419522
epoch: 141, train loss: 0.22489174636663228, acc: 0.8617852492008997; test loss: 0.7564544574049623, acc: 0.7182699125502245
epoch: 142, train loss: 0.22007257291744825, acc: 0.8641529537113768; test loss: 0.7575000113444643, acc: 0.7241786811628457
epoch: 143, train loss: 0.23020944105311514, acc: 0.8595951225287084; test loss: 0.7912602131035953, acc: 0.7135428976601277
epoch: 144, train loss: 0.22381897329776065, acc: 0.8640345684858529; test loss: 0.759724331786801, acc: 0.7218151737177972
epoch: 145, train loss: 0.219218784931977, acc: 0.8619036344264236; test loss: 0.8076074325117032, acc: 0.709997636492555
epoch: 146, train loss: 0.23293258095402908, acc: 0.85349828341423; test loss: 0.761292519566016, acc: 0.7125974946821082
epoch: 147, train loss: 0.2110779813454534, acc: 0.8662838877708062; test loss: 0.7628633358100435, acc: 0.722051524462302
epoch: 148, train loss: 0.22226609882023315, acc: 0.8620812122647094; test loss: 0.7265638712308446, acc: 0.7208697707397779
epoch: 149, train loss: 0.214873595856288, acc: 0.8650408429028057; test loss: 0.7496052066419345, acc: 0.7286693453084377
epoch: 150, train loss: 0.20973054659998852, acc: 0.865691961643187; test loss: 0.7640469525052537, acc: 0.726778539352399
epoch: 151, train loss: 0.20454995055424188, acc: 0.8682372439919498; test loss: 0.7744329445076394, acc: 0.7307965020089813
epoch: 152, train loss: 0.21239466349888345, acc: 0.8647448798389961; test loss: 0.7891730366799038, acc: 0.7277239423304184
epoch: 153, train loss: 0.20983876924416264, acc: 0.8651592281283296; test loss: 0.7674160593087385, acc: 0.722051524462302
epoch: 154, train loss: 0.20602242869993237, acc: 0.8693619036344264; test loss: 0.786026733081072, acc: 0.7277239423304184
epoch: 155, train loss: 0.20559475488093698, acc: 0.8675861252515686; test loss: 0.7687858106734425, acc: 0.7279602930749232
epoch: 156, train loss: 0.2092706708000922, acc: 0.8646264946134722; test loss: 0.7862140499539647, acc: 0.7177972110612149
epoch: 157, train loss: 0.20888562802310467, acc: 0.8630282940689002; test loss: 0.7991820301352537, acc: 0.725124084140865
epoch: 158, train loss: 0.20620202117627, acc: 0.8682372439919498; test loss: 0.7912376143920509, acc: 0.7182699125502245
epoch: 159, train loss: 0.20123100654393297, acc: 0.8684740144429975; test loss: 0.7733173747345562, acc: 0.7154337036161664
epoch: 160, train loss: 0.20959545300954388, acc: 0.8650408429028057; test loss: 0.7875343688242656, acc: 0.7255967856298747
epoch: 161, train loss: 0.2023676037329731, acc: 0.8690659405706168; test loss: 0.8097916030906329, acc: 0.7135428976601277
epoch: 162, train loss: 0.20095077184540175, acc: 0.8695394814727122; test loss: 0.7956428064631672, acc: 0.7263058378633893
epoch: 163, train loss: 0.18424874538498628, acc: 0.8748076240085237; test loss: 0.8065824257907673, acc: 0.7203970692507682
epoch: 164, train loss: 0.19062302129902586, acc: 0.8733870013022375; test loss: 0.8355162229923401, acc: 0.7078704797920113
epoch: 165, train loss: 0.19408846221669293, acc: 0.8710784894045223; test loss: 0.8393166162702271, acc: 0.7156700543606712
epoch: 166, train loss: 0.1812175520940181, acc: 0.8790694921273825; test loss: 0.8182875089007499, acc: 0.7187426140392342
epoch: 167, train loss: 0.18743071370545655, acc: 0.8758730910382384; test loss: 0.825770247548896, acc: 0.7166154573386906
epoch: 168, train loss: 0.18897822617115573, acc: 0.8739197348170948; test loss: 0.8590789680868648, acc: 0.7071614275584968
epoch: 169, train loss: 0.1911205071587668, acc: 0.8735645791405232; test loss: 0.823830388956559, acc: 0.7203970692507682
epoch 170, gamma increased to 3.
epoch: 170, train loss: 0.1450928837868876, acc: 0.870545755889665; test loss: 0.7390371476806554, acc: 0.7225242259513117
epoch: 171, train loss: 0.14029490926025237, acc: 0.8759914762637623; test loss: 0.6921672635370325, acc: 0.7225242259513117
epoch: 172, train loss: 0.1378694179708102, acc: 0.8779448324849058; test loss: 0.7154053082507825, acc: 0.7182699125502245
epoch: 173, train loss: 0.14313927829456116, acc: 0.8724991121108086; test loss: 0.7165876475949289, acc: 0.71756086031671
epoch: 174, train loss: 0.14705946476477566, acc: 0.8678228957026163; test loss: 0.6943489239870079, acc: 0.7118884424485937
epoch: 175, train loss: 0.14901714347990022, acc: 0.867290162187759; test loss: 0.7015556379909691, acc: 0.7059796738359726
epoch: 176, train loss: 0.14200115175102962, acc: 0.8751035870723334; test loss: 0.7363995771491479, acc: 0.703852517135429
epoch: 177, train loss: 0.13467702759085598, acc: 0.8769385580679531; test loss: 0.745839528963093, acc: 0.7128338454266131
epoch: 178, train loss: 0.13825062829685786, acc: 0.876169054102048; test loss: 0.7996147017027012, acc: 0.6991255022453321
epoch: 179, train loss: 0.14421703718929893, acc: 0.8716112229193796; test loss: 0.7422601727553503, acc: 0.7159064051051761
epoch: 180, train loss: 0.1354316108839899, acc: 0.8703089854386172; test loss: 0.7183910220882567, acc: 0.7123611439376034
epoch: 181, train loss: 0.14506196453893935, acc: 0.8716704155321416; test loss: 0.7095493002722264, acc: 0.722051524462302
epoch: 182, train loss: 0.13342535504578443, acc: 0.8753403575233811; test loss: 0.7039715370312563, acc: 0.7218151737177972
epoch: 183, train loss: 0.13389327659597902, acc: 0.8741565052681425; test loss: 0.7362239157947855, acc: 0.7107066887260695
Epoch   183: reducing learning rate of group 0 to 3.7500e-04.
epoch: 184, train loss: 0.11556911357545395, acc: 0.8855806795311945; test loss: 0.7129540285639265, acc: 0.7286693453084377
epoch: 185, train loss: 0.09929065831641523, acc: 0.8973008168580561; test loss: 0.7497190942834157, acc: 0.7229969274403214
epoch: 186, train loss: 0.09332654666897812, acc: 0.9012667219131052; test loss: 0.7731139771455401, acc: 0.71756086031671
epoch: 187, train loss: 0.09163488923790639, acc: 0.9000828696578667; test loss: 0.7533194607410327, acc: 0.7253604348853699
epoch: 188, train loss: 0.09043326427051128, acc: 0.9036344264235824; test loss: 0.7598144185371192, acc: 0.7255967856298747
epoch: 189, train loss: 0.09005820507835718, acc: 0.90387119687463; test loss: 0.7726196769393383, acc: 0.7201607185062633
epoch: 190, train loss: 0.09254925856544263, acc: 0.9015626849769148; test loss: 0.7737442158660943, acc: 0.7215788229732923
epoch: 191, train loss: 0.09098514370382982, acc: 0.9029241150704392; test loss: 0.7781162926615961, acc: 0.7277239423304184
epoch: 192, train loss: 0.09147924986925156, acc: 0.9012667219131052; test loss: 0.8091136747656408, acc: 0.7085795320255259
epoch: 193, train loss: 0.09131012255939151, acc: 0.9022138037172961; test loss: 0.7978515066003946, acc: 0.7156700543606712
epoch: 194, train loss: 0.09032478110210507, acc: 0.9018586480407245; test loss: 0.8121534604266641, acc: 0.7123611439376034
epoch: 195, train loss: 0.09044677880694629, acc: 0.9007931810110098; test loss: 0.7925843141006257, acc: 0.7274875915859135
epoch: 196, train loss: 0.0875614399940033, acc: 0.9045815082277732; test loss: 0.7854425944889784, acc: 0.7244150319073505
epoch: 197, train loss: 0.09152394223120758, acc: 0.9012667219131052; test loss: 0.7924668334162114, acc: 0.7246513826518554
epoch: 198, train loss: 0.0881340462060402, acc: 0.904640700840535; test loss: 0.8097208942597258, acc: 0.7222878752068069
epoch: 199, train loss: 0.08209839407665512, acc: 0.9051142417426306; test loss: 0.8205202890327146, acc: 0.7255967856298747
epoch: 200, train loss: 0.09152491690333643, acc: 0.8994909435302474; test loss: 0.8229181863432713, acc: 0.7140155991491374
best test acc 0.7307965020089813 at epoch 134.
****************************************************************
train report:
              precision    recall  f1-score   support

           0     0.9044    0.9479    0.9256      6100
           1     0.9604    0.8898    0.9238       926
           2     0.8270    0.9363    0.8782      2400
           3     0.9287    0.8339    0.8787       843
           4     0.8612    0.9703    0.9125       774
           5     0.9168    0.9332    0.9249      1512
           6     0.7686    0.7218    0.7445      1330
           7     0.8172    0.8274    0.8223       481
           8     0.7853    0.8384    0.8110       458
           9     0.9237    0.9381    0.9308       452
          10     0.9424    0.8215    0.8778       717
          11     0.8822    0.7868    0.8317       333
          12     0.4000    0.0067    0.0132       299
          13     0.9252    0.5056    0.6538       269

    accuracy                         0.8804     16894
   macro avg     0.8459    0.7827    0.7949     16894
weighted avg     0.8735    0.8804    0.8713     16894

---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.7739    0.8505    0.8104      1525
           1     0.8145    0.7759    0.7947       232
           2     0.7129    0.7604    0.7359       601
           3     0.8370    0.7299    0.7797       211
           4     0.7546    0.8402    0.7951       194
           5     0.7817    0.7672    0.7744       378
           6     0.4779    0.4865    0.4821       333
           7     0.6371    0.6529    0.6449       121
           8     0.5268    0.5130    0.5198       115
           9     0.7658    0.7456    0.7556       114
          10     0.7536    0.5778    0.6541       180
          11     0.6029    0.4881    0.5395        84
          12     0.0000    0.0000    0.0000        75
          13     0.8077    0.3088    0.4468        68

    accuracy                         0.7308      4231
   macro avg     0.6605    0.6069    0.6238      4231
weighted avg     0.7188    0.7308    0.7213      4231

---------------------------------------
program finished.
