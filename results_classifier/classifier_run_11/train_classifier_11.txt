seed:  666
save trained model at:  ../trained_models/trained_classifier_model_11.pt
save loss at:  ./results/train_classifier_results_11.json
how to merge clusters:  [[0, 9, 12, 25], [1, 5, 11, 22], 2, [3, 8, 13], 4, 6, [7, 19, 21], [10, 16], 15, 17, 18, 20, 23, 24, 26, [27, 30], 28, 29]
features to use:  ['x', 'y', 'z', 'r', 'theta', 'phi', 'sasa', 'charge', 'hydrophobicity', 'binding_probability', 'sequence_entropy']
number of epochs:  200
learning rate decay at epoch:  [100, 170]
batch size:  256
number of workers to load data:  36
device:  cuda
number of classes after merging:  18
number of pockets in training set:  17515
number of pockets in validation set:  3747
number of pockets in test set:  3772
first 5 pockets in train set of cluster 0 before merging (to verify reproducibility):
['4gi7C01', '6djqD00', '8icoC00', '1ofhB00', '2zceA00']
first 5 pockets in val set of cluster 0 before merging (to verify reproducibility):
['1w46A00', '4j97A00', '4iqlB01', '2wtkB00', '6o7zA00']
first 5 pockets in test set of cluster 0 before merging (to verify reproducibility):
['1tlzA00', '5t5iH01', '5t48A00', '4q5hA00', '5gjcA00']
model architecture:
MoNet(
  (embedding_net): JKEmbeddingNet(
    (conv0): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=11, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=11, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn0): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=96, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=96, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn2): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): GINMolecularConv(nn=Sequential(
      (0): Linear(in_features=96, out_features=96, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=96, out_features=96, bias=True)
    ))(edge_transformer=Sequential(
      (0): Linear(in_features=1, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=8, out_features=96, bias=True)
      (3): ELU(alpha=1.0)
    ))
    (bn3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (set2set): Set2Set(96, 192)
  )
  (fc1): Linear(in_features=192, out_features=96, bias=True)
  (fc2): Linear(in_features=96, out_features=18, bias=True)
)
optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0.0005
)
initial gamma of FocalLoss:  0
increase gamma of FocalLoss at epochs:  [15, 80, 150]
loss function:
FocalLoss(gamma=0, alpha=1, reduction=mean)
begin training...
epoch: 1, train loss: 2.2961514434711, acc: 0.33325720810733656, val loss: 2.075808035649711, acc: 0.39738457432612756, test loss: 2.0811918856356835, acc: 0.3920996818663839
epoch: 2, train loss: 1.9867159806691745, acc: 0.40673708250071366, val loss: 1.9305031368946883, acc: 0.4133973845743261, test loss: 1.9337701109788072, acc: 0.42974549310710497
epoch: 3, train loss: 1.8557267570373095, acc: 0.44253497002569225, val loss: 2.016306396481321, acc: 0.4243394715772618, test loss: 1.99759163760431, acc: 0.4292152704135737
epoch: 4, train loss: 1.75278449658833, acc: 0.4760491007707679, val loss: 1.6756120603626048, acc: 0.5012009607686149, test loss: 1.6607974771477385, acc: 0.5013255567338282
epoch: 5, train loss: 1.6821063631437794, acc: 0.49426206109049386, val loss: 1.6948927524791644, acc: 0.48225246864157995, test loss: 1.6766793201988601, acc: 0.4899257688229056
epoch: 6, train loss: 1.6250837795944715, acc: 0.5127604910077077, val loss: 1.659148329952923, acc: 0.4945289564985322, test loss: 1.6405049235039526, acc: 0.49893955461293743
epoch: 7, train loss: 1.5837762481717768, acc: 0.527033970882101, val loss: 1.6121487069009048, acc: 0.5022684814518281, test loss: 1.593832549209433, acc: 0.5156415694591728
epoch: 8, train loss: 1.543953179741396, acc: 0.5308021695689409, val loss: 1.7106772424826662, acc: 0.4819855884707766, test loss: 1.6803683514053975, acc: 0.4923117709437964
epoch: 9, train loss: 1.5275044255415917, acc: 0.5357693405652298, val loss: 1.493225887739916, acc: 0.5412329863891113, test loss: 1.4794392198924675, acc: 0.5482502651113468
epoch: 10, train loss: 1.4946029604417135, acc: 0.5469597487867542, val loss: 1.6642233694843651, acc: 0.49799839871897517, test loss: 1.6648382718338062, acc: 0.4878048780487805
epoch: 11, train loss: 1.4553693604952533, acc: 0.5593491293177276, val loss: 2.3558904203949913, acc: 0.4101948225246864, test loss: 2.358355581317954, acc: 0.41542948038176036
epoch: 12, train loss: 1.4308101771798707, acc: 0.5636882671995432, val loss: 1.4845942682923399, acc: 0.5529757139044569, test loss: 1.4889315021632206, acc: 0.5633616118769883
epoch: 13, train loss: 1.42364850419268, acc: 0.5690550956323152, val loss: 1.5620142349027333, acc: 0.5246864157993061, test loss: 1.561613477047529, acc: 0.531813361611877
epoch: 14, train loss: 1.394966350716589, acc: 0.5747644875820725, val loss: 1.3800099317888148, acc: 0.5759274085935415, test loss: 1.369970874847056, acc: 0.5792682926829268
epoch 15, gamma increased to 1.
epoch: 15, train loss: 1.124770844551008, acc: 0.5838424207821867, val loss: 1.2187433807188777, acc: 0.5511075527088337, test loss: 1.2011937647583881, acc: 0.5577942735949099
epoch: 16, train loss: 1.1028218535535717, acc: 0.5944618898087354, val loss: 1.2436447068472243, acc: 0.5476381104883907, test loss: 1.238531808205905, acc: 0.5482502651113468
epoch: 17, train loss: 1.102726968921255, acc: 0.593034541821296, val loss: 1.1789249170612328, acc: 0.5543101147584735, test loss: 1.179276558652536, acc: 0.5630965005302226
epoch: 18, train loss: 1.0750853142873105, acc: 0.6047958892377961, val loss: 1.1829400315805216, acc: 0.5623165198825727, test loss: 1.1880364198067788, acc: 0.5564687168610817
epoch: 19, train loss: 1.0730489750436603, acc: 0.600799314872966, val loss: 1.3065678316146365, acc: 0.5193488123832399, test loss: 1.2836980503664794, acc: 0.5331389183457051
epoch: 20, train loss: 1.0542241559381182, acc: 0.6047958892377961, val loss: 1.5247795327300862, acc: 0.4552975713904457, test loss: 1.5178764463103083, acc: 0.46049840933191943
epoch: 21, train loss: 1.0349028536567613, acc: 0.6126177562089637, val loss: 1.2223364400138275, acc: 0.5508406725380304, test loss: 1.2206272949975217, acc: 0.556203605514316
epoch: 22, train loss: 1.049078702776901, acc: 0.6065087068227234, val loss: 1.295774418543522, acc: 0.5313584200693888, test loss: 1.3138445869862465, acc: 0.5246553552492047
epoch: 23, train loss: 1.0098987922544584, acc: 0.6188980873536968, val loss: 1.073701079890096, acc: 0.6012810248198559, test loss: 1.0620432034024252, acc: 0.6002120890774125
epoch: 24, train loss: 0.992892706418017, acc: 0.6250642306594347, val loss: 1.0511702498595556, acc: 0.6044835868694955, test loss: 1.0536968561396998, acc: 0.6113467656415694
epoch: 25, train loss: 0.9888967418207838, acc: 0.6301455894947188, val loss: 1.1159097115007883, acc: 0.5916733386709367, test loss: 1.1380537183262005, acc: 0.5890774125132555
epoch: 26, train loss: 0.9873570036758806, acc: 0.6296888381387382, val loss: 1.1798587719026743, acc: 0.5673872431278356, test loss: 1.1975795656347832, acc: 0.5691940615058324
epoch: 27, train loss: 0.9766271873640735, acc: 0.6296317442192406, val loss: 1.1496837531085455, acc: 0.5623165198825727, test loss: 1.1524160828595449, acc: 0.5593849416755037
epoch: 28, train loss: 0.9615493959404556, acc: 0.6332286611475878, val loss: 1.0873242439446145, acc: 0.5964771817453963, test loss: 1.0842122305994448, acc: 0.5941145281018028
epoch: 29, train loss: 0.9516415681289054, acc: 0.6382529260633742, val loss: 1.3674211942389198, acc: 0.5126768081131572, test loss: 1.3910071751219344, acc: 0.5204135737009544
epoch: 30, train loss: 0.9343792473762811, acc: 0.6421353125892092, val loss: 1.0962319675686838, acc: 0.5874032559380838, test loss: 1.111089829801881, acc: 0.5967656415694592
epoch: 31, train loss: 0.9269824685827174, acc: 0.6468170139880103, val loss: 1.0116973975069337, acc: 0.6282359220709901, test loss: 1.0305645288869913, acc: 0.6261930010604454
epoch: 32, train loss: 0.9157011354646784, acc: 0.6493291464459036, val loss: 0.9701643347708359, acc: 0.6362423271950894, test loss: 0.9781464046284098, acc: 0.6336161187698833
epoch: 33, train loss: 0.8942626680948038, acc: 0.6575506708535541, val loss: 1.3764408036235303, acc: 0.5134774486255671, test loss: 1.371998702481089, acc: 0.5055673382820784
epoch: 34, train loss: 0.9115934998009704, acc: 0.6507564944333428, val loss: 1.1660933956515798, acc: 0.5684547638110489, test loss: 1.16995498332593, acc: 0.5689289501590669
epoch: 35, train loss: 0.884765321715641, acc: 0.6597773337139594, val loss: 1.0394315515354788, acc: 0.6148919135308246, test loss: 1.0422594036555366, acc: 0.6163838812301167
epoch: 36, train loss: 0.8741779444628092, acc: 0.6621752783328575, val loss: 1.0747268882661876, acc: 0.5978115825994129, test loss: 1.1073111509341325, acc: 0.5972958642629904
epoch: 37, train loss: 0.8731094948570965, acc: 0.6651441621467313, val loss: 1.2180230571073885, acc: 0.5724579663730984, test loss: 1.2474935244401464, acc: 0.5623011664899258
epoch: 38, train loss: 0.8399587043210366, acc: 0.6744504710248359, val loss: 1.183838763553873, acc: 0.5812650120096077, test loss: 1.2173644705010875, acc: 0.5686638388123012
epoch: 39, train loss: 0.8427041603264521, acc: 0.6727376534399087, val loss: 1.0264804532504381, acc: 0.6274352815585802, test loss: 1.0496817418904472, acc: 0.6142629904559915
epoch: 40, train loss: 0.8490295475461025, acc: 0.66988295746503, val loss: 0.9524323173545347, acc: 0.6338404056578596, test loss: 0.9840279255034688, acc: 0.6306998939554613
epoch: 41, train loss: 0.8288355600789653, acc: 0.6769055095632315, val loss: 0.9502256029855419, acc: 0.6453162530024019, test loss: 0.9769800702941506, acc: 0.6391834570519618
epoch: 42, train loss: 0.8263059371510745, acc: 0.6755352554952897, val loss: 1.0280003753190745, acc: 0.6159594342140379, test loss: 1.03828935936678, acc: 0.626458112407211
epoch: 43, train loss: 0.807049323108378, acc: 0.6868969454753069, val loss: 0.9771082572756304, acc: 0.6341072858286629, test loss: 1.019060743702052, acc: 0.6322905620360552
epoch: 44, train loss: 0.8081771287481138, acc: 0.6825007136739937, val loss: 0.9477361206821292, acc: 0.6378436082199093, test loss: 0.9532190388106087, acc: 0.6527041357370096
epoch: 45, train loss: 0.8041998983552515, acc: 0.6847273765343991, val loss: 0.9381071538775007, acc: 0.6437149719775821, test loss: 0.9525513021968708, acc: 0.647932131495228
epoch: 46, train loss: 0.7985020929767239, acc: 0.6871824150727948, val loss: 1.1643081454553699, acc: 0.585001334400854, test loss: 1.2129496021210073, acc: 0.5723753976670202
epoch: 47, train loss: 0.8021090856627536, acc: 0.6808449900085641, val loss: 1.565963561865117, acc: 0.46970910061382437, test loss: 1.5897648096843084, acc: 0.46527041357370097
epoch: 48, train loss: 0.7853047872998529, acc: 0.690893519840137, val loss: 0.9335348347138811, acc: 0.6503869762476648, test loss: 0.9572055840416399, acc: 0.6524390243902439
epoch: 49, train loss: 0.7701873541797123, acc: 0.6918070225520982, val loss: 0.886537800988166, acc: 0.6658660261542567, test loss: 0.9265859855194739, acc: 0.6641039236479321
epoch: 50, train loss: 0.7609076313349032, acc: 0.7012846131886954, val loss: 1.0002558689483936, acc: 0.6391780090739259, test loss: 1.0134342047973504, acc: 0.6428950159066809
epoch: 51, train loss: 0.7648499076855785, acc: 0.6974593205823579, val loss: 0.9762452117659297, acc: 0.6314384841206299, test loss: 0.9798239673056254, acc: 0.633085896076352
epoch: 52, train loss: 0.7820746343645613, acc: 0.6922066799885812, val loss: 0.9317193936118133, acc: 0.6471844141980251, test loss: 0.9339842199647161, acc: 0.6622481442205727
epoch: 53, train loss: 0.7707056603498401, acc: 0.6931201827005424, val loss: 0.9781733170044337, acc: 0.6418468107819589, test loss: 1.0135587412125233, acc: 0.644220572640509
epoch: 54, train loss: 0.7287020047256139, acc: 0.7069369112189552, val loss: 0.91111897652328, acc: 0.6629303442754203, test loss: 0.9482281443170121, acc: 0.6654294803817603
epoch: 55, train loss: 0.726808029152889, acc: 0.7100770767913217, val loss: 1.001105556941077, acc: 0.636776087536696, test loss: 0.9978432561787416, acc: 0.6383881230116649
epoch: 56, train loss: 0.7483972118440574, acc: 0.703339994290608, val loss: 1.0224608819517162, acc: 0.6183613557512677, test loss: 1.0124743058092394, acc: 0.6277836691410392
epoch: 57, train loss: 0.7171806985059875, acc: 0.7150442477876107, val loss: 0.9217554453311999, acc: 0.6642647451294369, test loss: 0.9160255700634241, acc: 0.6773594909862142
epoch: 58, train loss: 0.7096043843730259, acc: 0.720753639737368, val loss: 0.8875739751321524, acc: 0.6770749933279957, test loss: 0.916850611362073, acc: 0.6686108165429481
epoch: 59, train loss: 0.6990433830517958, acc: 0.7218955181273194, val loss: 1.0654816955192776, acc: 0.6207632772884975, test loss: 1.1148023438478958, acc: 0.616914103923648
epoch: 60, train loss: 0.7242027427549196, acc: 0.7127604910077077, val loss: 1.0302655285824323, acc: 0.610088070456365, test loss: 1.074022031411892, acc: 0.6102863202545069
epoch: 61, train loss: 0.7136578177405262, acc: 0.7147016842706252, val loss: 1.0617529302398014, acc: 0.6135575126768081, test loss: 1.0762586773137146, acc: 0.6214209968186638
epoch: 62, train loss: 0.6889616515545922, acc: 0.7237796174707394, val loss: 0.8595916445426443, acc: 0.6821457165732586, test loss: 0.8764310596097944, acc: 0.6805408271474019
epoch: 63, train loss: 0.6715077434986821, acc: 0.7262346560091351, val loss: 0.9638374459434899, acc: 0.6506538564184681, test loss: 0.9645711915490484, acc: 0.6540296924708378
epoch: 64, train loss: 0.6898367506485138, acc: 0.7225235512417928, val loss: 1.0247277430861925, acc: 0.6386442487323192, test loss: 1.0363542945109394, acc: 0.6373276776246023
epoch: 65, train loss: 0.6849137327754766, acc: 0.7256066228946617, val loss: 1.029648212550448, acc: 0.6439818521483853, test loss: 1.088809690950784, acc: 0.6450159066808059
epoch: 66, train loss: 0.6757097384196638, acc: 0.7281758492720525, val loss: 0.898491106957221, acc: 0.6720042700827329, test loss: 0.9288661229648367, acc: 0.6736479321314952
epoch: 67, train loss: 0.6706969073796797, acc: 0.7288609763060234, val loss: 0.9095378940761455, acc: 0.6728049105951428, test loss: 0.8991640322028895, acc: 0.6829268292682927
epoch: 68, train loss: 0.6784682194845628, acc: 0.7293748215815016, val loss: 0.8907584478481629, acc: 0.6706698692287163, test loss: 0.9174557591799335, acc: 0.6651643690349947
epoch: 69, train loss: 0.652623102864367, acc: 0.7349129317727662, val loss: 0.9121748773708704, acc: 0.6730717907659461, test loss: 0.9567104435169305, acc: 0.6741781548250265
epoch: 70, train loss: 0.664548386334352, acc: 0.7301741364544676, val loss: 0.9560582163113099, acc: 0.6551908193221244, test loss: 0.9453685561773759, acc: 0.6617179215270413
epoch: 71, train loss: 0.6547816253205282, acc: 0.7346845560947759, val loss: 0.9745653710239309, acc: 0.6493194555644516, test loss: 1.0199056895388525, acc: 0.6500530222693531
epoch: 72, train loss: 0.6524584900131574, acc: 0.7355980588067371, val loss: 0.9481991007387208, acc: 0.6554576994929276, test loss: 0.9632679053354112, acc: 0.6572110286320254
epoch: 73, train loss: 0.6604985035796387, acc: 0.73788181558664, val loss: 0.9666549271190837, acc: 0.6530557779556979, test loss: 0.9858097880920701, acc: 0.6463414634146342
epoch: 74, train loss: 0.6601135680139865, acc: 0.7337710533828147, val loss: 0.8735228290486596, acc: 0.6765412329863891, test loss: 0.8850238123476063, acc: 0.6802757158006363
epoch: 75, train loss: 0.6474718385854857, acc: 0.7391378818155866, val loss: 0.8926450919875916, acc: 0.6765412329863891, test loss: 0.9200006669045506, acc: 0.6707317073170732
epoch: 76, train loss: 0.6258349372800881, acc: 0.7446759920068513, val loss: 0.9538656958332372, acc: 0.6575927408593542, test loss: 0.9608257422391582, acc: 0.6664899257688229
epoch: 77, train loss: 0.6126463153686926, acc: 0.7497002569226378, val loss: 0.829212000569185, acc: 0.6906858820389645, test loss: 0.8545172090621415, acc: 0.6967126193001061
epoch: 78, train loss: 0.6087982246714457, acc: 0.746445903511276, val loss: 1.1837492099850535, acc: 0.6170269548972511, test loss: 1.2230768977394793, acc: 0.6102863202545069
epoch: 79, train loss: 0.6211625125896172, acc: 0.7465600913502712, val loss: 0.920533810674842, acc: 0.669602348545503, test loss: 0.9747998208787256, acc: 0.66118769883351
epoch 80, gamma increased to 2.
epoch: 80, train loss: 0.5082282038508161, acc: 0.7429060805024265, val loss: 0.8413255224871833, acc: 0.6546570589805177, test loss: 0.8716406938506336, acc: 0.6492576882290562
epoch: 81, train loss: 0.49325428307822117, acc: 0.7494147873251499, val loss: 0.7623756140557995, acc: 0.6586602615425674, test loss: 0.7995807634052264, acc: 0.6667550371155886
epoch: 82, train loss: 0.47713173891250454, acc: 0.7502141021981159, val loss: 0.7906826002489002, acc: 0.6701361088871097, test loss: 0.7833994691470522, acc: 0.6855779427359491
epoch: 83, train loss: 0.4878706907701941, acc: 0.7446188980873537, val loss: 0.7584212834847142, acc: 0.6728049105951428, test loss: 0.782495164416122, acc: 0.6765641569459173
epoch: 84, train loss: 0.48639206627148274, acc: 0.7459320582357979, val loss: 0.6971290526404074, acc: 0.6965572457966374, test loss: 0.7273556943410022, acc: 0.6967126193001061
epoch: 85, train loss: 0.46939410062779574, acc: 0.7526691407365116, val loss: 0.8202702359641619, acc: 0.6565252201761409, test loss: 0.8336483127618266, acc: 0.6548250265111347
epoch: 86, train loss: 0.46311568547717646, acc: 0.7552954610334, val loss: 0.7352024804823616, acc: 0.6818788364024553, test loss: 0.7705335105084918, acc: 0.676829268292683
epoch: 87, train loss: 0.4744014179675946, acc: 0.753011704253497, val loss: 0.8748736006118534, acc: 0.6263677608753669, test loss: 0.889444039709874, acc: 0.6272534464475079
epoch: 88, train loss: 0.46881530279776995, acc: 0.7524978589780188, val loss: 0.8261879182097306, acc: 0.6581265012009607, test loss: 0.8548619814161784, acc: 0.6595970307529162
epoch: 89, train loss: 0.4636999786427591, acc: 0.7539252069654582, val loss: 0.7471699756655911, acc: 0.6813450760608487, test loss: 0.7769585654439755, acc: 0.6826617179215271
epoch: 90, train loss: 0.4776234971176989, acc: 0.7476448758207251, val loss: 0.7862286294082467, acc: 0.6578596210301575, test loss: 0.8429494756150473, acc: 0.6638388123011665
epoch: 91, train loss: 0.4667103779714107, acc: 0.7516414501855553, val loss: 0.7478202021786904, acc: 0.6744061916199626, test loss: 0.7916321300625927, acc: 0.6778897136797455
epoch: 92, train loss: 0.4794672208855978, acc: 0.7490722238081644, val loss: 0.7943559193693863, acc: 0.6690685882038965, test loss: 0.8055643389359006, acc: 0.6672852598091198
epoch: 93, train loss: 0.476782278105289, acc: 0.7490722238081644, val loss: 0.8716742871505659, acc: 0.6327728849746463, test loss: 0.9145697434911799, acc: 0.6312301166489925
epoch: 94, train loss: 0.45274459599019323, acc: 0.7562089637453612, val loss: 0.6994494275471546, acc: 0.6944222044302109, test loss: 0.7222170150924715, acc: 0.7006892895015907
epoch: 95, train loss: 0.487040052616763, acc: 0.7430202683414217, val loss: 0.7493374384591699, acc: 0.6770749933279957, test loss: 0.797691402733642, acc: 0.6646341463414634
epoch: 96, train loss: 0.48960637800563517, acc: 0.7395375392520697, val loss: 1.1491736569085182, acc: 0.5511075527088337, test loss: 1.173207369993499, acc: 0.5516967126193001
epoch: 97, train loss: 0.4853414640336795, acc: 0.7474735940622323, val loss: 0.7404784694620219, acc: 0.6736055511075527, test loss: 0.7554324891888324, acc: 0.6749734888653235
epoch: 98, train loss: 0.4512342532855798, acc: 0.7559805880673708, val loss: 0.7489238638606809, acc: 0.6837469975980784, test loss: 0.7946334958455722, acc: 0.676033934252386
epoch: 99, train loss: 0.4420054637905941, acc: 0.7652298030259778, val loss: 0.8374716779980177, acc: 0.673872431278356, test loss: 0.8892033676580305, acc: 0.6694061505832449
epoch: 100, train loss: 0.3639692872413866, acc: 0.795318298601199, val loss: 0.657001286753025, acc: 0.7208433413397385, test loss: 0.7005242348222945, acc: 0.7314422057264051
epoch: 101, train loss: 0.3247913634167921, acc: 0.8126748501284613, val loss: 0.7252508161352386, acc: 0.7002935681878837, test loss: 0.7561413336659792, acc: 0.7075821845174973
epoch: 102, train loss: 0.30445969652149496, acc: 0.8157579217813303, val loss: 0.7361446031100087, acc: 0.7155057379236722, test loss: 0.784245207479119, acc: 0.7107635206786851
epoch: 103, train loss: 0.323294815709514, acc: 0.8094204967170996, val loss: 0.7999487532467279, acc: 0.6845476381104884, test loss: 0.8372824122518902, acc: 0.6977730646871686
epoch: 104, train loss: 0.3086781880854199, acc: 0.8139309163574079, val loss: 0.8542920871833372, acc: 0.6837469975980784, test loss: 0.8694258231752624, acc: 0.686373276776246
epoch: 105, train loss: 0.30347244721070854, acc: 0.8178133028832429, val loss: 0.7705901928955884, acc: 0.6973578863090473, test loss: 0.8025284405098487, acc: 0.7062566277836692
epoch: 106, train loss: 0.29639413097382544, acc: 0.815129888666857, val loss: 0.8103791203599374, acc: 0.6896183613557513, test loss: 0.8707545127241634, acc: 0.6993637327677624
epoch: 107, train loss: 0.30052886374875404, acc: 0.8165572366542964, val loss: 0.7800128450067896, acc: 0.6933546837469976, test loss: 0.8379379843350812, acc: 0.6996288441145281
epoch: 108, train loss: 0.295135037777528, acc: 0.8184413359977163, val loss: 0.748358294395628, acc: 0.7104350146784094, test loss: 0.8126184339867164, acc: 0.704931071049841
epoch: 109, train loss: 0.3039076897405809, acc: 0.8079360548101627, val loss: 0.9951826264325669, acc: 0.6423805711235655, test loss: 1.038942470024651, acc: 0.6481972428419936
epoch: 110, train loss: 0.28913208709898386, acc: 0.8231230373965173, val loss: 0.7663900985188379, acc: 0.6957566052842273, test loss: 0.8181751138458575, acc: 0.7059915164369035
epoch: 111, train loss: 0.3187685443372887, acc: 0.8139309163574079, val loss: 0.8652881714449522, acc: 0.6709367493995196, test loss: 0.8748680672994474, acc: 0.6749734888653235
epoch: 112, train loss: 0.29738358315691893, acc: 0.8170710819297745, val loss: 0.763916346242277, acc: 0.6992260475046704, test loss: 0.8005747979670794, acc: 0.7051961823966065
epoch: 113, train loss: 0.2668352330850595, acc: 0.8288324293462747, val loss: 0.8159478245527229, acc: 0.6946890846010142, test loss: 0.8489573395012039, acc: 0.6990986214209968
epoch: 114, train loss: 0.27596301277671376, acc: 0.8246074793034541, val loss: 0.8789616115639806, acc: 0.6754737123031759, test loss: 0.90300885895277, acc: 0.6834570519618239
epoch: 115, train loss: 0.28294772140611557, acc: 0.8250642306594348, val loss: 0.8768043906204472, acc: 0.6896183613557513, test loss: 0.866278992225105, acc: 0.6874337221633086
epoch: 116, train loss: 0.2734847704291582, acc: 0.829974307736226, val loss: 0.7821202773172314, acc: 0.7021617293835068, test loss: 0.7984322342361904, acc: 0.7099681866383881
epoch: 117, train loss: 0.2726040151709323, acc: 0.8295175563802455, val loss: 0.8152442645288512, acc: 0.6944222044302109, test loss: 0.8586066464477733, acc: 0.6961823966065748
epoch: 118, train loss: 0.2829796968240109, acc: 0.8225520982015415, val loss: 0.7934110507603674, acc: 0.7066986922871631, test loss: 0.811589488295457, acc: 0.7213679745493107
epoch: 119, train loss: 0.2615157834655926, acc: 0.8340850699400514, val loss: 0.7796889557531748, acc: 0.7173738991192954, test loss: 0.8436808692309156, acc: 0.7083775185577943
epoch: 120, train loss: 0.26539526577680544, acc: 0.8299172138167286, val loss: 0.8767446731611159, acc: 0.6904190018681612, test loss: 0.8824152026171396, acc: 0.7017497348886532
epoch: 121, train loss: 0.2902740461160005, acc: 0.8182129603197259, val loss: 0.8370397802286222, acc: 0.680811315719242, test loss: 0.8715959439727686, acc: 0.6871686108165429
epoch: 122, train loss: 0.2729391909176508, acc: 0.8247787610619469, val loss: 0.8477965313687718, acc: 0.692020282892981, test loss: 0.8761359621333166, acc: 0.6821314952279958
epoch: 123, train loss: 0.2899007546979973, acc: 0.8185555238367114, val loss: 0.7863147654532113, acc: 0.7008273285294903, test loss: 0.8233756404666355, acc: 0.6996288441145281
epoch: 124, train loss: 0.26638281473322595, acc: 0.827576363117328, val loss: 0.8042513958002238, acc: 0.7029623698959168, test loss: 0.8080572486555841, acc: 0.7142099681866384
epoch: 125, train loss: 0.24242954948715506, acc: 0.8397944618898088, val loss: 0.8346070750478047, acc: 0.6957566052842273, test loss: 0.8456907717319461, acc: 0.7030752916224814
epoch: 126, train loss: 0.25683103399960067, acc: 0.8312874678846702, val loss: 0.810419550888819, acc: 0.7072324526287697, test loss: 0.81522036420452, acc: 0.7112937433722163
epoch: 127, train loss: 0.2759921617671962, acc: 0.8243791036254638, val loss: 0.8061311815400298, acc: 0.7000266880170803, test loss: 0.8320263499040492, acc: 0.70864262990456
epoch: 128, train loss: 0.24436332902209337, acc: 0.8387096774193549, val loss: 0.8194272057482616, acc: 0.7064318121163597, test loss: 0.8510741502331018, acc: 0.7099681866383881
epoch: 129, train loss: 0.24027820134074424, acc: 0.8413359977162432, val loss: 0.8477781697340192, acc: 0.6954897251134241, test loss: 0.8477236721447391, acc: 0.7051961823966065
epoch: 130, train loss: 0.25134289196958004, acc: 0.8351127604910077, val loss: 0.9836456986914198, acc: 0.678142514011209, test loss: 0.9372328920556531, acc: 0.7009544008483564
epoch: 131, train loss: 0.2725623916174186, acc: 0.8211247502141023, val loss: 0.8939519973637297, acc: 0.6906858820389645, test loss: 0.8999532012393341, acc: 0.6998939554612937
epoch: 132, train loss: 0.2595914362108507, acc: 0.8302597773337139, val loss: 0.873687649905793, acc: 0.6904190018681612, test loss: 0.8990855070338649, acc: 0.6993637327677624
epoch: 133, train loss: 0.23536952380755613, acc: 0.8395660862118185, val loss: 0.8633111441393104, acc: 0.699759807846277, test loss: 0.8896624599509376, acc: 0.7112937433722163
epoch: 134, train loss: 0.23762080422340717, acc: 0.8399657436483015, val loss: 0.8888570212477965, acc: 0.685882038964505, test loss: 0.899381969033218, acc: 0.6983032873806999
epoch: 135, train loss: 0.24800617859052515, acc: 0.8405366828432772, val loss: 0.8665973851927, acc: 0.6994929276754737, test loss: 0.8433746699943017, acc: 0.70864262990456
epoch: 136, train loss: 0.2535161216804173, acc: 0.8311732800456751, val loss: 0.8703902618771144, acc: 0.6893514811849479, test loss: 0.9167942818755942, acc: 0.6964475079533404
epoch: 137, train loss: 0.23800272062067776, acc: 0.8408221524407651, val loss: 0.9988647038566929, acc: 0.671203629570323, test loss: 0.9913286437158999, acc: 0.6675503711558854
epoch: 138, train loss: 0.2554728690952769, acc: 0.8304310590922067, val loss: 0.8599645676598856, acc: 0.699759807846277, test loss: 0.8982057925372968, acc: 0.7012195121951219
epoch: 139, train loss: 0.22717264904283027, acc: 0.8454467599200685, val loss: 0.8704532209892161, acc: 0.6912196423805711, test loss: 0.8922734670143492, acc: 0.6980381760339343
epoch: 140, train loss: 0.2300665445781047, acc: 0.8432200970596632, val loss: 0.9447355353739982, acc: 0.6802775553776355, test loss: 0.941919192529559, acc: 0.6874337221633086
epoch: 141, train loss: 0.23896704904358623, acc: 0.8421353125892093, val loss: 0.8470824475857236, acc: 0.7066986922871631, test loss: 0.8679181639488277, acc: 0.7083775185577943
epoch: 142, train loss: 0.2172880710396398, acc: 0.8505281187553525, val loss: 0.8710140469935153, acc: 0.7024286095543101, test loss: 0.8756268234293383, acc: 0.7097030752916225
epoch: 143, train loss: 0.22477521831704111, acc: 0.8448187268055952, val loss: 0.8586908957784514, acc: 0.7093674939951962, test loss: 0.87591432336785, acc: 0.7197773064687168
epoch: 144, train loss: 0.21978432693857144, acc: 0.8491578646874108, val loss: 0.8444196946276707, acc: 0.7144382172404591, test loss: 0.8663996507355416, acc: 0.71898197242842
epoch: 145, train loss: 0.2095909853331062, acc: 0.8499571795603769, val loss: 0.9058790096194387, acc: 0.7010942087002936, test loss: 0.9383912961278209, acc: 0.7099681866383881
epoch: 146, train loss: 0.21202470476614418, acc: 0.8507564944333429, val loss: 0.9176528246395741, acc: 0.6949559647718174, test loss: 0.9408216259006731, acc: 0.7022799575821845
epoch: 147, train loss: 0.23517597696456508, acc: 0.8391664287753354, val loss: 0.8861357629983178, acc: 0.688550840672538, test loss: 0.9163698100841438, acc: 0.6959172852598091
epoch: 148, train loss: 0.22427590796343777, acc: 0.8431630031401656, val loss: 0.9347981781777555, acc: 0.685882038964505, test loss: 0.9743481415579706, acc: 0.6890243902439024
epoch: 149, train loss: 0.21425627004817796, acc: 0.8500142734798743, val loss: 0.8296101108563242, acc: 0.7000266880170803, test loss: 0.8734514189929496, acc: 0.7120890774125133
epoch 150, gamma increased to 3.
epoch: 150, train loss: 0.1723683828835211, acc: 0.8449329146445903, val loss: 0.7412847437666421, acc: 0.6994929276754737, test loss: 0.7553979212759914, acc: 0.7102332979851538
epoch: 151, train loss: 0.15587227108425458, acc: 0.8509848701113332, val loss: 0.7698340576300151, acc: 0.6965572457966374, test loss: 0.7813280300017, acc: 0.707051961823966
epoch: 152, train loss: 0.14984418565873722, acc: 0.8529260633742506, val loss: 0.7866608711252348, acc: 0.6976247664798505, test loss: 0.8091588619531528, acc: 0.7038706256627784
epoch: 153, train loss: 0.16990736192919137, acc: 0.8439623180131316, val loss: 0.745296570724699, acc: 0.6970910061382439, test loss: 0.7637988653820115, acc: 0.7120890774125133
epoch: 154, train loss: 0.17861265475124485, acc: 0.8405937767627748, val loss: 0.8725737646035204, acc: 0.6543901788097144, test loss: 0.9094117104943964, acc: 0.66118769883351
epoch: 155, train loss: 0.164522852112047, acc: 0.8445332572081073, val loss: 0.7793371874012882, acc: 0.6949559647718174, test loss: 0.8130360769694582, acc: 0.6922057264050901
epoch: 156, train loss: 0.16599007480712133, acc: 0.8451612903225807, val loss: 0.7856543748767528, acc: 0.7024286095543101, test loss: 0.8096268998729589, acc: 0.6951219512195121
epoch: 157, train loss: 0.17519743507618976, acc: 0.8389380530973451, val loss: 0.7282206229219317, acc: 0.70402989057913, test loss: 0.7495954121410657, acc: 0.7073170731707317
epoch: 158, train loss: 0.15829975564174234, acc: 0.8503568369968598, val loss: 0.7521837776363262, acc: 0.7029623698959168, test loss: 0.7789693183383153, acc: 0.6990986214209968
epoch: 159, train loss: 0.15849403451501795, acc: 0.8458464173565515, val loss: 0.7538900864992837, acc: 0.7149719775820657, test loss: 0.7692460278058988, acc: 0.7134146341463414
epoch: 160, train loss: 0.15881136032536544, acc: 0.8431630031401656, val loss: 0.7880023915448887, acc: 0.6880170803309315, test loss: 0.824069281524464, acc: 0.6924708377518558
epoch: 161, train loss: 0.1786994348916877, acc: 0.8350556665715101, val loss: 0.8527502100850093, acc: 0.6848145182812917, test loss: 0.8150817365686815, acc: 0.6914103923647932
epoch: 162, train loss: 0.14710473120467785, acc: 0.8494433342848987, val loss: 0.7987006344984842, acc: 0.692820923405391, test loss: 0.8254255411607211, acc: 0.7038706256627784
epoch: 163, train loss: 0.1400500132480826, acc: 0.8592634884384813, val loss: 0.7940889553671237, acc: 0.7072324526287697, test loss: 0.8187000046099989, acc: 0.7171261930010604
epoch: 164, train loss: 0.14861369033796326, acc: 0.8470453896660006, val loss: 0.7833663614266454, acc: 0.6989591673338671, test loss: 0.7883081015195472, acc: 0.7067868504772005
epoch: 165, train loss: 0.17174698670795907, acc: 0.8410505281187554, val loss: 0.8084855225998019, acc: 0.6973578863090473, test loss: 0.8408304634933633, acc: 0.700424178154825
epoch: 166, train loss: 0.18431100158598843, acc: 0.8353982300884956, val loss: 0.9683294825023226, acc: 0.6250333600213505, test loss: 1.038189901147642, acc: 0.619034994697773
epoch: 167, train loss: 0.17111176672983675, acc: 0.83882386525835, val loss: 0.7697060017831363, acc: 0.70402989057913, test loss: 0.7925650577909241, acc: 0.7102332979851538
epoch: 168, train loss: 0.15069855125910753, acc: 0.8484156437339423, val loss: 0.8502406713515241, acc: 0.6784093941820123, test loss: 0.885532513655888, acc: 0.6855779427359491
epoch: 169, train loss: 0.13950761449599042, acc: 0.8557807593491293, val loss: 0.8410404782820804, acc: 0.6864157993061115, test loss: 0.8788916443969387, acc: 0.6911452810180275
epoch: 170, train loss: 0.11315047753017152, acc: 0.8726805595204111, val loss: 0.8098218577494136, acc: 0.7157726180944756, test loss: 0.8407019964584237, acc: 0.7107635206786851
epoch: 171, train loss: 0.08826224619095713, acc: 0.8902083928061661, val loss: 0.8358479138212201, acc: 0.7112356551908193, test loss: 0.8532473687023272, acc: 0.7234888653234358
epoch: 172, train loss: 0.08060396589442999, acc: 0.8954610333999429, val loss: 0.8323337201726001, acc: 0.7163063784360822, test loss: 0.8661262302611059, acc: 0.7165959703075292
epoch: 173, train loss: 0.07843727384155014, acc: 0.8964887239508992, val loss: 0.8397552310418281, acc: 0.718708299973312, test loss: 0.8908646134531257, acc: 0.7237539766702015
epoch: 174, train loss: 0.07739678515903513, acc: 0.8980302597773338, val loss: 0.8292720033418092, acc: 0.7189751801441153, test loss: 0.8896462410403967, acc: 0.7237539766702015
epoch: 175, train loss: 0.07114525743418001, acc: 0.9028261490151299, val loss: 0.8531504890140547, acc: 0.7181745396317054, test loss: 0.89816543254468, acc: 0.7250795334040296
epoch: 176, train loss: 0.06513401710311584, acc: 0.9083642592063945, val loss: 0.8895902086455949, acc: 0.7200427008273286, test loss: 0.9006020362910638, acc: 0.71898197242842
epoch: 177, train loss: 0.07225715267064399, acc: 0.9045389666000571, val loss: 0.8997091068596658, acc: 0.7136375767280491, test loss: 0.9550802700087223, acc: 0.7123541887592789
epoch: 178, train loss: 0.07182416444718243, acc: 0.9034541821296032, val loss: 0.9116096915898401, acc: 0.7123031758740326, test loss: 0.9415998235107859, acc: 0.7208377518557795
epoch: 179, train loss: 0.08418373637903155, acc: 0.8921495860690837, val loss: 0.8677193689639008, acc: 0.7104350146784094, test loss: 0.9077280452623094, acc: 0.711558854718982
epoch: 180, train loss: 0.0818682741547121, acc: 0.8956894090779332, val loss: 0.8923195752265264, acc: 0.7058980517747532, test loss: 0.8898361841669011, acc: 0.7213679745493107
epoch: 181, train loss: 0.07707248385437311, acc: 0.8994005138452755, val loss: 0.9333232477374925, acc: 0.6893514811849479, test loss: 0.9544857029950252, acc: 0.6980381760339343
epoch: 182, train loss: 0.0882210626683506, acc: 0.888210105623751, val loss: 0.8881816528051543, acc: 0.7008273285294903, test loss: 0.9261519860867352, acc: 0.7128844114528102
epoch: 183, train loss: 0.08987404585646079, acc: 0.8889523265772196, val loss: 0.9079624290685193, acc: 0.7091006138243928, test loss: 0.9374356494855021, acc: 0.703340402969247
epoch: 184, train loss: 0.09028808422133544, acc: 0.8907793320011419, val loss: 0.8619254121215368, acc: 0.7077662129703763, test loss: 0.9003877471638636, acc: 0.7091728525980912
epoch: 185, train loss: 0.07281188100933722, acc: 0.9004282043962318, val loss: 0.873421824420647, acc: 0.7165732586068855, test loss: 0.9299894514417598, acc: 0.7158006362672322
epoch: 186, train loss: 0.07238877278407267, acc: 0.905224093634028, val loss: 0.9008584613000866, acc: 0.7085668534827863, test loss: 0.9476773099201482, acc: 0.7118239660657476
epoch: 187, train loss: 0.07408650918514634, acc: 0.8991721381672851, val loss: 0.9073946173107716, acc: 0.7088337336535896, test loss: 0.9384563482452425, acc: 0.7128844114528102
epoch: 188, train loss: 0.08474143401251974, acc: 0.8903225806451613, val loss: 0.9042365777514794, acc: 0.6893514811849479, test loss: 0.9183032578400564, acc: 0.7038706256627784
epoch: 189, train loss: 0.09490603124656373, acc: 0.8858692549243505, val loss: 0.8933320302533123, acc: 0.7042967707499332, test loss: 0.9143853071259289, acc: 0.7136797454931071
epoch: 190, train loss: 0.08426529701236586, acc: 0.8956323151584357, val loss: 0.924549558559099, acc: 0.7029623698959168, test loss: 0.9883741156799543, acc: 0.7038706256627784
epoch: 191, train loss: 0.10947110713498101, acc: 0.8779332001141879, val loss: 0.8229998059039884, acc: 0.7117694155324259, test loss: 0.8544899250517973, acc: 0.7099681866383881
epoch: 192, train loss: 0.08799534001130566, acc: 0.8886097630602341, val loss: 0.8839863857842459, acc: 0.7021617293835068, test loss: 0.8992873420138879, acc: 0.7110286320254506
epoch: 193, train loss: 0.07456595830208841, acc: 0.9005994861547245, val loss: 0.8952996250404051, acc: 0.7058980517747532, test loss: 0.9126375371299897, acc: 0.7057264050901378
epoch: 194, train loss: 0.07468677306946025, acc: 0.8992863260062803, val loss: 0.9043444436424282, acc: 0.6978916466506538, test loss: 0.9526076783304629, acc: 0.6998939554612937
epoch: 195, train loss: 0.06806624304634008, acc: 0.9023123037396518, val loss: 0.9423458396768074, acc: 0.7013610888710968, test loss: 0.9435612853233281, acc: 0.7142099681866384
epoch: 196, train loss: 0.07968480512393464, acc: 0.8971738509848701, val loss: 0.9941495309869925, acc: 0.6856151587937016, test loss: 1.0569785298623564, acc: 0.690880169671262
epoch: 197, train loss: 0.11068210375332813, acc: 0.8769055095632315, val loss: 0.8564462476264072, acc: 0.7034961302375233, test loss: 0.8943032854308759, acc: 0.7083775185577943
epoch: 198, train loss: 0.08740276297160363, acc: 0.8911789894376249, val loss: 0.8816541793475572, acc: 0.70322925006672, test loss: 0.9168969998304057, acc: 0.7062566277836692
epoch: 199, train loss: 0.07539711971683499, acc: 0.8984299172138167, val loss: 0.8738959832034939, acc: 0.7173738991192954, test loss: 0.9113962533997326, acc: 0.7213679745493107
epoch: 200, train loss: 0.07456273849103881, acc: 0.9030545246931202, val loss: 0.9421664224741203, acc: 0.6944222044302109, test loss: 0.9504738076494202, acc: 0.7067868504772005
best val acc 0.7208433413397385 at epoch 100.
****************************************************************
/opt/python/anaconda-2020.7/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
train report:
              precision    recall  f1-score   support

           0     0.9237    0.9638    0.9433      5337
           1     0.7258    0.7934    0.7581      2502
           2     0.9766    0.8765    0.9239       810
           3     0.8086    0.8310    0.8196      1840
           4     0.9488    0.8548    0.8994       737
           5     0.8475    0.9852    0.9112       677
           6     0.7752    0.9123    0.8382      1323
           7     0.6006    0.7178    0.6539       907
           8     0.8786    0.8599    0.8691       421
           9     0.8934    0.8778    0.8855       401
          10     0.8810    0.9722    0.9244       396
          11     0.9907    0.9578    0.9740       332
          12     0.8175    0.7288    0.7706       295
          13     0.9018    0.8522    0.8763       291
          14     0.0000    0.0000    0.0000       261
          15     0.9420    0.2632    0.4114       494
          16     0.6613    0.1602    0.2579       256
          17     0.8593    0.7277    0.7880       235

    accuracy                         0.8418     17515
   macro avg     0.8018    0.7408    0.7503     17515
weighted avg     0.8344    0.8418    0.8288     17515

train confusion matrix:
[[9.63837362e-01 1.10548998e-02 3.74742365e-04 2.99793892e-03
  0.00000000e+00 1.87371182e-04 1.87371182e-04 5.43376429e-03
  9.36855912e-03 2.81056773e-03 3.74742365e-04 0.00000000e+00
  1.87371182e-04 1.68634064e-03 0.00000000e+00 0.00000000e+00
  1.87371182e-04 1.31159828e-03]
 [3.59712230e-02 7.93365308e-01 7.99360512e-04 3.75699440e-02
  8.79296563e-03 7.99360512e-04 8.67306155e-02 2.63788969e-02
  0.00000000e+00 4.79616307e-03 0.00000000e+00 0.00000000e+00
  1.99840128e-03 3.99680256e-04 0.00000000e+00 3.99680256e-04
  0.00000000e+00 1.99840128e-03]
 [7.40740741e-03 9.87654321e-03 8.76543210e-01 0.00000000e+00
  0.00000000e+00 9.62962963e-02 3.70370370e-03 3.70370370e-03
  0.00000000e+00 1.23456790e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.23456790e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.47826087e-02 1.14130435e-01 5.43478261e-04 8.30978261e-01
  1.08695652e-03 5.43478261e-04 5.43478261e-04 1.30434783e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.08695652e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.71739130e-03
  0.00000000e+00 5.43478261e-04]
 [0.00000000e+00 7.59837178e-02 0.00000000e+00 5.42740841e-03
  8.54816825e-01 0.00000000e+00 1.35685210e-03 3.12075984e-02
  0.00000000e+00 0.00000000e+00 1.08548168e-02 0.00000000e+00
  1.62822252e-02 2.71370421e-03 0.00000000e+00 0.00000000e+00
  1.35685210e-03 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 5.90841950e-03 0.00000000e+00
  0.00000000e+00 9.85228951e-01 8.86262925e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.77928949e-03 4.30839002e-02 2.26757370e-03 7.55857899e-04
  7.55857899e-04 2.26757370e-02 9.12320484e-01 4.53514739e-03
  0.00000000e+00 1.51171580e-03 7.55857899e-04 0.00000000e+00
  3.02343159e-03 3.77928949e-03 0.00000000e+00 7.55857899e-04
  0.00000000e+00 0.00000000e+00]
 [6.06394708e-02 1.16868798e-01 0.00000000e+00 6.61521499e-03
  2.20507166e-03 1.10253583e-03 1.32304300e-02 7.17750827e-01
  0.00000000e+00 0.00000000e+00 2.53583241e-02 0.00000000e+00
  2.42557883e-02 2.20507166e-03 0.00000000e+00 0.00000000e+00
  1.76405733e-02 1.21278942e-02]
 [1.18764846e-01 7.12589074e-03 2.37529691e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  8.59857482e-01 7.12589074e-03 2.37529691e-03 0.00000000e+00
  0.00000000e+00 2.37529691e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [8.72817955e-02 2.24438903e-02 2.49376559e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 4.98753117e-03 2.49376559e-03
  0.00000000e+00 8.77805486e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.49376559e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [7.57575758e-03 2.52525253e-03 0.00000000e+00 2.52525253e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 7.57575758e-03
  0.00000000e+00 0.00000000e+00 9.72222222e-01 0.00000000e+00
  0.00000000e+00 7.57575758e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.02409639e-03 2.10843373e-02 3.01204819e-03 0.00000000e+00
  3.01204819e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.03614458e-03 0.00000000e+00 9.57831325e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.35593220e-02 3.72881356e-02 6.77966102e-03 0.00000000e+00
  1.01694915e-02 3.38983051e-03 6.77966102e-03 1.72881356e-01
  0.00000000e+00 0.00000000e+00 1.69491525e-02 0.00000000e+00
  7.28813559e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.38983051e-03 0.00000000e+00]
 [8.59106529e-02 6.87285223e-03 0.00000000e+00 0.00000000e+00
  3.43642612e-03 0.00000000e+00 1.37457045e-02 3.43642612e-03
  0.00000000e+00 2.06185567e-02 0.00000000e+00 3.43642612e-03
  1.03092784e-02 8.52233677e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.57088123e-01 6.16858238e-01 0.00000000e+00 1.41762452e-01
  7.66283525e-03 0.00000000e+00 3.83141762e-03 5.74712644e-02
  0.00000000e+00 0.00000000e+00 1.14942529e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.83141762e-03
  0.00000000e+00 0.00000000e+00]
 [1.61943320e-02 9.91902834e-02 0.00000000e+00 4.06882591e-01
  0.00000000e+00 1.01214575e-02 2.00404858e-01 2.02429150e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.02429150e-03 0.00000000e+00 2.63157895e-01
  0.00000000e+00 0.00000000e+00]
 [8.98437500e-02 3.12500000e-02 0.00000000e+00 7.81250000e-03
  0.00000000e+00 0.00000000e+00 3.90625000e-03 6.52343750e-01
  0.00000000e+00 0.00000000e+00 3.12500000e-02 0.00000000e+00
  3.90625000e-03 3.90625000e-03 0.00000000e+00 0.00000000e+00
  1.60156250e-01 1.56250000e-02]
 [5.95744681e-02 1.27659574e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 4.25531915e-03 0.00000000e+00 1.82978723e-01
  0.00000000e+00 0.00000000e+00 4.25531915e-03 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  8.51063830e-03 7.27659574e-01]]
---------------------------------------
validation report:
              precision    recall  f1-score   support

           0     0.8166    0.8880    0.8508      1143
           1     0.5814    0.6530    0.6151       536
           2     0.8828    0.7399    0.8050       173
           3     0.7021    0.6878    0.6949       394
           4     0.8692    0.7152    0.7847       158
           5     0.7529    0.9034    0.8213       145
           6     0.6796    0.8021    0.7358       283
           7     0.4340    0.5258    0.4755       194
           8     0.8133    0.6778    0.7394        90
           9     0.6000    0.6000    0.6000        85
          10     0.7917    0.9048    0.8444        84
          11     0.9649    0.7746    0.8594        71
          12     0.6222    0.4444    0.5185        63
          13     0.6552    0.6129    0.6333        62
          14     0.0000    0.0000    0.0000        56
          15     0.7407    0.1905    0.3030       105
          16     0.7778    0.1273    0.2187        55
          17     0.6087    0.5600    0.5833        50

    accuracy                         0.7208      3747
   macro avg     0.6830    0.6004    0.6157      3747
weighted avg     0.7172    0.7208    0.7081      3747

validation confusion matrix:
[[8.88013998e-01 3.32458443e-02 2.62467192e-03 1.74978128e-02
  8.74890639e-04 1.74978128e-03 2.62467192e-03 1.74978128e-02
  1.04986877e-02 1.13735783e-02 2.62467192e-03 0.00000000e+00
  1.74978128e-03 6.12423447e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 3.49956255e-03]
 [9.70149254e-02 6.52985075e-01 9.32835821e-03 7.08955224e-02
  1.11940299e-02 3.73134328e-03 8.20895522e-02 5.22388060e-02
  1.86567164e-03 9.32835821e-03 0.00000000e+00 0.00000000e+00
  3.73134328e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.59701493e-03]
 [2.89017341e-02 5.78034682e-03 7.39884393e-01 0.00000000e+00
  0.00000000e+00 1.56069364e-01 4.04624277e-02 5.78034682e-03
  0.00000000e+00 5.78034682e-03 0.00000000e+00 5.78034682e-03
  1.15606936e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [7.36040609e-02 1.57360406e-01 2.53807107e-03 6.87817259e-01
  2.53807107e-03 0.00000000e+00 5.07614213e-03 3.04568528e-02
  2.53807107e-03 5.07614213e-03 7.61421320e-03 0.00000000e+00
  2.53807107e-03 0.00000000e+00 0.00000000e+00 1.52284264e-02
  0.00000000e+00 7.61421320e-03]
 [1.26582278e-02 1.39240506e-01 6.32911392e-03 4.43037975e-02
  7.15189873e-01 0.00000000e+00 1.26582278e-02 3.79746835e-02
  0.00000000e+00 0.00000000e+00 1.26582278e-02 0.00000000e+00
  6.32911392e-03 1.26582278e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 6.89655172e-03 2.75862069e-02 0.00000000e+00
  0.00000000e+00 9.03448276e-01 4.82758621e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.37931034e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.76678445e-02 9.54063604e-02 3.53356890e-03 3.53356890e-03
  3.53356890e-03 3.18021201e-02 8.02120141e-01 1.76678445e-02
  0.00000000e+00 1.06007067e-02 0.00000000e+00 0.00000000e+00
  3.53356890e-03 7.06713781e-03 0.00000000e+00 3.53356890e-03
  0.00000000e+00 0.00000000e+00]
 [1.44329897e-01 1.54639175e-01 0.00000000e+00 3.60824742e-02
  1.03092784e-02 0.00000000e+00 2.06185567e-02 5.25773196e-01
  0.00000000e+00 0.00000000e+00 3.60824742e-02 5.15463918e-03
  2.06185567e-02 5.15463918e-03 0.00000000e+00 0.00000000e+00
  1.03092784e-02 3.09278351e-02]
 [2.44444444e-01 2.22222222e-02 0.00000000e+00 1.11111111e-02
  0.00000000e+00 0.00000000e+00 1.11111111e-02 1.11111111e-02
  6.77777778e-01 1.11111111e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.11111111e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.70588235e-01 2.35294118e-02 1.17647059e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 7.05882353e-02 1.17647059e-02
  0.00000000e+00 6.00000000e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.17647059e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.57142857e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.38095238e-02
  0.00000000e+00 0.00000000e+00 9.04761905e-01 0.00000000e+00
  1.19047619e-02 1.19047619e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.19047619e-02]
 [1.40845070e-02 1.26760563e-01 0.00000000e+00 1.40845070e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 7.04225352e-02 0.00000000e+00 7.74647887e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.34920635e-02 1.90476190e-01 0.00000000e+00 1.58730159e-02
  1.58730159e-02 0.00000000e+00 1.58730159e-02 2.22222222e-01
  0.00000000e+00 0.00000000e+00 1.58730159e-02 0.00000000e+00
  4.44444444e-01 1.58730159e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.41935484e-01 1.61290323e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 4.83870968e-02 1.61290323e-02
  0.00000000e+00 4.83870968e-02 0.00000000e+00 0.00000000e+00
  1.61290323e-02 6.12903226e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.67857143e-01 4.10714286e-01 1.78571429e-02 1.42857143e-01
  3.57142857e-02 0.00000000e+00 1.78571429e-02 7.14285714e-02
  0.00000000e+00 0.00000000e+00 1.78571429e-02 0.00000000e+00
  0.00000000e+00 1.78571429e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.14285714e-01 1.14285714e-01 0.00000000e+00 2.76190476e-01
  9.52380952e-03 2.85714286e-02 2.47619048e-01 9.52380952e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.52380952e-03 0.00000000e+00 1.90476190e-01
  0.00000000e+00 0.00000000e+00]
 [1.09090909e-01 1.63636364e-01 0.00000000e+00 3.63636364e-02
  1.81818182e-02 0.00000000e+00 0.00000000e+00 4.54545455e-01
  0.00000000e+00 1.81818182e-02 1.81818182e-02 0.00000000e+00
  3.63636364e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.27272727e-01 1.81818182e-02]
 [1.20000000e-01 2.00000000e-02 0.00000000e+00 0.00000000e+00
  2.00000000e-02 0.00000000e+00 0.00000000e+00 2.40000000e-01
  0.00000000e+00 0.00000000e+00 4.00000000e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.60000000e-01]]
---------------------------------------
test report: 
              precision    recall  f1-score   support

           0     0.8290    0.8847    0.8559      1145
           1     0.5848    0.6611    0.6206       537
           2     0.8608    0.7771    0.8168       175
           3     0.7057    0.6861    0.6958       395
           4     0.8828    0.7107    0.7875       159
           5     0.7701    0.9178    0.8375       146
           6     0.7226    0.8345    0.7745       284
           7     0.4520    0.5795    0.5079       195
           8     0.7935    0.8022    0.7978        91
           9     0.6173    0.5747    0.5952        87
          10     0.7404    0.8953    0.8105        86
          11     0.9062    0.8056    0.8529        72
          12     0.6000    0.5156    0.5546        64
          13     0.7391    0.5312    0.6182        64
          14     0.0000    0.0000    0.0000        57
          15     0.8261    0.1776    0.2923       107
          16     0.7500    0.1607    0.2647        56
          17     0.7727    0.6538    0.7083        52

    accuracy                         0.7314      3772
   macro avg     0.6974    0.6205    0.6328      3772
weighted avg     0.7290    0.7314    0.7187      3772

test confusion matrix:
[[8.84716157e-01 3.66812227e-02 3.49344978e-03 9.60698690e-03
  8.73362445e-04 0.00000000e+00 1.74672489e-03 2.09606987e-02
  1.39737991e-02 9.60698690e-03 5.24017467e-03 1.74672489e-03
  8.73362445e-04 6.98689956e-03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 3.49344978e-03]
 [8.00744879e-02 6.61080074e-01 7.44878957e-03 7.63500931e-02
  3.72439479e-03 7.44878957e-03 8.56610801e-02 5.21415270e-02
  1.86219739e-03 9.31098696e-03 0.00000000e+00 1.86219739e-03
  5.58659218e-03 3.72439479e-03 0.00000000e+00 1.86219739e-03
  0.00000000e+00 1.86219739e-03]
 [3.42857143e-02 2.28571429e-02 7.77142857e-01 0.00000000e+00
  0.00000000e+00 1.20000000e-01 1.71428571e-02 0.00000000e+00
  0.00000000e+00 2.28571429e-02 0.00000000e+00 0.00000000e+00
  5.71428571e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [8.86075949e-02 1.44303797e-01 5.06329114e-03 6.86075949e-01
  5.06329114e-03 0.00000000e+00 2.53164557e-03 4.81012658e-02
  0.00000000e+00 5.06329114e-03 2.53164557e-03 0.00000000e+00
  0.00000000e+00 5.06329114e-03 0.00000000e+00 5.06329114e-03
  0.00000000e+00 2.53164557e-03]
 [1.25786164e-02 9.43396226e-02 0.00000000e+00 2.51572327e-02
  7.10691824e-01 0.00000000e+00 1.88679245e-02 8.80503145e-02
  0.00000000e+00 0.00000000e+00 1.25786164e-02 1.25786164e-02
  2.51572327e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 2.05479452e-02 2.05479452e-02 0.00000000e+00
  0.00000000e+00 9.17808219e-01 4.10958904e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.11267606e-02 6.69014085e-02 1.05633803e-02 0.00000000e+00
  3.52112676e-03 3.16901408e-02 8.34507042e-01 3.52112676e-03
  0.00000000e+00 7.04225352e-03 3.52112676e-03 0.00000000e+00
  1.05633803e-02 0.00000000e+00 0.00000000e+00 3.52112676e-03
  0.00000000e+00 3.52112676e-03]
 [9.74358974e-02 1.38461538e-01 1.02564103e-02 4.10256410e-02
  1.53846154e-02 0.00000000e+00 3.58974359e-02 5.79487179e-01
  0.00000000e+00 0.00000000e+00 3.58974359e-02 0.00000000e+00
  3.07692308e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  5.12820513e-03 1.02564103e-02]
 [1.75824176e-01 1.09890110e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.09890110e-02
  8.02197802e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.41379310e-01 8.04597701e-02 1.14942529e-02 1.14942529e-02
  2.29885057e-02 0.00000000e+00 4.59770115e-02 0.00000000e+00
  1.14942529e-02 5.74712644e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.97674419e-02 0.00000000e+00 0.00000000e+00 1.16279070e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.32558140e-02
  0.00000000e+00 0.00000000e+00 8.95348837e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [8.33333333e-02 4.16666667e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.38888889e-02
  0.00000000e+00 5.55555556e-02 0.00000000e+00 8.05555556e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [4.68750000e-02 1.25000000e-01 1.56250000e-02 0.00000000e+00
  3.12500000e-02 0.00000000e+00 3.12500000e-02 2.03125000e-01
  0.00000000e+00 0.00000000e+00 1.56250000e-02 0.00000000e+00
  5.15625000e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.56250000e-02 0.00000000e+00]
 [2.50000000e-01 1.25000000e-01 1.56250000e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.56250000e-02 0.00000000e+00
  0.00000000e+00 3.12500000e-02 1.56250000e-02 0.00000000e+00
  1.56250000e-02 5.31250000e-01 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.57894737e-01 5.26315789e-01 0.00000000e+00 1.57894737e-01
  3.50877193e-02 0.00000000e+00 1.75438596e-02 5.26315789e-02
  1.75438596e-02 1.75438596e-02 1.75438596e-02 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [9.34579439e-02 1.86915888e-01 0.00000000e+00 3.08411215e-01
  0.00000000e+00 5.60747664e-02 1.40186916e-01 9.34579439e-03
  0.00000000e+00 0.00000000e+00 1.86915888e-02 9.34579439e-03
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.77570093e-01
  0.00000000e+00 0.00000000e+00]
 [1.25000000e-01 1.07142857e-01 0.00000000e+00 7.14285714e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.10714286e-01
  0.00000000e+00 0.00000000e+00 7.14285714e-02 0.00000000e+00
  3.57142857e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.60714286e-01 1.78571429e-02]
 [7.69230769e-02 3.84615385e-02 1.92307692e-02 1.92307692e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.34615385e-01
  0.00000000e+00 0.00000000e+00 1.92307692e-02 0.00000000e+00
  1.92307692e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  1.92307692e-02 6.53846154e-01]]
---------------------------------------
program finished.
